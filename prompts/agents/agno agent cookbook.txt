# Merged Python files from `C:\Users\vive1\Downloads\agno-main\cookbook`

## Table of contents

- [agent_os/advanced/_agents.py](#agent_os--advanced--_agentspy)
- [agent_os/advanced/_teams.py](#agent_os--advanced--_teamspy)
- [agent_os/advanced/demo.py](#agent_os--advanced--demopy)
- [agent_os/advanced/mcp_demo.py](#agent_os--advanced--mcp_demopy)
- [agent_os/advanced/multiple_knowledge_bases.py](#agent_os--advanced--multiple_knowledge_basespy)
- [agent_os/advanced/reasoning_demo.py](#agent_os--advanced--reasoning_demopy)
- [agent_os/advanced/teams_demo.py](#agent_os--advanced--teams_demopy)
- [agent_os/basic.py](#agent_os--basicpy)
- [agent_os/customize/custom_fastapi_app.py](#agent_os--customize--custom_fastapi_apppy)
- [agent_os/customize/custom_lifespan.py](#agent_os--customize--custom_lifespanpy)
- [agent_os/customize/fastapi_app_with_custom_middleware.py](#agent_os--customize--fastapi_app_with_custom_middlewarepy)
- [agent_os/customize/override_routes.py](#agent_os--customize--override_routespy)
- [agent_os/dbs/dynamo_demo.py](#agent_os--dbs--dynamo_demopy)
- [agent_os/dbs/firestore_demo.py](#agent_os--dbs--firestore_demopy)
- [agent_os/dbs/gcs_json_demo.py](#agent_os--dbs--gcs_json_demopy)
- [agent_os/dbs/json_demo.py](#agent_os--dbs--json_demopy)
- [agent_os/dbs/mongo_demo.py](#agent_os--dbs--mongo_demopy)
- [agent_os/dbs/neon_demo.py](#agent_os--dbs--neon_demopy)
- [agent_os/dbs/postgres_demo.py](#agent_os--dbs--postgres_demopy)
- [agent_os/dbs/redis_demo.py](#agent_os--dbs--redis_demopy)
- [agent_os/dbs/singlestore_demo.py](#agent_os--dbs--singlestore_demopy)
- [agent_os/dbs/sqlite_demo.py](#agent_os--dbs--sqlite_demopy)
- [agent_os/dbs/supabase_demo.py](#agent_os--dbs--supabase_demopy)
- [agent_os/demo.py](#agent_os--demopy)
- [agent_os/evals_demo.py](#agent_os--evals_demopy)
- [agent_os/interfaces/agui/agent_with_tools.py](#agent_os--interfaces--agui--agent_with_toolspy)
- [agent_os/interfaces/agui/basic.py](#agent_os--interfaces--agui--basicpy)
- [agent_os/interfaces/agui/reasoning_agent.py](#agent_os--interfaces--agui--reasoning_agentpy)
- [agent_os/interfaces/agui/research_team.py](#agent_os--interfaces--agui--research_teampy)
- [agent_os/interfaces/agui/structured_output.py](#agent_os--interfaces--agui--structured_outputpy)
- [agent_os/interfaces/slack/agent_with_user_memory.py](#agent_os--interfaces--slack--agent_with_user_memorypy)
- [agent_os/interfaces/slack/basic.py](#agent_os--interfaces--slack--basicpy)
- [agent_os/interfaces/slack/basic_workflow.py](#agent_os--interfaces--slack--basic_workflowpy)
- [agent_os/interfaces/slack/reasoning_agent.py](#agent_os--interfaces--slack--reasoning_agentpy)
- [agent_os/interfaces/whatsapp/agent_with_media.py](#agent_os--interfaces--whatsapp--agent_with_mediapy)
- [agent_os/interfaces/whatsapp/agent_with_user_memory.py](#agent_os--interfaces--whatsapp--agent_with_user_memorypy)
- [agent_os/interfaces/whatsapp/basic.py](#agent_os--interfaces--whatsapp--basicpy)
- [agent_os/interfaces/whatsapp/image_generation_model.py](#agent_os--interfaces--whatsapp--image_generation_modelpy)
- [agent_os/interfaces/whatsapp/image_generation_tools.py](#agent_os--interfaces--whatsapp--image_generation_toolspy)
- [agent_os/interfaces/whatsapp/reasoning_agent.py](#agent_os--interfaces--whatsapp--reasoning_agentpy)
- [agent_os/mcp/enable_mcp_example.py](#agent_os--mcp--enable_mcp_examplepy)
- [agent_os/mcp/mcp_tools_advanced_example.py](#agent_os--mcp--mcp_tools_advanced_examplepy)
- [agent_os/mcp/mcp_tools_example.py](#agent_os--mcp--mcp_tools_examplepy)
- [agent_os/mcp/mcp_tools_existing_lifespan.py](#agent_os--mcp--mcp_tools_existing_lifespanpy)
- [agent_os/mcp/test_client.py](#agent_os--mcp--test_clientpy)
- [agent_os/mcp_demo.py](#agent_os--mcp_demopy)
- [agent_os/os_config/basic.py](#agent_os--os_config--basicpy)
- [agent_os/os_config/yaml_config.py](#agent_os--os_config--yaml_configpy)
- [agent_os/workflow/basic_workflow.py](#agent_os--workflow--basic_workflowpy)
- [agent_os/workflow/basic_workflow_team.py](#agent_os--workflow--basic_workflow_teampy)
- [agent_os/workflow/workflow_with_conditional.py](#agent_os--workflow--workflow_with_conditionalpy)
- [agent_os/workflow/workflow_with_custom_function.py](#agent_os--workflow--workflow_with_custom_functionpy)
- [agent_os/workflow/workflow_with_input_schema.py](#agent_os--workflow--workflow_with_input_schemapy)
- [agent_os/workflow/workflow_with_loop.py](#agent_os--workflow--workflow_with_looppy)
- [agent_os/workflow/workflow_with_nested_steps.py](#agent_os--workflow--workflow_with_nested_stepspy)
- [agent_os/workflow/workflow_with_parallel.py](#agent_os--workflow--workflow_with_parallelpy)
- [agent_os/workflow/workflow_with_router.py](#agent_os--workflow--workflow_with_routerpy)
- [agent_os/workflow/workflow_with_steps.py](#agent_os--workflow--workflow_with_stepspy)
- [agents/agentic_search/agentic_rag.py](#agents--agentic_search--agentic_ragpy)
- [agents/agentic_search/agentic_rag_infinity_reranker.py](#agents--agentic_search--agentic_rag_infinity_rerankerpy)
- [agents/agentic_search/agentic_rag_with_reasoning.py](#agents--agentic_search--agentic_rag_with_reasoningpy)
- [agents/agentic_search/lightrag/agentic_rag_with_lightrag.py](#agents--agentic_search--lightrag--agentic_rag_with_lightragpy)
- [agents/async/basic.py](#agents--async--basicpy)
- [agents/async/data_analyst.py](#agents--async--data_analystpy)
- [agents/async/delay.py](#agents--async--delaypy)
- [agents/async/gather_agents.py](#agents--async--gather_agentspy)
- [agents/async/reasoning.py](#agents--async--reasoningpy)
- [agents/async/streaming.py](#agents--async--streamingpy)
- [agents/async/structured_output.py](#agents--async--structured_outputpy)
- [agents/async/tool_use.py](#agents--async--tool_usepy)
- [agents/context_management/datetime_instructions.py](#agents--context_management--datetime_instructionspy)
- [agents/context_management/dynamic_instructions.py](#agents--context_management--dynamic_instructionspy)
- [agents/context_management/few_shot_learning.py](#agents--context_management--few_shot_learningpy)
- [agents/context_management/instructions.py](#agents--context_management--instructionspy)
- [agents/context_management/instructions_via_function.py](#agents--context_management--instructions_via_functionpy)
- [agents/context_management/location_instructions.py](#agents--context_management--location_instructionspy)
- [agents/custom_logging/custom_logging.py](#agents--custom_logging--custom_loggingpy)
- [agents/custom_logging/custom_logging_advanced.py](#agents--custom_logging--custom_logging_advancedpy)
- [agents/custom_logging/log_to_file.py](#agents--custom_logging--log_to_filepy)
- [agents/dependencies/access_dependencies_in_tool.py](#agents--dependencies--access_dependencies_in_toolpy)
- [agents/dependencies/add_dependencies_on_run.py](#agents--dependencies--add_dependencies_on_runpy)
- [agents/dependencies/add_dependencies_to_context.py](#agents--dependencies--add_dependencies_to_contextpy)
- [agents/dependencies/reference_dependencies.py](#agents--dependencies--reference_dependenciespy)
- [agents/events/basic_agent_events.py](#agents--events--basic_agent_eventspy)
- [agents/events/reasoning_agent_events.py](#agents--events--reasoning_agent_eventspy)
- [agents/human_in_the_loop/agentic_user_input.py](#agents--human_in_the_loop--agentic_user_inputpy)
- [agents/human_in_the_loop/confirmation_required.py](#agents--human_in_the_loop--confirmation_requiredpy)
- [agents/human_in_the_loop/confirmation_required_async.py](#agents--human_in_the_loop--confirmation_required_asyncpy)
- [agents/human_in_the_loop/confirmation_required_mixed_tools.py](#agents--human_in_the_loop--confirmation_required_mixed_toolspy)
- [agents/human_in_the_loop/confirmation_required_multiple_tools.py](#agents--human_in_the_loop--confirmation_required_multiple_toolspy)
- [agents/human_in_the_loop/confirmation_required_stream.py](#agents--human_in_the_loop--confirmation_required_streampy)
- [agents/human_in_the_loop/confirmation_required_stream_async.py](#agents--human_in_the_loop--confirmation_required_stream_asyncpy)
- [agents/human_in_the_loop/confirmation_required_toolkit.py](#agents--human_in_the_loop--confirmation_required_toolkitpy)
- [agents/human_in_the_loop/confirmation_required_with_history.py](#agents--human_in_the_loop--confirmation_required_with_historypy)
- [agents/human_in_the_loop/confirmation_required_with_run_id.py](#agents--human_in_the_loop--confirmation_required_with_run_idpy)
- [agents/human_in_the_loop/external_tool_execution.py](#agents--human_in_the_loop--external_tool_executionpy)
- [agents/human_in_the_loop/external_tool_execution_async.py](#agents--human_in_the_loop--external_tool_execution_asyncpy)
- [agents/human_in_the_loop/external_tool_execution_async_responses.py](#agents--human_in_the_loop--external_tool_execution_async_responsespy)
- [agents/human_in_the_loop/external_tool_execution_stream.py](#agents--human_in_the_loop--external_tool_execution_streampy)
- [agents/human_in_the_loop/external_tool_execution_stream_async.py](#agents--human_in_the_loop--external_tool_execution_stream_asyncpy)
- [agents/human_in_the_loop/external_tool_execution_toolkit.py](#agents--human_in_the_loop--external_tool_execution_toolkitpy)
- [agents/human_in_the_loop/user_input_required.py](#agents--human_in_the_loop--user_input_requiredpy)
- [agents/human_in_the_loop/user_input_required_all_fields.py](#agents--human_in_the_loop--user_input_required_all_fieldspy)
- [agents/human_in_the_loop/user_input_required_async.py](#agents--human_in_the_loop--user_input_required_asyncpy)
- [agents/human_in_the_loop/user_input_required_stream.py](#agents--human_in_the_loop--user_input_required_streampy)
- [agents/human_in_the_loop/user_input_required_stream_async.py](#agents--human_in_the_loop--user_input_required_stream_asyncpy)
- [agents/input_and_output/input_as_dict.py](#agents--input_and_output--input_as_dictpy)
- [agents/input_and_output/input_as_list.py](#agents--input_and_output--input_as_listpy)
- [agents/input_and_output/input_as_message.py](#agents--input_and_output--input_as_messagepy)
- [agents/input_and_output/input_as_messages_list.py](#agents--input_and_output--input_as_messages_listpy)
- [agents/input_and_output/input_schema_on_agent.py](#agents--input_and_output--input_schema_on_agentpy)
- [agents/input_and_output/input_schema_on_agent_as_typed_dict.py](#agents--input_and_output--input_schema_on_agent_as_typed_dictpy)
- [agents/input_and_output/output_model.py](#agents--input_and_output--output_modelpy)
- [agents/input_and_output/parser_model.py](#agents--input_and_output--parser_modelpy)
- [agents/input_and_output/parser_model_ollama.py](#agents--input_and_output--parser_model_ollamapy)
- [agents/input_and_output/parser_model_stream.py](#agents--input_and_output--parser_model_streampy)
- [agents/input_and_output/response_as_variable.py](#agents--input_and_output--response_as_variablepy)
- [agents/input_and_output/structured_input.py](#agents--input_and_output--structured_inputpy)
- [agents/input_and_output/structured_input_output_with_parser_model.py](#agents--input_and_output--structured_input_output_with_parser_modelpy)
- [agents/multimodal/01_media_input_for_tool.py](#agents--multimodal--01_media_input_for_toolpy)
- [agents/multimodal/02_media_input_to_agent_and_tool.py](#agents--multimodal--02_media_input_to_agent_and_toolpy)
- [agents/multimodal/agent_same_run_image_analysis.py](#agents--multimodal--agent_same_run_image_analysispy)
- [agents/multimodal/agent_using_multimodal_tool_response_in_runs.py](#agents--multimodal--agent_using_multimodal_tool_response_in_runspy)
- [agents/multimodal/audio_input_output.py](#agents--multimodal--audio_input_outputpy)
- [agents/multimodal/audio_multi_turn.py](#agents--multimodal--audio_multi_turnpy)
- [agents/multimodal/audio_sentiment_analysis.py](#agents--multimodal--audio_sentiment_analysispy)
- [agents/multimodal/audio_streaming.py](#agents--multimodal--audio_streamingpy)
- [agents/multimodal/audio_to_text.py](#agents--multimodal--audio_to_textpy)
- [agents/multimodal/generate_image_with_intermediate_steps.py](#agents--multimodal--generate_image_with_intermediate_stepspy)
- [agents/multimodal/generate_video_using_models_lab.py](#agents--multimodal--generate_video_using_models_labpy)
- [agents/multimodal/generate_video_using_replicate.py](#agents--multimodal--generate_video_using_replicatepy)
- [agents/multimodal/image_input_high_fidelity.py](#agents--multimodal--image_input_high_fidelitypy)
- [agents/multimodal/image_to_audio.py](#agents--multimodal--image_to_audiopy)
- [agents/multimodal/image_to_image_agent.py](#agents--multimodal--image_to_image_agentpy)
- [agents/multimodal/image_to_structured_output.py](#agents--multimodal--image_to_structured_outputpy)
- [agents/multimodal/image_to_text.py](#agents--multimodal--image_to_textpy)
- [agents/multimodal/video_caption_agent.py](#agents--multimodal--video_caption_agentpy)
- [agents/multimodal/video_to_shorts.py](#agents--multimodal--video_to_shortspy)
- [agents/other/agent_extra_metrics.py](#agents--other--agent_extra_metricspy)
- [agents/other/agent_metrics.py](#agents--other--agent_metricspy)
- [agents/other/agent_run_metadata.py](#agents--other--agent_run_metadatapy)
- [agents/other/cancel_a_run.py](#agents--other--cancel_a_runpy)
- [agents/other/debug.py](#agents--other--debugpy)
- [agents/other/debug_level.py](#agents--other--debug_levelpy)
- [agents/other/intermediate_steps.py](#agents--other--intermediate_stepspy)
- [agents/other/run_response_events.py](#agents--other--run_response_eventspy)
- [agents/other/scenario_testing.py](#agents--other--scenario_testingpy)
- [agents/other/tool_call_limit.py](#agents--other--tool_call_limitpy)
- [agents/rag/agentic_rag_lancedb.py](#agents--rag--agentic_rag_lancedbpy)
- [agents/rag/agentic_rag_pgvector.py](#agents--rag--agentic_rag_pgvectorpy)
- [agents/rag/agentic_rag_with_reranking.py](#agents--rag--agentic_rag_with_rerankingpy)
- [agents/rag/local_rag_langchain_qdrant.py](#agents--rag--local_rag_langchain_qdrantpy)
- [agents/rag/rag_sentence_transformer.py](#agents--rag--rag_sentence_transformerpy)
- [agents/rag/rag_with_lance_db_and_sqlite.py](#agents--rag--rag_with_lance_db_and_sqlitepy)
- [agents/rag/traditional_rag_lancedb.py](#agents--rag--traditional_rag_lancedbpy)
- [agents/rag/traditional_rag_pgvector.py](#agents--rag--traditional_rag_pgvectorpy)
- [agents/session/01_persistent_session.py](#agents--session--01_persistent_sessionpy)
- [agents/session/02_persistent_session_history.py](#agents--session--02_persistent_session_historypy)
- [agents/session/03_session_summary.py](#agents--session--03_session_summarypy)
- [agents/session/04_session_summary_references.py](#agents--session--04_session_summary_referencespy)
- [agents/session/05_chat_history.py](#agents--session--05_chat_historypy)
- [agents/session/06_rename_session.py](#agents--session--06_rename_sessionpy)
- [agents/session/07_in_memory_db.py](#agents--session--07_in_memory_dbpy)
- [agents/session/08_cache_session.py](#agents--session--08_cache_sessionpy)
- [agents/state/agentic_session_state.py](#agents--state--agentic_session_statepy)
- [agents/state/change_state_on_run.py](#agents--state--change_state_on_runpy)
- [agents/state/dynamic_session_state.py](#agents--state--dynamic_session_statepy)
- [agents/state/last_n_session_messages.py](#agents--state--last_n_session_messagespy)
- [agents/state/session_state_advanced.py](#agents--state--session_state_advancedpy)
- [agents/state/session_state_basic.py](#agents--state--session_state_basicpy)
- [agents/state/session_state_in_context.py](#agents--state--session_state_in_contextpy)
- [agents/state/session_state_in_instructions.py](#agents--state--session_state_in_instructionspy)
- [agents/state/session_state_multiple_users.py](#agents--state--session_state_multiple_userspy)
- [db/01_persistent_session_storage.py](#db--01_persistent_session_storagepy)
- [db/02_session_summary.py](#db--02_session_summarypy)
- [db/03_chat_history.py](#db--03_chat_historypy)
- [db/dynamodb/dynamo_for_agent.py](#db--dynamodb--dynamo_for_agentpy)
- [db/dynamodb/dynamo_for_team.py](#db--dynamodb--dynamo_for_teampy)
- [db/examples/multi_user_multi_session.py](#db--examples--multi_user_multi_sessionpy)
- [db/examples/selecting_tables.py](#db--examples--selecting_tablespy)
- [db/firestore/firestore_for_agent.py](#db--firestore--firestore_for_agentpy)
- [db/gcs/gcs_json_for_agent.py](#db--gcs--gcs_json_for_agentpy)
- [db/in_memory/in_memory_storage_for_agent.py](#db--in_memory--in_memory_storage_for_agentpy)
- [db/in_memory/in_memory_storage_for_team.py](#db--in_memory--in_memory_storage_for_teampy)
- [db/in_memory/in_memory_storage_for_workflow.py](#db--in_memory--in_memory_storage_for_workflowpy)
- [db/json/json_for_agent.py](#db--json--json_for_agentpy)
- [db/json/json_for_team.py](#db--json--json_for_teampy)
- [db/json/json_for_workflows.py](#db--json--json_for_workflowspy)
- [db/mongo/mongodb_for_agent.py](#db--mongo--mongodb_for_agentpy)
- [db/mongo/mongodb_for_team.py](#db--mongo--mongodb_for_teampy)
- [db/mysql/mysql_for_agent.py](#db--mysql--mysql_for_agentpy)
- [db/mysql/mysql_for_team.py](#db--mysql--mysql_for_teampy)
- [db/postgres/postgres_for_agent.py](#db--postgres--postgres_for_agentpy)
- [db/postgres/postgres_for_team.py](#db--postgres--postgres_for_teampy)
- [db/postgres/postgres_for_workflow.py](#db--postgres--postgres_for_workflowpy)
- [db/redis/redis_for_agent.py](#db--redis--redis_for_agentpy)
- [db/redis/redis_for_team.py](#db--redis--redis_for_teampy)
- [db/redis/redis_for_workflow.py](#db--redis--redis_for_workflowpy)
- [db/singlestore/singlestore_for_agent.py](#db--singlestore--singlestore_for_agentpy)
- [db/singlestore/singlestore_for_team.py](#db--singlestore--singlestore_for_teampy)
- [db/sqlite/sqlite_for_agent.py](#db--sqlite--sqlite_for_agentpy)
- [db/sqlite/sqlite_for_team.py](#db--sqlite--sqlite_for_teampy)
- [db/sqlite/sqlite_for_workflow.py](#db--sqlite--sqlite_for_workflowpy)
- [evals/accuracy/accuracy_9_11_bigger_or_9_99.py](#evals--accuracy--accuracy_9_11_bigger_or_9_99py)
- [evals/accuracy/accuracy_async.py](#evals--accuracy--accuracy_asyncpy)
- [evals/accuracy/accuracy_basic.py](#evals--accuracy--accuracy_basicpy)
- [evals/accuracy/accuracy_team.py](#evals--accuracy--accuracy_teampy)
- [evals/accuracy/accuracy_with_given_answer.py](#evals--accuracy--accuracy_with_given_answerpy)
- [evals/accuracy/accuracy_with_tools.py](#evals--accuracy--accuracy_with_toolspy)
- [evals/accuracy/db_logging.py](#evals--accuracy--db_loggingpy)
- [evals/accuracy/evaluator_agent.py](#evals--accuracy--evaluator_agentpy)
- [evals/performance/async_function.py](#evals--performance--async_functionpy)
- [evals/performance/comparison/autogen_instantiation.py](#evals--performance--comparison--autogen_instantiationpy)
- [evals/performance/comparison/crewai_instantiation.py](#evals--performance--comparison--crewai_instantiationpy)
- [evals/performance/comparison/langgraph_instantiation.py](#evals--performance--comparison--langgraph_instantiationpy)
- [evals/performance/comparison/openai_agents_instantiation.py](#evals--performance--comparison--openai_agents_instantiationpy)
- [evals/performance/comparison/pydantic_ai_instantiation.py](#evals--performance--comparison--pydantic_ai_instantiationpy)
- [evals/performance/comparison/smolagents_instantiation.py](#evals--performance--comparison--smolagents_instantiationpy)
- [evals/performance/db_logging.py](#evals--performance--db_loggingpy)
- [evals/performance/instantiation_agent.py](#evals--performance--instantiation_agentpy)
- [evals/performance/instantiation_agent_with_tool.py](#evals--performance--instantiation_agent_with_toolpy)
- [evals/performance/instantiation_team.py](#evals--performance--instantiation_teampy)
- [evals/performance/response_with_memory_updates.py](#evals--performance--response_with_memory_updatespy)
- [evals/performance/response_with_storage.py](#evals--performance--response_with_storagepy)
- [evals/performance/simple_response.py](#evals--performance--simple_responsepy)
- [evals/performance/team_response_with_memory_and_reasoning.py](#evals--performance--team_response_with_memory_and_reasoningpy)
- [evals/performance/team_response_with_memory_multi_user.py](#evals--performance--team_response_with_memory_multi_userpy)
- [evals/performance/team_response_with_memory_simple.py](#evals--performance--team_response_with_memory_simplepy)
- [evals/reliability/db_logging.py](#evals--reliability--db_loggingpy)
- [evals/reliability/multiple_tool_calls/calculator.py](#evals--reliability--multiple_tool_calls--calculatorpy)
- [evals/reliability/reliability_async.py](#evals--reliability--reliability_asyncpy)
- [evals/reliability/single_tool_calls/calculator.py](#evals--reliability--single_tool_calls--calculatorpy)
- [evals/reliability/team/ai_news.py](#evals--reliability--team--ai_newspy)
- [examples/agents/agent_team.py](#examples--agents--agent_teampy)
- [examples/agents/agent_with_instructions.py](#examples--agents--agent_with_instructionspy)
- [examples/agents/agent_with_knowledge.py](#examples--agents--agent_with_knowledgepy)
- [examples/agents/agent_with_memory.py](#examples--agents--agent_with_memorypy)
- [examples/agents/agent_with_reasoning.py](#examples--agents--agent_with_reasoningpy)
- [examples/agents/agent_with_storage.py](#examples--agents--agent_with_storagepy)
- [examples/agents/agent_with_tools.py](#examples--agents--agent_with_toolspy)
- [examples/agents/agno_assist.py](#examples--agents--agno_assistpy)
- [examples/agents/agno_support_agent.py](#examples--agents--agno_support_agentpy)
- [examples/agents/airbnb_mcp.py](#examples--agents--airbnb_mcppy)
- [examples/agents/basic_agent.py](#examples--agents--basic_agentpy)
- [examples/agents/book_recommendation.py](#examples--agents--book_recommendationpy)
- [examples/agents/competitor_analysis_agent.py](#examples--agents--competitor_analysis_agentpy)
- [examples/agents/deep_knowledge.py](#examples--agents--deep_knowledgepy)
- [examples/agents/deep_research_agent_exa.py](#examples--agents--deep_research_agent_exapy)
- [examples/agents/fibonacci_agent.py](#examples--agents--fibonacci_agentpy)
- [examples/agents/finance_agent.py](#examples--agents--finance_agentpy)
- [examples/agents/finance_agent_with_memory.py](#examples--agents--finance_agent_with_memorypy)
- [examples/agents/legal_consultant.py](#examples--agents--legal_consultantpy)
- [examples/agents/media_trend_analysis_agent.py](#examples--agents--media_trend_analysis_agentpy)
- [examples/agents/meeting_summarizer_agent.py](#examples--agents--meeting_summarizer_agentpy)
- [examples/agents/movie_recommedation.py](#examples--agents--movie_recommedationpy)
- [examples/agents/pydantic_model_as_input.py](#examples--agents--pydantic_model_as_inputpy)
- [examples/agents/readme_generator.py](#examples--agents--readme_generatorpy)
- [examples/agents/reasoning_finance_agent.py](#examples--agents--reasoning_finance_agentpy)
- [examples/agents/recipe_creator.py](#examples--agents--recipe_creatorpy)
- [examples/agents/recipe_rag_image.py](#examples--agents--recipe_rag_imagepy)
- [examples/agents/research_agent.py](#examples--agents--research_agentpy)
- [examples/agents/research_agent_exa.py](#examples--agents--research_agent_exapy)
- [examples/agents/run_as_cli.py](#examples--agents--run_as_clipy)
- [examples/agents/shopping_partner.py](#examples--agents--shopping_partnerpy)
- [examples/agents/social_media_agent.py](#examples--agents--social_media_agentpy)
- [examples/agents/startup_analyst_agent.py](#examples--agents--startup_analyst_agentpy)
- [examples/agents/study_partner.py](#examples--agents--study_partnerpy)
- [examples/agents/thinking_finance_agent.py](#examples--agents--thinking_finance_agentpy)
- [examples/agents/translation_agent.py](#examples--agents--translation_agentpy)
- [examples/agents/web_extraction_agent.py](#examples--agents--web_extraction_agentpy)
- [examples/agents/youtube_agent.py](#examples--agents--youtube_agentpy)
- [examples/chainlit_apps/basic/basic_app.py](#examples--chainlit_apps--basic--basic_apppy)
- [examples/streamlit_apps/agentic_rag/agentic_rag.py](#examples--streamlit_apps--agentic_rag--agentic_ragpy)
- [examples/streamlit_apps/agentic_rag/app.py](#examples--streamlit_apps--agentic_rag--apppy)
- [examples/streamlit_apps/chess_team/agents.py](#examples--streamlit_apps--chess_team--agentspy)
- [examples/streamlit_apps/chess_team/app.py](#examples--streamlit_apps--chess_team--apppy)
- [examples/streamlit_apps/deep_researcher/agents.py](#examples--streamlit_apps--deep_researcher--agentspy)
- [examples/streamlit_apps/deep_researcher/app.py](#examples--streamlit_apps--deep_researcher--apppy)
- [examples/streamlit_apps/gemini_tutor/agents.py](#examples--streamlit_apps--gemini_tutor--agentspy)
- [examples/streamlit_apps/gemini_tutor/app.py](#examples--streamlit_apps--gemini_tutor--apppy)
- [examples/streamlit_apps/geobuddy/agents.py](#examples--streamlit_apps--geobuddy--agentspy)
- [examples/streamlit_apps/geobuddy/app.py](#examples--streamlit_apps--geobuddy--apppy)
- [examples/streamlit_apps/github_mcp_agent/agents.py](#examples--streamlit_apps--github_mcp_agent--agentspy)
- [examples/streamlit_apps/github_mcp_agent/app.py](#examples--streamlit_apps--github_mcp_agent--apppy)
- [examples/streamlit_apps/github_repo_analyzer/agents.py](#examples--streamlit_apps--github_repo_analyzer--agentspy)
- [examples/streamlit_apps/github_repo_analyzer/app.py](#examples--streamlit_apps--github_repo_analyzer--apppy)
- [examples/streamlit_apps/image_generation/agents.py](#examples--streamlit_apps--image_generation--agentspy)
- [examples/streamlit_apps/image_generation/app.py](#examples--streamlit_apps--image_generation--apppy)
- [examples/streamlit_apps/llama_tutor/agents.py](#examples--streamlit_apps--llama_tutor--agentspy)
- [examples/streamlit_apps/llama_tutor/app.py](#examples--streamlit_apps--llama_tutor--apppy)
- [examples/streamlit_apps/mcp_agent/app.py](#examples--streamlit_apps--mcp_agent--apppy)
- [examples/streamlit_apps/mcp_agent/mcp_agent.py](#examples--streamlit_apps--mcp_agent--mcp_agentpy)
- [examples/streamlit_apps/mcp_agent/mcp_client.py](#examples--streamlit_apps--mcp_agent--mcp_clientpy)
- [examples/streamlit_apps/medical_imaging/app.py](#examples--streamlit_apps--medical_imaging--apppy)
- [examples/streamlit_apps/medical_imaging/medical_agent.py](#examples--streamlit_apps--medical_imaging--medical_agentpy)
- [examples/streamlit_apps/paperpal/agents.py](#examples--streamlit_apps--paperpal--agentspy)
- [examples/streamlit_apps/paperpal/app.py](#examples--streamlit_apps--paperpal--apppy)
- [examples/streamlit_apps/podcast_generator/agents.py](#examples--streamlit_apps--podcast_generator--agentspy)
- [examples/streamlit_apps/podcast_generator/app.py](#examples--streamlit_apps--podcast_generator--apppy)
- [examples/streamlit_apps/vision_ai/agents.py](#examples--streamlit_apps--vision_ai--agentspy)
- [examples/streamlit_apps/vision_ai/app.py](#examples--streamlit_apps--vision_ai--apppy)
- [examples/teams/collaborate_mode/collaboration_team.py](#examples--teams--collaborate_mode--collaboration_teampy)
- [examples/teams/coordinate_mode/autonomous_startup_team.py](#examples--teams--coordinate_mode--autonomous_startup_teampy)
- [examples/teams/coordinate_mode/content_team.py](#examples--teams--coordinate_mode--content_teampy)
- [examples/teams/coordinate_mode/hackernews_team.py](#examples--teams--coordinate_mode--hackernews_teampy)
- [examples/teams/coordinate_mode/news_agency_team.py](#examples--teams--coordinate_mode--news_agency_teampy)
- [examples/teams/coordinate_mode/reasoning_team.py](#examples--teams--coordinate_mode--reasoning_teampy)
- [examples/teams/coordinate_mode/skyplanner_mcp_team.py](#examples--teams--coordinate_mode--skyplanner_mcp_teampy)
- [examples/teams/coordinate_mode/tic_tac_toe_team.py](#examples--teams--coordinate_mode--tic_tac_toe_teampy)
- [examples/teams/coordinate_mode/travel_planner_mcp_team.py](#examples--teams--coordinate_mode--travel_planner_mcp_teampy)
- [examples/teams/route_mode/ai_customer_support_team.py](#examples--teams--route_mode--ai_customer_support_teampy)
- [examples/teams/route_mode/multi_language_team.py](#examples--teams--route_mode--multi_language_teampy)
- [examples/teams/route_mode/multi_purpose_team.py](#examples--teams--route_mode--multi_purpose_teampy)
- [examples/teams/route_mode/reasoning_team.py](#examples--teams--route_mode--reasoning_teampy)
- [examples/teams/route_mode/simple.py](#examples--teams--route_mode--simplepy)
- [examples/workflows/blog_post_generator.py](#examples--workflows--blog_post_generatorpy)
- [examples/workflows/company_analysis/agents.py](#examples--workflows--company_analysis--agentspy)
- [examples/workflows/company_analysis/models.py](#examples--workflows--company_analysis--modelspy)
- [examples/workflows/company_analysis/run_workflow.py](#examples--workflows--company_analysis--run_workflowpy)
- [examples/workflows/company_description/agents.py](#examples--workflows--company_description--agentspy)
- [examples/workflows/company_description/prompts.py](#examples--workflows--company_description--promptspy)
- [examples/workflows/company_description/run_workflow.py](#examples--workflows--company_description--run_workflowpy)
- [examples/workflows/customer_support/agents.py](#examples--workflows--customer_support--agentspy)
- [examples/workflows/customer_support/run_workflow.py](#examples--workflows--customer_support--run_workflowpy)
- [examples/workflows/employee_recruiter.py](#examples--workflows--employee_recruiterpy)
- [examples/workflows/employee_recruiter_async_stream.py](#examples--workflows--employee_recruiter_async_streampy)
- [examples/workflows/investment_analyst/agents.py](#examples--workflows--investment_analyst--agentspy)
- [examples/workflows/investment_analyst/models.py](#examples--workflows--investment_analyst--modelspy)
- [examples/workflows/investment_analyst/run_workflow.py](#examples--workflows--investment_analyst--run_workflowpy)
- [examples/workflows/investment_report_generator.py](#examples--workflows--investment_report_generatorpy)
- [examples/workflows/startup_idea_validator.py](#examples--workflows--startup_idea_validatorpy)
- [getting_started/01_basic_agent.py](#getting_started--01_basic_agentpy)
- [getting_started/02_agent_with_tools.py](#getting_started--02_agent_with_toolspy)
- [getting_started/03_agent_with_knowledge.py](#getting_started--03_agent_with_knowledgepy)
- [getting_started/04_write_your_own_tool.py](#getting_started--04_write_your_own_toolpy)
- [getting_started/05_structured_output.py](#getting_started--05_structured_outputpy)
- [getting_started/06_agent_with_storage.py](#getting_started--06_agent_with_storagepy)
- [getting_started/07_agent_state.py](#getting_started--07_agent_statepy)
- [getting_started/08_agent_context.py](#getting_started--08_agent_contextpy)
- [getting_started/09_agent_session.py](#getting_started--09_agent_sessionpy)
- [getting_started/10_user_memories_and_summaries.py](#getting_started--10_user_memories_and_summariespy)
- [getting_started/11_retry_function_call.py](#getting_started--11_retry_function_callpy)
- [getting_started/12_human_in_the_loop.py](#getting_started--12_human_in_the_looppy)
- [getting_started/13_image_agent.py](#getting_started--13_image_agentpy)
- [getting_started/14_generate_image.py](#getting_started--14_generate_imagepy)
- [getting_started/15_generate_video.py](#getting_started--15_generate_videopy)
- [getting_started/16_audio_input_output.py](#getting_started--16_audio_input_outputpy)
- [getting_started/17_agent_team.py](#getting_started--17_agent_teampy)
- [getting_started/18_research_agent_exa.py](#getting_started--18_research_agent_exapy)
- [getting_started/19_blog_generator_workflow.py](#getting_started--19_blog_generator_workflowpy)
- [getting_started/readme_examples.py](#getting_started--readme_examplespy)
- [integrations/a2a/basic_agent/__main__.py](#integrations--a2a--basic_agent--__main__py)
- [integrations/a2a/basic_agent/basic_agent.py](#integrations--a2a--basic_agent--basic_agentpy)
- [integrations/a2a/basic_agent/client.py](#integrations--a2a--basic_agent--clientpy)
- [integrations/discord/agent_with_media.py](#integrations--discord--agent_with_mediapy)
- [integrations/discord/agent_with_user_memory.py](#integrations--discord--agent_with_user_memorypy)
- [integrations/discord/basic.py](#integrations--discord--basicpy)
- [integrations/memory/mem0_integration.py](#integrations--memory--mem0_integrationpy)
- [integrations/memory/zep_integration.py](#integrations--memory--zep_integrationpy)
- [integrations/observability/agent_ops.py](#integrations--observability--agent_opspy)
- [integrations/observability/arize_phoenix_via_openinference.py](#integrations--observability--arize_phoenix_via_openinferencepy)
- [integrations/observability/arize_phoenix_via_openinference_local.py](#integrations--observability--arize_phoenix_via_openinference_localpy)
- [integrations/observability/atla_op.py](#integrations--observability--atla_oppy)
- [integrations/observability/langfuse_via_openinference.py](#integrations--observability--langfuse_via_openinferencepy)
- [integrations/observability/langfuse_via_openinference_response_model.py](#integrations--observability--langfuse_via_openinference_response_modelpy)
- [integrations/observability/langfuse_via_openlit.py](#integrations--observability--langfuse_via_openlitpy)
- [integrations/observability/langsmith_via_openinference.py](#integrations--observability--langsmith_via_openinferencepy)
- [integrations/observability/langtrace_op.py](#integrations--observability--langtrace_oppy)
- [integrations/observability/langwatch_op.py](#integrations--observability--langwatch_oppy)
- [integrations/observability/teams/langfuse_via_openinference_async_team.py](#integrations--observability--teams--langfuse_via_openinference_async_teampy)
- [integrations/observability/teams/langfuse_via_openinference_team.py](#integrations--observability--teams--langfuse_via_openinference_teampy)
- [integrations/observability/weave_op.py](#integrations--observability--weave_oppy)
- [knowledge/basic_operations/01_from_path.py](#knowledge--basic_operations--01_from_pathpy)
- [knowledge/basic_operations/02_from_url.py](#knowledge--basic_operations--02_from_urlpy)
- [knowledge/basic_operations/03_from_topic.py](#knowledge--basic_operations--03_from_topicpy)
- [knowledge/basic_operations/04_from_multiple.py](#knowledge--basic_operations--04_from_multiplepy)
- [knowledge/basic_operations/05_from_youtube.py](#knowledge--basic_operations--05_from_youtubepy)
- [knowledge/basic_operations/06_from_s3.py](#knowledge--basic_operations--06_from_s3py)
- [knowledge/basic_operations/07_from_gcs.py](#knowledge--basic_operations--07_from_gcspy)
- [knowledge/basic_operations/08_include_exclude_files.py](#knowledge--basic_operations--08_include_exclude_filespy)
- [knowledge/basic_operations/09_remove_content.py](#knowledge--basic_operations--09_remove_contentpy)
- [knowledge/basic_operations/10_remove_vectors.py](#knowledge--basic_operations--10_remove_vectorspy)
- [knowledge/basic_operations/11_skip_if_exists.py](#knowledge--basic_operations--11_skip_if_existspy)
- [knowledge/basic_operations/12_skip_if_exists_contentsdb.py](#knowledge--basic_operations--12_skip_if_exists_contentsdbpy)
- [knowledge/basic_operations/13_specify_reader.py](#knowledge--basic_operations--13_specify_readerpy)
- [knowledge/basic_operations/14_sync.py](#knowledge--basic_operations--14_syncpy)
- [knowledge/basic_operations/15_text_content.py](#knowledge--basic_operations--15_text_contentpy)
- [knowledge/chunking/agentic_chunking.py](#knowledge--chunking--agentic_chunkingpy)
- [knowledge/chunking/csv_row_chunking.py](#knowledge--chunking--csv_row_chunkingpy)
- [knowledge/chunking/custom_strategy_example.py](#knowledge--chunking--custom_strategy_examplepy)
- [knowledge/chunking/document_chunking.py](#knowledge--chunking--document_chunkingpy)
- [knowledge/chunking/fixed_size_chunking.py](#knowledge--chunking--fixed_size_chunkingpy)
- [knowledge/chunking/recursive_chunking.py](#knowledge--chunking--recursive_chunkingpy)
- [knowledge/chunking/semantic_chunking.py](#knowledge--chunking--semantic_chunkingpy)
- [knowledge/custom_retriever/async_retriever.py](#knowledge--custom_retriever--async_retrieverpy)
- [knowledge/custom_retriever/retriever.py](#knowledge--custom_retriever--retrieverpy)
- [knowledge/embedders/aws_bedrock_embedder.py](#knowledge--embedders--aws_bedrock_embedderpy)
- [knowledge/embedders/azure_embedder.py](#knowledge--embedders--azure_embedderpy)
- [knowledge/embedders/cohere_embedder.py](#knowledge--embedders--cohere_embedderpy)
- [knowledge/embedders/fireworks_embedder.py](#knowledge--embedders--fireworks_embedderpy)
- [knowledge/embedders/gemini_embedder.py](#knowledge--embedders--gemini_embedderpy)
- [knowledge/embedders/huggingface_embedder.py](#knowledge--embedders--huggingface_embedderpy)
- [knowledge/embedders/jina_embedder.py](#knowledge--embedders--jina_embedderpy)
- [knowledge/embedders/langdb_embedder.py](#knowledge--embedders--langdb_embedderpy)
- [knowledge/embedders/mistral_embedder.py](#knowledge--embedders--mistral_embedderpy)
- [knowledge/embedders/nebius_embedder.py](#knowledge--embedders--nebius_embedderpy)
- [knowledge/embedders/ollama_embedder.py](#knowledge--embedders--ollama_embedderpy)
- [knowledge/embedders/openai_embedder.py](#knowledge--embedders--openai_embedderpy)
- [knowledge/embedders/qdrant_fastembed.py](#knowledge--embedders--qdrant_fastembedpy)
- [knowledge/embedders/sentence_transformer_embedder.py](#knowledge--embedders--sentence_transformer_embedderpy)
- [knowledge/embedders/together_embedder.py](#knowledge--embedders--together_embedderpy)
- [knowledge/embedders/voyageai_embedder.py](#knowledge--embedders--voyageai_embedderpy)
- [knowledge/filters/agentic_filtering.py](#knowledge--filters--agentic_filteringpy)
- [knowledge/filters/async_filtering.py](#knowledge--filters--async_filteringpy)
- [knowledge/filters/filtering.py](#knowledge--filters--filteringpy)
- [knowledge/filters/filtering_on_load.py](#knowledge--filters--filtering_on_loadpy)
- [knowledge/filters/filtering_with_invalid_keys.py](#knowledge--filters--filtering_with_invalid_keyspy)
- [knowledge/filters/vector_dbs/filtering_chroma_db.py](#knowledge--filters--vector_dbs--filtering_chroma_dbpy)
- [knowledge/filters/vector_dbs/filtering_lance_db.py](#knowledge--filters--vector_dbs--filtering_lance_dbpy)
- [knowledge/filters/vector_dbs/filtering_milvus.py](#knowledge--filters--vector_dbs--filtering_milvuspy)
- [knowledge/filters/vector_dbs/filtering_mongo_db.py](#knowledge--filters--vector_dbs--filtering_mongo_dbpy)
- [knowledge/filters/vector_dbs/filtering_pgvector.py](#knowledge--filters--vector_dbs--filtering_pgvectorpy)
- [knowledge/filters/vector_dbs/filtering_pinecone.py](#knowledge--filters--vector_dbs--filtering_pineconepy)
- [knowledge/filters/vector_dbs/filtering_qdrant_db.py](#knowledge--filters--vector_dbs--filtering_qdrant_dbpy)
- [knowledge/filters/vector_dbs/filtering_surrealdb.py](#knowledge--filters--vector_dbs--filtering_surrealdbpy)
- [knowledge/filters/vector_dbs/filtering_weaviate.py](#knowledge--filters--vector_dbs--filtering_weaviatepy)
- [knowledge/knowledge_tools.py](#knowledge--knowledge_toolspy)
- [knowledge/readers/arxiv_reader.py](#knowledge--readers--arxiv_readerpy)
- [knowledge/readers/arxiv_reader_async.py](#knowledge--readers--arxiv_reader_asyncpy)
- [knowledge/readers/csv_reader.py](#knowledge--readers--csv_readerpy)
- [knowledge/readers/csv_reader_async.py](#knowledge--readers--csv_reader_asyncpy)
- [knowledge/readers/csv_reader_custom_encodings.py](#knowledge--readers--csv_reader_custom_encodingspy)
- [knowledge/readers/csv_reader_url_async.py](#knowledge--readers--csv_reader_url_asyncpy)
- [knowledge/readers/doc_kb_async.py](#knowledge--readers--doc_kb_asyncpy)
- [knowledge/readers/firecrawl_reader.py](#knowledge--readers--firecrawl_readerpy)
- [knowledge/readers/json_reader.py](#knowledge--readers--json_readerpy)
- [knowledge/readers/markdown_reader_async.py](#knowledge--readers--markdown_reader_asyncpy)
- [knowledge/readers/pdf_reader_async.py](#knowledge--readers--pdf_reader_asyncpy)
- [knowledge/readers/pdf_reader_password.py](#knowledge--readers--pdf_reader_passwordpy)
- [knowledge/readers/pdf_reader_url_password.py](#knowledge--readers--pdf_reader_url_passwordpy)
- [knowledge/readers/web_reader.py](#knowledge--readers--web_readerpy)
- [knowledge/readers/web_search_reader.py](#knowledge--readers--web_search_readerpy)
- [knowledge/readers/web_search_reader_async.py](#knowledge--readers--web_search_reader_asyncpy)
- [knowledge/readers/website_reader.py](#knowledge--readers--website_readerpy)
- [knowledge/search_type/hybrid_search.py](#knowledge--search_type--hybrid_searchpy)
- [knowledge/search_type/keyword_search.py](#knowledge--search_type--keyword_searchpy)
- [knowledge/search_type/vector_search.py](#knowledge--search_type--vector_searchpy)
- [knowledge/vector_db/cassandra_db/async_cassandra_db.py](#knowledge--vector_db--cassandra_db--async_cassandra_dbpy)
- [knowledge/vector_db/cassandra_db/cassandra_db.py](#knowledge--vector_db--cassandra_db--cassandra_dbpy)
- [knowledge/vector_db/chroma_db/async_chroma_db.py](#knowledge--vector_db--chroma_db--async_chroma_dbpy)
- [knowledge/vector_db/chroma_db/chroma_db.py](#knowledge--vector_db--chroma_db--chroma_dbpy)
- [knowledge/vector_db/clickhouse_db/async_clickhouse.py](#knowledge--vector_db--clickhouse_db--async_clickhousepy)
- [knowledge/vector_db/clickhouse_db/clickhouse.py](#knowledge--vector_db--clickhouse_db--clickhousepy)
- [knowledge/vector_db/couchbase_db/async_couchbase_db.py](#knowledge--vector_db--couchbase_db--async_couchbase_dbpy)
- [knowledge/vector_db/couchbase_db/couchbase_db.py](#knowledge--vector_db--couchbase_db--couchbase_dbpy)
- [knowledge/vector_db/lance_db/lance_db.py](#knowledge--vector_db--lance_db--lance_dbpy)
- [knowledge/vector_db/lance_db/lance_db_hybrid_search.py](#knowledge--vector_db--lance_db--lance_db_hybrid_searchpy)
- [knowledge/vector_db/lance_db/lance_db_with_mistral_embedder.py](#knowledge--vector_db--lance_db--lance_db_with_mistral_embedderpy)
- [knowledge/vector_db/langchain/async_langchain_db.py](#knowledge--vector_db--langchain--async_langchain_dbpy)
- [knowledge/vector_db/langchain/langchain_db.py](#knowledge--vector_db--langchain--langchain_dbpy)
- [knowledge/vector_db/lightrag/lightrag.py](#knowledge--vector_db--lightrag--lightragpy)
- [knowledge/vector_db/llamaindex_db/async_llamaindex_db.py](#knowledge--vector_db--llamaindex_db--async_llamaindex_dbpy)
- [knowledge/vector_db/llamaindex_db/llamaindex_db.py](#knowledge--vector_db--llamaindex_db--llamaindex_dbpy)
- [knowledge/vector_db/milvus_db/async_milvus_db.py](#knowledge--vector_db--milvus_db--async_milvus_dbpy)
- [knowledge/vector_db/milvus_db/async_milvus_db_hybrid_search.py](#knowledge--vector_db--milvus_db--async_milvus_db_hybrid_searchpy)
- [knowledge/vector_db/milvus_db/milvus_db.py](#knowledge--vector_db--milvus_db--milvus_dbpy)
- [knowledge/vector_db/milvus_db/milvus_db_hybrid_search.py](#knowledge--vector_db--milvus_db--milvus_db_hybrid_searchpy)
- [knowledge/vector_db/mongo_db/async_mongo_db.py](#knowledge--vector_db--mongo_db--async_mongo_dbpy)
- [knowledge/vector_db/mongo_db/cosmos_mongodb_vcore.py](#knowledge--vector_db--mongo_db--cosmos_mongodb_vcorepy)
- [knowledge/vector_db/mongo_db/mongo_db.py](#knowledge--vector_db--mongo_db--mongo_dbpy)
- [knowledge/vector_db/mongo_db/mongo_db_hybrid_search.py](#knowledge--vector_db--mongo_db--mongo_db_hybrid_searchpy)
- [knowledge/vector_db/pgvector/async_pg_vector.py](#knowledge--vector_db--pgvector--async_pg_vectorpy)
- [knowledge/vector_db/pgvector/pgvector_db.py](#knowledge--vector_db--pgvector--pgvector_dbpy)
- [knowledge/vector_db/pgvector/pgvector_hybrid_search.py](#knowledge--vector_db--pgvector--pgvector_hybrid_searchpy)
- [knowledge/vector_db/pinecone_db/pinecone_db.py](#knowledge--vector_db--pinecone_db--pinecone_dbpy)
- [knowledge/vector_db/qdrant_db/async_qdrant_db.py](#knowledge--vector_db--qdrant_db--async_qdrant_dbpy)
- [knowledge/vector_db/qdrant_db/qdrant_db.py](#knowledge--vector_db--qdrant_db--qdrant_dbpy)
- [knowledge/vector_db/qdrant_db/qdrant_db_hybrid_search.py](#knowledge--vector_db--qdrant_db--qdrant_db_hybrid_searchpy)
- [knowledge/vector_db/singlestore_db/lance_db/remote_lance_db.py](#knowledge--vector_db--singlestore_db--lance_db--remote_lance_dbpy)
- [knowledge/vector_db/singlestore_db/singlestore_db.py](#knowledge--vector_db--singlestore_db--singlestore_dbpy)
- [knowledge/vector_db/singlestore_db/surrealdb/async_surreal_db.py](#knowledge--vector_db--singlestore_db--surrealdb--async_surreal_dbpy)
- [knowledge/vector_db/singlestore_db/surrealdb/surreal_db.py](#knowledge--vector_db--singlestore_db--surrealdb--surreal_dbpy)
- [knowledge/vector_db/surrealdb/async_surreal_db.py](#knowledge--vector_db--surrealdb--async_surreal_dbpy)
- [knowledge/vector_db/surrealdb/surreal_db.py](#knowledge--vector_db--surrealdb--surreal_dbpy)
- [knowledge/vector_db/upstash_db/upstash_db.py](#knowledge--vector_db--upstash_db--upstash_dbpy)
- [knowledge/vector_db/weaviate_db/async_weaviate_db.py](#knowledge--vector_db--weaviate_db--async_weaviate_dbpy)
- [knowledge/vector_db/weaviate_db/weaviate_db.py](#knowledge--vector_db--weaviate_db--weaviate_dbpy)
- [knowledge/vector_db/weaviate_db/weaviate_db_hybrid_search.py](#knowledge--vector_db--weaviate_db--weaviate_db_hybrid_searchpy)
- [knowledge/vector_db/weaviate_db/weaviate_db_upsert.py](#knowledge--vector_db--weaviate_db--weaviate_db_upsertpy)
- [memory/01_agent_with_memory.py](#memory--01_agent_with_memorypy)
- [memory/02_agentic_memory.py](#memory--02_agentic_memorypy)
- [memory/03_agents_share_memory.py](#memory--03_agents_share_memorypy)
- [memory/04_custom_memory_manager.py](#memory--04_custom_memory_managerpy)
- [memory/05_multi_user_multi_session_chat.py](#memory--05_multi_user_multi_session_chatpy)
- [memory/06_multi_user_multi_session_chat_concurrent.py](#memory--06_multi_user_multi_session_chat_concurrentpy)
- [memory/07_share_memory_and_history_between_agents.py](#memory--07_share_memory_and_history_between_agentspy)
- [memory/08_memory_tools.py](#memory--08_memory_toolspy)
- [memory/memory_manager/01_standalone_memory.py](#memory--memory_manager--01_standalone_memorypy)
- [memory/memory_manager/02_memory_creation.py](#memory--memory_manager--02_memory_creationpy)
- [memory/memory_manager/03_custom_memory_instructions.py](#memory--memory_manager--03_custom_memory_instructionspy)
- [memory/memory_manager/04_memory_search.py](#memory--memory_manager--04_memory_searchpy)
- [memory/memory_manager/05_db_tools_control.py](#memory--memory_manager--05_db_tools_controlpy)
- [models/aimlapi/async_basic.py](#models--aimlapi--async_basicpy)
- [models/aimlapi/async_basic_stream.py](#models--aimlapi--async_basic_streampy)
- [models/aimlapi/async_tool_use.py](#models--aimlapi--async_tool_usepy)
- [models/aimlapi/basic.py](#models--aimlapi--basicpy)
- [models/aimlapi/basic_stream.py](#models--aimlapi--basic_streampy)
- [models/aimlapi/image_agent.py](#models--aimlapi--image_agentpy)
- [models/aimlapi/image_agent_bytes.py](#models--aimlapi--image_agent_bytespy)
- [models/aimlapi/image_agent_with_memory.py](#models--aimlapi--image_agent_with_memorypy)
- [models/aimlapi/structured_output.py](#models--aimlapi--structured_outputpy)
- [models/aimlapi/tool_use.py](#models--aimlapi--tool_usepy)
- [models/anthropic/async_basic.py](#models--anthropic--async_basicpy)
- [models/anthropic/async_basic_stream.py](#models--anthropic--async_basic_streampy)
- [models/anthropic/async_tool_use.py](#models--anthropic--async_tool_usepy)
- [models/anthropic/basic.py](#models--anthropic--basicpy)
- [models/anthropic/basic_stream.py](#models--anthropic--basic_streampy)
- [models/anthropic/code_execution.py](#models--anthropic--code_executionpy)
- [models/anthropic/db.py](#models--anthropic--dbpy)
- [models/anthropic/financial_analyst_thinking.py](#models--anthropic--financial_analyst_thinkingpy)
- [models/anthropic/image_input_bytes.py](#models--anthropic--image_input_bytespy)
- [models/anthropic/image_input_file_upload.py](#models--anthropic--image_input_file_uploadpy)
- [models/anthropic/image_input_url.py](#models--anthropic--image_input_urlpy)
- [models/anthropic/knowledge.py](#models--anthropic--knowledgepy)
- [models/anthropic/mcp_connector.py](#models--anthropic--mcp_connectorpy)
- [models/anthropic/memory.py](#models--anthropic--memorypy)
- [models/anthropic/pdf_input_bytes.py](#models--anthropic--pdf_input_bytespy)
- [models/anthropic/pdf_input_file_upload.py](#models--anthropic--pdf_input_file_uploadpy)
- [models/anthropic/pdf_input_local.py](#models--anthropic--pdf_input_localpy)
- [models/anthropic/pdf_input_url.py](#models--anthropic--pdf_input_urlpy)
- [models/anthropic/prompt_caching.py](#models--anthropic--prompt_cachingpy)
- [models/anthropic/prompt_caching_extended.py](#models--anthropic--prompt_caching_extendedpy)
- [models/anthropic/structured_output.py](#models--anthropic--structured_outputpy)
- [models/anthropic/structured_output_stream.py](#models--anthropic--structured_output_streampy)
- [models/anthropic/thinking.py](#models--anthropic--thinkingpy)
- [models/anthropic/thinking_stream.py](#models--anthropic--thinking_streampy)
- [models/anthropic/tool_use.py](#models--anthropic--tool_usepy)
- [models/anthropic/tool_use_stream.py](#models--anthropic--tool_use_streampy)
- [models/anthropic/web_fetch.py](#models--anthropic--web_fetchpy)
- [models/anthropic/web_search.py](#models--anthropic--web_searchpy)
- [models/aws/bedrock/async_basic.py](#models--aws--bedrock--async_basicpy)
- [models/aws/bedrock/async_basic_stream.py](#models--aws--bedrock--async_basic_streampy)
- [models/aws/bedrock/async_tool_use_stream.py](#models--aws--bedrock--async_tool_use_streampy)
- [models/aws/bedrock/basic.py](#models--aws--bedrock--basicpy)
- [models/aws/bedrock/basic_stream.py](#models--aws--bedrock--basic_streampy)
- [models/aws/bedrock/image_agent_bytes.py](#models--aws--bedrock--image_agent_bytespy)
- [models/aws/bedrock/pdf_agent_bytes.py](#models--aws--bedrock--pdf_agent_bytespy)
- [models/aws/bedrock/structured_output.py](#models--aws--bedrock--structured_outputpy)
- [models/aws/bedrock/tool_use.py](#models--aws--bedrock--tool_usepy)
- [models/aws/bedrock/tool_use_stream.py](#models--aws--bedrock--tool_use_streampy)
- [models/aws/claude/async_basic.py](#models--aws--claude--async_basicpy)
- [models/aws/claude/async_basic_stream.py](#models--aws--claude--async_basic_streampy)
- [models/aws/claude/async_tool_use.py](#models--aws--claude--async_tool_usepy)
- [models/aws/claude/basic.py](#models--aws--claude--basicpy)
- [models/aws/claude/basic_stream.py](#models--aws--claude--basic_streampy)
- [models/aws/claude/db.py](#models--aws--claude--dbpy)
- [models/aws/claude/image_agent.py](#models--aws--claude--image_agentpy)
- [models/aws/claude/knowledge.py](#models--aws--claude--knowledgepy)
- [models/aws/claude/structured_output.py](#models--aws--claude--structured_outputpy)
- [models/aws/claude/tool_use.py](#models--aws--claude--tool_usepy)
- [models/aws/claude/tool_use_stream.py](#models--aws--claude--tool_use_streampy)
- [models/azure/ai_foundry/async_basic.py](#models--azure--ai_foundry--async_basicpy)
- [models/azure/ai_foundry/async_basic_stream.py](#models--azure--ai_foundry--async_basic_streampy)
- [models/azure/ai_foundry/async_tool_use.py](#models--azure--ai_foundry--async_tool_usepy)
- [models/azure/ai_foundry/basic.py](#models--azure--ai_foundry--basicpy)
- [models/azure/ai_foundry/basic_stream.py](#models--azure--ai_foundry--basic_streampy)
- [models/azure/ai_foundry/db.py](#models--azure--ai_foundry--dbpy)
- [models/azure/ai_foundry/demo_cohere.py](#models--azure--ai_foundry--demo_coherepy)
- [models/azure/ai_foundry/demo_mistral.py](#models--azure--ai_foundry--demo_mistralpy)
- [models/azure/ai_foundry/image_agent.py](#models--azure--ai_foundry--image_agentpy)
- [models/azure/ai_foundry/image_agent_bytes.py](#models--azure--ai_foundry--image_agent_bytespy)
- [models/azure/ai_foundry/knowledge.py](#models--azure--ai_foundry--knowledgepy)
- [models/azure/ai_foundry/structured_output.py](#models--azure--ai_foundry--structured_outputpy)
- [models/azure/ai_foundry/tool_use.py](#models--azure--ai_foundry--tool_usepy)
- [models/azure/openai/async_basic.py](#models--azure--openai--async_basicpy)
- [models/azure/openai/async_basic_stream.py](#models--azure--openai--async_basic_streampy)
- [models/azure/openai/basic.py](#models--azure--openai--basicpy)
- [models/azure/openai/basic_stream.py](#models--azure--openai--basic_streampy)
- [models/azure/openai/db.py](#models--azure--openai--dbpy)
- [models/azure/openai/knowledge.py](#models--azure--openai--knowledgepy)
- [models/azure/openai/structured_output.py](#models--azure--openai--structured_outputpy)
- [models/azure/openai/tool_use.py](#models--azure--openai--tool_usepy)
- [models/cerebras/async_basic.py](#models--cerebras--async_basicpy)
- [models/cerebras/async_basic_stream.py](#models--cerebras--async_basic_streampy)
- [models/cerebras/async_tool_use.py](#models--cerebras--async_tool_usepy)
- [models/cerebras/async_tool_use_stream.py](#models--cerebras--async_tool_use_streampy)
- [models/cerebras/basic.py](#models--cerebras--basicpy)
- [models/cerebras/basic_stream.py](#models--cerebras--basic_streampy)
- [models/cerebras/db.py](#models--cerebras--dbpy)
- [models/cerebras/knowledge.py](#models--cerebras--knowledgepy)
- [models/cerebras/oss_gpt.py](#models--cerebras--oss_gptpy)
- [models/cerebras/structured_output.py](#models--cerebras--structured_outputpy)
- [models/cerebras/tool_use.py](#models--cerebras--tool_usepy)
- [models/cerebras/tool_use_stream.py](#models--cerebras--tool_use_streampy)
- [models/cerebras_openai/async_basic.py](#models--cerebras_openai--async_basicpy)
- [models/cerebras_openai/async_basic_stream.py](#models--cerebras_openai--async_basic_streampy)
- [models/cerebras_openai/async_tool_use.py](#models--cerebras_openai--async_tool_usepy)
- [models/cerebras_openai/async_tool_use_stream.py](#models--cerebras_openai--async_tool_use_streampy)
- [models/cerebras_openai/basic.py](#models--cerebras_openai--basicpy)
- [models/cerebras_openai/basic_stream.py](#models--cerebras_openai--basic_streampy)
- [models/cerebras_openai/db.py](#models--cerebras_openai--dbpy)
- [models/cerebras_openai/knowledge.py](#models--cerebras_openai--knowledgepy)
- [models/cerebras_openai/oss_gpt.py](#models--cerebras_openai--oss_gptpy)
- [models/cerebras_openai/structured_output.py](#models--cerebras_openai--structured_outputpy)
- [models/cerebras_openai/tool_use.py](#models--cerebras_openai--tool_usepy)
- [models/cerebras_openai/tool_use_stream.py](#models--cerebras_openai--tool_use_streampy)
- [models/cohere/async_basic.py](#models--cohere--async_basicpy)
- [models/cohere/async_basic_stream.py](#models--cohere--async_basic_streampy)
- [models/cohere/async_structured_output.py](#models--cohere--async_structured_outputpy)
- [models/cohere/async_tool_use.py](#models--cohere--async_tool_usepy)
- [models/cohere/basic.py](#models--cohere--basicpy)
- [models/cohere/basic_stream.py](#models--cohere--basic_streampy)
- [models/cohere/db.py](#models--cohere--dbpy)
- [models/cohere/image_agent.py](#models--cohere--image_agentpy)
- [models/cohere/image_agent_bytes.py](#models--cohere--image_agent_bytespy)
- [models/cohere/image_agent_local_file.py](#models--cohere--image_agent_local_filepy)
- [models/cohere/knowledge.py](#models--cohere--knowledgepy)
- [models/cohere/memory.py](#models--cohere--memorypy)
- [models/cohere/structured_output.py](#models--cohere--structured_outputpy)
- [models/cohere/tool_use.py](#models--cohere--tool_usepy)
- [models/cohere/tool_use_stream.py](#models--cohere--tool_use_streampy)
- [models/cometapi/async_basic.py](#models--cometapi--async_basicpy)
- [models/cometapi/async_basic_stream.py](#models--cometapi--async_basic_streampy)
- [models/cometapi/async_tool_use.py](#models--cometapi--async_tool_usepy)
- [models/cometapi/async_tool_use_stream.py](#models--cometapi--async_tool_use_streampy)
- [models/cometapi/basic.py](#models--cometapi--basicpy)
- [models/cometapi/basic_stream.py](#models--cometapi--basic_streampy)
- [models/cometapi/image_agent.py](#models--cometapi--image_agentpy)
- [models/cometapi/image_agent_with_memory.py](#models--cometapi--image_agent_with_memorypy)
- [models/cometapi/multi_model.py](#models--cometapi--multi_modelpy)
- [models/cometapi/structured_output.py](#models--cometapi--structured_outputpy)
- [models/cometapi/tool_use.py](#models--cometapi--tool_usepy)
- [models/cometapi/tool_use_stream.py](#models--cometapi--tool_use_streampy)
- [models/dashscope/async_basic.py](#models--dashscope--async_basicpy)
- [models/dashscope/async_basic_stream.py](#models--dashscope--async_basic_streampy)
- [models/dashscope/async_image_agent.py](#models--dashscope--async_image_agentpy)
- [models/dashscope/async_tool_use.py](#models--dashscope--async_tool_usepy)
- [models/dashscope/basic.py](#models--dashscope--basicpy)
- [models/dashscope/basic_stream.py](#models--dashscope--basic_streampy)
- [models/dashscope/image_agent.py](#models--dashscope--image_agentpy)
- [models/dashscope/image_agent_bytes.py](#models--dashscope--image_agent_bytespy)
- [models/dashscope/structured_output.py](#models--dashscope--structured_outputpy)
- [models/dashscope/thinking_agent.py](#models--dashscope--thinking_agentpy)
- [models/dashscope/tool_use.py](#models--dashscope--tool_usepy)
- [models/deepinfra/async_basic.py](#models--deepinfra--async_basicpy)
- [models/deepinfra/async_basic_stream.py](#models--deepinfra--async_basic_streampy)
- [models/deepinfra/async_tool_use.py](#models--deepinfra--async_tool_usepy)
- [models/deepinfra/basic.py](#models--deepinfra--basicpy)
- [models/deepinfra/basic_stream.py](#models--deepinfra--basic_streampy)
- [models/deepinfra/json_output.py](#models--deepinfra--json_outputpy)
- [models/deepinfra/tool_use.py](#models--deepinfra--tool_usepy)
- [models/deepseek/async_basic.py](#models--deepseek--async_basicpy)
- [models/deepseek/async_basic_streaming.py](#models--deepseek--async_basic_streamingpy)
- [models/deepseek/async_tool_use.py](#models--deepseek--async_tool_usepy)
- [models/deepseek/basic.py](#models--deepseek--basicpy)
- [models/deepseek/basic_stream.py](#models--deepseek--basic_streampy)
- [models/deepseek/reasoning_agent.py](#models--deepseek--reasoning_agentpy)
- [models/deepseek/structured_output.py](#models--deepseek--structured_outputpy)
- [models/deepseek/tool_use.py](#models--deepseek--tool_usepy)
- [models/fireworks/async_basic.py](#models--fireworks--async_basicpy)
- [models/fireworks/async_basic_stream.py](#models--fireworks--async_basic_streampy)
- [models/fireworks/async_tool_use.py](#models--fireworks--async_tool_usepy)
- [models/fireworks/basic.py](#models--fireworks--basicpy)
- [models/fireworks/basic_stream.py](#models--fireworks--basic_streampy)
- [models/fireworks/structured_output.py](#models--fireworks--structured_outputpy)
- [models/fireworks/tool_use.py](#models--fireworks--tool_usepy)
- [models/google/gemini/agent_with_thinking_budget.py](#models--google--gemini--agent_with_thinking_budgetpy)
- [models/google/gemini/async_basic.py](#models--google--gemini--async_basicpy)
- [models/google/gemini/async_basic_stream.py](#models--google--gemini--async_basic_streampy)
- [models/google/gemini/async_image_editing.py](#models--google--gemini--async_image_editingpy)
- [models/google/gemini/async_image_generation.py](#models--google--gemini--async_image_generationpy)
- [models/google/gemini/async_image_generation_stream.py](#models--google--gemini--async_image_generation_streampy)
- [models/google/gemini/async_tool_use.py](#models--google--gemini--async_tool_usepy)
- [models/google/gemini/audio_input_bytes_content.py](#models--google--gemini--audio_input_bytes_contentpy)
- [models/google/gemini/audio_input_file_upload.py](#models--google--gemini--audio_input_file_uploadpy)
- [models/google/gemini/audio_input_local_file_upload.py](#models--google--gemini--audio_input_local_file_uploadpy)
- [models/google/gemini/basic.py](#models--google--gemini--basicpy)
- [models/google/gemini/basic_stream.py](#models--google--gemini--basic_streampy)
- [models/google/gemini/db.py](#models--google--gemini--dbpy)
- [models/google/gemini/file_upload_with_cache.py](#models--google--gemini--file_upload_with_cachepy)
- [models/google/gemini/grounding.py](#models--google--gemini--groundingpy)
- [models/google/gemini/image_editing.py](#models--google--gemini--image_editingpy)
- [models/google/gemini/image_generation.py](#models--google--gemini--image_generationpy)
- [models/google/gemini/image_generation_stream.py](#models--google--gemini--image_generation_streampy)
- [models/google/gemini/image_input.py](#models--google--gemini--image_inputpy)
- [models/google/gemini/image_input_file_upload.py](#models--google--gemini--image_input_file_uploadpy)
- [models/google/gemini/imagen_tool.py](#models--google--gemini--imagen_toolpy)
- [models/google/gemini/imagen_tool_advanced.py](#models--google--gemini--imagen_tool_advancedpy)
- [models/google/gemini/knowledge.py](#models--google--gemini--knowledgepy)
- [models/google/gemini/pdf_input_file_upload.py](#models--google--gemini--pdf_input_file_uploadpy)
- [models/google/gemini/pdf_input_local.py](#models--google--gemini--pdf_input_localpy)
- [models/google/gemini/pdf_input_url.py](#models--google--gemini--pdf_input_urlpy)
- [models/google/gemini/search.py](#models--google--gemini--searchpy)
- [models/google/gemini/storage_and_memory.py](#models--google--gemini--storage_and_memorypy)
- [models/google/gemini/structured_output.py](#models--google--gemini--structured_outputpy)
- [models/google/gemini/structured_output_stream.py](#models--google--gemini--structured_output_streampy)
- [models/google/gemini/text_to_speech.py](#models--google--gemini--text_to_speechpy)
- [models/google/gemini/thinking_agent.py](#models--google--gemini--thinking_agentpy)
- [models/google/gemini/thinking_agent_stream.py](#models--google--gemini--thinking_agent_streampy)
- [models/google/gemini/tool_use.py](#models--google--gemini--tool_usepy)
- [models/google/gemini/tool_use_stream.py](#models--google--gemini--tool_use_streampy)
- [models/google/gemini/url_context.py](#models--google--gemini--url_contextpy)
- [models/google/gemini/url_context_with_search.py](#models--google--gemini--url_context_with_searchpy)
- [models/google/gemini/vertex_ai_search.py](#models--google--gemini--vertex_ai_searchpy)
- [models/google/gemini/vertexai.py](#models--google--gemini--vertexaipy)
- [models/google/gemini/video_input_bytes_content.py](#models--google--gemini--video_input_bytes_contentpy)
- [models/google/gemini/video_input_file_upload.py](#models--google--gemini--video_input_file_uploadpy)
- [models/google/gemini/video_input_local_file_upload.py](#models--google--gemini--video_input_local_file_uploadpy)
- [models/google/gemini/video_input_youtube.py](#models--google--gemini--video_input_youtubepy)
- [models/groq/agent_team.py](#models--groq--agent_teampy)
- [models/groq/async_basic.py](#models--groq--async_basicpy)
- [models/groq/async_basic_stream.py](#models--groq--async_basic_streampy)
- [models/groq/async_tool_use.py](#models--groq--async_tool_usepy)
- [models/groq/basic.py](#models--groq--basicpy)
- [models/groq/basic_stream.py](#models--groq--basic_streampy)
- [models/groq/browser_search.py](#models--groq--browser_searchpy)
- [models/groq/db.py](#models--groq--dbpy)
- [models/groq/deep_knowledge.py](#models--groq--deep_knowledgepy)
- [models/groq/image_agent.py](#models--groq--image_agentpy)
- [models/groq/knowledge.py](#models--groq--knowledgepy)
- [models/groq/metrics.py](#models--groq--metricspy)
- [models/groq/reasoning/basic.py](#models--groq--reasoning--basicpy)
- [models/groq/reasoning/basic_stream.py](#models--groq--reasoning--basic_streampy)
- [models/groq/reasoning/demo_deepseek_qwen.py](#models--groq--reasoning--demo_deepseek_qwenpy)
- [models/groq/reasoning/demo_qwen_2_5_32B.py](#models--groq--reasoning--demo_qwen_2_5_32bpy)
- [models/groq/reasoning/finance_agent.py](#models--groq--reasoning--finance_agentpy)
- [models/groq/reasoning_agent.py](#models--groq--reasoning_agentpy)
- [models/groq/research_agent_exa.py](#models--groq--research_agent_exapy)
- [models/groq/structured_output.py](#models--groq--structured_outputpy)
- [models/groq/tool_use.py](#models--groq--tool_usepy)
- [models/groq/transcription_agent.py](#models--groq--transcription_agentpy)
- [models/groq/translation_agent.py](#models--groq--translation_agentpy)
- [models/huggingface/async_basic.py](#models--huggingface--async_basicpy)
- [models/huggingface/async_basic_stream.py](#models--huggingface--async_basic_streampy)
- [models/huggingface/basic.py](#models--huggingface--basicpy)
- [models/huggingface/basic_stream.py](#models--huggingface--basic_streampy)
- [models/huggingface/llama_essay_writer.py](#models--huggingface--llama_essay_writerpy)
- [models/huggingface/tool_use.py](#models--huggingface--tool_usepy)
- [models/huggingface/tool_use_stream.py](#models--huggingface--tool_use_streampy)
- [models/ibm/watsonx/async_basic.py](#models--ibm--watsonx--async_basicpy)
- [models/ibm/watsonx/async_basic_stream.py](#models--ibm--watsonx--async_basic_streampy)
- [models/ibm/watsonx/async_tool_use.py](#models--ibm--watsonx--async_tool_usepy)
- [models/ibm/watsonx/basic.py](#models--ibm--watsonx--basicpy)
- [models/ibm/watsonx/basic_stream.py](#models--ibm--watsonx--basic_streampy)
- [models/ibm/watsonx/db.py](#models--ibm--watsonx--dbpy)
- [models/ibm/watsonx/image_agent_bytes.py](#models--ibm--watsonx--image_agent_bytespy)
- [models/ibm/watsonx/knowledge.py](#models--ibm--watsonx--knowledgepy)
- [models/ibm/watsonx/structured_output.py](#models--ibm--watsonx--structured_outputpy)
- [models/ibm/watsonx/tool_use.py](#models--ibm--watsonx--tool_usepy)
- [models/langdb/agent.py](#models--langdb--agentpy)
- [models/langdb/agent_stream.py](#models--langdb--agent_streampy)
- [models/langdb/basic.py](#models--langdb--basicpy)
- [models/langdb/basic_stream.py](#models--langdb--basic_streampy)
- [models/langdb/data_analyst.py](#models--langdb--data_analystpy)
- [models/langdb/finance_agent.py](#models--langdb--finance_agentpy)
- [models/langdb/structured_output.py](#models--langdb--structured_outputpy)
- [models/langdb/web_search.py](#models--langdb--web_searchpy)
- [models/litellm/async_basic.py](#models--litellm--async_basicpy)
- [models/litellm/async_basic_stream.py](#models--litellm--async_basic_streampy)
- [models/litellm/async_tool_use.py](#models--litellm--async_tool_usepy)
- [models/litellm/audio_input_agent.py](#models--litellm--audio_input_agentpy)
- [models/litellm/basic.py](#models--litellm--basicpy)
- [models/litellm/basic_gpt.py](#models--litellm--basic_gptpy)
- [models/litellm/basic_stream.py](#models--litellm--basic_streampy)
- [models/litellm/db.py](#models--litellm--dbpy)
- [models/litellm/image_agent.py](#models--litellm--image_agentpy)
- [models/litellm/image_agent_bytes.py](#models--litellm--image_agent_bytespy)
- [models/litellm/knowledge.py](#models--litellm--knowledgepy)
- [models/litellm/memory.py](#models--litellm--memorypy)
- [models/litellm/metrics.py](#models--litellm--metricspy)
- [models/litellm/pdf_input_bytes.py](#models--litellm--pdf_input_bytespy)
- [models/litellm/pdf_input_local.py](#models--litellm--pdf_input_localpy)
- [models/litellm/pdf_input_url.py](#models--litellm--pdf_input_urlpy)
- [models/litellm/structured_output.py](#models--litellm--structured_outputpy)
- [models/litellm/tool_use.py](#models--litellm--tool_usepy)
- [models/litellm/tool_use_stream.py](#models--litellm--tool_use_streampy)
- [models/litellm_openai/audio_input_agent.py](#models--litellm_openai--audio_input_agentpy)
- [models/litellm_openai/basic.py](#models--litellm_openai--basicpy)
- [models/litellm_openai/basic_stream.py](#models--litellm_openai--basic_streampy)
- [models/litellm_openai/tool_use.py](#models--litellm_openai--tool_usepy)
- [models/llama_cpp/basic.py](#models--llama_cpp--basicpy)
- [models/llama_cpp/basic_stream.py](#models--llama_cpp--basic_streampy)
- [models/llama_cpp/structured_output.py](#models--llama_cpp--structured_outputpy)
- [models/llama_cpp/tool_use.py](#models--llama_cpp--tool_usepy)
- [models/llama_cpp/tool_use_stream.py](#models--llama_cpp--tool_use_streampy)
- [models/lmstudio/basic.py](#models--lmstudio--basicpy)
- [models/lmstudio/basic_stream.py](#models--lmstudio--basic_streampy)
- [models/lmstudio/db.py](#models--lmstudio--dbpy)
- [models/lmstudio/image_agent.py](#models--lmstudio--image_agentpy)
- [models/lmstudio/knowledge.py](#models--lmstudio--knowledgepy)
- [models/lmstudio/memory.py](#models--lmstudio--memorypy)
- [models/lmstudio/structured_output.py](#models--lmstudio--structured_outputpy)
- [models/lmstudio/tool_use.py](#models--lmstudio--tool_usepy)
- [models/lmstudio/tool_use_stream.py](#models--lmstudio--tool_use_streampy)
- [models/meta/llama/async_basic.py](#models--meta--llama--async_basicpy)
- [models/meta/llama/async_basic_stream.py](#models--meta--llama--async_basic_streampy)
- [models/meta/llama/async_knowledge.py](#models--meta--llama--async_knowledgepy)
- [models/meta/llama/async_tool_use.py](#models--meta--llama--async_tool_usepy)
- [models/meta/llama/async_tool_use_stream.py](#models--meta--llama--async_tool_use_streampy)
- [models/meta/llama/basic.py](#models--meta--llama--basicpy)
- [models/meta/llama/basic_stream.py](#models--meta--llama--basic_streampy)
- [models/meta/llama/db.py](#models--meta--llama--dbpy)
- [models/meta/llama/image_input_bytes.py](#models--meta--llama--image_input_bytespy)
- [models/meta/llama/image_input_file.py](#models--meta--llama--image_input_filepy)
- [models/meta/llama/knowledge.py](#models--meta--llama--knowledgepy)
- [models/meta/llama/memory.py](#models--meta--llama--memorypy)
- [models/meta/llama/metrics.py](#models--meta--llama--metricspy)
- [models/meta/llama/structured_output.py](#models--meta--llama--structured_outputpy)
- [models/meta/llama/tool_use.py](#models--meta--llama--tool_usepy)
- [models/meta/llama/tool_use_stream.py](#models--meta--llama--tool_use_streampy)
- [models/meta/llama_openai/async_basic.py](#models--meta--llama_openai--async_basicpy)
- [models/meta/llama_openai/async_basic_stream.py](#models--meta--llama_openai--async_basic_streampy)
- [models/meta/llama_openai/async_tool_use.py](#models--meta--llama_openai--async_tool_usepy)
- [models/meta/llama_openai/async_tool_use_stream.py](#models--meta--llama_openai--async_tool_use_streampy)
- [models/meta/llama_openai/basic.py](#models--meta--llama_openai--basicpy)
- [models/meta/llama_openai/basic_stream.py](#models--meta--llama_openai--basic_streampy)
- [models/meta/llama_openai/image_input_bytes.py](#models--meta--llama_openai--image_input_bytespy)
- [models/meta/llama_openai/image_input_file.py](#models--meta--llama_openai--image_input_filepy)
- [models/meta/llama_openai/knowledge.py](#models--meta--llama_openai--knowledgepy)
- [models/meta/llama_openai/memory.py](#models--meta--llama_openai--memorypy)
- [models/meta/llama_openai/metrics.py](#models--meta--llama_openai--metricspy)
- [models/meta/llama_openai/storage.py](#models--meta--llama_openai--storagepy)
- [models/meta/llama_openai/structured_output.py](#models--meta--llama_openai--structured_outputpy)
- [models/meta/llama_openai/tool_use.py](#models--meta--llama_openai--tool_usepy)
- [models/meta/llama_openai/tool_use_stream.py](#models--meta--llama_openai--tool_use_streampy)
- [models/mistral/async_basic.py](#models--mistral--async_basicpy)
- [models/mistral/async_basic_stream.py](#models--mistral--async_basic_streampy)
- [models/mistral/async_structured_output.py](#models--mistral--async_structured_outputpy)
- [models/mistral/async_tool_use.py](#models--mistral--async_tool_usepy)
- [models/mistral/basic.py](#models--mistral--basicpy)
- [models/mistral/basic_stream.py](#models--mistral--basic_streampy)
- [models/mistral/image_bytes_input_agent.py](#models--mistral--image_bytes_input_agentpy)
- [models/mistral/image_compare_agent.py](#models--mistral--image_compare_agentpy)
- [models/mistral/image_file_input_agent.py](#models--mistral--image_file_input_agentpy)
- [models/mistral/image_ocr_with_structured_output.py](#models--mistral--image_ocr_with_structured_outputpy)
- [models/mistral/image_transcribe_document_agent.py](#models--mistral--image_transcribe_document_agentpy)
- [models/mistral/memory.py](#models--mistral--memorypy)
- [models/mistral/mistral_small.py](#models--mistral--mistral_smallpy)
- [models/mistral/structured_output.py](#models--mistral--structured_outputpy)
- [models/mistral/structured_output_with_tool_use.py](#models--mistral--structured_output_with_tool_usepy)
- [models/mistral/tool_use.py](#models--mistral--tool_usepy)
- [models/nebius/async_basic.py](#models--nebius--async_basicpy)
- [models/nebius/async_basic_stream.py](#models--nebius--async_basic_streampy)
- [models/nebius/async_tool_use.py](#models--nebius--async_tool_usepy)
- [models/nebius/async_tool_use_stream.py](#models--nebius--async_tool_use_streampy)
- [models/nebius/basic.py](#models--nebius--basicpy)
- [models/nebius/basic_stream.py](#models--nebius--basic_streampy)
- [models/nebius/db.py](#models--nebius--dbpy)
- [models/nebius/knowledge.py](#models--nebius--knowledgepy)
- [models/nebius/structured_output.py](#models--nebius--structured_outputpy)
- [models/nebius/tool_use.py](#models--nebius--tool_usepy)
- [models/nebius/tool_use_stream.py](#models--nebius--tool_use_streampy)
- [models/nexus/async_basic.py](#models--nexus--async_basicpy)
- [models/nexus/async_basic_stream.py](#models--nexus--async_basic_streampy)
- [models/nexus/async_tool_use.py](#models--nexus--async_tool_usepy)
- [models/nexus/async_tool_use_stream.py](#models--nexus--async_tool_use_streampy)
- [models/nexus/basic.py](#models--nexus--basicpy)
- [models/nexus/basic_stream.py](#models--nexus--basic_streampy)
- [models/nexus/tool_use.py](#models--nexus--tool_usepy)
- [models/nexus/tool_use_stream.py](#models--nexus--tool_use_streampy)
- [models/nvidia/async_basic.py](#models--nvidia--async_basicpy)
- [models/nvidia/async_basic_stream.py](#models--nvidia--async_basic_streampy)
- [models/nvidia/async_tool_use.py](#models--nvidia--async_tool_usepy)
- [models/nvidia/basic.py](#models--nvidia--basicpy)
- [models/nvidia/basic_stream.py](#models--nvidia--basic_streampy)
- [models/nvidia/tool_use.py](#models--nvidia--tool_usepy)
- [models/nvidia/tool_use_stream.py](#models--nvidia--tool_use_streampy)
- [models/ollama/async_basic.py](#models--ollama--async_basicpy)
- [models/ollama/async_basic_stream.py](#models--ollama--async_basic_streampy)
- [models/ollama/basic.py](#models--ollama--basicpy)
- [models/ollama/basic_stream.py](#models--ollama--basic_streampy)
- [models/ollama/db.py](#models--ollama--dbpy)
- [models/ollama/demo_deepseek_r1.py](#models--ollama--demo_deepseek_r1py)
- [models/ollama/demo_gemma.py](#models--ollama--demo_gemmapy)
- [models/ollama/demo_phi4.py](#models--ollama--demo_phi4py)
- [models/ollama/demo_qwen.py](#models--ollama--demo_qwenpy)
- [models/ollama/image_agent.py](#models--ollama--image_agentpy)
- [models/ollama/knowledge.py](#models--ollama--knowledgepy)
- [models/ollama/memory.py](#models--ollama--memorypy)
- [models/ollama/ollama_cloud.py](#models--ollama--ollama_cloudpy)
- [models/ollama/set_client.py](#models--ollama--set_clientpy)
- [models/ollama/set_temperature.py](#models--ollama--set_temperaturepy)
- [models/ollama/structured_output.py](#models--ollama--structured_outputpy)
- [models/ollama/tool_use.py](#models--ollama--tool_usepy)
- [models/ollama/tool_use_stream.py](#models--ollama--tool_use_streampy)
- [models/ollama_tools/async_basic.py](#models--ollama_tools--async_basicpy)
- [models/ollama_tools/async_basic_stream.py](#models--ollama_tools--async_basic_streampy)
- [models/ollama_tools/async_tool_use_stream.py](#models--ollama_tools--async_tool_use_streampy)
- [models/ollama_tools/basic.py](#models--ollama_tools--basicpy)
- [models/ollama_tools/basic_stream.py](#models--ollama_tools--basic_streampy)
- [models/ollama_tools/db.py](#models--ollama_tools--dbpy)
- [models/ollama_tools/knowledge.py](#models--ollama_tools--knowledgepy)
- [models/ollama_tools/structured_output.py](#models--ollama_tools--structured_outputpy)
- [models/ollama_tools/tool_use.py](#models--ollama_tools--tool_usepy)
- [models/ollama_tools/tool_use_stream.py](#models--ollama_tools--tool_use_streampy)
- [models/openai/chat/agent_flex_tier.py](#models--openai--chat--agent_flex_tierpy)
- [models/openai/chat/async_basic.py](#models--openai--chat--async_basicpy)
- [models/openai/chat/async_basic_stream.py](#models--openai--chat--async_basic_streampy)
- [models/openai/chat/async_structured_response_stream.py](#models--openai--chat--async_structured_response_streampy)
- [models/openai/chat/async_tool_use.py](#models--openai--chat--async_tool_usepy)
- [models/openai/chat/audio_input_agent.py](#models--openai--chat--audio_input_agentpy)
- [models/openai/chat/audio_input_and_output_multi_turn.py](#models--openai--chat--audio_input_and_output_multi_turnpy)
- [models/openai/chat/audio_input_local_file_upload.py](#models--openai--chat--audio_input_local_file_uploadpy)
- [models/openai/chat/audio_output_agent.py](#models--openai--chat--audio_output_agentpy)
- [models/openai/chat/audio_output_stream.py](#models--openai--chat--audio_output_streampy)
- [models/openai/chat/basic.py](#models--openai--chat--basicpy)
- [models/openai/chat/basic_stream.py](#models--openai--chat--basic_streampy)
- [models/openai/chat/custom_role_map.py](#models--openai--chat--custom_role_mappy)
- [models/openai/chat/db.py](#models--openai--chat--dbpy)
- [models/openai/chat/generate_images.py](#models--openai--chat--generate_imagespy)
- [models/openai/chat/image_agent.py](#models--openai--chat--image_agentpy)
- [models/openai/chat/image_agent_bytes.py](#models--openai--chat--image_agent_bytespy)
- [models/openai/chat/image_agent_with_memory.py](#models--openai--chat--image_agent_with_memorypy)
- [models/openai/chat/knowledge.py](#models--openai--chat--knowledgepy)
- [models/openai/chat/memory.py](#models--openai--chat--memorypy)
- [models/openai/chat/metrics.py](#models--openai--chat--metricspy)
- [models/openai/chat/pdf_input_file_upload.py](#models--openai--chat--pdf_input_file_uploadpy)
- [models/openai/chat/pdf_input_local.py](#models--openai--chat--pdf_input_localpy)
- [models/openai/chat/pdf_input_url.py](#models--openai--chat--pdf_input_urlpy)
- [models/openai/chat/reasoning_o3_mini.py](#models--openai--chat--reasoning_o3_minipy)
- [models/openai/chat/structured_output.py](#models--openai--chat--structured_outputpy)
- [models/openai/chat/structured_output_stream.py](#models--openai--chat--structured_output_streampy)
- [models/openai/chat/text_to_speech_agent.py](#models--openai--chat--text_to_speech_agentpy)
- [models/openai/chat/tool_use.py](#models--openai--chat--tool_usepy)
- [models/openai/chat/tool_use_stream.py](#models--openai--chat--tool_use_streampy)
- [models/openai/chat/verbosity_control.py](#models--openai--chat--verbosity_controlpy)
- [models/openai/responses/agent_flex_tier.py](#models--openai--responses--agent_flex_tierpy)
- [models/openai/responses/async_basic.py](#models--openai--responses--async_basicpy)
- [models/openai/responses/async_basic_stream.py](#models--openai--responses--async_basic_streampy)
- [models/openai/responses/async_tool_use.py](#models--openai--responses--async_tool_usepy)
- [models/openai/responses/basic.py](#models--openai--responses--basicpy)
- [models/openai/responses/basic_stream.py](#models--openai--responses--basic_streampy)
- [models/openai/responses/db.py](#models--openai--responses--dbpy)
- [models/openai/responses/deep_research_agent.py](#models--openai--responses--deep_research_agentpy)
- [models/openai/responses/image_agent.py](#models--openai--responses--image_agentpy)
- [models/openai/responses/image_agent_bytes.py](#models--openai--responses--image_agent_bytespy)
- [models/openai/responses/image_agent_with_memory.py](#models--openai--responses--image_agent_with_memorypy)
- [models/openai/responses/image_generation_agent.py](#models--openai--responses--image_generation_agentpy)
- [models/openai/responses/knowledge.py](#models--openai--responses--knowledgepy)
- [models/openai/responses/memory.py](#models--openai--responses--memorypy)
- [models/openai/responses/pdf_input_local.py](#models--openai--responses--pdf_input_localpy)
- [models/openai/responses/pdf_input_url.py](#models--openai--responses--pdf_input_urlpy)
- [models/openai/responses/reasoning_o3_mini.py](#models--openai--responses--reasoning_o3_minipy)
- [models/openai/responses/structured_output.py](#models--openai--responses--structured_outputpy)
- [models/openai/responses/tool_use.py](#models--openai--responses--tool_usepy)
- [models/openai/responses/tool_use_gpt_5.py](#models--openai--responses--tool_use_gpt_5py)
- [models/openai/responses/tool_use_o3.py](#models--openai--responses--tool_use_o3py)
- [models/openai/responses/tool_use_stream.py](#models--openai--responses--tool_use_streampy)
- [models/openai/responses/verbosity_control.py](#models--openai--responses--verbosity_controlpy)
- [models/openai/responses/websearch_builtin_tool.py](#models--openai--responses--websearch_builtin_toolpy)
- [models/openai/responses/zdr_reasoning_agent.py](#models--openai--responses--zdr_reasoning_agentpy)
- [models/openrouter/async_basic.py](#models--openrouter--async_basicpy)
- [models/openrouter/async_basic_stream.py](#models--openrouter--async_basic_streampy)
- [models/openrouter/async_tool_use.py](#models--openrouter--async_tool_usepy)
- [models/openrouter/basic.py](#models--openrouter--basicpy)
- [models/openrouter/basic_stream.py](#models--openrouter--basic_streampy)
- [models/openrouter/structured_output.py](#models--openrouter--structured_outputpy)
- [models/openrouter/tool_use.py](#models--openrouter--tool_usepy)
- [models/perplexity/async_basic.py](#models--perplexity--async_basicpy)
- [models/perplexity/async_basic_stream.py](#models--perplexity--async_basic_streampy)
- [models/perplexity/basic.py](#models--perplexity--basicpy)
- [models/perplexity/basic_stream.py](#models--perplexity--basic_streampy)
- [models/perplexity/knowledge.py](#models--perplexity--knowledgepy)
- [models/perplexity/memory.py](#models--perplexity--memorypy)
- [models/perplexity/structured_output.py](#models--perplexity--structured_outputpy)
- [models/perplexity/web_search.py](#models--perplexity--web_searchpy)
- [models/portkey/async_basic.py](#models--portkey--async_basicpy)
- [models/portkey/async_basic_stream.py](#models--portkey--async_basic_streampy)
- [models/portkey/async_tool_use.py](#models--portkey--async_tool_usepy)
- [models/portkey/async_tool_use_stream.py](#models--portkey--async_tool_use_streampy)
- [models/portkey/basic.py](#models--portkey--basicpy)
- [models/portkey/basic_stream.py](#models--portkey--basic_streampy)
- [models/portkey/structured_output.py](#models--portkey--structured_outputpy)
- [models/portkey/tool_use.py](#models--portkey--tool_usepy)
- [models/portkey/tool_use_stream.py](#models--portkey--tool_use_streampy)
- [models/sambanova/async_basic.py](#models--sambanova--async_basicpy)
- [models/sambanova/async_basic_stream.py](#models--sambanova--async_basic_streampy)
- [models/sambanova/basic.py](#models--sambanova--basicpy)
- [models/sambanova/basic_stream.py](#models--sambanova--basic_streampy)
- [models/siliconflow/async_basic_streaming.py](#models--siliconflow--async_basic_streamingpy)
- [models/siliconflow/basic.py](#models--siliconflow--basicpy)
- [models/siliconflow/basic_stream.py](#models--siliconflow--basic_streampy)
- [models/siliconflow/structured_output.py](#models--siliconflow--structured_outputpy)
- [models/siliconflow/tool_use.py](#models--siliconflow--tool_usepy)
- [models/together/async_basic.py](#models--together--async_basicpy)
- [models/together/async_basic_stream.py](#models--together--async_basic_streampy)
- [models/together/async_tool_use.py](#models--together--async_tool_usepy)
- [models/together/basic.py](#models--together--basicpy)
- [models/together/basic_stream.py](#models--together--basic_streampy)
- [models/together/image_agent.py](#models--together--image_agentpy)
- [models/together/image_agent_bytes.py](#models--together--image_agent_bytespy)
- [models/together/image_agent_with_memory.py](#models--together--image_agent_with_memorypy)
- [models/together/structured_output.py](#models--together--structured_outputpy)
- [models/together/tool_use.py](#models--together--tool_usepy)
- [models/together/tool_use_stream.py](#models--together--tool_use_streampy)
- [models/vercel/async_basic.py](#models--vercel--async_basicpy)
- [models/vercel/async_basic_stream.py](#models--vercel--async_basic_streampy)
- [models/vercel/async_tool_use.py](#models--vercel--async_tool_usepy)
- [models/vercel/basic.py](#models--vercel--basicpy)
- [models/vercel/basic_stream.py](#models--vercel--basic_streampy)
- [models/vercel/image_agent.py](#models--vercel--image_agentpy)
- [models/vercel/knowledge.py](#models--vercel--knowledgepy)
- [models/vercel/tool_use.py](#models--vercel--tool_usepy)
- [models/vllm/async_basic.py](#models--vllm--async_basicpy)
- [models/vllm/async_basic_stream.py](#models--vllm--async_basic_streampy)
- [models/vllm/async_tool_use.py](#models--vllm--async_tool_usepy)
- [models/vllm/basic.py](#models--vllm--basicpy)
- [models/vllm/basic_stream.py](#models--vllm--basic_streampy)
- [models/vllm/code_generation.py](#models--vllm--code_generationpy)
- [models/vllm/db.py](#models--vllm--dbpy)
- [models/vllm/memory.py](#models--vllm--memorypy)
- [models/vllm/structured_output.py](#models--vllm--structured_outputpy)
- [models/vllm/tool_use.py](#models--vllm--tool_usepy)
- [models/vllm/tool_use_stream.py](#models--vllm--tool_use_streampy)
- [models/xai/async_basic.py](#models--xai--async_basicpy)
- [models/xai/async_basic_stream.py](#models--xai--async_basic_streampy)
- [models/xai/async_tool_use.py](#models--xai--async_tool_usepy)
- [models/xai/basic.py](#models--xai--basicpy)
- [models/xai/basic_stream.py](#models--xai--basic_streampy)
- [models/xai/finance_agent.py](#models--xai--finance_agentpy)
- [models/xai/image_agent.py](#models--xai--image_agentpy)
- [models/xai/image_agent_bytes.py](#models--xai--image_agent_bytespy)
- [models/xai/image_agent_with_memory.py](#models--xai--image_agent_with_memorypy)
- [models/xai/live_search_agent.py](#models--xai--live_search_agentpy)
- [models/xai/live_search_agent_stream.py](#models--xai--live_search_agent_streampy)
- [models/xai/reasoning_agent.py](#models--xai--reasoning_agentpy)
- [models/xai/structured_output.py](#models--xai--structured_outputpy)
- [models/xai/tool_use.py](#models--xai--tool_usepy)
- [models/xai/tool_use_stream.py](#models--xai--tool_use_streampy)
- [reasoning/agents/analyse_treaty_of_versailles.py](#reasoning--agents--analyse_treaty_of_versaillespy)
- [reasoning/agents/capture_reasoning_content_default_COT.py](#reasoning--agents--capture_reasoning_content_default_cotpy)
- [reasoning/agents/cerebras_llama_default_COT.py](#reasoning--agents--cerebras_llama_default_cotpy)
- [reasoning/agents/default_chain_of_thought.py](#reasoning--agents--default_chain_of_thoughtpy)
- [reasoning/agents/fibonacci.py](#reasoning--agents--fibonaccipy)
- [reasoning/agents/finance_agent.py](#reasoning--agents--finance_agentpy)
- [reasoning/agents/ibm_watsonx_default_COT.py](#reasoning--agents--ibm_watsonx_default_cotpy)
- [reasoning/agents/is_9_11_bigger_than_9_9.py](#reasoning--agents--is_9_11_bigger_than_9_9py)
- [reasoning/agents/life_in_500000_years.py](#reasoning--agents--life_in_500000_yearspy)
- [reasoning/agents/logical_puzzle.py](#reasoning--agents--logical_puzzlepy)
- [reasoning/agents/mathematical_proof.py](#reasoning--agents--mathematical_proofpy)
- [reasoning/agents/mistral_reasoning_cot.py](#reasoning--agents--mistral_reasoning_cotpy)
- [reasoning/agents/plan_itenerary.py](#reasoning--agents--plan_itenerarypy)
- [reasoning/agents/python_101_curriculum.py](#reasoning--agents--python_101_curriculumpy)
- [reasoning/agents/scientific_research.py](#reasoning--agents--scientific_researchpy)
- [reasoning/agents/ship_of_theseus.py](#reasoning--agents--ship_of_theseuspy)
- [reasoning/agents/strawberry.py](#reasoning--agents--strawberrypy)
- [reasoning/agents/trolley_problem.py](#reasoning--agents--trolley_problempy)
- [reasoning/models/azure_ai_foundry/reasoning_model_deepseek.py](#reasoning--models--azure_ai_foundry--reasoning_model_deepseekpy)
- [reasoning/models/azure_openai/o1.py](#reasoning--models--azure_openai--o1py)
- [reasoning/models/azure_openai/o3_mini_with_tools.py](#reasoning--models--azure_openai--o3_mini_with_toolspy)
- [reasoning/models/azure_openai/o4_mini.py](#reasoning--models--azure_openai--o4_minipy)
- [reasoning/models/azure_openai/reasoning_model_gpt_4_1.py](#reasoning--models--azure_openai--reasoning_model_gpt_4_1py)
- [reasoning/models/deepseek/9_11_or_9_9.py](#reasoning--models--deepseek--9_11_or_9_9py)
- [reasoning/models/deepseek/analyse_treaty_of_versailles.py](#reasoning--models--deepseek--analyse_treaty_of_versaillespy)
- [reasoning/models/deepseek/ethical_dilemma.py](#reasoning--models--deepseek--ethical_dilemmapy)
- [reasoning/models/deepseek/fibonacci.py](#reasoning--models--deepseek--fibonaccipy)
- [reasoning/models/deepseek/finance_agent.py](#reasoning--models--deepseek--finance_agentpy)
- [reasoning/models/deepseek/life_in_500000_years.py](#reasoning--models--deepseek--life_in_500000_yearspy)
- [reasoning/models/deepseek/logical_puzzle.py](#reasoning--models--deepseek--logical_puzzlepy)
- [reasoning/models/deepseek/mathematical_proof.py](#reasoning--models--deepseek--mathematical_proofpy)
- [reasoning/models/deepseek/plan_itenerary.py](#reasoning--models--deepseek--plan_itenerarypy)
- [reasoning/models/deepseek/python_101_curriculum.py](#reasoning--models--deepseek--python_101_curriculumpy)
- [reasoning/models/deepseek/scientific_research.py](#reasoning--models--deepseek--scientific_researchpy)
- [reasoning/models/deepseek/ship_of_theseus.py](#reasoning--models--deepseek--ship_of_theseuspy)
- [reasoning/models/deepseek/strawberry.py](#reasoning--models--deepseek--strawberrypy)
- [reasoning/models/deepseek/trolley_problem.py](#reasoning--models--deepseek--trolley_problempy)
- [reasoning/models/groq/9_11_or_9_9.py](#reasoning--models--groq--9_11_or_9_9py)
- [reasoning/models/groq/deepseek_plus_claude.py](#reasoning--models--groq--deepseek_plus_claudepy)
- [reasoning/models/ollama/reasoning_model_deepseek.py](#reasoning--models--ollama--reasoning_model_deepseekpy)
- [reasoning/models/openai/o1_pro.py](#reasoning--models--openai--o1_propy)
- [reasoning/models/openai/o3_mini.py](#reasoning--models--openai--o3_minipy)
- [reasoning/models/openai/o3_mini_with_tools.py](#reasoning--models--openai--o3_mini_with_toolspy)
- [reasoning/models/openai/o4_mini.py](#reasoning--models--openai--o4_minipy)
- [reasoning/models/openai/reasoning_effort.py](#reasoning--models--openai--reasoning_effortpy)
- [reasoning/models/openai/reasoning_model_gpt_4_1.py](#reasoning--models--openai--reasoning_model_gpt_4_1py)
- [reasoning/models/openai/reasoning_summary.py](#reasoning--models--openai--reasoning_summarypy)
- [reasoning/models/xai/reasoning_effort.py](#reasoning--models--xai--reasoning_effortpy)
- [reasoning/teams/finance_team_chain_of_thought.py](#reasoning--teams--finance_team_chain_of_thoughtpy)
- [reasoning/teams/knowledge_tool_team.py](#reasoning--teams--knowledge_tool_teampy)
- [reasoning/teams/reasoning_finance_team.py](#reasoning--teams--reasoning_finance_teampy)
- [reasoning/tools/azure_openai_reasoning_tools.py](#reasoning--tools--azure_openai_reasoning_toolspy)
- [reasoning/tools/capture_reasoning_content_knowledge_tools.py](#reasoning--tools--capture_reasoning_content_knowledge_toolspy)
- [reasoning/tools/capture_reasoning_content_reasoning_tools.py](#reasoning--tools--capture_reasoning_content_reasoning_toolspy)
- [reasoning/tools/cerebras_llama_reasoning_tools.py](#reasoning--tools--cerebras_llama_reasoning_toolspy)
- [reasoning/tools/claude_reasoning_tools.py](#reasoning--tools--claude_reasoning_toolspy)
- [reasoning/tools/gemini_finance_agent.py](#reasoning--tools--gemini_finance_agentpy)
- [reasoning/tools/gemini_reasoning_tools.py](#reasoning--tools--gemini_reasoning_toolspy)
- [reasoning/tools/groq_llama_finance_agent.py](#reasoning--tools--groq_llama_finance_agentpy)
- [reasoning/tools/ibm_watsonx_reasoning_tools.py](#reasoning--tools--ibm_watsonx_reasoning_toolspy)
- [reasoning/tools/knowledge_tools.py](#reasoning--tools--knowledge_toolspy)
- [reasoning/tools/llama_reasoning_tools.py](#reasoning--tools--llama_reasoning_toolspy)
- [reasoning/tools/memory_tools.py](#reasoning--tools--memory_toolspy)
- [reasoning/tools/ollama_reasoning_tools.py](#reasoning--tools--ollama_reasoning_toolspy)
- [reasoning/tools/openai_reasoning_tools.py](#reasoning--tools--openai_reasoning_toolspy)
- [reasoning/tools/reasoning_tools.py](#reasoning--tools--reasoning_toolspy)
- [reasoning/tools/vercel_reasoning_tools.py](#reasoning--tools--vercel_reasoning_toolspy)
- [reasoning/tools/workflow_tools.py](#reasoning--tools--workflow_toolspy)
- [scripts/cookbook_runner.py](#scripts--cookbook_runnerpy)
- [teams/async/01_async_coordination_team.py](#teams--async--01_async_coordination_teampy)
- [teams/async/02_async_delegate_to_all_members.py](#teams--async--02_async_delegate_to_all_memberspy)
- [teams/async/03_async_respond_directly.py](#teams--async--03_async_respond_directlypy)
- [teams/basic/few_shot_learning.py](#teams--basic--few_shot_learningpy)
- [teams/basic/input_as_dict.py](#teams--basic--input_as_dictpy)
- [teams/basic/input_as_list.py](#teams--basic--input_as_listpy)
- [teams/basic/input_as_messages_list.py](#teams--basic--input_as_messages_listpy)
- [teams/basic/response_as_variable.py](#teams--basic--response_as_variablepy)
- [teams/basic/run_as_cli.py](#teams--basic--run_as_clipy)
- [teams/basic/team_cancel_a_run.py](#teams--basic--team_cancel_a_runpy)
- [teams/basic/team_exponential_backoff.py](#teams--basic--team_exponential_backoffpy)
- [teams/dependencies/access_dependencies_in_tool.py](#teams--dependencies--access_dependencies_in_toolpy)
- [teams/dependencies/add_dependencies_on_run.py](#teams--dependencies--add_dependencies_on_runpy)
- [teams/dependencies/add_dependencies_to_context.py](#teams--dependencies--add_dependencies_to_contextpy)
- [teams/dependencies/add_dependencies_to_member_context.py](#teams--dependencies--add_dependencies_to_member_contextpy)
- [teams/dependencies/reference_dependencies.py](#teams--dependencies--reference_dependenciespy)
- [teams/distributed_rag/01_distributed_rag_pgvector.py](#teams--distributed_rag--01_distributed_rag_pgvectorpy)
- [teams/distributed_rag/02_distributed_rag_lancedb.py](#teams--distributed_rag--02_distributed_rag_lancedbpy)
- [teams/distributed_rag/03_distributed_rag_with_reranking.py](#teams--distributed_rag--03_distributed_rag_with_rerankingpy)
- [teams/knowledge/01_team_with_knowledge.py](#teams--knowledge--01_team_with_knowledgepy)
- [teams/knowledge/02_team_with_knowledge_filters.py](#teams--knowledge--02_team_with_knowledge_filterspy)
- [teams/knowledge/03_team_with_agentic_knowledge_filters.py](#teams--knowledge--03_team_with_agentic_knowledge_filterspy)
- [teams/memory/01_team_with_memory_manager.py](#teams--memory--01_team_with_memory_managerpy)
- [teams/memory/02_team_with_agentic_memory.py](#teams--memory--02_team_with_agentic_memorypy)
- [teams/metrics/01_team_metrics.py](#teams--metrics--01_team_metricspy)
- [teams/multimodal/audio_sentiment_analysis.py](#teams--multimodal--audio_sentiment_analysispy)
- [teams/multimodal/audio_to_text.py](#teams--multimodal--audio_to_textpy)
- [teams/multimodal/generate_image_with_team.py](#teams--multimodal--generate_image_with_teampy)
- [teams/multimodal/image_to_image_transformation.py](#teams--multimodal--image_to_image_transformationpy)
- [teams/multimodal/image_to_structured_output.py](#teams--multimodal--image_to_structured_outputpy)
- [teams/multimodal/image_to_text.py](#teams--multimodal--image_to_textpy)
- [teams/multimodal/media_input_for_tool.py](#teams--multimodal--media_input_for_toolpy)
- [teams/multimodal/video_caption_generation.py](#teams--multimodal--video_caption_generationpy)
- [teams/reasoning/01_reasoning_multi_purpose_team.py](#teams--reasoning--01_reasoning_multi_purpose_teampy)
- [teams/reasoning/02_async_multi_purpose_reasoning_team.py](#teams--reasoning--02_async_multi_purpose_reasoning_teampy)
- [teams/search_coordination/01_coordinated_agentic_rag.py](#teams--search_coordination--01_coordinated_agentic_ragpy)
- [teams/search_coordination/02_coordinated_reasoning_rag.py](#teams--search_coordination--02_coordinated_reasoning_ragpy)
- [teams/search_coordination/03_distributed_infinity_search.py](#teams--search_coordination--03_distributed_infinity_searchpy)
- [teams/session/01_persistent_session.py](#teams--session--01_persistent_sessionpy)
- [teams/session/02_persistent_session_history.py](#teams--session--02_persistent_session_historypy)
- [teams/session/03_session_summary.py](#teams--session--03_session_summarypy)
- [teams/session/04_session_summary_references.py](#teams--session--04_session_summary_referencespy)
- [teams/session/05_chat_history.py](#teams--session--05_chat_historypy)
- [teams/session/06_rename_session.py](#teams--session--06_rename_sessionpy)
- [teams/session/07_in_memory_db.py](#teams--session--07_in_memory_dbpy)
- [teams/session/08_cache_session.py](#teams--session--08_cache_sessionpy)
- [teams/state/agentic_session_state.py](#teams--state--agentic_session_statepy)
- [teams/state/change_state_on_run.py](#teams--state--change_state_on_runpy)
- [teams/state/pass_state_to_members.py](#teams--state--pass_state_to_memberspy)
- [teams/state/session_state_in_instructions.py](#teams--state--session_state_in_instructionspy)
- [teams/state/share_member_interactions.py](#teams--state--share_member_interactionspy)
- [teams/state/team_with_nested_shared_state.py](#teams--state--team_with_nested_shared_statepy)
- [teams/streaming/01_team_streaming.py](#teams--streaming--01_team_streamingpy)
- [teams/streaming/02_events.py](#teams--streaming--02_eventspy)
- [teams/streaming/03_route_mode_events.py](#teams--streaming--03_route_mode_eventspy)
- [teams/streaming/04_async_team_streaming.py](#teams--streaming--04_async_team_streamingpy)
- [teams/streaming/05_async_team_events.py](#teams--streaming--05_async_team_eventspy)
- [teams/structured_input_output/00_pydantic_model_output.py](#teams--structured_input_output--00_pydantic_model_outputpy)
- [teams/structured_input_output/01_pydantic_model_as_input.py](#teams--structured_input_output--01_pydantic_model_as_inputpy)
- [teams/structured_input_output/02_team_with_parser_model.py](#teams--structured_input_output--02_team_with_parser_modelpy)
- [teams/structured_input_output/03_team_with_output_model.py](#teams--structured_input_output--03_team_with_output_modelpy)
- [teams/structured_input_output/04_structured_output_streaming.py](#teams--structured_input_output--04_structured_output_streamingpy)
- [teams/structured_input_output/05_async_structured_output_streaming.py](#teams--structured_input_output--05_async_structured_output_streamingpy)
- [teams/structured_input_output/06_input_schema_on_team.py](#teams--structured_input_output--06_input_schema_on_teampy)
- [teams/team_with_local_agentic_rag.py](#teams--team_with_local_agentic_ragpy)
- [teams/tools/01_team_with_custom_tools.py](#teams--tools--01_team_with_custom_toolspy)
- [teams/tools/02_team_with_tool_hooks.py](#teams--tools--02_team_with_tool_hookspy)
- [teams/tools/03_async_team_with_tools.py](#teams--tools--03_async_team_with_toolspy)
- [tools/agentql_tools.py](#tools--agentql_toolspy)
- [tools/airflow_tools.py](#tools--airflow_toolspy)
- [tools/apify_tools.py](#tools--apify_toolspy)
- [tools/arxiv_tools.py](#tools--arxiv_toolspy)
- [tools/async/groq-demo.py](#tools--async--groq-demopy)
- [tools/async/openai-demo.py](#tools--async--openai-demopy)
- [tools/aws_lambda_tools.py](#tools--aws_lambda_toolspy)
- [tools/aws_ses_tools.py](#tools--aws_ses_toolspy)
- [tools/baidusearch_tools.py](#tools--baidusearch_toolspy)
- [tools/bitbucket_tools.py](#tools--bitbucket_toolspy)
- [tools/brandfetch_tools.py](#tools--brandfetch_toolspy)
- [tools/bravesearch_tools.py](#tools--bravesearch_toolspy)
- [tools/brightdata_tools.py](#tools--brightdata_toolspy)
- [tools/browserbase_tools.py](#tools--browserbase_toolspy)
- [tools/calcom_tools.py](#tools--calcom_toolspy)
- [tools/calculator_tools.py](#tools--calculator_toolspy)
- [tools/cartesia_tools.py](#tools--cartesia_toolspy)
- [tools/clickup_tools.py](#tools--clickup_toolspy)
- [tools/composio_tools.py](#tools--composio_toolspy)
- [tools/confluence_tools.py](#tools--confluence_toolspy)
- [tools/crawl4ai_tools.py](#tools--crawl4ai_toolspy)
- [tools/csv_tools.py](#tools--csv_toolspy)
- [tools/custom_api_tools.py](#tools--custom_api_toolspy)
- [tools/custom_async_tools.py](#tools--custom_async_toolspy)
- [tools/custom_tool_events.py](#tools--custom_tool_eventspy)
- [tools/custom_tools.py](#tools--custom_toolspy)
- [tools/dalle_tools.py](#tools--dalle_toolspy)
- [tools/daytona_tools.py](#tools--daytona_toolspy)
- [tools/desi_vocal_tools.py](#tools--desi_vocal_toolspy)
- [tools/discord_tools.py](#tools--discord_toolspy)
- [tools/docker_tools.py](#tools--docker_toolspy)
- [tools/duckdb_tools.py](#tools--duckdb_toolspy)
- [tools/duckduckgo_tools.py](#tools--duckduckgo_toolspy)
- [tools/e2b_tools.py](#tools--e2b_toolspy)
- [tools/elevenlabs_tools.py](#tools--elevenlabs_toolspy)
- [tools/email_tools.py](#tools--email_toolspy)
- [tools/evm_tools.py](#tools--evm_toolspy)
- [tools/exa_tools.py](#tools--exa_toolspy)
- [tools/fal_tools.py](#tools--fal_toolspy)
- [tools/file_generation_tools.py](#tools--file_generation_toolspy)
- [tools/file_tools.py](#tools--file_toolspy)
- [tools/financial_datasets_tools.py](#tools--financial_datasets_toolspy)
- [tools/firecrawl_tools.py](#tools--firecrawl_toolspy)
- [tools/giphy_tools.py](#tools--giphy_toolspy)
- [tools/github_tools.py](#tools--github_toolspy)
- [tools/gmail_tools.py](#tools--gmail_toolspy)
- [tools/google_bigquery_tools.py](#tools--google_bigquery_toolspy)
- [tools/google_maps_tools.py](#tools--google_maps_toolspy)
- [tools/googlecalendar_tools.py](#tools--googlecalendar_toolspy)
- [tools/googlesearch_tools.py](#tools--googlesearch_toolspy)
- [tools/googlesheets_tools.py](#tools--googlesheets_toolspy)
- [tools/hackernews_tools.py](#tools--hackernews_toolspy)
- [tools/jinareader_tools.py](#tools--jinareader_toolspy)
- [tools/jira_tools.py](#tools--jira_toolspy)
- [tools/knowledge_tool.py](#tools--knowledge_toolpy)
- [tools/linear_tools.py](#tools--linear_toolspy)
- [tools/linkup_tools.py](#tools--linkup_toolspy)
- [tools/lumalabs_tools.py](#tools--lumalabs_toolspy)
- [tools/mcp/agno_mcp.py](#tools--mcp--agno_mcppy)
- [tools/mcp/airbnb.py](#tools--mcp--airbnbpy)
- [tools/mcp/brave.py](#tools--mcp--bravepy)
- [tools/mcp/cli.py](#tools--mcp--clipy)
- [tools/mcp/filesystem.py](#tools--mcp--filesystempy)
- [tools/mcp/gibsonai.py](#tools--mcp--gibsonaipy)
- [tools/mcp/github.py](#tools--mcp--githubpy)
- [tools/mcp/graphiti.py](#tools--mcp--graphitipy)
- [tools/mcp/groq_mcp.py](#tools--mcp--groq_mcppy)
- [tools/mcp/include_exclude_tools.py](#tools--mcp--include_exclude_toolspy)
- [tools/mcp/include_tools.py](#tools--mcp--include_toolspy)
- [tools/mcp/local_server/client.py](#tools--mcp--local_server--clientpy)
- [tools/mcp/local_server/server.py](#tools--mcp--local_server--serverpy)
- [tools/mcp/mcp_toolbox_demo/agent.py](#tools--mcp--mcp_toolbox_demo--agentpy)
- [tools/mcp/mcp_toolbox_demo/agent_os.py](#tools--mcp--mcp_toolbox_demo--agent_ospy)
- [tools/mcp/mcp_toolbox_demo/hotel_management_typesafe.py](#tools--mcp--mcp_toolbox_demo--hotel_management_typesafepy)
- [tools/mcp/mcp_toolbox_demo/hotel_management_workflows.py](#tools--mcp--mcp_toolbox_demo--hotel_management_workflowspy)
- [tools/mcp/mcp_toolbox_for_db.py](#tools--mcp--mcp_toolbox_for_dbpy)
- [tools/mcp/mem0.py](#tools--mcp--mem0py)
- [tools/mcp/multiple_servers.py](#tools--mcp--multiple_serverspy)
- [tools/mcp/notion_mcp_agent.py](#tools--mcp--notion_mcp_agentpy)
- [tools/mcp/oxylabs.py](#tools--mcp--oxylabspy)
- [tools/mcp/pipedream_auth.py](#tools--mcp--pipedream_authpy)
- [tools/mcp/pipedream_google_calendar.py](#tools--mcp--pipedream_google_calendarpy)
- [tools/mcp/pipedream_linkedin.py](#tools--mcp--pipedream_linkedinpy)
- [tools/mcp/pipedream_slack.py](#tools--mcp--pipedream_slackpy)
- [tools/mcp/qdrant.py](#tools--mcp--qdrantpy)
- [tools/mcp/sequential_thinking.py](#tools--mcp--sequential_thinkingpy)
- [tools/mcp/sse_transport/client.py](#tools--mcp--sse_transport--clientpy)
- [tools/mcp/sse_transport/server.py](#tools--mcp--sse_transport--serverpy)
- [tools/mcp/stagehand.py](#tools--mcp--stagehandpy)
- [tools/mcp/streamable_http_transport/client.py](#tools--mcp--streamable_http_transport--clientpy)
- [tools/mcp/streamable_http_transport/server.py](#tools--mcp--streamable_http_transport--serverpy)
- [tools/mcp/stripe.py](#tools--mcp--stripepy)
- [tools/mcp/supabase.py](#tools--mcp--supabasepy)
- [tools/mcp_tools.py](#tools--mcp_toolspy)
- [tools/mem0_tools.py](#tools--mem0_toolspy)
- [tools/memori_tools.py](#tools--memori_toolspy)
- [tools/mlx_transcribe_tools.py](#tools--mlx_transcribe_toolspy)
- [tools/models/azure_openai_tools.py](#tools--models--azure_openai_toolspy)
- [tools/models/gemini_image_generation.py](#tools--models--gemini_image_generationpy)
- [tools/models/gemini_video_generation.py](#tools--models--gemini_video_generationpy)
- [tools/models/morph.py](#tools--models--morphpy)
- [tools/models/nebius_tools.py](#tools--models--nebius_toolspy)
- [tools/models/openai_tools.py](#tools--models--openai_toolspy)
- [tools/models_lab_tools.py](#tools--models_lab_toolspy)
- [tools/moviepy_video_tools.py](#tools--moviepy_video_toolspy)
- [tools/multiple_tools.py](#tools--multiple_toolspy)
- [tools/neo4j_tools.py](#tools--neo4j_toolspy)
- [tools/newspaper4k_tools.py](#tools--newspaper4k_toolspy)
- [tools/newspaper_tools.py](#tools--newspaper_toolspy)
- [tools/openbb_tools.py](#tools--openbb_toolspy)
- [tools/opencv_tools.py](#tools--opencv_toolspy)
- [tools/openweather_tools.py](#tools--openweather_toolspy)
- [tools/other/add_tool_after_initialization.py](#tools--other--add_tool_after_initializationpy)
- [tools/other/cache_tool_calls.py](#tools--other--cache_tool_callspy)
- [tools/other/complex_input_types.py](#tools--other--complex_input_typespy)
- [tools/other/custom_tool_manipulate_session_state.py](#tools--other--custom_tool_manipulate_session_statepy)
- [tools/other/human_in_the_loop.py](#tools--other--human_in_the_looppy)
- [tools/other/include_exclude_tools.py](#tools--other--include_exclude_toolspy)
- [tools/other/include_exclude_tools_custom_toolkit.py](#tools--other--include_exclude_tools_custom_toolkitpy)
- [tools/other/retry_tool_call.py](#tools--other--retry_tool_callpy)
- [tools/other/retry_tool_call_from_post_hook.py](#tools--other--retry_tool_call_from_post_hookpy)
- [tools/other/stop_after_tool_call.py](#tools--other--stop_after_tool_callpy)
- [tools/other/stop_agent_exception.py](#tools--other--stop_agent_exceptionpy)
- [tools/oxylabs_tools.py](#tools--oxylabs_toolspy)
- [tools/pandas_tools.py](#tools--pandas_toolspy)
- [tools/postgres_tools.py](#tools--postgres_toolspy)
- [tools/pubmed_tools.py](#tools--pubmed_toolspy)
- [tools/python_function.py](#tools--python_functionpy)
- [tools/python_function_as_tool.py](#tools--python_function_as_toolpy)
- [tools/python_tools.py](#tools--python_toolspy)
- [tools/reddit_tools.py](#tools--reddit_toolspy)
- [tools/replicate_tools.py](#tools--replicate_toolspy)
- [tools/resend_tools.py](#tools--resend_toolspy)
- [tools/scrapegraph_tools.py](#tools--scrapegraph_toolspy)
- [tools/searxng_tools.py](#tools--searxng_toolspy)
- [tools/serpapi_tools.py](#tools--serpapi_toolspy)
- [tools/serper_tools.py](#tools--serper_toolspy)
- [tools/shell_tools.py](#tools--shell_toolspy)
- [tools/slack_tools.py](#tools--slack_toolspy)
- [tools/sleep_tools.py](#tools--sleep_toolspy)
- [tools/spider_tools.py](#tools--spider_toolspy)
- [tools/sql_tools.py](#tools--sql_toolspy)
- [tools/tavily_tools.py](#tools--tavily_toolspy)
- [tools/telegram_tools.py](#tools--telegram_toolspy)
- [tools/todoist_tools.py](#tools--todoist_toolspy)
- [tools/tool_calls_accesing_agent.py](#tools--tool_calls_accesing_agentpy)
- [tools/tool_decorator/async_tool_decorator.py](#tools--tool_decorator--async_tool_decoratorpy)
- [tools/tool_decorator/cache_tool_calls.py](#tools--tool_decorator--cache_tool_callspy)
- [tools/tool_decorator/stop_after_tool_call.py](#tools--tool_decorator--stop_after_tool_callpy)
- [tools/tool_decorator/tool_decorator.py](#tools--tool_decorator--tool_decoratorpy)
- [tools/tool_decorator/tool_decorator_async.py](#tools--tool_decorator--tool_decorator_asyncpy)
- [tools/tool_decorator/tool_decorator_with_hook.py](#tools--tool_decorator--tool_decorator_with_hookpy)
- [tools/tool_decorator/tool_decorator_with_instructions.py](#tools--tool_decorator--tool_decorator_with_instructionspy)
- [tools/tool_hooks/async_pre_and_post_hooks.py](#tools--tool_hooks--async_pre_and_post_hookspy)
- [tools/tool_hooks/pre_and_post_hooks.py](#tools--tool_hooks--pre_and_post_hookspy)
- [tools/tool_hooks/tool_hook.py](#tools--tool_hooks--tool_hookpy)
- [tools/tool_hooks/tool_hook_async.py](#tools--tool_hooks--tool_hook_asyncpy)
- [tools/tool_hooks/tool_hook_in_toolkit.py](#tools--tool_hooks--tool_hook_in_toolkitpy)
- [tools/tool_hooks/tool_hook_in_toolkit_async.py](#tools--tool_hooks--tool_hook_in_toolkit_asyncpy)
- [tools/tool_hooks/tool_hook_in_toolkit_with_state.py](#tools--tool_hooks--tool_hook_in_toolkit_with_statepy)
- [tools/tool_hooks/tool_hook_in_toolkit_with_state_nested.py](#tools--tool_hooks--tool_hook_in_toolkit_with_state_nestedpy)
- [tools/tool_hooks/tool_hooks_in_toolkit_nested.py](#tools--tool_hooks--tool_hooks_in_toolkit_nestedpy)
- [tools/tool_hooks/tool_hooks_in_toolkit_nested_async.py](#tools--tool_hooks--tool_hooks_in_toolkit_nested_asyncpy)
- [tools/trafilatura_tools.py](#tools--trafilatura_toolspy)
- [tools/trello_tools.py](#tools--trello_toolspy)
- [tools/twilio_tools.py](#tools--twilio_toolspy)
- [tools/valyu_tools.py](#tools--valyu_toolspy)
- [tools/visualization_tools.py](#tools--visualization_toolspy)
- [tools/web_tools.py](#tools--web_toolspy)
- [tools/webbrowser_tools.py](#tools--webbrowser_toolspy)
- [tools/webex_tools.py](#tools--webex_toolspy)
- [tools/website_tools.py](#tools--website_toolspy)
- [tools/website_tools_knowledge.py](#tools--website_tools_knowledgepy)
- [tools/whatsapp_tools.py](#tools--whatsapp_toolspy)
- [tools/wikipedia_tools.py](#tools--wikipedia_toolspy)
- [tools/x_tools.py](#tools--x_toolspy)
- [tools/yfinance_tools.py](#tools--yfinance_toolspy)
- [tools/youtube_tools.py](#tools--youtube_toolspy)
- [tools/zendesk_tools.py](#tools--zendesk_toolspy)
- [tools/zep_async_tools.py](#tools--zep_async_toolspy)
- [tools/zep_tools.py](#tools--zep_toolspy)
- [tools/zoom_tools.py](#tools--zoom_toolspy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_functions_and_agents.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_functions_and_agentspy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_functions_and_agents_stream.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_functions_and_agents_streampy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_steps.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_stepspy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_steps_stream.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_steps_streampy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/async/workflow_using_steps.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--async--workflow_using_stepspy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_functions_and_agents.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_functions_and_agentspy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_functions_and_agents_stream.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_functions_and_agents_streampy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_steps.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_stepspy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_steps_stream.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_steps_streampy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_using_steps.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_using_stepspy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_using_steps_nested.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_using_steps_nestedpy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_with_file_input.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_with_file_inputpy)
- [workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_with_session_metrics.py](#workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_with_session_metricspy)
- [workflows/_01_basic_workflows/_02_step_with_function/async/step_with_function_additional_data.py](#workflows--_01_basic_workflows--_02_step_with_function--async--step_with_function_additional_datapy)
- [workflows/_01_basic_workflows/_02_step_with_function/async/step_with_function_stream.py](#workflows--_01_basic_workflows--_02_step_with_function--async--step_with_function_streampy)
- [workflows/_01_basic_workflows/_02_step_with_function/sync/step_with_function.py](#workflows--_01_basic_workflows--_02_step_with_function--sync--step_with_functionpy)
- [workflows/_01_basic_workflows/_02_step_with_function/sync/step_with_function_additional_data.py](#workflows--_01_basic_workflows--_02_step_with_function--sync--step_with_function_additional_datapy)
- [workflows/_01_basic_workflows/_02_step_with_function/sync/step_with_function_stream.py](#workflows--_01_basic_workflows--_02_step_with_function--sync--step_with_function_streampy)
- [workflows/_01_basic_workflows/_03_function_instead_of_steps/async/function_instead_of_steps.py](#workflows--_01_basic_workflows--_03_function_instead_of_steps--async--function_instead_of_stepspy)
- [workflows/_01_basic_workflows/_03_function_instead_of_steps/async/function_instead_of_steps_stream.py](#workflows--_01_basic_workflows--_03_function_instead_of_steps--async--function_instead_of_steps_streampy)
- [workflows/_01_basic_workflows/_03_function_instead_of_steps/sync/function_instead_of_steps.py](#workflows--_01_basic_workflows--_03_function_instead_of_steps--sync--function_instead_of_stepspy)
- [workflows/_01_basic_workflows/_03_function_instead_of_steps/sync/function_instead_of_steps_stream.py](#workflows--_01_basic_workflows--_03_function_instead_of_steps--sync--function_instead_of_steps_streampy)
- [workflows/_02_workflows_conditional_execution/async/condition_and_parallel_steps.py](#workflows--_02_workflows_conditional_execution--async--condition_and_parallel_stepspy)
- [workflows/_02_workflows_conditional_execution/async/condition_and_parallel_steps_stream.py](#workflows--_02_workflows_conditional_execution--async--condition_and_parallel_steps_streampy)
- [workflows/_02_workflows_conditional_execution/async/condition_steps_workflow_stream.py](#workflows--_02_workflows_conditional_execution--async--condition_steps_workflow_streampy)
- [workflows/_02_workflows_conditional_execution/async/condition_with_list_of_steps.py](#workflows--_02_workflows_conditional_execution--async--condition_with_list_of_stepspy)
- [workflows/_02_workflows_conditional_execution/sync/condition_and_parallel_steps.py](#workflows--_02_workflows_conditional_execution--sync--condition_and_parallel_stepspy)
- [workflows/_02_workflows_conditional_execution/sync/condition_and_parallel_steps_stream.py](#workflows--_02_workflows_conditional_execution--sync--condition_and_parallel_steps_streampy)
- [workflows/_02_workflows_conditional_execution/sync/condition_steps_workflow_stream.py](#workflows--_02_workflows_conditional_execution--sync--condition_steps_workflow_streampy)
- [workflows/_02_workflows_conditional_execution/sync/condition_with_list_of_steps.py](#workflows--_02_workflows_conditional_execution--sync--condition_with_list_of_stepspy)
- [workflows/_03_workflows_loop_execution/async/loop_steps_workflow.py](#workflows--_03_workflows_loop_execution--async--loop_steps_workflowpy)
- [workflows/_03_workflows_loop_execution/async/loop_steps_workflow_stream.py](#workflows--_03_workflows_loop_execution--async--loop_steps_workflow_streampy)
- [workflows/_03_workflows_loop_execution/async/loop_with_parallel_steps_stream.py](#workflows--_03_workflows_loop_execution--async--loop_with_parallel_steps_streampy)
- [workflows/_03_workflows_loop_execution/sync/loop_steps_workflow.py](#workflows--_03_workflows_loop_execution--sync--loop_steps_workflowpy)
- [workflows/_03_workflows_loop_execution/sync/loop_steps_workflow_stream.py](#workflows--_03_workflows_loop_execution--sync--loop_steps_workflow_streampy)
- [workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps.py](#workflows--_03_workflows_loop_execution--sync--loop_with_parallel_stepspy)
- [workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps_stream.py](#workflows--_03_workflows_loop_execution--sync--loop_with_parallel_steps_streampy)
- [workflows/_04_workflows_parallel_execution/async/parallel_and_condition_steps_stream.py](#workflows--_04_workflows_parallel_execution--async--parallel_and_condition_steps_streampy)
- [workflows/_04_workflows_parallel_execution/async/parallel_steps_workflow.py](#workflows--_04_workflows_parallel_execution--async--parallel_steps_workflowpy)
- [workflows/_04_workflows_parallel_execution/async/parallel_steps_workflow_stream.py](#workflows--_04_workflows_parallel_execution--async--parallel_steps_workflow_streampy)
- [workflows/_04_workflows_parallel_execution/sync/parallel_and_condition_steps_stream.py](#workflows--_04_workflows_parallel_execution--sync--parallel_and_condition_steps_streampy)
- [workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow.py](#workflows--_04_workflows_parallel_execution--sync--parallel_steps_workflowpy)
- [workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow_stream.py](#workflows--_04_workflows_parallel_execution--sync--parallel_steps_workflow_streampy)
- [workflows/_05_workflows_conditional_branching/async/router_steps_workflow.py](#workflows--_05_workflows_conditional_branching--async--router_steps_workflowpy)
- [workflows/_05_workflows_conditional_branching/async/router_steps_workflow_stream.py](#workflows--_05_workflows_conditional_branching--async--router_steps_workflow_streampy)
- [workflows/_05_workflows_conditional_branching/async/router_with_loop_steps.py](#workflows--_05_workflows_conditional_branching--async--router_with_loop_stepspy)
- [workflows/_05_workflows_conditional_branching/async/selector_for_image_video_generation_pipeline.py](#workflows--_05_workflows_conditional_branching--async--selector_for_image_video_generation_pipelinepy)
- [workflows/_05_workflows_conditional_branching/sync/router_steps_workflow.py](#workflows--_05_workflows_conditional_branching--sync--router_steps_workflowpy)
- [workflows/_05_workflows_conditional_branching/sync/router_steps_workflow_stream.py](#workflows--_05_workflows_conditional_branching--sync--router_steps_workflow_streampy)
- [workflows/_05_workflows_conditional_branching/sync/router_with_loop_steps.py](#workflows--_05_workflows_conditional_branching--sync--router_with_loop_stepspy)
- [workflows/_05_workflows_conditional_branching/sync/selector_for_image_video_generation_pipelines.py](#workflows--_05_workflows_conditional_branching--sync--selector_for_image_video_generation_pipelinespy)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/pydantic_model_as_input.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--pydantic_model_as_inputpy)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_agent.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_agentpy)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_agent_stream.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_agent_streampy)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_functionpy)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function_1.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_function_1py)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function_2.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_function_2py)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_team.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_teampy)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_team_stream.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_team_streampy)
- [workflows/_06_advanced_concepts/_01_structured_io_at_each_level/workflow_with_input_schema.py](#workflows--_06_advanced_concepts--_01_structured_io_at_each_level--workflow_with_input_schemapy)
- [workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_agents.py](#workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_agentspy)
- [workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_condition.py](#workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_conditionpy)
- [workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_loop.py](#workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_looppy)
- [workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_parallel.py](#workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_parallelpy)
- [workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_router.py](#workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_routerpy)
- [workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_step.py](#workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_steppy)
- [workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_steps.py](#workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_stepspy)
- [workflows/_06_advanced_concepts/_03_access_previous_step_outputs/access_multiple_previous_step_output_stream_2.py](#workflows--_06_advanced_concepts--_03_access_previous_step_outputs--access_multiple_previous_step_output_stream_2py)
- [workflows/_06_advanced_concepts/_03_access_previous_step_outputs/access_multiple_previous_steps_output_stream.py](#workflows--_06_advanced_concepts--_03_access_previous_step_outputs--access_multiple_previous_steps_output_streampy)
- [workflows/_06_advanced_concepts/_03_access_previous_step_outputs/access_multiple_previous_steps_output_stream_1.py](#workflows--_06_advanced_concepts--_03_access_previous_step_outputs--access_multiple_previous_steps_output_stream_1py)
- [workflows/_06_advanced_concepts/_04_shared_session_state/access_session_state_in_custom_function_step_stream.py](#workflows--_06_advanced_concepts--_04_shared_session_state--access_session_state_in_custom_function_step_streampy)
- [workflows/_06_advanced_concepts/_04_shared_session_state/access_session_state_in_custom_python_function_step.py](#workflows--_06_advanced_concepts--_04_shared_session_state--access_session_state_in_custom_python_function_steppy)
- [workflows/_06_advanced_concepts/_04_shared_session_state/shared_session_state_with_agent.py](#workflows--_06_advanced_concepts--_04_shared_session_state--shared_session_state_with_agentpy)
- [workflows/_06_advanced_concepts/_04_shared_session_state/shared_session_state_with_team.py](#workflows--_06_advanced_concepts--_04_shared_session_state--shared_session_state_with_teampy)
- [workflows/_06_advanced_concepts/_05_background_execution/background_execution_poll.py](#workflows--_06_advanced_concepts--_05_background_execution--background_execution_pollpy)
- [workflows/_06_advanced_concepts/_05_background_execution/background_execution_using_websocket/websocket_client.py](#workflows--_06_advanced_concepts--_05_background_execution--background_execution_using_websocket--websocket_clientpy)
- [workflows/_06_advanced_concepts/_05_background_execution/background_execution_using_websocket/websocket_server.py](#workflows--_06_advanced_concepts--_05_background_execution--background_execution_using_websocket--websocket_serverpy)
- [workflows/_06_advanced_concepts/_06_other/rename_workflow_session.py](#workflows--_06_advanced_concepts--_06_other--rename_workflow_sessionpy)
- [workflows/_06_advanced_concepts/_06_other/store_events_and_events_to_skip_in_a_workflow.py](#workflows--_06_advanced_concepts--_06_other--store_events_and_events_to_skip_in_a_workflowpy)
- [workflows/_06_advanced_concepts/_06_other/workflow_cancel_a_run.py](#workflows--_06_advanced_concepts--_06_other--workflow_cancel_a_runpy)
- [workflows/_06_advanced_concepts/_06_other/workflow_metrics_on_run_response.py](#workflows--_06_advanced_concepts--_06_other--workflow_metrics_on_run_responsepy)
- [workflows/_06_advanced_concepts/_06_other/workflow_tools.py](#workflows--_06_advanced_concepts--_06_other--workflow_toolspy)
- [workflows/_06_advanced_concepts/_06_other/workflow_with_image_input.py](#workflows--_06_advanced_concepts--_06_other--workflow_with_image_inputpy)

---

<a name="agent_os--advanced--_agentspy"></a>

### `agent_os/advanced/_agents.py`

```python
from datetime import datetime
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.anthropic.claude import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.file import FileTools
from agno.vectordb.pgvector.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

AGENT_DESCRIPTION = dedent("""\
    You are Sage, a cutting-edge Answer Engine built to deliver precise, context-rich, and engaging responses.
    You have the following tools at your disposal:
      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.
      - ExaTools for structured, in-depth analysis.
      - FileTools for saving the output upon user confirmation.

    Your response should always be clear, concise, and detailed. Blend direct answers with extended analysis,
    supporting evidence, illustrative examples, and clarifications on common misconceptions. Engage the user
    with follow-up questions, such as asking if they'd like to save the answer.

    <critical>
    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.
    - You must provide sources, whenever you provide a data point or a statistic.
    - When the user asks a follow-up question, you can use the previous answer as context.
    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.
    </critical>\
""")

AGENT_INSTRUCTIONS = dedent("""\
    Here's how you should answer the user's question:

    1. Gather Relevant Information
      - First, carefully analyze the query to identify the intent of the user.
      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.
      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.
      - Combine the insights from both tools to craft a comprehensive and balanced answer.
      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.
      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.

    2. Construct Your Response
      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query.
      - **Then expand** the answer by including:
           A clear explanation with context and definitions.
           Supporting evidence such as statistics, real-world examples, and data points.
           Clarifications that address common misconceptions.
      - Expand the answer only if the query requires more detail. Simple questions like: "What is the weather in Tokyo?" or "What is the capital of France?" don't need an in-depth analysis.
      - Ensure the response is structured so that it provides quick answers as well as in-depth analysis for further exploration.

    3. Enhance Engagement
      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)"
      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.

    4. Final Quality Check & Presentation 
      - Review your response to ensure clarity, depth, and engagement.
      - Strive to be both informative for quick queries and thorough for detailed exploration.

    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\
""")

EXPECTED_OUTPUT_TEMPLATE = dedent("""\
    {# If this is the first message, include the question title #}
    {% if this is the first message %}
    ## {An engaging title for this report. Keep it short.}
    {% endif %}

    **{A clear and direct response that answers the question.}**

    {# If the query requires more detail, include the sections below #}
    {% if detailed_response %}

    ### {Secion title}
    {Add detailed analysis & explanation in this section}
    {A comprehensive breakdown covering key insights, context, and definitions.}

    ### {Section title}
    {Add evidence & support in this section}
    {Add relevant data points and statistics in this section}
    {Add links or names of reputable sources supporting the answer in this section}

    ### {Section title}
    {Add real-world examples or case studies that help illustrate the key points in this section}

    ### {Section title}
    {Add clarifications addressing any common misunderstandings related to the topic in this section}

    ### {Section title}
    {Add further details, implications, or suggestions for ongoing exploration in this section}
    {% endif %}

    {Add any more sections you think are relevant, covering all the aspects of the query}

    ### Sources
    - [1] {Source 1 url}
    - [2] {Source 2 url}
    - [3] {Source 3 url}
    - {any more sources you think are relevant}

    Generated by Sage on: {current_time}

    Stay curious and keep exploring \
    """)

sage = Agent(
    name="Sage",
    id="sage",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=PostgresDb(db_url=db_url, session_table="sage_sessions"),
    tools=[
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"),
            type="keyword",
            num_results=10,
        ),
        DuckDuckGoTools(
            timeout=20,
            fixed_max_results=5,
        ),
        FileTools(base_dir=Path(__file__).parent),
    ],
    # Allow Sage to read both chat history and tool call history for better context.
    read_chat_history=True,
    # Append previous conversation responses into the new messages for context.
    add_history_to_context=True,
    num_history_runs=5,
    add_datetime_to_context=True,
    add_name_to_context=True,
    enable_user_memories=True,
    description=AGENT_DESCRIPTION,
    instructions=AGENT_INSTRUCTIONS,
    expected_output=EXPECTED_OUTPUT_TEMPLATE,
    markdown=True,
)

knowledge = Knowledge(
    name="Agno Docs",
    contents_db=PostgresDb(db_url=db_url, knowledge_table="agno-assist-knowledge"),
    vector_db=PgVector(
        db_url=db_url,
        table_name="agno_assist_knowledge",
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

agno_assist = Agent(
    name="Agno Assist",
    model=Claude(id="claude-3-7-sonnet-latest"),
    description="You help answer questions about the Agno framework.",
    instructions="Search your knowledge before answering the question.",
    knowledge=knowledge,
    db=PostgresDb(db_url=db_url, session_table="agno_assist_sessions"),
    add_history_to_context=True,
    add_datetime_to_context=True,
    markdown=True,
)
```

---

<a name="agent_os--advanced--_teamspy"></a>

### `agent_os/advanced/_teams.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=PostgresDb(db_url=db_url, session_table="web_agent_sessions"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=PostgresDb(db_url=db_url, session_table="finance_agent_sessions"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=["Use tables to display data"],
)

finance_reasoning_team = Team(
    name="Reasoning Team Leader",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=PostgresDb(db_url=db_url, session_table="finance_reasoning_team_sessions"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[ReasoningTools(add_instructions=True)],
    markdown=True,
    show_members_responses=True,
)
```

---

<a name="agent_os--advanced--demopy"></a>

### `agent_os/advanced/demo.py`

```python
"""
AgentOS Demo

Set the OS_SECURITY_KEY environment variable to your OS security key to enable authentication.
"""

from _agents import agno_assist, sage  # type: ignore[import-not-found]
from _teams import finance_reasoning_team  # type: ignore[import-not-found]
from agno.db.postgres.postgres import PostgresDb  # noqa: F401
from agno.eval.accuracy import AccuracyEval
from agno.models.anthropic.claude import Claude
from agno.os import AgentOS

# Database connection
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create the AgentOS
agent_os = AgentOS(
    os_id="agentos-demo",
    agents=[sage, agno_assist],
    teams=[finance_reasoning_team],
)
app = agent_os.get_app()

# Uncomment to create a memory
# agno_agent.print_response("I love astronomy, specifically the science behind nebulae")


if __name__ == "__main__":
    # Setting up and running an eval for our agent
    evaluation = AccuracyEval(
        db=agno_assist.db,
        name="Calculator Evaluation",
        model=Claude(id="claude-3-7-sonnet-latest"),
        agent=agno_assist,
        input="Should I post my password online? Answer yes or no.",
        expected_output="No",
        num_iterations=1,
    )

    # evaluation.run(print_results=False)

    # Setup knowledge
    # agno_assist.knowledge.add_content(name="Agno Docs", url="https://docs.agno.com/llms-full.txt", skip_if_exists=True)

    # Simple run to generate and record a session
    agent_os.serve(app="demo:app", reload=True)
```

---

<a name="agent_os--advanced--mcp_demopy"></a>

### `agent_os/advanced/mcp_demo.py`

```python
"""This example shows how to run an Agent using our MCP integration in the Agno OS.

For this example to run you need:
- Create a GitHub personal access token following these steps:
    - https://github.com/modelcontextprotocol/servers/tree/main/src/github#setup
- Set the GITHUB_TOKEN environment variable: `export GITHUB_TOKEN=<Your GitHub access token>`
- Run: `pip install agno mcp openai` to install the dependencies
"""

from contextlib import asynccontextmanager
from os import getenv
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.mcp import MCPTools
from fastapi import FastAPI
from mcp import StdioServerParameters

agent_storage_file: str = "tmp/agents.db"


# MCP server parameters setup
github_token = getenv("GITHUB_TOKEN") or getenv("GITHUB_ACCESS_TOKEN")
if not github_token:
    raise ValueError("GITHUB_TOKEN environment variable is required")

server_params = StdioServerParameters(
    command="npx",
    args=["-y", "@modelcontextprotocol/server-github"],
)


# This is required to start the MCP connection correctly in the FastAPI lifecycle
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage MCP connection lifecycle inside a FastAPI app"""
    global mcp_tools

    # Startuplogic: connect to our MCP server
    mcp_tools = MCPTools(server_params=server_params)
    await mcp_tools.connect()

    # Add the MCP tools to our Agent
    agent.tools = [mcp_tools]

    yield

    # Shutdown: Close MCP connection
    await mcp_tools.close()


agent = Agent(
    name="MCP GitHub Agent",
    instructions=dedent("""\
        You are a GitHub assistant. Help users explore repositories and their activity.

        - Use headings to organize your responses
        - Be concise and focus on relevant information\
    """),
    model=OpenAIChat(id="gpt-4o"),
    db=SqliteDb(db_file=agent_storage_file),
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

# Setup our AgentOS app
agent_os = AgentOS(
    description="Example OS setup",
    agents=[agent],
)
app = agent_os.get_app()


if __name__ == "__main__":
    agent_os.serve(app="mcp_demo:app", reload=True)
```

---

<a name="agent_os--advanced--multiple_knowledge_basespy"></a>

### `agent_os/advanced/multiple_knowledge_bases.py`

```python
from agno.agent import Agent
from agno.db.json import JsonDb
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


vector_db = PgVector(table_name="vectors", db_url=db_url)
secondary_vector_db = PgVector(table_name="more_vectors", db_url=db_url)
contents_db = JsonDb(db_path="./agno_json_data", knowledge_table="main_knowledge")
secondary_contents_db = JsonDb(
    db_path="./agno_json_data_2", knowledge_table="secondary_knowledge"
)

# Create knowledge bases
knowledge_base = Knowledge(
    name="Main Knowledge Base",
    description="A simple knowledge base",
    contents_db=contents_db,
    vector_db=vector_db,
)

main_agent = Agent(
    name="Main Agent",
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    add_datetime_to_context=True,
    markdown=True,
    db=contents_db,
)

agent_os = AgentOS(
    description="Example app for basic agent with knowledge capabilities",
    os_id="knowledge-demo",
    agents=[main_agent],
)
app = agent_os.get_app()

if __name__ == "__main__":
    """ Run your AgentOS:
    Now you can interact with your knowledge base using the API. Examples:
    - http://localhost:8001/knowledge/{id}/documents
    - http://localhost:8001/knowledge/{id}/documents/123
    - http://localhost:8001/knowledge/{id}/documents?agent_id=123
    - http://localhost:8001/knowledge/{id}/documents?limit=10&page=0&sort_by=created_at&sort_order=desc
    """
    agent_os.serve(app="multiple_knowledge_bases:app", reload=True)
```

---

<a name="agent_os--advanced--reasoning_demopy"></a>

### `agent_os/advanced/reasoning_demo.py`

```python
"""Run `pip install openai exa_py ddgs yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api python-docx agno` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.knowledge import KnowledgeTools
from agno.tools.reasoning import ReasoningTools
from agno.vectordb.lancedb import LanceDb, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url)


finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    id="finance-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        DuckDuckGoTools(
            enable_news=True,
        )
    ],
    instructions=["Always use tables to display data"],
    db=db,
    add_history_to_context=True,
    num_history_runs=5,
    add_datetime_to_context=True,
    markdown=True,
)

cot_agent = Agent(
    name="Chain-of-Thought Agent",
    role="Answer basic questions",
    id="cot-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
    reasoning=True,
)

reasoning_model_agent = Agent(
    name="Reasoning Model Agent",
    role="Reasoning about Math",
    id="reasoning-model-agent",
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(id="o3-mini"),
    instructions=["You are a reasoning agent that can reason about math."],
    markdown=True,
    db=db,
)

reasoning_tool_agent = Agent(
    name="Reasoning Tool Agent",
    role="Answer basic questions",
    id="reasoning-tool-agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
    tools=[ReasoningTools()],
)


web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    id="web_agent",
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_context=True,
    db=db,
)

agno_docs = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
    ),
)

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    enable_think=True,
    enable_search=True,
    enable_analyze=True,
    add_few_shot=True,
)
knowledge_agent = Agent(
    id="knowledge_agent",
    name="Knowledge Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[knowledge_tools],
    markdown=True,
    db=db,
)

reasoning_finance_team = Team(
    name="Reasoning Finance Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[
        web_agent,
        finance_agent,
    ],
    # reasoning=True,
    tools=[ReasoningTools(add_instructions=True)],
    # uncomment it to use knowledge tools
    # tools=[knowledge_tools],
    id="reasoning_finance_team",
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    show_members_responses=True,
    add_datetime_to_context=True,
    db=db,
)


# Setup our AgentOS app
agent_os = AgentOS(
    description="Example OS setup",
    agents=[
        finance_agent,
        cot_agent,
        reasoning_model_agent,
        reasoning_tool_agent,
        knowledge_agent,
    ],
    teams=[reasoning_finance_team],
)
app = agent_os.get_app()


if __name__ == "__main__":
    agno_docs.add_content(name="Agno Docs", url="https://www.paulgraham.com/read.html")
    agent_os.serve(app="reasoning_demo:app", reload=True)
```

---

<a name="agent_os--advanced--teams_demopy"></a>

### `agent_os/advanced/teams_demo.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.yfinance import YFinanceTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url)

file_agent = Agent(
    name="File Upload Agent",
    id="file-upload-agent",
    role="Answer questions about the uploaded files",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=db,
    enable_user_memories=True,
    instructions=[
        "You are an AI agent that can analyze files.",
        "You are given a file and you need to answer questions about the file.",
    ],
    markdown=True,
)

video_agent = Agent(
    name="Video Understanding Agent",
    model=Gemini(id="gemini-2.0-flash"),
    id="video-understanding-agent",
    role="Answer questions about video files",
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
    add_datetime_to_context=True,
    markdown=True,
)

audio_agent = Agent(
    name="Audio Understanding Agent",
    id="audio-understanding-agent",
    role="Answer questions about audio files",
    model=OpenAIChat(id="gpt-4o-audio-preview"),
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
    add_datetime_to_context=True,
    markdown=True,
)

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    id="web_agent",
    instructions=[
        "You are an experienced web researcher and news analyst! ",
    ],
    enable_user_memories=True,
    markdown=True,
    db=db,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    id="finance_agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)
    ],
    instructions=[
        "You are a skilled financial analyst with expertise in market data! ",
        "Follow these steps when analyzing financial data:",
        "Start with the latest stock price, trading volume, and daily range",
        "Present detailed analyst recommendations and consensus target prices",
        "Include key metrics: P/E ratio, market cap, 52-week range",
        "Analyze trading patterns and volume trends",
    ],
    enable_user_memories=True,
    markdown=True,
    db=db,
)

simple_agent = Agent(
    name="Simple Agent",
    role="Simple agent",
    id="simple_agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=["You are a simple agent"],
    enable_user_memories=True,
    db=db,
)

research_agent = Agent(
    name="Research Agent",
    role="Research agent",
    id="research_agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=["You are a research agent"],
    tools=[DuckDuckGoTools(), ExaTools()],
    enable_user_memories=True,
    db=db,
)

research_team = Team(
    name="Research Team",
    description="A team of agents that research the web",
    members=[research_agent, simple_agent],
    model=OpenAIChat(id="gpt-4o"),
    id="research_team",
    instructions=[
        "You are the lead researcher of a research team! ",
    ],
    enable_user_memories=True,
    add_datetime_to_context=True,
    markdown=True,
    db=db,
)

multimodal_team = Team(
    name="Multimodal Team",
    description="A team of agents that can handle multiple modalities",
    members=[file_agent, audio_agent, video_agent],
    model=OpenAIChat(id="gpt-4o"),
    determine_input_for_members=False,
    respond_directly=True,
    id="multimodal_team",
    instructions=[
        "You are the lead editor of a prestigious financial news desk! ",
    ],
    enable_user_memories=True,
    db=db,
)
financial_news_team = Team(
    name="Financial News Team",
    description="A team of agents that search the web for financial news and analyze it.",
    members=[
        web_agent,
        finance_agent,
        research_agent,
        file_agent,
        audio_agent,
        video_agent,
    ],
    model=OpenAIChat(id="gpt-4o"),
    respond_directly=True,
    id="financial_news_team",
    instructions=[
        "You are the lead editor of a prestigious financial news desk! ",
        "If you are given a file send it to the file agent.",
        "If you are given an audio file send it to the audio agent.",
        "If you are given a video file send it to the video agent.",
        "Use USD as currency.",
        "If the user is just being conversational, you should respond directly WITHOUT forwarding a task to a member.",
    ],
    add_datetime_to_context=True,
    markdown=True,
    show_members_responses=True,
    db=db,
    enable_user_memories=True,
    expected_output="A good financial news report.",
)


# Setup our AgentOS app
agent_os = AgentOS(
    description="Example OS setup",
    agents=[
        simple_agent,
        web_agent,
        finance_agent,
        research_agent,
    ],
    teams=[research_team, multimodal_team, financial_news_team],
)
app = agent_os.get_app()


if __name__ == "__main__":
    agent_os.serve(app="teams_demo:app", reload=True)
```

---

<a name="agent_os--basicpy"></a>

### `agent_os/basic.py`

```python
"""Minimal example for AgentOS."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Setup the database
db = PostgresDb(id="basic-db", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

# Setup basic agents, teams and workflows
basic_agent = Agent(
    name="Basic Agent",
    db=db,
    enable_session_summaries=True,
    enable_user_memories=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
basic_team = Team(
    id="basic-team",
    name="Basic Team",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[basic_agent],
    enable_user_memories=True,
)
basic_workflow = Workflow(
    id="basic-workflow",
    name="Basic Workflow",
    description="Just a simple workflow",
    db=db,
    steps=[
        Step(
            name="step1",
            description="Just a simple step",
            agent=basic_agent,
        )
    ],
)

# Setup our AgentOS app
agent_os = AgentOS(
    description="Example app for basic agent with playground capabilities",
    agents=[basic_agent],
    teams=[basic_team],
    workflows=[basic_workflow],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="basic:app", reload=True)
```

---

<a name="agent_os--customize--custom_fastapi_apppy"></a>

### `agent_os/customize/custom_fastapi_app.py`

```python
"""
Example AgentOS app with a custom FastAPI app with basic routes.

You can also run this using the FastAPI cli (pip install fastapi["standard"]):
```
fastapi run custom_fastapi_app.py
```
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from fastapi import FastAPI

# Setup the database
db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

web_research_agent = Agent(
    id="web-research-agent",
    name="Web Research Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

# Custom FastAPI app
app: FastAPI = FastAPI(
    title="Custom FastAPI App",
    version="1.0.0",
)


# Add your own routes
@app.post("/customers")
async def get_customers():
    return [
        {
            "id": 1,
            "name": "John Doe",
            "email": "john.doe@example.com",
        },
        {
            "id": 2,
            "name": "Jane Doe",
            "email": "jane.doe@example.com",
        },
    ]


# Setup our AgentOS app by passing your FastAPI app
# Use route_prefix to avoid conflicts with your custom routes
agent_os = AgentOS(
    description="Example app with custom routers",
    agents=[web_research_agent],
    fastapi_app=app,
)

# Alternatively, add all routes from AgentOS app to the current app
# for route in agent_os.get_routes():
#     app.router.routes.append(route)

app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    With this setup:
    - API docs: http://localhost:7777/docs

    """
    agent_os.serve(app="custom_fastapi_app:app", reload=True)
```

---

<a name="agent_os--customize--custom_lifespanpy"></a>

### `agent_os/customize/custom_lifespan.py`

```python
"""
Example AgentOS app where the agent has a custom lifespan.
"""

from contextlib import asynccontextmanager

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.utils.log import log_info

# Setup the database
db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

# Setup basic agents, teams and workflows
agno_support_agent = Agent(
    id="example-agent",
    name="Example Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    markdown=True,
)


@asynccontextmanager
async def lifespan(app):
    log_info("Starting My FastAPI App")
    yield
    log_info("Stopping My FastAPI App")


agent_os = AgentOS(
    description="Example app with custom lifespan",
    agents=[agno_support_agent],
    lifespan=lifespan,
)


app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

    You can see test your AgentOS at:
    http://localhost:7777/docs

    """
    # Don't use reload=True here, this can cause issues with the lifespan
    agent_os.serve(app="custom_lifespan:app")
```

---

<a name="agent_os--customize--fastapi_app_with_custom_middlewarepy"></a>

### `agent_os/customize/fastapi_app_with_custom_middleware.py`

```python
"""
AgentOS middleware example - Rate Limiting & Request Logging.

This shows how to add custom middleware to your AgentOS.
"""

import time
from collections import defaultdict, deque
from typing import Dict

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from fastapi import Request, Response
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware


# === Rate Limiting Middleware ===
class RateLimitMiddleware(BaseHTTPMiddleware):
    """
    Rate limiting middleware that limits requests per IP address.
    """

    def __init__(self, app, requests_per_minute: int = 60, window_size: int = 60):
        super().__init__(app)
        self.requests_per_minute = requests_per_minute
        self.window_size = window_size
        # Store request timestamps per IP
        self.request_history: Dict[str, deque] = defaultdict(lambda: deque())

    async def dispatch(self, request: Request, call_next) -> Response:
        # Get client IP
        client_ip = request.client.host if request.client else "unknown"
        current_time = time.time()

        # Clean old requests outside the window
        history = self.request_history[client_ip]
        while history and current_time - history[0] > self.window_size:
            history.popleft()

        # Check if rate limit exceeded
        if len(history) >= self.requests_per_minute:
            return JSONResponse(
                status_code=429,
                content={
                    "detail": f"Rate limit exceeded. Max {self.requests_per_minute} requests per minute."
                },
            )

        # Add current request to history
        history.append(current_time)

        # Add rate limit headers
        response = await call_next(request)
        response.headers["X-RateLimit-Limit"] = str(self.requests_per_minute)
        response.headers["X-RateLimit-Remaining"] = str(
            self.requests_per_minute - len(history)
        )
        response.headers["X-RateLimit-Reset"] = str(
            int(current_time + self.window_size)
        )

        return response


# === Request/Response Logging Middleware ===
class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """
    Request/response logging middleware with timing and basic info.
    """

    def __init__(self, app, log_body: bool = False, log_headers: bool = False):
        super().__init__(app)
        self.log_body = log_body
        self.log_headers = log_headers
        self.request_count = 0

    async def dispatch(self, request: Request, call_next) -> Response:
        self.request_count += 1
        start_time = time.time()

        # Basic request info
        client_ip = request.client.host if request.client else "unknown"
        print(
            f" Request #{self.request_count}: {request.method} {request.url.path} from {client_ip}"
        )

        # Optional: Log headers
        if self.log_headers:
            print(f" Headers: {dict(request.headers)}")

        # Optional: Log request body
        if self.log_body and request.method in ["POST", "PUT", "PATCH"]:
            body = await request.body()
            if body:
                print(f" Body: {body.decode()}")

        # Process request
        response = await call_next(request)

        # Log response info
        duration = time.time() - start_time
        status_emoji = "" if response.status_code < 400 else ""
        print(
            f"{status_emoji} Response: {response.status_code} in {duration * 1000:.1f}ms"
        )

        # Add request count to response header
        response.headers["X-Request-Count"] = str(self.request_count)

        return response


# === Setup database and agent ===
db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

agent = Agent(
    id="demo-agent",
    name="Demo Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent_os = AgentOS(
    description="Essential middleware demo with rate limiting and logging",
    agents=[agent],
)

app = agent_os.get_app()

# Add custom middleware
app.add_middleware(
    RateLimitMiddleware,
    requests_per_minute=10,
    window_size=60,
)

app.add_middleware(
    RequestLoggingMiddleware,
    log_body=False,
    log_headers=False,
)

if __name__ == "__main__":
    """
    Run the essential middleware demo using AgentOS serve method.
    
    Features:
    1. Rate Limiting (10 requests/minute)
    2. Request/Response Logging
    
    Test commands:
    
    1. Basic request:
       curl http://localhost:7777/test
    
    2. Test rate limiting:
       for i in {1..15}; do curl http://localhost:7777/config; done
    
    3. Check rate limit headers:
       curl -v http://localhost:7777/config
    
    Look for:
    - Console logs showing request/response info
    - Rate limit headers: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset
    - Request count header: X-Request-Count
    - 429 errors when rate limit exceeded
    """

    agent_os.serve(
        app="fastapi_app_with_custom_middleware:app",
        host="localhost",
        port=7777,
        reload=True,
    )
```

---

<a name="agent_os--customize--override_routespy"></a>

### `agent_os/customize/override_routes.py`

```python
"""
Example AgentOS app with a custom FastAPI app with conflicting routes.

This example demonstrates the `replace_routes=False` functionality which allows your
custom routes to take precedence over conflicting AgentOS routes.

When `replace_routes=False`:
- Your custom routes (/, /health) will be preserved
- Conflicting AgentOS routes will be skipped
- Non-conflicting AgentOS routes will still be added

When `replace_routes=True` (default):
- AgentOS routes will override your custom routes
- Warnings will be logged about the conflicts
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from fastapi import FastAPI
from starlette.middleware.cors import CORSMiddleware

# Setup the database
db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

web_research_agent = Agent(
    id="web-research-agent",
    name="Web Research Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

# Custom FastAPI app
app: FastAPI = FastAPI(
    title="Custom FastAPI App",
    version="1.0.0",
)

# Add Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Custom landing page (conflicts with AgentOS home route)
@app.get("/")
async def get_custom_home():
    return {
        "message": "Custom FastAPI App",
        "note": "Using replace_routes=True to preserve custom routes",
    }


# Custom health endpoint (conflicts with AgentOS health route)
@app.get("/health")
async def get_custom_health():
    return {"status": "custom_ok", "note": "This is your custom health endpoint"}


# Setup our AgentOS app by passing your FastAPI app
# Use replace_routes=False to preserve your custom routes over AgentOS routes
agent_os = AgentOS(
    description="Example app with route replacement",
    agents=[web_research_agent],
    fastapi_app=app,
    replace_routes=False,  # Skip conflicting AgentOS routes, keep your custom routes
)

app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

    With replace_routes=True:
    - Your custom routes are preserved: http://localhost:7777/ and http://localhost:7777/health
    - AgentOS routes are available at other paths: http://localhost:7777/sessions, etc.
    - Conflicting AgentOS routes (GET / and GET /health) are skipped
    - API docs: http://localhost:7777/docs

    Try changing replace_routes=False to see AgentOS routes override your custom ones.
    """
    agent_os.serve(app="override_routes:app", reload=True)
```

---

<a name="agent_os--dbs--dynamo_demopy"></a>

### `agent_os/dbs/dynamo_demo.py`

```python
"""Example showing how to use AgentOS with a DynamoDB database

Set the following environment variables to connect to your DynamoDb instance:
- AWS_REGION
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY

Or pass those parameters when initializing the DynamoDb instance.

Run `pip install boto3` to install dependencies.
"""

from agno.agent import Agent
from agno.db.dynamo import DynamoDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

# Setup the DynamoDB database
db = DynamoDb()

# Setup a basic agent and a basic team
basic_agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
basic_team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[basic_agent],
    debug_mode=True,
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=basic_agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[basic_agent],
    teams=[basic_team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="dynamo_demo:app", reload=True)
```

---

<a name="agent_os--dbs--firestore_demopy"></a>

### `agent_os/dbs/firestore_demo.py`

```python
"""Example showing how to use AgentOS with a Firestore database"""

from agno.agent import Agent
from agno.db.firestore import FirestoreDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

PROJECT_ID = "agno-os-test"

# Setup the Firestore database
db = FirestoreDb(
    project_id=PROJECT_ID,
    session_collection="sessions",
    eval_collection="eval_runs",
    memory_collection="user_memories",
    metrics_collection="metrics",
    knowledge_collection="knowledge",
)

# Setup a basic agent and a basic team
basic_agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
basic_team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    members=[basic_agent],
    debug_mode=True,
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=basic_agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example app for basic agent with Firestore database capabilities",
    os_id="firestore-app",
    agents=[basic_agent],
    teams=[basic_team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    basic_agent.run("Please remember I really like French food")
    agent_os.serve(app="firestore_demo:app", reload=True)
```

---

<a name="agent_os--dbs--gcs_json_demopy"></a>

### `agent_os/dbs/gcs_json_demo.py`

```python
"""
Example showing how to use AgentOS with JSON files hosted in GCS as database.

GCS JSON Database Setup:
- Uses JSON files stored in Google Cloud Storage as a lightweight database
- Only requires a GCS bucket name - authentication follows the standard GCP patterns:
  * Local development: `gcloud auth application-default login`
  * Production: Set GOOGLE_APPLICATION_CREDENTIALS env var to service account key path
  * GCP instances: Uses instance metadata automatically
- Optional prefix parameter for organizing files (defaults to empty string)
- Automatically creates JSON files in the bucket as needed

Prerequisites:
1. Create a GCS bucket
2. Ensure proper GCS permissions
3. Install google-cloud-storage: `pip install google-cloud-storage`
"""

from agno.agent import Agent
from agno.db.gcs_json import GcsJsonDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

# Setup the GCS JSON database
db = GcsJsonDb(bucket_name="agno_tests")


# Setup a basic agent and a basic team
agent = Agent(
    name="JSON Demo Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

team = Team(
    id="basic-team",
    name="JSON Demo Team",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[agent],
    debug_mode=True,
)

# Evaluation example
evaluation = AccuracyEval(
    db=db,
    name="JSON Demo Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=agent,
    input="What is 2 + 2?",
    expected_output="4",
    num_iterations=1,
)
# evaluation.run(print_results=True)

# Create the AgentOS instance
agent_os = AgentOS(
    os_id="json-demo-app",
    description="Example app using JSON file database for simple deployments and demos",
    agents=[agent],
    teams=[team],
)

app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="gcs_json_demo:app", reload=True)
```

---

<a name="agent_os--dbs--json_demopy"></a>

### `agent_os/dbs/json_demo.py`

```python
"""Example showing how to use AgentOS with JSON files as database"""

from agno.agent import Agent
from agno.db.json import JsonDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

# Setup the JSON database
db = JsonDb(db_path="./agno_json_data")

# Setup a basic agent and a basic team
agent = Agent(
    name="JSON Demo Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

team = Team(
    id="basic-team",
    name="JSON Demo Team",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[agent],
    debug_mode=True,
)

# Evaluation example
evaluation = AccuracyEval(
    db=db,
    name="JSON Demo Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=agent,
    input="What is 2 + 2?",
    expected_output="4",
    num_iterations=1,
)
# evaluation.run(print_results=True)

# Create the AgentOS instance
agent_os = AgentOS(
    os_id="json-demo-app",
    description="Example app using JSON file database for simple deployments and demos",
    agents=[agent],
    teams=[team],
)

app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="json_demo:app", reload=True)
```

---

<a name="agent_os--dbs--mongo_demopy"></a>

### `agent_os/dbs/mongo_demo.py`

```python
"""Example showing how to use AgentOS with a Mongo database"""

from agno.agent import Agent
from agno.db.mongo import MongoDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

# Setup the Mongo database
db = MongoDb(db_url="mongodb://localhost:27017")

# Setup a basic agent and a basic team
basic_agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
basic_team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[basic_agent],
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=basic_agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[basic_agent],
    teams=[basic_team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="mongo_demo:app", reload=True)
```

---

<a name="agent_os--dbs--neon_demopy"></a>

### `agent_os/dbs/neon_demo.py`

```python
"""Example showing how to use AgentOS with Neon as database"""

from os import getenv

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

NEON_DB_URL = getenv("NEON_DB_URL")

db = PostgresDb(db_url=NEON_DB_URL)

# Setup a basic agent and a basic team
agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    enable_user_memories=True,
    members=[agent],
    debug_mode=True,
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[agent],
    teams=[team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="neon_demo:app", reload=True)
```

---

<a name="agent_os--dbs--postgres_demopy"></a>

### `agent_os/dbs/postgres_demo.py`

```python
"""Example showing how to use AgentOS with Redis as database"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

# Setup the Redis database
db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    session_table="sessions",
    eval_table="eval_runs",
    memory_table="user_memories",
    metrics_table="metrics",
)

# Setup a basic agent and a basic team
agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    enable_user_memories=True,
    members=[agent],
    debug_mode=True,
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[agent],
    teams=[team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="postgres_demo:app", reload=True)
```

---

<a name="agent_os--dbs--redis_demopy"></a>

### `agent_os/dbs/redis_demo.py`

```python
"""Example showing how to use AgentOS with Redis as database"""

from agno.agent import Agent
from agno.db.redis import RedisDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

# Setup the Redis database
db = RedisDb(
    db_url="redis://localhost:6379",
    session_table="sessions_new",
    metrics_table="metrics_new",
)

# Setup a basic agent and a basic team
agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[agent],
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[agent],
    teams=[team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="redis_demo:app", reload=True)
```

---

<a name="agent_os--dbs--singlestore_demopy"></a>

### `agent_os/dbs/singlestore_demo.py`

```python
"""Example showing how to use AgentOS with SingleStore as database"""

from agno.agent import Agent
from agno.db.singlestore import SingleStoreDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

SINGLE_STORE_DB_URL = "mysql+pymysql://root:ai@localhost:3306/ai"

# Setup the SingleStore database
db = SingleStoreDb(
    db_url=SINGLE_STORE_DB_URL,
    session_table="sessions",
    eval_table="eval_runs",
    memory_table="user_memories",
    metrics_table="metrics",
)

# Setup a basic agent and a basic team
agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[agent],
    debug_mode=True,
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[agent],
    teams=[team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent.run("Remember my favorite color is dark green")
    agent_os.serve(app="singlestore_demo:app", reload=True)
```

---

<a name="agent_os--dbs--sqlite_demopy"></a>

### `agent_os/dbs/sqlite_demo.py`

```python
"""Example showing how to use AgentOS with a SQLite database"""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

# Setup the SQLite database
db = SqliteDb(
    db_file="agno.db",
    session_table="sessions",
    eval_table="eval_runs",
    memory_table="user_memories",
    metrics_table="metrics",
)


# Setup a basic agent and a basic team
basic_agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
team_agent = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[basic_agent],
    debug_mode=True,
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=basic_agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[basic_agent],
    teams=[team_agent],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="sqlite_demo:app", reload=True)
```

---

<a name="agent_os--dbs--supabase_demopy"></a>

### `agent_os/dbs/supabase_demo.py`

```python
from os import getenv

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team

SUPABASE_PROJECT = getenv("SUPABASE_PROJECT")
SUPABASE_PASSWORD = getenv("SUPABASE_PASSWORD")

SUPABASE_DB_URL = (
    f"postgresql://postgres:{SUPABASE_PASSWORD}@db.{SUPABASE_PROJECT}:5432/postgres"
)

# Setup the Redis database
db = PostgresDb(db_url=SUPABASE_DB_URL)

# Setup a basic agent and a basic team
agent = Agent(
    name="Basic Agent",
    id="basic-agent",
    enable_user_memories=True,
    enable_session_summaries=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
team = Team(
    id="basic-team",
    name="Team Agent",
    model=OpenAIChat(id="gpt-4o"),
    enable_user_memories=True,
    members=[agent],
    debug_mode=True,
)

# Evals
evaluation = AccuracyEval(
    db=db,
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    agent=agent,
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
)
# evaluation.run(print_results=True)

agent_os = AgentOS(
    description="Example OS setup",
    agents=[agent],
    teams=[team],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent.run("What is the weather in Tokyo?")
    agent_os.serve(app="supabase_demo:app", reload=True)
```

---

<a name="agent_os--demopy"></a>

### `agent_os/demo.py`

```python
"""
AgentOS Demo

Set the OS_SECURITY_KEY environment variable to your OS security key to enable authentication.

Prerequisites:
pip install -U fastapi uvicorn sqlalchemy pgvector psycopg openai ddgs yfinance
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.mcp import MCPTools
from agno.vectordb.pgvector import PgVector

# Database connection
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create Postgres-backed memory store
db = PostgresDb(db_url=db_url)

# Create Postgres-backed vector store
vector_db = PgVector(
    db_url=db_url,
    table_name="agno_docs",
)
knowledge = Knowledge(
    name="Agno Docs",
    contents_db=db,
    vector_db=vector_db,
)

# Create your agents
agno_agent = Agent(
    name="Agno Agent",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")],
    db=db,
    enable_user_memories=True,
    knowledge=knowledge,
    markdown=True,
)

simple_agent = Agent(
    name="Simple Agent",
    role="Simple agent",
    id="simple_agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=["You are a simple agent"],
    db=db,
    enable_user_memories=True,
)

research_agent = Agent(
    name="Research Agent",
    role="Research agent",
    id="research_agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=["You are a research agent"],
    tools=[DuckDuckGoTools()],
    db=db,
    enable_user_memories=True,
)

# Create a team
research_team = Team(
    name="Research Team",
    description="A team of agents that research the web",
    members=[research_agent, simple_agent],
    model=OpenAIChat(id="gpt-5-mini"),
    id="research_team",
    instructions=[
        "You are the lead researcher of a research team! ",
    ],
    db=db,
    enable_user_memories=True,
    add_datetime_to_context=True,
    markdown=True,
)

# Create the AgentOS
agent_os = AgentOS(
    os_id="agentos-demo",
    agents=[agno_agent],
    teams=[research_team],
)
app = agent_os.get_app()


if __name__ == "__main__":
    agent_os.serve(app="demo:app", port=7777)
```

---

<a name="agent_os--evals_demopy"></a>

### `agent_os/evals_demo.py`

```python
"""Simple example creating a session and using the AgentOS with a SessionApp to expose it"""

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team
from agno.tools.calculator import CalculatorTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

# Setup the agent
basic_agent = Agent(
    id="basic-agent",
    name="Calculator Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    markdown=True,
    instructions="You are an assistant that can answer arithmetic questions. Always use the Calculator tools you have.",
    tools=[CalculatorTools()],
)

basic_team = Team(
    name="Basic Team",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[basic_agent],
)

# Setting up and running an eval for our agent
evaluation = AccuracyEval(
    db=db,  # Pass the database to the evaluation. Results will be stored in the database.
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-4o"),
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
    # Agent or team to evaluate:
    agent=basic_agent,
    # team=basic_team,
)
# evaluation.run(print_results=True)

# Setup the Agno API App
agent_os = AgentOS(
    description="Example app for basic agent with eval capabilities",
    os_id="eval-demo",
    agents=[basic_agent],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """ Run your AgentOS:
    Now you can interact with your eval runs using the API. Examples:
    - http://localhost:8001/eval/{index}/eval-runs
    - http://localhost:8001/eval/{index}/eval-runs/123
    - http://localhost:8001/eval/{index}/eval-runs?agent_id=123
    - http://localhost:8001/eval/{index}/eval-runs?limit=10&page=0&sort_by=created_at&sort_order=desc
    - http://localhost:8001/eval/{index}/eval-runs/accuracy
    - http://localhost:8001/eval/{index}/eval-runs/performance
    - http://localhost:8001/eval/{index}/eval-runs/reliability
    """
    agent_os.serve(app="evals_demo:app", reload=True)
```

---

<a name="agent_os--interfaces--agui--agent_with_toolspy"></a>

### `agent_os/interfaces/agui/agent_with_tools.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.interfaces.agui import AGUI
from agno.tools import tool
from agno.tools.duckduckgo import DuckDuckGoTools


# Frontend Tools
@tool(external_execution=True)
def generate_haiku(
    english: List[str], japanese: List[str], image_names: List[str]
) -> str:
    """Generate a haiku in Japanese and English and display it in the frontend."""
    return "Haiku generated and displayed in frontend"


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        DuckDuckGoTools(),
        generate_haiku,
    ],
    description="You are a helpful AI assistant with both backend and frontend capabilities. You can search the web, create beautiful haikus, modify the UI, ask for user confirmations, and create visualizations.",
    instructions="""
    You are a versatile AI assistant with the following capabilities:

    **Backend Tools (executed on server):**
    - Web search using DuckDuckGo for finding current information

    **Frontend Tools (executed in browser):**
    - Generate beautiful haikus with English/Japanese text and images
    - Change the chat background color or style for better user experience
    - Ask users for confirmation before important actions
    - Create charts and visualizations from data

    **When to use frontend tools:**
    - Use generate_haiku when users ask for poems or creative content

    Always be helpful, creative, and use the most appropriate tool for each request!
    """,
    add_datetime_to_context=True,
    add_history_to_context=True,
    add_location_to_context=True,
    timezone_identifier="Etc/UTC",
    markdown=True,
    debug_mode=True,
)


# Setup your AgentOS app
agent_os = AgentOS(
    agents=[agent],
    interfaces=[AGUI(agent=agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.
    
    You can see the configuration and available apps at:
    http://localhost:9001/config
    
    Use Port 9001 to configure Dojo endpoint.
    """
    agent_os.serve(app="agent_with_tools:app", port=9001, reload=True)
```

---

<a name="agent_os--interfaces--agui--basicpy"></a>

### `agent_os/interfaces/agui/basic.py`

```python
from agno.agent.agent import Agent
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.interfaces.agui import AGUI

chat_agent = Agent(
    name="Assistant",
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a helpful AI assistant.",
    add_datetime_to_context=True,
    markdown=True,
)

# Setup your AgentOS app
agent_os = AgentOS(
    agents=[chat_agent],
    interfaces=[AGUI(agent=chat_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:9001/config

    """
    agent_os.serve(app="basic:app", reload=True, port=9001)
```

---

<a name="agent_os--interfaces--agui--reasoning_agentpy"></a>

### `agent_os/interfaces/agui/reasoning_agent.py`

```python
from agno.agent.agent import Agent
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.interfaces.agui import AGUI
from agno.tools.duckduckgo import DuckDuckGoTools

chat_agent = Agent(
    name="Assistant",
    model=OpenAIChat(id="o4-mini"),
    instructions="You are a helpful AI assistant.",
    add_datetime_to_context=True,
    add_history_to_context=True,
    add_location_to_context=True,
    timezone_identifier="Etc/UTC",
    markdown=True,
    tools=[DuckDuckGoTools()],
)

# Setup your AgentOS app
agent_os = AgentOS(
    agents=[chat_agent],
    interfaces=[AGUI(agent=chat_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:9001/config

    """
    agent_os.serve(app="reasoning_agent:app", reload=True, port=9001)
```

---

<a name="agent_os--interfaces--agui--research_teampy"></a>

### `agent_os/interfaces/agui/research_team.py`

```python
from agno.agent.agent import Agent
from agno.models.openai import OpenAIChat
from agno.os.app import AgentOS
from agno.os.interfaces.agui.agui import AGUI
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

researcher = Agent(
    name="researcher",
    role="Research Assistant",
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a research assistant. Find information and provide detailed analysis.",
    tools=[DuckDuckGoTools()],
    markdown=True,
)

writer = Agent(
    name="writer",
    role="Content Writer",
    model=OpenAIChat(id="o4-mini"),
    instructions="You are a content writer. Create well-structured content based on research.",
    tools=[DuckDuckGoTools()],
    markdown=True,
)

research_team = Team(
    members=[researcher, writer],
    name="research_team",
    instructions="""
    You are a research team that helps users with research and content creation.
    First, use the researcher to gather information, then use the writer to create content.
    """,
    show_members_responses=True,
    get_member_information_tool=True,
    add_member_tools_to_context=True,
    add_history_to_context=True,
)

# Setup our AgentOS app
agent_os = AgentOS(
    teams=[research_team],
    interfaces=[AGUI(team=research_team)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:9001/config
    
    Use Port 9001 for Dojo compatibility.
    """
    agent_os.serve(app="research_team:app", reload=True, port=9001)
```

---

<a name="agent_os--interfaces--agui--structured_outputpy"></a>

### `agent_os/interfaces/agui/structured_output.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.interfaces.agui import AGUI
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


chat_agent = Agent(
    name="Output Schema Agent",
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    markdown=True,
    output_schema=MovieScript,
)


# Setup your AgentOS app
agent_os = AgentOS(
    agents=[chat_agent],
    interfaces=[AGUI(agent=chat_agent)],
)
app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:9001/config

    """

    agent_os.serve(app="structured_output:app", port=9001, reload=True)
```

---

<a name="agent_os--interfaces--slack--agent_with_user_memorypy"></a>

### `agent_os/interfaces/slack/agent_with_user_memory.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.memory.manager import MemoryManager
from agno.models.anthropic.claude import Claude
from agno.os.app import AgentOS
from agno.os.interfaces.slack import Slack
from agno.tools.googlesearch import GoogleSearchTools

agent_db = SqliteDb(session_table="agent_sessions", db_file="tmp/persistent_memory.db")

memory_manager = MemoryManager(
    memory_capture_instructions="""\
                    Collect User's name,
                    Collect Information about user's passion and hobbies,
                    Collect Information about the users likes and dislikes,
                    Collect information about what the user is doing with their life right now
                """,
    model=Claude(id="claude-3-5-sonnet-20241022"),
)


personal_agent = Agent(
    name="Basic Agent",
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[GoogleSearchTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
    db=agent_db,
    memory_manager=memory_manager,
    enable_user_memories=True,
    instructions=dedent("""
        You are a personal AI friend in a slack chat, your purpose is to chat with the user about things and make them feel good.
        First introduce yourself and ask for their name then, ask about themeselves, their hobbies, what they like to do and what they like to talk about.
        Use Google Search tool to find latest infromation about things in the conversations
        You may sometimes recieve messages prepenned with group message when that is the message then reply to whole group instead of treating them as from a single user
                        """),
    debug_mode=True,
)


# Setup our AgentOS app
agent_os = AgentOS(
    agents=[personal_agent],
    interfaces=[Slack(agent=personal_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="agent_with_user_memory:app", reload=True)
```

---

<a name="agent_os--interfaces--slack--basicpy"></a>

### `agent_os/interfaces/slack/basic.py`

```python
from agno.agent import Agent
from agno.db.sqlite.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os.app import AgentOS
from agno.os.interfaces.slack import Slack

agent_db = SqliteDb(session_table="agent_sessions", db_file="tmp/persistent_memory.db")

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=agent_db,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
)

# Setup our AgentOS app
agent_os = AgentOS(
    agents=[basic_agent],
    interfaces=[Slack(agent=basic_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="basic:app", reload=True)
```

---

<a name="agent_os--interfaces--slack--basic_workflowpy"></a>

### `agent_os/interfaces/slack/basic_workflow.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.interfaces.slack import Slack
from agno.tools.googlesearch import GoogleSearchTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents for the workflow
researcher_agent = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    role="Search the web and gather comprehensive research on the given topic",
    instructions=[
        "Search for the most recent and relevant information",
        "Focus on credible sources and key insights",
        "Summarize findings clearly and concisely",
    ],
)

writer_agent = Agent(
    name="Content Writer",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create engaging content based on research findings",
    instructions=[
        "Write in a clear, engaging, and professional tone",
        "Structure content with proper headings and bullet points",
        "Include key insights from the research",
        "Keep content informative yet accessible",
    ],
)

# Create workflow steps
research_step = Step(
    name="Research Step",
    agent=researcher_agent,
)

writing_step = Step(
    name="Writing Step",
    agent=writer_agent,
)

# Create the workflow
content_workflow = Workflow(
    name="Content Creation Workflow",
    description="Research and create content on any topic via Slack",
    db=PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"),
    steps=[research_step, writing_step],
    session_id="slack_workflow_session",
)

# Create AgentOS with Slack interface for the workflow
agent_os = AgentOS(
    workflows=[content_workflow],
    interfaces=[Slack(workflow=content_workflow)],
)

app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="basic_workflow:app", port=8000, reload=True)
```

---

<a name="agent_os--interfaces--slack--reasoning_agentpy"></a>

### `agent_os/interfaces/slack/reasoning_agent.py`

```python
from agno.agent import Agent
from agno.db.sqlite.sqlite import SqliteDb
from agno.models.anthropic.claude import Claude
from agno.os.app import AgentOS
from agno.os.interfaces.slack.slack import Slack
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent_db = SqliteDb(session_table="agent_sessions", db_file="tmp/persistent_memory.db")

reasoning_finance_agent = Agent(
    name="Reasoning Finance Agent",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=agent_db,
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables to display data. When you use thinking tools, keep the thinking brief.",
    add_datetime_to_context=True,
    markdown=True,
)

# Setup our AgentOS app
agent_os = AgentOS(
    agents=[reasoning_finance_agent],
    interfaces=[Slack(agent=reasoning_finance_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="reasoning_agent:app", reload=True)
```

---

<a name="agent_os--interfaces--whatsapp--agent_with_mediapy"></a>

### `agent_os/interfaces/whatsapp/agent_with_media.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.google import Gemini
from agno.os.app import AgentOS
from agno.os.interfaces.whatsapp import Whatsapp

agent_db = SqliteDb(db_file="tmp/persistent_memory.db")
media_agent = Agent(
    name="Media Agent",
    model=Gemini(id="gemini-2.0-flash"),
    db=agent_db,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

# Setup our AgentOS app
agent_os = AgentOS(
    agents=[media_agent],
    interfaces=[Whatsapp(agent=media_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="agent_with_media:app", reload=True)
```

---

<a name="agent_os--interfaces--whatsapp--agent_with_user_memorypy"></a>

### `agent_os/interfaces/whatsapp/agent_with_user_memory.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.memory.manager import MemoryManager
from agno.models.google import Gemini
from agno.os.app import AgentOS
from agno.os.interfaces.whatsapp import Whatsapp
from agno.tools.googlesearch import GoogleSearchTools

agent_db = SqliteDb(db_file="tmp/persistent_memory.db")

memory_manager = MemoryManager(
    memory_capture_instructions="""\
                    Collect User's name,
                    Collect Information about user's passion and hobbies,
                    Collect Information about the users likes and dislikes,
                    Collect information about what the user is doing with their life right now
                """,
    model=Gemini(id="gemini-2.0-flash"),
)


personal_agent = Agent(
    name="Basic Agent",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[GoogleSearchTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
    db=agent_db,
    memory_manager=memory_manager,
    enable_agentic_memory=True,
    instructions=dedent("""
        You are a personal AI friend of the user, your purpose is to chat with the user about things and make them feel good.
        First introduce yourself and ask for their name then, ask about themeselves, their hobbies, what they like to do and what they like to talk about.
        Use Google Search tool to find latest infromation about things in the conversations
                        """),
    debug_mode=True,
)


# Setup our AgentOS app
agent_os = AgentOS(
    agents=[personal_agent],
    interfaces=[Whatsapp(agent=personal_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="agent_with_user_memory:app", reload=True)
```

---

<a name="agent_os--interfaces--whatsapp--basicpy"></a>

### `agent_os/interfaces/whatsapp/basic.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os.app import AgentOS
from agno.os.interfaces.whatsapp import Whatsapp

agent_db = SqliteDb(db_file="tmp/persistent_memory.db")
basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=agent_db,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)


# Setup our AgentOS app
agent_os = AgentOS(
    agents=[basic_agent],
    interfaces=[Whatsapp(agent=basic_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="basic:app", reload=True)
```

---

<a name="agent_os--interfaces--whatsapp--image_generation_modelpy"></a>

### `agent_os/interfaces/whatsapp/image_generation_model.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.google import Gemini
from agno.os.app import AgentOS
from agno.os.interfaces.whatsapp import Whatsapp

agent_db = SqliteDb(db_file="tmp/persistent_memory.db")
image_agent = Agent(
    id="image_generation_model",
    db=agent_db,
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    ),
    debug_mode=True,
)


# Setup our AgentOS app
agent_os = AgentOS(
    agents=[image_agent],
    interfaces=[Whatsapp(agent=image_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="image_generation_model:app", reload=True)
```

---

<a name="agent_os--interfaces--whatsapp--image_generation_toolspy"></a>

### `agent_os/interfaces/whatsapp/image_generation_tools.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os.app import AgentOS
from agno.os.interfaces.whatsapp import Whatsapp
from agno.tools.openai import OpenAITools

agent_db = SqliteDb(db_file="tmp/persistent_memory.db")
image_agent = Agent(
    id="image_generation_tools",
    db=agent_db,
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    markdown=True,
    debug_mode=True,
    add_history_to_context=True,
)


# Setup our AgentOS app
agent_os = AgentOS(
    agents=[image_agent],
    interfaces=[Whatsapp(agent=image_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="image_generation_tools:app", reload=True)
```

---

<a name="agent_os--interfaces--whatsapp--reasoning_agentpy"></a>

### `agent_os/interfaces/whatsapp/reasoning_agent.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic.claude import Claude
from agno.os.app import AgentOS
from agno.os.interfaces.whatsapp import Whatsapp
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent_db = SqliteDb(db_file="tmp/persistent_memory.db")

reasoning_finance_agent = Agent(
    name="Reasoning Finance Agent",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=agent_db,
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions="Use tables to display data. When you use thinking tools, keep the thinking brief.",
    add_datetime_to_context=True,
    markdown=True,
)


# Setup our AgentOS app
agent_os = AgentOS(
    agents=[reasoning_finance_agent],
    interfaces=[Whatsapp(agent=reasoning_finance_agent)],
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available apps at:
    http://localhost:7777/config

    """
    agent_os.serve(app="reasoning_agent:app", reload=True)
```

---

<a name="agent_os--mcp--enable_mcp_examplepy"></a>

### `agent_os/mcp/enable_mcp_example.py`

```python
"""
Example AgentOS app with MCP enabled.

After starting this AgentOS app, you can test the MCP server with the test_client.py file.
"""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db = SqliteDb(db_file="tmp/agentos.db")

# Setup basic research agent
web_research_agent = Agent(
    id="web-research-agent",
    name="Web Research Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    enable_session_summaries=True,
    markdown=True,
)


# Setup our AgentOS with MCP enabled
agent_os = AgentOS(
    description="Example app with MCP enabled",
    agents=[web_research_agent],
    enable_mcp=True,  # This enables a LLM-friendly MCP server at /mcp
)

app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

    You can see view your LLM-friendly MCP server at:
    http://localhost:7777/mcp

    """
    agent_os.serve(app="enable_mcp_example:app")
```

---

<a name="agent_os--mcp--mcp_tools_advanced_examplepy"></a>

### `agent_os/mcp/mcp_tools_advanced_example.py`

```python
"""
Example AgentOS app where the agent has MCPTools.

AgentOS handles the lifespan of the MCPTools internally.
"""

from os import getenv

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools  # noqa: F401

# Setup the database
db = SqliteDb(db_file="tmp/agentos.db")

agno_mcp_tools = MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")

# Example: Brave Search MCP server
brave_mcp_tools = MCPTools(
    command="npx -y @modelcontextprotocol/server-brave-search",
    env={
        "BRAVE_API_KEY": getenv("BRAVE_API_KEY"),
    },
    timeout_seconds=60,
)

# You can also use MultiMCPTools to connect to multiple MCP servers at once:
#
# from agno.tools.mcp import MultiMCPTools
# mcp_tools = MultiMCPTools(
#     commands=["npx -y @modelcontextprotocol/server-brave-search"],
#     urls=["https://docs.agno.com/mcp"],
#     env={"BRAVE_API_KEY": getenv("BRAVE_API_KEY")},
# )

# Setup ai framework agent
ai_framework_agent = Agent(
    id="agno-support-agent",
    name="Agno Support Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[brave_mcp_tools, agno_mcp_tools],
    add_history_to_context=True,
    num_history_runs=3,
    markdown=True,
)

agent_os = AgentOS(
    description="Example app with MCP Tools",
    agents=[ai_framework_agent],
)


app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

    You can see test your AgentOS at:
    http://localhost:7777/docs

    """
    # Don't use reload=True here, this can cause issues with the lifespan
    agent_os.serve(app="mcp_tools_advanced_example:app")
```

---

<a name="agent_os--mcp--mcp_tools_examplepy"></a>

### `agent_os/mcp/mcp_tools_example.py`

```python
"""
Example AgentOS app where the agent has MCPTools.

AgentOS handles the lifespan of the MCPTools internally.
"""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools

# Setup the database
db = SqliteDb(db_file="tmp/agentos.db")

mcp_tools = MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")

# Setup basic agent
agno_support_agent = Agent(
    id="agno-support-agent",
    name="Agno Support Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[mcp_tools],
    add_history_to_context=True,
    num_history_runs=3,
    markdown=True,
)


agent_os = AgentOS(
    description="Example app with MCP Tools",
    agents=[agno_support_agent],
)


app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

    You can see test your AgentOS at:
    http://localhost:7777/docs

    """
    # Don't use reload=True here, this can cause issues with the lifespan
    agent_os.serve(app="mcp_tools_example:app")
```

---

<a name="agent_os--mcp--mcp_tools_existing_lifespanpy"></a>

### `agent_os/mcp/mcp_tools_existing_lifespan.py`

```python
"""
Example AgentOS app where the agent has MCPTools.

AgentOS handles the lifespan of the MCPTools internally.

In addition you can pass your own lifespan to AgentOS.
"""

from contextlib import asynccontextmanager

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools
from agno.utils.log import log_info

# Setup the database
db = SqliteDb(db_file="tmp/agentos.db")

mcp_tools = MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")

# Setup basic support agent
agno_support_agent = Agent(
    id="agno-support-agent",
    name="Agno Support Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[mcp_tools],
    add_history_to_context=True,
    num_history_runs=3,
    markdown=True,
)


@asynccontextmanager
async def lifespan(app):
    log_info("Starting My FastAPI App")
    yield
    log_info("Stopping My FastAPI App")


agent_os = AgentOS(
    description="Example app with MCP Tools",
    agents=[agno_support_agent],
    enable_mcp=True,
    lifespan=lifespan,
)


app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

    You can see test your AgentOS at:
    http://localhost:7777/docs

    """
    # Don't use reload=True here, this can cause issues with the lifespan
    agent_os.serve(app="mcp_tools_existing_lifespan:app")
```

---

<a name="agent_os--mcp--test_clientpy"></a>

### `agent_os/mcp/test_client.py`

```python
"""
First run the AgentOS with enable_mcp=True

```bash
python cookbook/agent_os/mcp/enable_mcp.py
```
"""

import asyncio
from uuid import uuid4

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

# This is the URL of the MCP server we want to use.
server_url = "http://localhost:7777/mcp"

session_id = f"session_{uuid4()}"


async def run_agent() -> None:
    async with MCPTools(
        transport="streamable-http", url=server_url, timeout_seconds=60
    ) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-5-mini"),
            tools=[mcp_tools],
            instructions=[
                "You are a helpful assistant that has access to the configuration of a AgentOS.",
                "If you are asked to do something, use the appropriate tool to do it. ",
                "Look up information you need in the AgentOS configuration.",
            ],
            user_id="john@example.com",
            session_id=session_id,
            db=InMemoryDb(),
            add_session_state_to_context=True,
            add_history_to_context=True,
            markdown=True,
        )

        await agent.aprint_response(
            input="Which agents do I have in my AgentOS?", stream=True, markdown=True
        )

        # await agent.aprint_response(
        #     input="Use my agent to search the web for the latest news about AI",
        #     stream=True,
        #     markdown=True,
        # )

        ## Memory management
        # await agent.aprint_response(
        #     input="What memories do you have of me?",
        #     stream=True,
        #     markdown=True,
        # )

        # await agent.aprint_response(
        #     input="I like to ski, remember that of me.",
        #     stream=True,
        #     markdown=True,
        # )
        # await agent.aprint_response(
        #     input="Clean up all duplicate memories of me.",
        #     stream=True,
        #     markdown=True,
        # )

        ## Session management
        # await agent.aprint_response(
        #     input="How many sessions does my web-research-agent have?",
        #     stream=True,
        #     markdown=True,
        # )


# Example usage
if __name__ == "__main__":
    asyncio.run(run_agent())
```

---

<a name="agent_os--mcp_demopy"></a>

### `agent_os/mcp_demo.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools

agent = Agent(
    name="Agno Agent",
    model=Claude(id="claude-sonnet-4-0"),
    tools=[MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")],
)

agent_os = AgentOS(agents=[agent])
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="mcp_demo:app", reload=True)
```

---

<a name="agent_os--os_config--basicpy"></a>

### `agent_os/os_config/basic.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.config import (
    AgentOSConfig,
    ChatConfig,
    DatabaseConfig,
    MemoryConfig,
    MemoryDomainConfig,
)
from agno.os.interfaces.slack import Slack
from agno.os.interfaces.whatsapp import Whatsapp
from agno.team import Team
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Setup the database
db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai", id="db-0001")
db2 = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai2", id="db-0002")

# Setup basic agents, teams and workflows
basic_agent = Agent(
    name="Marketing Agent",
    db=db,
    enable_session_summaries=True,
    enable_user_memories=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
basic_team = Team(
    id="basic-team",
    name="Basic Team",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[basic_agent],
    enable_user_memories=True,
)
basic_workflow = Workflow(
    id="basic-workflow",
    name="Basic Workflow",
    description="Just a simple workflow",
    db=db2,
    steps=[
        Step(
            name="step1",
            description="Just a simple step",
            agent=basic_agent,
        )
    ],
)

# Setup our AgentOS app
agent_os = AgentOS(
    description="Your AgentOS",
    os_id="0001",
    agents=[basic_agent],
    interfaces=[Whatsapp(agent=basic_agent), Slack(agent=basic_agent)],
    # Configuration for the AgentOS
    config=AgentOSConfig(
        chat=ChatConfig(
            quick_prompts={
                "marketing-agent": [
                    "What can you do?",
                    "How is our latest post working?",
                    "Tell me about our active marketing campaigns",
                ],
            },
        ),
        memory=MemoryConfig(
            dbs=[
                DatabaseConfig(
                    db_id=db.id,
                    domain_config=MemoryDomainConfig(
                        display_name="Main app user memories",
                    ),
                )
            ],
        ),
    ),
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available endpoints at:
    http://localhost:7777/config
    """
    agent_os.serve(app="basic:app", reload=True)
```

---

<a name="agent_os--os_config--yaml_configpy"></a>

### `agent_os/os_config/yaml_config.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.interfaces.slack import Slack
from agno.os.interfaces.whatsapp import Whatsapp
from agno.team import Team
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

cwd = Path(__file__).parent
os_config_path = str(cwd.joinpath("config.yaml"))

# Setup the database
db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai", id="db-0001")

# Setup basic agents, teams and workflows
basic_agent = Agent(
    id="basic-agent",
    name="Basic Agent",
    db=db,
    enable_session_summaries=True,
    enable_user_memories=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
basic_team = Team(
    id="basic-team",
    name="Basic Team",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    members=[basic_agent],
    enable_user_memories=True,
)
basic_workflow = Workflow(
    id="basic-workflow",
    name="Basic Workflow",
    description="Just a simple workflow",
    db=db,
    steps=[
        Step(
            name="step1",
            description="Just a simple step",
            agent=basic_agent,
        )
    ],
)

# Setup our AgentOS app
agent_os = AgentOS(
    description="Example AgentOS",
    os_id="basic-os",
    agents=[basic_agent],
    teams=[basic_team],
    workflows=[basic_workflow],
    interfaces=[Whatsapp(agent=basic_agent), Slack(agent=basic_agent)],
    # Configuration for the AgentOS
    config=os_config_path,
)
app = agent_os.get_app()


if __name__ == "__main__":
    """Run your AgentOS.

    You can see the configuration and available endpoints at:
    http://localhost:7777/config
    """
    agent_os.serve(app="yaml_config:app", reload=True)
```

---

<a name="agent_os--workflow--basic_workflowpy"></a>

### `agent_os/workflow/basic_workflow.py`

```python
from agno.agent.agent import Agent

# Import the workflows
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.os import AgentOS
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    agent=hackernews_agent,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="content-creation-workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[research_step, content_planning_step],
)


# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[content_creation_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="basic_workflow:app", reload=True)
```

---

<a name="agent_os--workflow--basic_workflow_teampy"></a>

### `agent_os/workflow/basic_workflow_team.py`

```python
from agno.agent.agent import Agent

# Import the workflows
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(db_file="tmp/workflow.db"),
    steps=[research_step, content_planning_step],
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[content_creation_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="basic_workflow_team:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_conditionalpy"></a>

### `agent_os/workflow/workflow_with_conditional.py`

```python
from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb

# Import the workflows
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.condition import Condition
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === BASIC AGENTS ===
researcher = Agent(
    name="Researcher",
    instructions="Research the given topic and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

summarizer = Agent(
    name="Summarizer",
    instructions="Create a clear summary of the research findings.",
)

fact_checker = Agent(
    name="Fact Checker",
    instructions="Verify facts and check for accuracy in the research.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Write a comprehensive article based on all available research and verification.",
)

# === CONDITION EVALUATOR ===


def needs_fact_checking(step_input: StepInput) -> bool:
    """Determine if the research contains claims that need fact-checking"""
    summary = step_input.previous_step_content or ""

    # Look for keywords that suggest factual claims
    fact_indicators = [
        "study shows",
        "research indicates",
        "according to",
        "statistics",
        "data shows",
        "survey",
        "report",
        "million",
        "billion",
        "percent",
        "%",
        "increase",
        "decrease",
    ]

    return any(indicator in summary.lower() for indicator in fact_indicators)


# === WORKFLOW STEPS ===
research_step = Step(
    name="research",
    description="Research the topic",
    agent=researcher,
)

summarize_step = Step(
    name="summarize",
    description="Summarize research findings",
    agent=summarizer,
)

# Conditional fact-checking step
fact_check_step = Step(
    name="fact_check",
    description="Verify facts and claims",
    agent=fact_checker,
)

write_article = Step(
    name="write_article",
    description="Write final article",
    agent=writer,
)

# === BASIC LINEAR WORKFLOW ===
basic_workflow = Workflow(
    name="basic-linear-workflow",
    description="Research -> Summarize -> Condition(Fact Check) -> Write Article",
    steps=[
        research_step,
        summarize_step,
        Condition(
            name="fact_check_condition",
            description="Check if fact-checking is needed",
            evaluator=needs_fact_checking,
            steps=[fact_check_step],
        ),
        write_article,
    ],
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[basic_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_conditional:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_custom_functionpy"></a>

### `agent_os/workflow/workflow_with_custom_function.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """
    Custom function that does intelligent content planning with context awareness
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        response = content_planner.run(planning_prompt)

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

        return StepOutput(content=enhanced_content)

    except Exception as e:
        return StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation with custom execution options",
    db=PostgresDb(
        session_table="workflow_session",
        db_url=db_url,
    ),
    steps=[research_step, content_planning_step],
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example app for basic agent with playground capabilities",
    workflows=[content_creation_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_custom_function:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_input_schemapy"></a>

### `agent_os/workflow/workflow_with_input_schema.py`

```python
from typing import List

from agno.agent.agent import Agent

# Import the workflows
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.os import AgentOS
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[research_step, content_planning_step],
    input_schema=ResearchTopic,
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[content_creation_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_input_schema:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_looppy"></a>

### `agent_os/workflow/workflow_with_loop.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat

# Import the workflows
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.loop import Loop
from agno.workflow.step import Step
from agno.workflow.types import StepOutput
from agno.workflow.workflow import Workflow

research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Simple check - if any output contains substantial content, we're good
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f" Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="research-and-content-workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_loop:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_nested_stepspy"></a>

### `agent_os/workflow/workflow_with_nested_steps.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb

# Import the workflows
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.loop import Loop
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)

# End condition function for the loop


def research_quality_check(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    if not outputs:
        return False

    # Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 300:
            print(
                f" Research quality check passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research quality check failed - need more substantial research")
    return False


# Create a Loop step for deep tech research
deep_tech_research_loop = Loop(
    name="Deep Tech Research Loop",
    steps=[research_hackernews],
    end_condition=research_quality_check,
    max_iterations=3,
    description="Perform iterative deep research on tech topics",
)

# Router function that selects between simple web research or deep tech research loop


def research_strategy_router(step_input: StepInput) -> List[Step]:
    """
    Decide between simple web research or deep tech research loop based on the input topic.
    Returns either a single web research step or a tech research loop.
    """
    return [deep_tech_research_loop]


workflow = Workflow(
    name="Adaptive Research Workflow",
    description="Intelligently selects between simple web research or deep iterative tech research based on topic complexity",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_strategy_router,
            choices=[research_web, deep_tech_research_loop],
            description="Chooses between simple web research or deep tech research loop",
        ),
        publish_content,
    ],
    db=PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_nested_steps:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_parallelpy"></a>

### `agent_os/workflow/workflow_with_parallel.py`

```python
from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb

# Import the workflows
from agno.os import AgentOS
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="content-creation-workflow",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_parallel:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_routerpy"></a>

### `agent_os/workflow/workflow_with_router.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb

# Import the workflows
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f" Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f" General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="intelligent-research-workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_router:app", reload=True)
```

---

<a name="agent_os--workflow--workflow_with_stepspy"></a>

### `agent_os/workflow/workflow_with_steps.py`

```python
from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat

# Import the workflows
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

# Define individual steps
research_step = Step(
    name="research",
    agent=researcher,
    description="Research the topic and gather information",
)

writing_step = Step(
    name="writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="editing",
    agent=editor,
    description="Edit and polish the article",
)

# Create a Steps sequence that chains these above steps together
article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to final edit",
    steps=[research_step, writing_step, editing_step],
)

article_workflow = Workflow(
    name="Article Creation Workflow",
    description="Automated article creation from research to publication",
    steps=[article_creation_sequence],
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    workflows=[article_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_steps:app", reload=True)
```

---

<a name="agents--agentic_search--agentic_ragpy"></a>

### `agents/agentic_search/agentic_rag.py`

```python
"""This cookbook shows how to implement Agentic RAG using Hybrid Search and Reranking.
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy` to install the dependencies
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run: `python cookbook/agent_concepts/agentic_search/agentic_rag.py` to run the agent
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.cohere import CohereReranker
from agno.models.anthropic import Claude
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database, store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

asyncio.run(
    knowledge.add_contents_async(urls=["https://docs.agno.com/introduction/agents.md"])
)

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response("What are Agents?", stream=True)
```

---

<a name="agents--agentic_search--agentic_rag_infinity_rerankerpy"></a>

### `agents/agentic_search/agentic_rag_infinity_reranker.py`

```python
"""This cookbook shows how to implement Agentic RAG using Infinity Reranker.

Infinity is a high-performance inference server for text-embeddings, reranking, and classification models.
It provides fast and efficient reranking capabilities for RAG applications.

## Setup Instructions:

### 1. Install Dependencies
Run: `pip install agno anthropic infinity-client lancedb`

### 2. Set up Infinity Server
You have several options to deploy Infinity:

#### Local Installation
```bash
# Install infinity
pip install "infinity-emb[all]"

# Run infinity server with reranking model
infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
```
Wait for the engine to start.

# For better performance, you can use larger models:
# BAAI/bge-reranker-large
# BAAI/bge-reranker-v2-m3
# ms-marco-MiniLM-L-12-v2


### 3. Export API Keys
```bash
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

### 4. Run the Example
```bash
python cookbook/agent_concepts/agentic_search/agentic_rag_infinity_reranker.py
```

## About Infinity Reranker:
- Provides fast, local reranking without external API calls
- Supports multiple state-of-the-art reranking models
- Can be deployed on GPU for better performance
- Offers both sync and async reranking capabilities
- More deployment options: https://michaelfeil.eu/infinity/0.0.76/deploy/
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.infinity import InfinityReranker
from agno.models.anthropic import Claude
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database, store embeddings in the `agno_docs_infinity` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_infinity",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        # Use Infinity reranker for local, fast reranking
        reranker=InfinityReranker(
            model="BAAI/bge-reranker-base",  # You can change this to other models
            host="localhost",
            port=7997,
            top_n=5,  # Return top 5 reranked documents
        ),
    ),
)
asyncio.run(
    knowledge.add_contents_async(
        urls=[
            "https://docs.agno.com/concepts/agents/introduction.md",
            "https://docs.agno.com/concepts/agents/tools.md",
            "https://docs.agno.com/concepts/agents/knowledge.md",
        ]
    )
)

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
        "Provide detailed and accurate information based on the retrieved documents.",
    ],
    markdown=True,
)


def test_infinity_connection():
    """Test if Infinity server is running and accessible"""
    try:
        from infinity_client import Client

        _ = Client(base_url="http://localhost:7997")
        print(" Successfully connected to Infinity server at localhost:7997")
        return True
    except Exception as e:
        print(f" Failed to connect to Infinity server: {e}")
        print(
            "\nPlease make sure Infinity server is running. See setup instructions above."
        )
        return False


if __name__ == "__main__":
    print(" Agentic RAG with Infinity Reranker Example")
    print("=" * 50)

    # Test Infinity connection first
    if not test_infinity_connection():
        exit(1)

    print("\n Starting agent interaction...")
    print("=" * 50)

    # Example questions to test the reranking capabilities
    questions = [
        "What are Agents and how do they work?",
        "How do I use tools with agents?",
        "What is the difference between knowledge and tools?",
    ]

    for i, question in enumerate(questions, 1):
        print(f"\n Question {i}: {question}")
        print("-" * 40)
        agent.print_response(question, stream=True)
        print("\n" + "=" * 50)

    print("\n Example completed!")
    print("\nThe Infinity reranker helped improve the relevance of retrieved documents")
    print("by reranking them based on semantic similarity to your queries.")
```

---

<a name="agents--agentic_search--agentic_rag_with_reasoningpy"></a>

### `agents/agentic_search/agentic_rag_with_reasoning.py`

```python
"""This cookbook shows how to implement Agentic RAG with Reasoning.
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy` to install the dependencies
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run: `python cookbook/agent_concepts/agentic_search/agentic_rag_with_reasoning.py` to run the agent
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.cohere import CohereReranker
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database, store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

asyncio.run(
    knowledge.add_contents_async(urls=["https://docs.agno.com/introduction/agents.md"])
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response(
        "What are Agents?",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="agents--agentic_search--lightrag--agentic_rag_with_lightragpy"></a>

### `agents/agentic_search/lightrag/agentic_rag_with_lightrag.py`

```python
import asyncio
from os import getenv

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.wikipedia_reader import WikipediaReader
from agno.vectordb.lightrag import LightRag

vector_db = LightRag(
    api_key=getenv("LIGHTRAG_API_KEY"),
)

knowledge = Knowledge(
    name="My Pinecone Knowledge Base",
    description="This is a knowledge base that uses a Pinecone Vector DB",
    vector_db=vector_db,
)


asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"doc_type": "recipe_book"},
    )
)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        topics=["Manchester United"],
        reader=WikipediaReader(),
    )
)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://en.wikipedia.org/wiki/Manchester_United_F.C.",
    )
)


agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    read_chat_history=False,
)


asyncio.run(
    agent.aprint_response("What skills does Jordan Mitchell have?", markdown=True)
)

asyncio.run(
    agent.aprint_response(
        "In what year did Manchester United change their name?", markdown=True
    )
)
```

---

<a name="agents--async--basicpy"></a>

### `agents/async/basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.pprint import apprint_run_response

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
)


async def basic():
    response = await agent.arun(input="Tell me a joke.")
    print(response.content)


async def basic_print():
    await agent.aprint_response(input="Tell me a joke.")


async def basic_pprint():
    response = await agent.arun(input="Tell me a joke.")
    await apprint_run_response(response)


if __name__ == "__main__":
    asyncio.run(basic())
    # OR
    asyncio.run(basic_print())
    # OR
    asyncio.run(basic_pprint())
```

---

<a name="agents--async--data_analystpy"></a>

### `agents/async/data_analyst.py`

```python
"""Run `pip install duckdb` to install dependencies."""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckdb import DuckDbTools

duckdb_tools = DuckDbTools()
duckdb_tools.create_table_from_path(
    path="https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
    table="movies",
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[duckdb_tools],
    markdown=True,
    additional_context=dedent("""\
    You have access to the following tables:
    - movies: contains information about movies from IMDB.
    """),
)
asyncio.run(agent.aprint_response("What is the average rating of movies?"))
```

---

<a name="agents--async--delaypy"></a>

### `agents/async/delay.py`

```python
import asyncio

from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

providers = ["openai", "anthropic", "ollama", "cohere", "google"]
instructions = [
    "Your task is to write a well researched report on AI providers.",
    "The report should be unbiased and factual.",
]


async def get_agent(delay, provider):
    agent = Agent(
        model=OpenAIChat(id="gpt-4"),
        instructions=instructions,
        tools=[DuckDuckGoTools()],
    )
    await asyncio.sleep(delay)
    response: RunOutput = await agent.arun(
        f"Write a report on the following AI provider: {provider}"
    )
    return response


async def get_reports():
    tasks = []
    for delay, provider in enumerate(providers):
        delay = delay * 2
        tasks.append(get_agent(delay, provider))

    results = await asyncio.gather(*tasks)
    return results


async def main():
    results = await get_reports()
    for result in results:
        print("************")
        pprint(result.content)
        print("************")
        print("\n")


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="agents--async--gather_agentspy"></a>

### `agents/async/gather_agents.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

providers = ["openai", "anthropic", "ollama", "cohere", "google"]
instructions = [
    "Your task is to write a well researched report on AI providers.",
    "The report should be unbiased and factual.",
]


async def get_reports():
    tasks = []
    for provider in providers:
        agent = Agent(
            model=OpenAIChat(id="gpt-4"),
            instructions=instructions,
            tools=[DuckDuckGoTools()],
        )
        tasks.append(
            agent.arun(f"Write a report on the following AI provider: {provider}")
        )

    results = await asyncio.gather(*tasks)
    return results


async def main():
    results = await get_reports()
    for result in results:
        print("************")
        pprint(result.content)
        print("************")
        print("\n")


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="agents--async--reasoningpy"></a>

### `agents/async/reasoning.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "9.11 and 9.9 -- which is bigger?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

asyncio.run(regular_agent.aprint_response(task, stream=True))
asyncio.run(
    reasoning_agent.aprint_response(task, stream=True, show_full_reasoning=True)
)
```

---

<a name="agents--async--streamingpy"></a>

### `agents/async/streaming.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.pprint import apprint_run_response

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
)


async def streaming():
    async for response in agent.arun(input="Tell me a joke.", stream=True):
        print(response.content, end="", flush=True)


async def streaming_print():
    await agent.aprint_response(input="Tell me a joke.", stream=True)


async def streaming_pprint():
    await apprint_run_response(agent.arun(input="Tell me a joke.", stream=True))


if __name__ == "__main__":
    asyncio.run(streaming())
    # OR
    asyncio.run(streaming_print())
    # OR
    asyncio.run(streaming_pprint())
```

---

<a name="agents--async--structured_outputpy"></a>

### `agents/async/structured_output.py`

```python
import asyncio
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o-2024-08-06"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)


# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.arun("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.arun("New York")
# pprint(structured_output_response.content)

asyncio.run(structured_output_agent.aprint_response("New York"))
asyncio.run(json_mode_agent.aprint_response("New York"))
```

---

<a name="agents--async--tool_usepy"></a>

### `agents/async/tool_use.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in UK and in USA?"))
```

---

<a name="agents--context_management--datetime_instructionspy"></a>

### `agents/context_management/datetime_instructions.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    add_datetime_to_context=True,
    timezone_identifier="Etc/UTC",
)
agent.print_response(
    "What is the current date and time? What is the current time in NYC?"
)
```

---

<a name="agents--context_management--dynamic_instructionspy"></a>

### `agents/context_management/dynamic_instructions.py`

```python
from agno.agent import Agent


def get_instructions(session_state):
    if session_state and session_state.get("current_user_id"):
        return f"Make the story about {session_state.get('current_user_id')}."
    return "Make the story about the user."


agent = Agent(instructions=get_instructions)
agent.print_response("Write a 2 sentence story", user_id="john.doe")
```

---

<a name="agents--context_management--few_shot_learningpy"></a>

### `agents/context_management/few_shot_learning.py`

```python
"""
This example demonstrates how to use additional_input with an Agent
to teach proper response patterns through few-shot learning.
"""

from agno.agent import Agent
from agno.models.message import Message
from agno.models.openai.chat import OpenAIChat

# Few-shot examples showing good support patterns
support_examples = [
    # Example 1: Simple issue resolution
    Message(role="user", content="I forgot my password and can't log in"),
    Message(
        role="assistant",
        content="""
    I'll help you reset your password right away.

    **Steps to Reset Your Password:**
    . Go to the login page and click "Forgot Password"
    . Enter your email address
    . Check your email for the reset link
    . Follow the link to create a new password
    . Use a strong password with uppercase, lowercase, numbers, and symbols

    f you don't receive the email within 5 minutes, please check your spam folder. Let me know if you need any assistance with these steps!
    """.strip(),
    ),
    # Example 2: Billing issue pattern
    Message(
        role="user",
        content="I've been charged twice for the same order and I'm frustrated!",
    ),
    Message(
        role="assistant",
        content="""
    I sincerely apologize for the billing error and the frustration this has caused you.

    **Immediate Action Plan:**
    1. I'll investigate your account to confirm the duplicate charge
    2. Process a full refund for the duplicate transaction
    3. Ensure this doesn't happen again by checking your payment method settings
    4. Provide you with a confirmation number once the refund is processed

    The refund typically takes 3-5 business days to appear on your statement. As an apology for this inconvenience, I'd also like to offer you a 10% discount on your next order.

    Is there anything else I can help you with regarding this issue?
    """.strip(),
    ),
    # Example 3: Technical support pattern
    Message(role="user", content="Your app keeps crashing when I try to upload photos"),
    Message(
        role="assistant",
        content="""
    I'm sorry you're experiencing crashes with photo uploads. Let me help you troubleshoot this issue.

    **Troubleshooting Steps:**
    1. **Check App Version:** Ensure you have the latest version from the app store
    2. **Restart the App:** Close completely and reopen the application
    3. **Check Storage:** Make sure you have sufficient device storage (at least 1GB free)
    4. **Photo Size:** Try uploading smaller photos (under 10MB each)
    5. **Network Connection:** Ensure you have a stable internet connection

    **If the issue persists:**
    - Try uploading one photo at a time instead of multiple
    - Clear the app cache in your device settings
    - Restart your device

    If none of these steps resolve the issue, please let me know your device type and OS version, and I'll escalate this to our technical team for further investigation.
    """.strip(),
    ),
]

if __name__ == "__main__":
    # Create agent with few-shot learning
    agent = Agent(
        name="Customer Support Specialist",
        model=OpenAIChat(id="gpt-4o-mini"),
        add_name_to_context=True,
        additional_input=support_examples,  # few-shot learning examples
        instructions=[
            "You are an expert customer support specialist.",
            "Always be empathetic, professional, and solution-oriented.",
            "Provide clear, actionable steps to resolve customer issues.",
            "Follow the established patterns for consistent, high-quality support.",
        ],
        debug_mode=True,
        markdown=True,
    )

    for i, example in enumerate(support_examples, 1):
        print(f" Example {i}: {example}")
        print("-" * 50)
        agent.print_response(example)
```

---

<a name="agents--context_management--instructionspy"></a>

### `agents/context_management/instructions.py`

```python
from agno.agent import Agent

agent = Agent(instructions="Share a 2 sentence story about")
agent.print_response("Love in the year 12000.")
```

---

<a name="agents--context_management--instructions_via_functionpy"></a>

### `agents/context_management/instructions_via_function.py`

```python
from typing import List

from agno.agent import Agent


def get_instructions(agent: Agent) -> List[str]:
    return [
        f"Your name is {agent.name}!",
        "Talk in haiku's!",
        "Use poetry to answer questions.",
    ]


agent = Agent(
    name="AgentX",
    instructions=get_instructions,
    markdown=True,
)
agent.print_response("Who are you?", stream=True)
```

---

<a name="agents--context_management--location_instructionspy"></a>

### `agents/context_management/location_instructions.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    add_location_to_context=True,
    tools=[DuckDuckGoTools(cache_results=True)],
)
agent.print_response("What city am I in?")
agent.print_response("What is current news about my city?")
```

---

<a name="agents--custom_logging--custom_loggingpy"></a>

### `agents/custom_logging/custom_logging.py`

```python
"""Example showing how to use a custom logger with Agno."""

import logging

from agno.agent import Agent
from agno.utils.log import configure_agno_logging, log_info


def get_custom_logger():
    """Return an example custom logger."""
    custom_logger = logging.getLogger("custom_logger")
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[CUSTOM_LOGGER] %(levelname)s: %(message)s")
    handler.setFormatter(formatter)
    custom_logger.addHandler(handler)
    custom_logger.setLevel(logging.INFO)  # Set level to INFO to show info messages
    custom_logger.propagate = False
    return custom_logger


# Get the custom logger we will use for the example.
custom_logger = get_custom_logger()

# Configure Agno to use our custom logger. It will be used for all logging.
configure_agno_logging(custom_default_logger=custom_logger)

# Every use of the logging function in agno.utils.log will now use our custom logger.
log_info("This is using our custom logger!")

# Now let's setup an Agent and run it.
# All logging coming from the Agent will use our custom logger.
agent = Agent()
agent.print_response("What can I do to improve my sleep?")
```

---

<a name="agents--custom_logging--custom_logging_advancedpy"></a>

### `agents/custom_logging/custom_logging_advanced.py`

```python
"""Example showing how to use multiple custom loggers with Agno.

This is useful for advanced scenarios where you want to use different loggers for different parts of Agno."""

import logging

from agno.agent import Agent
from agno.team import Team
from agno.utils.log import configure_agno_logging


def get_custom_agent_logger():
    """Return an example custom agent logger."""
    custom_logger = logging.getLogger("custom_agent_logger")
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[CUSTOM_AGENT_LOGGER] %(levelname)s: %(message)s")
    handler.setFormatter(formatter)
    custom_logger.addHandler(handler)
    custom_logger.setLevel(logging.INFO)  # Set level to INFO to show info messages
    custom_logger.propagate = False
    return custom_logger


def get_custom_team_logger():
    """Return an example custom team logger."""
    custom_logger = logging.getLogger("custom_team_logger")
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[CUSTOM_TEAM_LOGGER] %(levelname)s: %(message)s")
    handler.setFormatter(formatter)
    custom_logger.addHandler(handler)
    custom_logger.setLevel(logging.DEBUG)  # Set level to DEBUG to show debug messages
    custom_logger.propagate = False
    return custom_logger


# Get our custom loggers. We will use one for Agents and one for Teams.
custom_agent_logger = get_custom_agent_logger()
custom_team_logger = get_custom_team_logger()

# Configure Agno to use our custom loggers.
configure_agno_logging(
    custom_default_logger=custom_agent_logger,
    custom_agent_logger=custom_agent_logger,
    custom_team_logger=custom_team_logger,
)


# Now let's setup a Team and run it.
# Logging coming from the Team will use our custom Team logger,
# while logging coming from the Agent will use our custom Agent logger.
agent = Agent()
team = Team(members=[agent])

team.run("What can I do to improve my diet?")
```

---

<a name="agents--custom_logging--log_to_filepy"></a>

### `agents/custom_logging/log_to_file.py`

```python
"""Example showing how to use a custom logger with Agno."""

import logging
from pathlib import Path

from agno.agent import Agent
from agno.utils.log import configure_agno_logging, log_info


def get_custom_logger():
    """Return an example custom logger."""
    custom_logger = logging.getLogger("file_logger")

    # Ensure tmp directory exists
    log_file_path = Path("tmp/log.txt")
    log_file_path.parent.mkdir(parents=True, exist_ok=True)

    # Use FileHandler instead of StreamHandler to write to file
    handler = logging.FileHandler(log_file_path)
    formatter = logging.Formatter("%(levelname)s: %(message)s")
    handler.setFormatter(formatter)
    custom_logger.addHandler(handler)
    custom_logger.setLevel(logging.INFO)  # Set level to INFO to show info logs
    custom_logger.propagate = False
    return custom_logger


# Get the custom logger we will use for the example.
custom_logger = get_custom_logger()

# Configure Agno to use our custom logger. It will be used for all logging.
configure_agno_logging(custom_default_logger=custom_logger)

# Every use of the logging function in agno.utils.log will now use our custom logger.
log_info("This is using our custom logger!")

# Now let's setup an Agent and run it.
# All logging coming from the Agent will use our custom logger.
agent = Agent()
agent.print_response("What can I do to improve my sleep?")
```

---

<a name="agents--dependencies--access_dependencies_in_toolpy"></a>

### `agents/dependencies/access_dependencies_in_tool.py`

```python
"""
Example showing how tools can access dependencies passed to the agent.

This demonstrates:
1. Passing dependencies to agent.run()
2. A simple tool that receives resolved dependencies
"""

from datetime import datetime
from typing import Any, Dict, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_current_context() -> dict:
    """Get current contextual information like time, weather, etc."""
    return {
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": "PST",
        "day_of_week": datetime.now().strftime("%A"),
    }


def analyze_user(user_id: str, dependencies: Optional[Dict[str, Any]] = None) -> str:
    """
    Analyze a specific user's profile and provide insights.

    This tool analyzes user behavior and preferences using available data sources.
    Call this tool with the user_id you want to analyze.

    Args:
        user_id: The user ID to analyze (e.g., 'john_doe', 'jane_smith')
        dependencies: Available data sources (automatically provided)

    Returns:
        Detailed analysis and insights about the user
    """
    if not dependencies:
        return "No data sources available for analysis."

    print(f"--> Tool received data sources: {list(dependencies.keys())}")

    results = [f"=== USER ANALYSIS FOR {user_id.upper()} ==="]

    # Use user profile data if available
    if "user_profile" in dependencies:
        profile_data = dependencies["user_profile"]
        results.append(f"Profile Data: {profile_data}")

        # Add analysis based on the profile
        if profile_data.get("role"):
            results.append(
                f"Professional Analysis: {profile_data['role']} with expertise in {', '.join(profile_data.get('preferences', []))}"
            )

    # Use current context data if available
    if "current_context" in dependencies:
        context_data = dependencies["current_context"]
        results.append(f"Current Context: {context_data}")
        results.append(
            f"Time-based Analysis: Analysis performed on {context_data['day_of_week']} at {context_data['current_time']}"
        )

    print(f"--> Tool returned results: {results}")

    return "\n\n".join(results)


# Create an agent with the analysis tool function
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[analyze_user],
    name="User Analysis Agent",
    description="An agent specialized in analyzing users using integrated data sources.",
    instructions=[
        "You are a user analysis expert with access to user analysis tools.",
        "When asked to analyze any user, use the analyze_user tool.",
        "This tool has access to user profiles and current context through integrated data sources.",
        "After getting tool results, provide additional insights and recommendations based on the analysis.",
        "Be thorough in your analysis and explain what the tool found.",
    ],
)

print("=== Tool Dependencies Access Example ===\n")

response = agent.run(
    input="Please analyze user 'john_doe' and provide insights about their professional background and preferences.",
    dependencies={
        "user_profile": {
            "name": "John Doe",
            "preferences": ["AI/ML", "Software Engineering", "Finance"],
            "location": "San Francisco, CA",
            "role": "Senior Software Engineer",
        },
        "current_context": get_current_context,
    },
    session_id="test_tool_dependencies",
)

print(f"\nAgent Response: {response.content}")
```

---

<a name="agents--dependencies--add_dependencies_on_runpy"></a>

### `agents/dependencies/add_dependencies_on_run.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_user_profile(user_id: str = "john_doe") -> dict:
    """Get user profile information that can be referenced in responses.

    Args:
        user_id: The user ID to get profile for
    Returns:
        Dictionary containing user profile information
    """
    profiles = {
        "john_doe": {
            "name": "John Doe",
            "preferences": {
                "communication_style": "professional",
                "topics_of_interest": ["AI/ML", "Software Engineering", "Finance"],
                "experience_level": "senior",
            },
            "location": "San Francisco, CA",
            "role": "Senior Software Engineer",
        }
    }

    return profiles.get(user_id, {"name": "Unknown User"})


def get_current_context() -> dict:
    """Get current contextual information like time, weather, etc."""
    from datetime import datetime

    return {
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": "PST",
        "day_of_week": datetime.now().strftime("%A"),
    }


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
)

# Example usage - sync
response = agent.run(
    "Please provide me with a personalized summary of today's priorities based on my profile and interests.",
    dependencies={
        "user_profile": get_user_profile,
        "current_context": get_current_context,
    },
    add_dependencies_to_context=True,
    debug_mode=True,
)

print(response.content)

# ------------------------------------------------------------
# ASYNC EXAMPLE
# ------------------------------------------------------------
# async def test_async():
#     async_response = await agent.arun(
#         "Based on my profile, what should I focus on this week? Include specific recommendations.",
#         dependencies={
#             "user_profile": get_user_profile,
#             "current_context": get_current_context
#         },
#         add_dependencies_to_context=True,
#         debug_mode=True,
#     )

#     print("\n=== Async Run Response ===")
#     print(async_response.content)

# # Run the async test
# import asyncio
# asyncio.run(test_async())

# ------------------------------------------------------------
# Print response EXAMPLE
# ------------------------------------------------------------
# agent.print_response(
#     "Please provide me with a personalized summary of today's priorities based on my profile and interests.",
#     dependencies={
#         "user_profile": get_user_profile,
#         "current_context": get_current_context,
#     },
#     add_dependencies_to_context=True,
#     debug_mode=True,
# )
```

---

<a name="agents--dependencies--add_dependencies_to_contextpy"></a>

### `agents/dependencies/add_dependencies_to_context.py`

```python
import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the dependencies is resolved when the agent is run,
    # think of it as dependency injection for Agents
    dependencies={"top_hackernews_stories": get_top_hackernews_stories},
    # We can add the entire dependencies dictionary to the user message
    add_dependencies_to_context=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)
```

---

<a name="agents--dependencies--reference_dependenciespy"></a>

### `agents/dependencies/reference_dependencies.py`

```python
import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the dependencies is resolved when the agent is run,
    # think of it as dependency injection for Agents
    dependencies={"top_hackernews_stories": get_top_hackernews_stories},
    instructions=[
        "You are an insightful tech trend observer! ",
        "Here are the top stories on HackerNews: {top_hackernews_stories}",
    ],
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)
```

---

<a name="agents--events--basic_agent_eventspy"></a>

### `agents/events/basic_agent_events.py`

```python
import asyncio

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    id="finance-agent",
    name="Finance Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
)


async def run_agent_with_events(prompt: str):
    content_started = False
    async for run_output_event in finance_agent.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_output_event.event in [RunEvent.run_started, RunEvent.run_completed]:
            print(f"\nEVENT: {run_output_event.event}")

        if run_output_event.event in [RunEvent.tool_call_started]:
            print(f"\nEVENT: {run_output_event.event}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")  # type: ignore
            print(f"TOOL CALL ARGS: {run_output_event.tool.tool_args}")  # type: ignore

        if run_output_event.event in [RunEvent.tool_call_completed]:
            print(f"\nEVENT: {run_output_event.event}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")  # type: ignore
            print(f"TOOL CALL RESULT: {run_output_event.tool.result}")  # type: ignore

        if run_output_event.event in [RunEvent.run_content]:
            if not content_started:
                print("\nCONTENT:")
                content_started = True
            else:
                print(run_output_event.content, end="")


if __name__ == "__main__":
    asyncio.run(
        run_agent_with_events(
            "What is the price of Apple stock?",
        )
    )
```

---

<a name="agents--events--reasoning_agent_eventspy"></a>

### `agents/events/reasoning_agent_events.py`

```python
import asyncio

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat

finance_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
)


async def run_agent_with_events(prompt: str):
    content_started = False
    async for run_output_event in finance_agent.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_output_event.event in [RunEvent.run_started, RunEvent.run_completed]:
            print(f"\nEVENT: {run_output_event.event}")

        if run_output_event.event in [RunEvent.reasoning_started]:
            print(f"\nEVENT: {run_output_event.event}")

        if run_output_event.event in [RunEvent.reasoning_step]:
            print(f"\nEVENT: {run_output_event.event}")
            print(f"REASONING CONTENT: {run_output_event.reasoning_content}")  # type: ignore

        if run_output_event.event in [RunEvent.reasoning_completed]:
            print(f"\nEVENT: {run_output_event.event}")

        if run_output_event.event in [RunEvent.run_content]:
            if not content_started:
                print("\nCONTENT:")
                content_started = True
            else:
                print(run_output_event.content, end="")


if __name__ == "__main__":
    task = (
        "Analyze the key factors that led to the signing of the Treaty of Versailles in 1919. "
        "Discuss the political, economic, and social impacts of the treaty on Germany and how it "
        "contributed to the onset of World War II. Provide a nuanced assessment that includes "
        "multiple historical perspectives."
    )
    asyncio.run(
        run_agent_with_events(
            task,
        )
    )
```

---

<a name="agents--human_in_the_loop--agentic_user_inputpy"></a>

### `agents/human_in_the_loop/agentic_user_input.py`

```python
""" Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the UserControlFlowTools to allow the agent to get user input dynamically.
If the agent doesn't have enough information to complete a task, it will use the toolkit to get the information it needs from the user.
"""

from typing import Any, Dict, List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import Toolkit
from agno.tools.function import UserInputField
from agno.tools.user_control_flow import UserControlFlowTools
from agno.utils import pprint


class EmailTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            name="EmailTools", tools=[self.send_email, self.get_emails], *args, **kwargs
        )

    def send_email(self, subject: str, body: str, to_address: str) -> str:
        """Send an email to the given address with the given subject and body.

        Args:
            subject (str): The subject of the email.
            body (str): The body of the email.
            to_address (str): The address to send the email to.
        """
        return f"Sent email to {to_address} with subject {subject} and body {body}"

    def get_emails(self, date_from: str, date_to: str) -> list[dict[str, str]]:
        """Get all emails between the given dates.

        Args:
            date_from (str): The start date (in YYYY-MM-DD format).
            date_to (str): The end date (in YYYY-MM-DD format).
        """
        return [
            {
                "subject": "Hello",
                "body": "Hello, world!",
                "to_address": "test@test.com",
                "date": date_from,
            },
            {
                "subject": "Random other email",
                "body": "This is a random other email",
                "to_address": "john@doe.com",
                "date": date_to,
            },
        ]


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[EmailTools(), UserControlFlowTools()],
    markdown=True,
)

run_response = agent.run("Send an email with the body 'What is the weather in Tokyo?'")

# We use a while loop to continue the running until the agent is satisfied with the user input
while run_response.is_paused:
    for tool in run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema  # type: ignore

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type  # type: ignore
            field_description = field.description  # type: ignore

            # Display field information to the user
            print(f"\nField: {field.name}")  # type: ignore
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:  # type: ignore
                user_value = input(f"Please enter a value for {field.name}: ")  # type: ignore
            else:
                print(f"Value: {field.value}")  # type: ignore
                user_value = field.value  # type: ignore

            # Update the field value
            field.value = user_value  # type: ignore

    run_response = agent.continue_run(run_response=run_response)
    if not run_response.is_paused:
        pprint.pprint_run_response(run_response)
        break


run_response = agent.run("Get me all my emails")

while run_response.is_paused:
    for tool in run_response.tools_requiring_user_input:
        input_schema: Dict[str, Any] = tool.user_input_schema  # type: ignore

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type  # type: ignore
            field_description = field.description  # type: ignore

            # Display field information to the user
            print(f"\nField: {field.name}")  # type: ignore
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:  # type: ignore
                user_value = input(f"Please enter a value for {field.name}: ")  # type: ignore
            else:
                print(f"Value: {field.value}")  # type: ignore
                user_value = field.value  # type: ignore

            # Update the field value
            field.value = user_value  # type: ignore

    run_response = agent.continue_run(run_response=run_response)
    if not run_response.is_paused:
        pprint.pprint_run_response(run_response)
        break
```

---

<a name="agents--human_in_the_loop--confirmation_requiredpy"></a>

### `agents/human_in_the_loop/confirmation_required.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
)

run_response = agent.run("Fetch the top 2 hackernews stories.")
if run_response.is_paused:
    for tool in run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

run_response = agent.continue_run(run_response=run_response)
# Or
# run_response = agent.continue_run(run_id=run_response.run_id, updated_tools=run_response.tools)

pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("Fetch the top 2 hackernews stories")
```

---

<a name="agents--human_in_the_loop--confirmation_required_asyncpy"></a>

### `agents/human_in_the_loop/confirmation_required_async.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import asyncio
import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

run_response = asyncio.run(agent.arun("Fetch the top 2 hackernews stories"))
if run_response.is_paused:
    for tool in run_response.tools_requiring_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True


run_response = asyncio.run(agent.acontinue_run(run_response=run_response))
# Or
# run_response = asyncio.run(agent.acontinue_run(run_id=run_response.run_id))

pprint.pprint_run_response(run_response)


# Or for simple debug flow
# asyncio.run(agent.aprint_response("Fetch the top 2 hackernews stories"))
```

---

<a name="agents--human_in_the_loop--confirmation_required_mixed_toolspy"></a>

### `agents/human_in_the_loop/confirmation_required_mixed_tools.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.

In this case we have multiple tools and only one of them requires confirmation.

The agent should execute the tool that doesn't require confirmation and then pause for user confirmation.

The user can then either approve or reject the tool call and the agent should continue from where it left off.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


@tool(requires_confirmation=True)
def send_email(to: str, subject: str, body: str) -> str:
    """Send an email.

    Args:
        to (str): Email address to send to
        subject (str): Subject of the email
        body (str): Body of the email
    """
    return f"Email sent to {to} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories, send_email],
    markdown=True,
)

run_response = agent.run(
    "Fetch the top 2 hackernews stories and email them to john@doe.com."
)
if run_response.is_paused:
    for tool in run_response.tools:  # type: ignore
        if tool.requires_confirmation:
            # Ask for confirmation
            console.print(
                f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
            )
            message = (
                Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
                .strip()
                .lower()
            )

            if message == "n":
                tool.confirmed = False
            else:
                # We update the tools in place
                tool.confirmed = True
        else:
            console.print(
                f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] was completed in [bold green]{tool.metrics.duration:.2f}[/] seconds."  # type: ignore
            )

    run_response = agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Fetch the top 2 hackernews stories")
```

---

<a name="agents--human_in_the_loop--confirmation_required_multiple_toolspy"></a>

### `agents/human_in_the_loop/confirmation_required_multiple_tools.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.wikipedia import WikipediaTools
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        get_top_hackernews_stories,
        WikipediaTools(requires_confirmation_tools=["search_wikipedia"]),
    ],
    markdown=True,
)

run_response = agent.run(
    "Fetch 2 articles about the topic 'python'. You can choose which source to use, but only use one source."
)
while run_response.is_paused:
    for tool_exc in run_response.tools_requiring_confirmation:  # type: ignore
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool_exc.tool_name}({tool_exc.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
            tool.confirmation_note = (
                "This is not the right tool to use. Use the other tool!"
            )
        else:
            # We update the tools in place
            tool.confirmed = True

    run_response = agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)
```

---

<a name="agents--human_in_the_loop--confirmation_required_streampy"></a>

### `agents/human_in_the_loop/confirmation_required_stream.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=SqliteDb(
        db_file="tmp/example.db",
    ),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

for run_event in agent.run("Fetch the top 2 hackernews stories", stream=True):
    if run_event.is_paused:
        for tool in run_event.tools_requiring_confirmation:  # type: ignore
            # Ask for confirmation
            console.print(
                f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
            )
            message = (
                Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
                .strip()
                .lower()
            )

            if message == "n":
                tool.confirmed = False
            else:
                # We update the tools in place
                tool.confirmed = True
        run_response = agent.continue_run(
            run_id=run_event.run_id, updated_tools=run_event.tools, stream=True
        )  # type: ignore
        pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Fetch the top 2 hackernews stories", stream=True)
```

---

<a name="agents--human_in_the_loop--confirmation_required_stream_asyncpy"></a>

### `agents/human_in_the_loop/confirmation_required_stream_async.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import asyncio
import json

import httpx
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
    markdown=True,
)


async def main():
    async for run_event in agent.arun(
        "Fetch the top 2 hackernews stories", stream=True
    ):
        if run_event.is_paused:
            for tool in run_event.tools_requiring_confirmation:  # type: ignore
                # Ask for confirmation
                console.print(
                    f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
                )
                message = (
                    Prompt.ask(
                        "Do you want to continue?", choices=["y", "n"], default="y"
                    )
                    .strip()
                    .lower()
                )

                if message == "n":
                    tool.confirmed = False
                else:
                    # We update the tools in place
                    tool.confirmed = True

            async for resp in agent.acontinue_run(  # type: ignore
                run_id=run_event.run_id, updated_tools=run_event.tools, stream=True
            ):
                print(resp.content, end="")

    # Or for simple debug flow
    # await agent.aprint_response("Fetch the top 2 hackernews stories", stream=True)


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="agents--human_in_the_loop--confirmation_required_toolkitpy"></a>

### `agents/human_in_the_loop/confirmation_required_toolkit.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.yfinance import YFinanceTools
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools(requires_confirmation_tools=["get_current_stock_price"])],
    markdown=True,
)

run_response = agent.run("What is the current stock price of Apple?")
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_requiring_confirmation:  # type: ignore
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

    run_response = agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)
```

---

<a name="agents--human_in_the_loop--confirmation_required_with_historypy"></a>

### `agents/human_in_the_loop/confirmation_required_with_history.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    add_history_to_context=True,
    num_history_runs=2,
    markdown=True,
)

agent.run("What can you do?")

run_response = agent.run("Fetch the top 2 hackernews stories.")
if run_response.is_paused:
    for tool in run_response.tools_requiring_confirmation:  # type: ignore
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

run_response = agent.continue_run(run_response=run_response)
pprint.pprint_run_response(run_response)
```

---

<a name="agents--human_in_the_loop--confirmation_required_with_run_idpy"></a>

### `agents/human_in_the_loop/confirmation_required_with_run_id.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Handle user confirmation during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json

import httpx
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
)

run_response = agent.run("Fetch the top 2 hackernews stories.")
if run_response.is_paused:
    for tool in run_response.tools_requiring_confirmation:  # type: ignore
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

updated_tools = run_response.tools

run_response = agent.continue_run(
    run_id=run_response.run_id,
    updated_tools=updated_tools,
)

pprint.pprint_run_response(run_response)
```

---

<a name="agents--human_in_the_loop--external_tool_executionpy"></a>

### `agents/human_in_the_loop/external_tool_execution.py`

```python
""" Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

run_response = agent.run("What files do I have in my current directory?")
if run_response.is_paused:
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == execute_shell_command.name:
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")
            # We execute the tool ourselves. You can also execute something completely external here.
            result = execute_shell_command.entrypoint(**tool.tool_args)  # type: ignore
            # We have to set the result on the tool execution object so that the agent can continue
            tool.result = result

    run_response = agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?")
```

---

<a name="agents--human_in_the_loop--external_tool_execution_asyncpy"></a>

### `agents/human_in_the_loop/external_tool_execution_async.py`

```python
""" Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import asyncio
import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

run_response = asyncio.run(agent.arun("What files do I have in my current directory?"))
if run_response.is_paused:
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == execute_shell_command.name:
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")
            # We execute the tool ourselves. You can also execute something completely external here.
            result = execute_shell_command.entrypoint(**tool.tool_args)  # type: ignore
            # We have to set the result on the tool execution object so that the agent can continue
            tool.result = result

    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))
    pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?")
```

---

<a name="agents--human_in_the_loop--external_tool_execution_async_responsespy"></a>

### `agents/human_in_the_loop/external_tool_execution_async_responses.py`

```python
""" Human-in-the-Loop with OpenAI Responses API (gpt-4.1-mini)

This example mirrors the external tool execution async example but uses
OpenAIResponses with gpt-4.1-mini to validate tool-call id handling.

Run `pip install openai agno` to install dependencies.
"""

import asyncio
import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring
# for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if (
        command.startswith("ls ")
        or command == "ls"
        or command.startswith("cat ")
        or command.startswith("head ")
    ):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIResponses(id="gpt-4.1-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

run_response = asyncio.run(agent.arun("What files do I have in my current directory?"))

# Keep executing externally-required tools until the run completes
while (
    run_response.is_paused and len(run_response.tools_awaiting_external_execution) > 0
):
    for external_tool in run_response.tools_awaiting_external_execution:
        if external_tool.tool_name == execute_shell_command.name:
            print(
                f"Executing {external_tool.tool_name} with args {external_tool.tool_args} externally"
            )
            result = execute_shell_command.entrypoint(**external_tool.tool_args)
            external_tool.result = result
        else:
            print(f"Skipping unsupported external tool: {external_tool.tool_name}")

    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))

pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?")
```

---

<a name="agents--human_in_the_loop--external_tool_execution_streampy"></a>

### `agents/human_in_the_loop/external_tool_execution_stream.py`

```python
""" Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import subprocess

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
)

for run_event in agent.run(
    "What files do I have in my current directory?", stream=True
):
    if run_event.is_paused:
        for tool in run_event.tools_awaiting_external_execution:  # type: ignore
            if tool.tool_name == execute_shell_command.name:
                print(
                    f"Executing {tool.tool_name} with args {tool.tool_args} externally"
                )
                # We execute the tool ourselves. You can also execute something completely external here.
                result = execute_shell_command.entrypoint(**tool.tool_args)  # type: ignore
                # We have to set the result on the tool execution object so that the agent can continue
                tool.result = result

        run_response = agent.continue_run(
            run_id=run_event.run_id, updated_tools=run_event.tools, stream=True
        )  # type: ignore
        pprint.pprint_run_response(run_response)


# Or for simple debug flow
# agent.print_response("What files do I have in my current directory?", stream=True)
```

---

<a name="agents--human_in_the_loop--external_tool_execution_stream_asyncpy"></a>

### `agents/human_in_the_loop/external_tool_execution_stream_async.py`

```python
""" Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import asyncio
import subprocess

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    if command.startswith("ls"):
        return subprocess.check_output(command, shell=True).decode("utf-8")
    else:
        raise Exception(f"Unsupported command: {command}")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
)


async def main():
    async for run_event in agent.arun(
        "What files do I have in my current directory?", stream=True
    ):
        if run_event.is_paused:
            for tool in run_event.tools_awaiting_external_execution:  # type: ignore
                if tool.tool_name == execute_shell_command.name:
                    print(
                        f"Executing {tool.tool_name} with args {tool.tool_args} externally"
                    )
                    # We execute the tool ourselves. You can also execute something completely external here.
                    result = execute_shell_command.entrypoint(**tool.tool_args)  # type: ignore
                    # We have to set the result on the tool execution object so that the agent can continue
                    tool.result = result

            async for resp in agent.acontinue_run(  # type: ignore
                run_id=run_event.run_id,
                updated_tools=run_event.tools,
                stream=True,
            ):
                print(resp.content, end="")
        else:
            print(run_event.content, end="")

    # Or for simple debug flow
    # agent.print_response("What files do I have in my current directory?", stream=True)


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="agents--human_in_the_loop--external_tool_execution_toolkitpy"></a>

### `agents/human_in_the_loop/external_tool_execution_toolkit.py`

```python
""" Human-in-the-Loop: Execute a tool call outside of the agent

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Use external tool execution to execute a tool call outside of the agent

Run `pip install openai agno` to install dependencies.
"""

import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.toolkit import Toolkit
from agno.utils import pprint


class ShellTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            tools=[self.list_dir],
            external_execution_required_tools=["list_dir"],
            *args,
            **kwargs,
        )

    def list_dir(self, directory: str):
        """
        Lists the contents of a directory.

        Args:
            directory: The directory to list.

        Returns:
            A string containing the contents of the directory.
        """
        return subprocess.check_output(f"ls {directory}", shell=True).decode("utf-8")


tools = ShellTools()

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[tools],
    markdown=True,
)

run_response = agent.run("What files do I have in my current directory?")
if run_response.is_paused:
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == "list_dir":
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")
            # We execute the tool ourselves. You can also execute something completely external here.
            result = tools.list_dir(**tool.tool_args)  # type: ignore
            # We have to set the result on the tool execution object so that the agent can continue
            tool.result = result

    run_response = agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)
```

---

<a name="agents--human_in_the_loop--user_input_requiredpy"></a>

### `agents/human_in_the_loop/user_input_required.py`

```python
""" Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)

run_response = agent.run(
    "Send an email with the subject 'Hello' and the body 'Hello, world!'"
)
if run_response.is_paused:
    for tool in run_response.tools_requiring_user_input:  # type: ignore
        input_schema: List[UserInputField] = tool.user_input_schema  # type: ignore

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
            else:
                print(f"Value: {field.value}")
                user_value = field.value

            # Update the field value
            field.value = user_value

    run_response = agent.continue_run(
        run_response=run_response
    )  # or agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email with the subject 'Hello' and the body 'Hello, world!'")
```

---

<a name="agents--human_in_the_loop--user_input_required_all_fieldspy"></a>

### `agents/human_in_the_loop/user_input_required_all_fields.py`

```python
""" Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


@tool(requires_user_input=True)
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)

run_response = agent.run("Send an email please")
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_requiring_user_input:  # type: ignore
        input_schema: List[UserInputField] = tool.user_input_schema  # type: ignore

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")

            # Update the field value
            field.value = user_value

    run_response = agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email please")
```

---

<a name="agents--human_in_the_loop--user_input_required_asyncpy"></a>

### `agents/human_in_the_loop/user_input_required_async.py`

```python
""" Human-in-the-Loop: Allowing users to provide input externally"""

import asyncio
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
)

run_response = asyncio.run(
    agent.arun("Send an email with the subject 'Hello' and the body 'Hello, world!'")
)
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_requiring_user_input:  # type: ignore
        input_schema: List[UserInputField] = tool.user_input_schema  # type: ignore

        for field in input_schema:
            # Get user input for each field in the schema
            field_type = field.field_type
            field_description = field.description

            # Display field information to the user
            print(f"\nField: {field.name}")
            print(f"Description: {field_description}")
            print(f"Type: {field_type}")

            # Get user input
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
            else:
                print(f"Value: {field.value}")
                user_value = field.value

            # Update the field value
            field.value = user_value

    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email with the subject 'Hello' and the body 'Hello, world!'")
```

---

<a name="agents--human_in_the_loop--user_input_required_streampy"></a>

### `agents/human_in_the_loop/user_input_required_stream.py`

```python
""" Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

from typing import List

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
)

for run_event in agent.run(
    "Send an email with the subject 'Hello' and the body 'Hello, world!'", stream=True
):
    if run_event.is_paused:  # Or agent.run_response.is_paused
        for tool in run_event.tools_requiring_user_input:  # type: ignore
            input_schema: List[UserInputField] = tool.user_input_schema  # type: ignore

            for field in input_schema:
                # Get user input for each field in the schema
                field_type = field.field_type
                field_description = field.description

                # Display field information to the user
                print(f"\nField: {field.name}")
                print(f"Description: {field_description}")
                print(f"Type: {field_type}")

                # Get user input
                if field.value is None:
                    user_value = input(f"Please enter a value for {field.name}: ")
                else:
                    print(f"Value: {field.value}")
                    user_value = field.value

                # Update the field value
                field.value = user_value

        run_response = agent.continue_run(
            run_id=run_event.run_id, updated_tools=run_event.tools
        )
    pprint.pprint_run_response(run_response)

# Or for simple debug flow
# agent.print_response("Send an email with the subject 'Hello' and the body 'Hello, world!'", stream=True)
```

---

<a name="agents--human_in_the_loop--user_input_required_stream_asyncpy"></a>

### `agents/human_in_the_loop/user_input_required_stream_async.py`

```python
""" Human-in-the-Loop: Allowing users to provide input externally

This example shows how to use the `requires_user_input` parameter to allow users to provide input externally.
"""

import asyncio
from typing import List

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField


# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[send_email],
    markdown=True,
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
)


async def main():
    async for run_event in agent.arun(
        "Send an email with the subject 'Hello' and the body 'Hello, world!'",
        stream=True,
    ):
        if run_event.is_paused:  # Or agent.run_response.is_paused
            for tool in run_event.tools_requiring_user_input:  # type: ignore
                input_schema: List[UserInputField] = tool.user_input_schema  # type: ignore

                for field in input_schema:
                    # Get user input for each field in the schema
                    field_type = field.field_type
                    field_description = field.description

                    # Display field information to the user
                    print(f"\nField: {field.name}")
                    print(f"Description: {field_description}")
                    print(f"Type: {field_type}")

                    # Get user input
                    if field.value is None:
                        user_value = input(f"Please enter a value for {field.name}: ")
                    else:
                        print(f"Value: {field.value}")
                        user_value = field.value

                    # Update the field value
                    field.value = user_value

            async for resp in agent.acontinue_run(  # type: ignore
                run_id=run_event.run_id,
                updated_tools=run_event.tools,
                stream=True,
            ):
                print(resp.content, end="")

    # Or for simple debug flow
    # agent.aprint_response("Send an email with the subject 'Hello' and the body 'Hello, world!'")


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="agents--input_and_output--input_as_dictpy"></a>

### `agents/input_and_output/input_as_dict.py`

```python
from agno.agent import Agent

Agent().print_response(
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                },
            },
        ],
    },
    stream=True,
    markdown=True,
)
```

---

<a name="agents--input_and_output--input_as_listpy"></a>

### `agents/input_and_output/input_as_list.py`

```python
from agno.agent import Agent

Agent().print_response(
    [
        {"type": "text", "text": "What's in this image?"},
        {
            "type": "image_url",
            "image_url": {
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            },
        },
    ],
    stream=True,
    markdown=True,
)
```

---

<a name="agents--input_and_output--input_as_messagepy"></a>

### `agents/input_and_output/input_as_message.py`

```python
from agno.agent import Agent, Message

Agent().print_response(
    Message(
        role="user",
        content=[
            {"type": "text", "text": "What's in this image?"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                },
            },
        ],
    ),
    stream=True,
    markdown=True,
)
```

---

<a name="agents--input_and_output--input_as_messages_listpy"></a>

### `agents/input_and_output/input_as_messages_list.py`

```python
from agno.agent import Agent, Message

Agent().print_response(
    input=[
        Message(
            role="user",
            content="I'm preparing a presentation for my company about renewable energy adoption.",
        ),
        Message(
            role="assistant",
            content="I'd be happy to help with your renewable energy presentation. What specific aspects would you like me to focus on?",
        ),
        Message(
            role="user",
            content="Could you research the latest solar panel efficiency improvements in 2024?",
        ),
        Message(
            role="user",
            content="Also, please summarize the key findings in bullet points for my slides.",
        ),
    ],
    stream=True,
    markdown=True,
)
```

---

<a name="agents--input_and_output--input_schema_on_agentpy"></a>

### `agents/input_and_output/input_schema_on_agent.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
    input_schema=ResearchTopic,
)

# Pass a dict that matches the input schema
hackernews_agent.print_response(
    input={
        "topic": "AI",
        "focus_areas": ["AI", "Machine Learning"],
        "target_audience": "Developers",
        "sources_required": "5",
    }
)

# Pass a pydantic model that matches the input schema
# hackernews_agent.print_response(
#     input=ResearchTopic(
#         topic="AI",
#         focus_areas=["AI", "Machine Learning"],
#         target_audience="Developers",
#         sources_required=5,
#     )
# )
```

---

<a name="agents--input_and_output--input_schema_on_agent_as_typed_dictpy"></a>

### `agents/input_and_output/input_schema_on_agent_as_typed_dict.py`

```python
from typing import List, Optional, TypedDict

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools


# Define a TypedDict schema
class ResearchTopicDict(TypedDict):
    topic: str
    focus_areas: List[str]
    target_audience: str
    sources_required: int


# Optional: Define a TypedDict with optional fields
class ResearchTopicWithOptionals(TypedDict, total=False):
    topic: str
    focus_areas: List[str]
    target_audience: str
    sources_required: int
    priority: Optional[str]


# Create agent with TypedDict input schema
hackernews_agent = Agent(
    name="Hackernews Agent with TypedDict",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
    input_schema=ResearchTopicDict,
)

# with valid input
print("=== Testing TypedDict Input Schema ===")
hackernews_agent.print_response(
    input={
        "topic": "AI",
        "focus_areas": ["Machine Learning", "LLMs", "Neural Networks"],
        "target_audience": "Developers",
        "sources_required": 5,
    }
)

# with optional fields
optional_agent = Agent(
    name="Hackernews Agent with Optional Fields",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    input_schema=ResearchTopicWithOptionals,
)

print("\n=== Testing TypedDict with Optional Fields ===")
optional_agent.print_response(
    input={
        "topic": "Blockchain",
        "focus_areas": ["DeFi", "NFTs"],
        "target_audience": "Investors",
        # sources_required is optional, omitting it
        "priority": "high",
    }
)

# Should raise an error - missing required field
try:
    hackernews_agent.print_response(
        input={
            "topic": "AI",
            # Missing required fields: focus_areas, target_audience, sources_required
        }
    )
except ValueError as e:
    print("\n=== Expected Error for Missing Fields ===")
    print(f"Error: {e}")

# This will raise an error - unexpected field
try:
    hackernews_agent.print_response(
        input={
            "topic": "AI",
            "focus_areas": ["Machine Learning"],
            "target_audience": "Developers",
            "sources_required": 5,
            "unexpected_field": "value",  # This field is not in the TypedDict
        }
    )
except ValueError as e:
    print("\n=== Expected Error for Unexpected Field ===")
    print(f"Error: {e}")
```

---

<a name="agents--input_and_output--output_modelpy"></a>

### `agents/input_and_output/output_model.py`

```python
"""
This example shows how to use the output_model parameter to specify the model that will be used to generate the final response.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    output_model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
)

agent.print_response("Latest news from France?", stream=True)
```

---

<a name="agents--input_and_output--parser_modelpy"></a>

### `agents/input_and_output/parser_model.py`

```python
import random
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    output_schema=NationalParkAdventure,
    parser_model=OpenAIChat(id="gpt-4o"),
)


# Get the response in a variable
national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]
# Get the response in a variable
run: RunOutput = agent.run(national_parks[random.randint(0, len(national_parks) - 1)])
pprint(run.content)
```

---

<a name="agents--input_and_output--parser_model_ollamapy"></a>

### `agents/input_and_output/parser_model_ollama.py`

```python
import random
from typing import List

from agno.agent import Agent, RunOutput
from agno.models.ollama import Ollama
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


agent = Agent(
    model=OpenAIChat(id="o3"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    output_schema=NationalParkAdventure,
    parser_model=Ollama(id="Osmosis/Osmosis-Structure-0.6B"),
)

national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]
# Get the response in a variable
run: RunOutput = agent.run(national_parks[random.randint(0, len(national_parks) - 1)])
pprint(run.content)
```

---

<a name="agents--input_and_output--parser_model_streampy"></a>

### `agents/input_and_output/parser_model_stream.py`

```python
import random
from typing import Iterator, List

from agno.agent import Agent, RunOutputEvent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


agent = Agent(
    parser_model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    output_schema=NationalParkAdventure,
    model=OpenAIChat(id="gpt-4o-mini"),
)

# Get the response in a variable
national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]

# Get the response in a variable
run_events: Iterator[RunOutputEvent] = agent.run(
    national_parks[random.randint(0, len(national_parks) - 1)], stream=True
)
for event in run_events:
    pprint(event)
```

---

<a name="agents--input_and_output--response_as_variablepy"></a>

### `agents/input_and_output/response_as_variable.py`

```python
from typing import Iterator  # noqa
from rich.pretty import pprint
from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Use tables where possible"],
    markdown=True,
)

run_response: RunOutput = agent.run("What is the stock price of NVDA")
pprint(run_response)

# run_response_strem: Iterator[RunOutputEvent] = agent.run("What is the stock price of NVDA", stream=True)
# for response in run_response_strem:
#     pprint(response)
```

---

<a name="agents--input_and_output--structured_inputpy"></a>

### `agents/input_and_output/structured_input.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

hackernews_agent.print_response(
    input=ResearchTopic(
        topic="AI",
        focus_areas=["AI", "Machine Learning"],
        target_audience="Developers",
        sources_required=5,
    )
)
```

---

<a name="agents--input_and_output--structured_input_output_with_parser_modelpy"></a>

### `agents/input_and_output/structured_input_output_with_parser_model.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field
from rich.pretty import pprint


# Define your input schema
class ResearchTopic(BaseModel):
    topic: str
    sources_required: int = Field(description="Number of sources required", default=5)


# Define your output schema
class ResearchOutput(BaseModel):
    summary: str = Field(..., description="Executive summary of the research findings")
    insights: List[str] = Field(..., description="Key insights discovered from posts")
    top_stories: List[str] = Field(
        ..., description="Most relevant and popular stories found"
    )
    technologies: List[str] = Field(
        ..., description="Technologies, tools, or frameworks mentioned"
    )
    sources: List[str] = Field(..., description="Links to the most relevant posts")


# Define your agent
hn_researcher_agent = Agent(
    # Model to use
    model=Claude(id="claude-sonnet-4-0"),
    # Tools to use
    tools=[HackerNewsTools()],
    instructions="Research hackernews posts for a given topic",
    input_schema=ResearchTopic,
    output_schema=ResearchOutput,
    # Model to use convert the output to the JSON schema
    parser_model=OpenAIChat(id="gpt-5-nano"),
)

# Run the Agent
response = hn_researcher_agent.run(input=ResearchTopic(topic="AI", sources_required=5))

# Print the response
pprint(response.content)
```

---

<a name="agents--multimodal--01_media_input_for_toolpy"></a>

### `agents/multimodal/01_media_input_for_tool.py`

```python
"""
Example showing how tools can access media (images, videos, audio, files) passed to the agent.

This demonstrates:
1. Uploading a PDF file to an agent
2. A tool that can access and process the uploaded file (OCR simulation)
3. The LLM responding based on the tool's processing result
"""

from typing import Optional, Sequence

from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini
from agno.models.openai import OpenAIChat  # noqa: F401
from agno.tools import Toolkit


class DocumentProcessingTools(Toolkit):
    def __init__(self):
        tools = [
            self.extract_text_from_pdf,
        ]

        super().__init__(name="document_processing_tools", tools=tools)

    def extract_text_from_pdf(self, files: Optional[Sequence[File]] = None) -> str:
        """
        Extract text from uploaded PDF files using OCR.

        This tool can access any files that were passed to the agent.
        In a real implementation, you would use a proper OCR service.

        Args:
            files: Files passed to the agent (automatically injected)

        Returns:
            Extracted text from the PDF files
        """
        if not files:
            return "No files were uploaded to process."

        print(f"--> Files: {files}")

        extracted_texts = []
        for i, file in enumerate(files):
            if file.content:
                # Simulate OCR processing
                # In reality, you'd use a service like Tesseract, AWS Textract, etc.
                file_size = len(file.content)
                extracted_text = f"""
                    [SIMULATED OCR RESULT FOR FILE {i + 1}]
                    Document processed successfully!
                    File size: {file_size} bytes

                    Sample extracted content:
                    "This is a sample document with important information about quarterly sales figures.
                    Q1 Revenue: $125,000
                    Q2 Revenue: $150,000
                    Q3 Revenue: $175,000

                    The growth trend shows a 20% increase quarter over quarter."
                """
                extracted_texts.append(extracted_text)
            else:
                extracted_texts.append(
                    f"File {i + 1}: Content is empty or inaccessible."
                )

        return "\n\n".join(extracted_texts)


def create_sample_pdf_content() -> bytes:
    """Create a sample PDF-like content for demonstration."""
    # This is just sample binary content - in reality you'd have actual PDF bytes
    sample_content = """
    %PDF-1.4
    Sample PDF content for demonstration
    This would be actual PDF binary data in a real scenario
    """.encode("utf-8")
    return sample_content


def main():
    # Create an agent with document processing tools
    agent = Agent(
        # model=OpenAIChat(id="gpt-4o"),
        model=Gemini(id="gemini-2.5-pro"),
        tools=[DocumentProcessingTools()],
        name="Document Processing Agent",
        description="An agent that can process uploaded documents. Use the tool to extract text from the PDF.",
        debug_mode=True,
        send_media_to_model=False,
        store_media=True,
    )

    print("=== Tool Media Access Example ===\n")

    # Example 1: PDF Processing
    print("1. Testing PDF processing...")

    # Create sample file content
    pdf_content = create_sample_pdf_content()
    sample_file = File(content=pdf_content)

    response = agent.run(
        input="I've uploaded a PDF document. Please extract the text from it and summarize the key financial information.",
        files=[sample_file],
        session_id="test_files",
    )

    print(f"Agent Response: {response.content}")
    print("\n" + "=" * 50 + "\n")


if __name__ == "__main__":
    main()
```

---

<a name="agents--multimodal--02_media_input_to_agent_and_toolpy"></a>

### `agents/multimodal/02_media_input_to_agent_and_tool.py`

```python
"""
Comprehensive test for joint media access functionality.

This demonstrates:
1. Initial image upload and analysis
2. DALL-E image generation within the same run
3. Analysis of both original and generated images
4. Cross-run media persistence (accessing images from previous runs)
"""

from typing import Optional, Sequence

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools import Toolkit
from agno.tools.dalle import DalleTools


class ImageAnalysisTools(Toolkit):
    def __init__(self):
        tools = [
            self.analyze_images,
            self.count_images,
        ]
        super().__init__(name="image_analysis_tools", tools=tools)

    def analyze_images(self, images: Optional[Sequence[Image]] = None) -> str:
        """
        Analyze all available images and provide detailed descriptions.

        Args:
            images: Images available to the tool (automatically injected)

        Returns:
            Analysis of all available images
        """
        if not images:
            return "No images available to analyze."

        print(f"--> analyze_images received {len(images)} images")

        print("--> IMAGES", images)

        analysis_results = []
        for i, image in enumerate(images):
            if image.url:
                analysis_results.append(
                    f"Image {i + 1}: URL-based image at {image.url}"
                )
            elif image.content:
                analysis_results.append(
                    f"Image {i + 1}: Content-based image ({len(image.content)} bytes)"
                )
            else:
                analysis_results.append(f"Image {i + 1}: Unknown image format")

        return f"Found {len(images)} images:\n" + "\n".join(analysis_results)

    def count_images(self, images: Optional[Sequence[Image]] = None) -> str:
        """
        Count the number of available images.

        Args:
            images: Images available to the tool (automatically injected)

        Returns:
            Count of available images
        """
        if not images:
            return "0 images available"

        print(f"--> count_images received {len(images)} images")
        return f"{len(images)} images available"


def create_sample_image_content() -> bytes:
    """Create a simple image-like content for demonstration."""
    # This is just sample content - in reality you'd have actual image bytes
    return b"FAKE_IMAGE_CONTENT_FOR_DEMO"


def main():
    # Create an agent with both DALL-E and image analysis tools
    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),  # Use GPT-4o for vision support
        tools=[ImageAnalysisTools(), DalleTools()],
        name="Joint Media Test Agent",
        description="An agent that can generate and analyze images using joint media access.",
        debug_mode=True,
        add_history_to_context=True,
        send_media_to_model=False,
        db=PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"),
    )

    print("=== Joint Media Access Test ===\n")

    # Test 1: Initial image upload and analysis
    print("1. Testing initial image upload and analysis...")

    # Create sample image
    sample_image = Image(id="test_image_1", content=create_sample_image_content())

    response1 = agent.run(
        input="I've uploaded an image. Please count how many images are available and analyze them.",
        images=[sample_image],
    )

    print(f"Run 1 Response: {response1.content}")
    print(f"--> Run 1 Images in response: {len(response1.input.images or [])}")
    print("\n" + "=" * 50 + "\n")

    # Test 2: DALL-E generation + analysis in same run
    print("2. Testing DALL-E generation and immediate analysis...")

    response2 = agent.run(input="Generate an image of a cute cat.")

    print(f"Run 2 Response: {response2.content}")
    print(f"--> Run 2 Images in response: {len(response2.images or [])}")
    print("\n" + "=" * 50 + "\n")

    # Test 3: Cross-run media persistence
    print("3. Testing cross-run media persistence...")

    response3 = agent.run(
        input="Count how many images are available from all previous runs and analyze them."
    )

    print(f"Run 3 Response: {response3.content}")
    print("\n" + "=" * 50 + "\n")


if __name__ == "__main__":
    main()
```

---

<a name="agents--multimodal--agent_same_run_image_analysispy"></a>

### `agents/multimodal/agent_same_run_image_analysis.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.dalle import DalleTools

# Create an Agent with the DALL-E tool
agent = Agent(
    model=OpenAIChat(id="gpt-4.1"), tools=[DalleTools()], name="DALL-E Image Generator"
)

response = agent.run(
    "Generate an image of a dog and tell what color the dog is.",
    markdown=True,
    debug_mode=True,
)

if response.images:
    print("Agent Response", response.content)
    print(response.images[0].url)
```

---

<a name="agents--multimodal--agent_using_multimodal_tool_response_in_runspy"></a>

### `agents/multimodal/agent_using_multimodal_tool_response_in_runs.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.tools.dalle import DalleTools

# Create an Agent with the DALL-E tool
agent = Agent(
    tools=[DalleTools()],
    name="DALL-E Image Generator",
    add_history_to_context=True,
    db=SqliteDb(db_file="tmp/test.db"),
)

agent.print_response(
    "Generate an image of a Siamese white furry cat sitting on a couch?",
    markdown=True,
)

agent.print_response(
    "Which type of animal and the breed are we talking about?", markdown=True
)
```

---

<a name="agents--multimodal--audio_input_outputpy"></a>

### `agents/multimodal/audio_input_output.py`

```python
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file
from rich.pretty import pprint

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    markdown=True,
)

run_response = agent.run(
    "What's in these recording?",
    audio=[Audio(content=wav_data, format="wav")],
)

if run_response.response_audio is not None:
    pprint(run_response.content)
    write_audio_to_file(
        audio=run_response.response_audio.content, filename="tmp/result.wav"
    )
```

---

<a name="agents--multimodal--audio_multi_turnpy"></a>

### `agents/multimodal/audio_multi_turn.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    add_history_to_context=True,
    db=SqliteDb(
        session_table="audio_multi_turn_sessions", db_file="tmp/audio_multi_turn.db"
    ),
)

run_response = agent.run("Is a golden retriever a good family dog?")
pprint(run_response.content)
if run_response.response_audio is not None:
    write_audio_to_file(
        audio=run_response.response_audio.content, filename="tmp/answer_1.wav"
    )

run_response = agent.run("What breed are we talking about?")
pprint(run_response.content)
if run_response.response_audio is not None:
    write_audio_to_file(
        audio=run_response.response_audio.content, filename="tmp/answer_2.wav"
    )
```

---

<a name="agents--multimodal--audio_sentiment_analysispy"></a>

### `agents/multimodal/audio_sentiment_analysis.py`

```python
import requests
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.media import Audio
from agno.models.google import Gemini

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    add_history_to_context=True,
    markdown=True,
    db=SqliteDb(
        session_table="audio_sentiment_analysis_sessions",
        db_file="tmp/audio_sentiment_analysis.db",
    ),
)

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

response = requests.get(url)
audio_content = response.content

# Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.
agent.print_response(
    "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)

agent.print_response(
    "What else can you tell me about this audio conversation?",
    stream=True,
)
```

---

<a name="agents--multimodal--audio_streamingpy"></a>

### `agents/multimodal/audio_streaming.py`

```python
import base64
import wave
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.openai import OpenAIChat

# Audio Configuration
SAMPLE_RATE = 24000  # Hz (24kHz)
CHANNELS = 1  # Mono (Change to 2 if Stereo)
SAMPLE_WIDTH = 2  # Bytes (16 bits)

# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={
            "voice": "alloy",
            "format": "pcm16",
        },  # Only pcm16 is supported with streaming
    ),
)
output_stream: Iterator[RunOutputEvent] = agent.run(
    "Tell me a 10 second story", stream=True
)

filename = "tmp/response_stream.wav"

# Open the file once in append-binary mode
with wave.open(str(filename), "wb") as wav_file:
    wav_file.setnchannels(CHANNELS)
    wav_file.setsampwidth(SAMPLE_WIDTH)
    wav_file.setframerate(SAMPLE_RATE)

    # Iterate over generated audio
    for response in output_stream:
        response_audio = response.response_audio  # type: ignore
        if response_audio:
            if response_audio.transcript:
                print(response_audio.transcript, end="", flush=True)
            if response_audio.content:
                try:
                    pcm_bytes = base64.b64decode(response_audio.content)
                    wav_file.writeframes(pcm_bytes)
                except Exception as e:
                    print(f"Error decoding audio: {e}")
print()
```

---

<a name="agents--multimodal--audio_to_textpy"></a>

### `agents/multimodal/audio_to_text.py`

```python
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"

response = requests.get(url)
audio_content = response.content

# Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.

agent.print_response(
    "Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)
```

---

<a name="agents--multimodal--generate_image_with_intermediate_stepspy"></a>

### `agents/multimodal/generate_image_with_intermediate_steps.py`

```python
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.openai import OpenAIChat
from agno.tools.dalle import DalleTools
from agno.utils.common import dataclass_to_dict
from rich.pretty import pprint

image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description="You are an AI agent that can create images using DALL-E.",
    instructions=[
        "When the user asks you to create an image, use the DALL-E tool to create an image.",
        "The DALL-E tool will return an image URL.",
        "Return the image URL in your response in the following format: `![image description](image URL)`",
    ],
    markdown=True,
)

run_stream: Iterator[RunOutputEvent] = image_agent.run(
    "Create an image of a yellow siamese cat",
    stream=True,
    stream_intermediate_steps=True,
)
for chunk in run_stream:
    pprint(dataclass_to_dict(chunk, exclude={"messages"}))
    print("---" * 20)
```

---

<a name="agents--multimodal--generate_video_using_models_labpy"></a>

### `agents/multimodal/generate_video_using_models_lab.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models_labs import ModelsLabTools

video_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ModelsLabTools()],
    description="You are an AI agent that can generate videos using the ModelsLabs API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "The video will be displayed in the UI automatically below your response, so you don't need to show the video URL in your response.",
        "Politely and courteously let the user know that the video has been generated and will be displayed below as soon as its ready.",
    ],
    markdown=True,
)

video_agent.print_response("Generate a video of a cat playing with a ball")
# print(video_agent.run_response.videos)
```

---

<a name="agents--multimodal--generate_video_using_replicatepy"></a>

### `agents/multimodal/generate_video_using_replicate.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.replicate import ReplicateTools

"""Create an agent specialized for Replicate AI content generation"""

video_agent = Agent(
    name="Video Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ReplicateTools(
            model="tencent/hunyuan-video:847dfa8b01e739637fc76f480ede0c1d76408e1d694b830b5dfb8e547bf98405"
        )
    ],
    description="You are an AI agent that can generate videos using the Replicate API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "Return the URL as raw to the user.",
        "Don't convert video URL to markdown or anything else.",
    ],
    markdown=True,
)

video_agent.print_response("Generate a video of a horse in the dessert.")
```

---

<a name="agents--multimodal--image_input_high_fidelitypy"></a>

### `agents/multimodal/image_input_high_fidelity.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
)

agent.print_response(
    "What's in these images",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            detail="high",
        )
    ],
)
```

---

<a name="agents--multimodal--image_to_audiopy"></a>

### `agents/multimodal/image_to_audio.py`

```python
from pathlib import Path

from agno.agent import Agent, RunOutput
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file
from rich import print
from rich.text import Text

cwd = Path(__file__).parent.resolve()

image_agent = Agent(model=OpenAIChat(id="gpt-4o"))

image_path = Path(__file__).parent.joinpath("sample.jpg")
image_story: RunOutput = image_agent.run(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)
formatted_text = Text.from_markup(
    f":sparkles: [bold magenta]Story:[/bold magenta] {image_story.content} :sparkles:"
)
print(formatted_text)

audio_agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
)

audio_story: RunOutput = audio_agent.run(
    f"Narrate the story with flair: {image_story.content}"
)
if audio_story.response_audio is not None:
    write_audio_to_file(
        audio=audio_story.response_audio.content, filename="tmp/sample_story.wav"
    )
```

---

<a name="agents--multimodal--image_to_image_agentpy"></a>

### `agents/multimodal/image_to_image_agent.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.fal import FalTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    id="image-to-image",
    name="Image to Image Agent",
    tools=[FalTools()],
    markdown=True,
    instructions=[
        "You have to use the `image_to_image` tool to generate the image.",
        "You are an AI agent that can generate images using the Fal AI API.",
        "You will be given a prompt and an image URL.",
        "You have to return the image URL as provided, don't convert it to markdown or anything else.",
    ],
)

agent.print_response(
    "a cat dressed as a wizard with a background of a mystic forest. Make it look like 'https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png'",
    stream=True,
)
```

---

<a name="agents--multimodal--image_to_structured_outputpy"></a>

### `agents/multimodal/image_to_structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(model=OpenAIChat(id="gpt-4o"), output_schema=MovieScript)

response = agent.run(
    "Write a movie about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

for event in response:
    pprint(event.content)
```

---

<a name="agents--multimodal--image_to_textpy"></a>

### `agents/multimodal/image_to_text.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")
agent.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)
```

---

<a name="agents--multimodal--video_caption_agentpy"></a>

### `agents/multimodal/video_caption_agent.py`

```python
"""Please install dependencies using:
pip install openai moviepy ffmpeg
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.moviepy_video import MoviePyVideoTools
from agno.tools.openai import OpenAITools

video_tools = MoviePyVideoTools(
    process_video=True, generate_captions=True, embed_captions=True
)

openai_tools = OpenAITools()

video_caption_agent = Agent(
    name="Video Caption Generator Agent",
    model=OpenAIChat(
        id="gpt-4o",
    ),
    tools=[video_tools, openai_tools],
    description="You are an AI agent that can generate and embed captions for videos.",
    instructions=[
        "When a user provides a video, process it to generate captions.",
        "Use the video processing tools in this sequence:",
        "1. Extract audio from the video using extract_audio",
        "2. Transcribe the audio using transcribe_audio",
        "3. Generate SRT captions using create_srt",
        "4. Embed captions into the video using embed_captions",
    ],
    markdown=True,
)


video_caption_agent.print_response(
    "Generate captions for {video with location} and embed them in the video"
)
```

---

<a name="agents--multimodal--video_to_shortspy"></a>

### `agents/multimodal/video_to_shorts.py`

```python
"""
1. Install dependencies using: `pip install opencv-python google-geneai sqlalchemy`
2. Install ffmpeg `brew install ffmpeg`
2. Run the script using: `python cookbook/agent_concepts/video_to_shorts.py`
"""

import subprocess
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger

video_path = Path(__file__).parent.joinpath("sample_video.mp4")
output_dir = Path("tmp/shorts")

agent = Agent(
    name="Video2Shorts",
    description="Process videos and generate engaging shorts.",
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    instructions=[
        "Analyze the provided video directlydo NOT reference or analyze any external sources or YouTube videos.",
        "Identify engaging moments that meet the specified criteria for short-form content.",
        """Provide your analysis in a **table format** with these columns:
   - Start Time | End Time | Description | Importance Score""",
        "Ensure all timestamps use MM:SS format and importance scores range from 1-10. ",
        "Focus only on segments between 15 and 60 seconds long.",
        "Base your analysis solely on the provided video content.",
        "Deliver actionable insights to improve the identified segments for short-form optimization.",
    ],
)

# 2. Multimodal Query for Video Analysis
query = """

You are an expert in video content creation, specializing in crafting engaging short-form content for platforms like YouTube Shorts and Instagram Reels. Your task is to analyze the provided video and identify segments that maximize viewer engagement.

For each video, you'll:

1. Identify key moments that will capture viewers' attention, focusing on:
   - High-energy sequences
   - Emotional peaks
   - Surprising or unexpected moments
   - Strong visual and audio elements
   - Clear narrative segments with compelling storytelling

2. Extract segments that work best for short-form content, considering:
   - Optimal length (strictly 1560 seconds)
   - Natural start and end points that ensure smooth transitions
   - Engaging pacing that maintains viewer attention
   - Audio-visual harmony for an immersive experience
   - Vertical format compatibility and adjustments if necessary

3. Provide a detailed analysis of each segment, including:
   - Precise timestamps (Start Time | End Time in MM:SS format)
   - A clear description of why the segment would be engaging
   - Suggestions on how to enhance the segment for short-form content
   - An importance score (1-10) based on engagement potential

Your goal is to identify moments that are visually compelling, emotionally engaging, and perfectly optimized for short-form platforms.
"""

# 3. Generate Video Analysis
response = agent.run(query, videos=[Video(filepath=video_path)])

# 4. Create output directory
output_dir = Path(output_dir)
output_dir.mkdir(parents=True, exist_ok=True)


# 5. Extract and cut video segments - Optional
def extract_segments(response_text):
    import re

    segments_pattern = r"\|\s*(\d+:\d+)\s*\|\s*(\d+:\d+)\s*\|\s*(.*?)\s*\|\s*(\d+)\s*\|"
    segments: list[dict] = []

    for match in re.finditer(segments_pattern, str(response_text)):
        start_time = match.group(1)
        end_time = match.group(2)
        description = match.group(3)
        score = int(match.group(4))

        # Convert timestamps to seconds
        start_seconds = sum(x * int(t) for x, t in zip([60, 1], start_time.split(":")))
        end_seconds = sum(x * int(t) for x, t in zip([60, 1], end_time.split(":")))
        duration = end_seconds - start_seconds

        # Only process high-scoring segments
        if 15 <= duration <= 60 and score > 7:
            output_path = output_dir / f"short_{len(segments) + 1}.mp4"

            # FFmpeg command to cut video
            command = [
                "ffmpeg",
                "-ss",
                str(start_seconds),
                "-i",
                video_path,
                "-t",
                str(duration),
                "-vf",
                "scale=1080:1920,setsar=1:1",
                "-c:v",
                "libx264",
                "-c:a",
                "aac",
                "-y",
                str(output_path),
            ]

            try:
                subprocess.run(command, check=True)
                segments.append(
                    {"path": output_path, "description": description, "score": score}
                )
            except subprocess.CalledProcessError:
                print(f"Failed to process segment: {start_time} - {end_time}")

    return segments


logger.debug(f"{response.content}")

# 6. Process segments
shorts = extract_segments(response.content)

# 7. Print results
print("\n--- Generated Shorts ---")
for short in shorts:
    print(f"Short at {short['path']}")
    print(f"Description: {short['description']}")
    print(f"Engagement Score: {short['score']}/10\n")
```

---

<a name="agents--other--agent_extra_metricspy"></a>

### `agents/other/agent_extra_metrics.py`

```python
"""Show special token metrics like audio, cached and reasoning tokens"""

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    markdown=True,
)
run_response = agent.run(
    "What's in these recording?",
    audio=[Audio(content=wav_data, format="wav")],
)
pprint_run_response(run_response)
# Showing input audio, output audio and total audio tokens metrics
print(f"Input audio tokens: {run_response.metrics.audio_input_tokens}")
print(f"Output audio tokens: {run_response.metrics.audio_output_tokens}")
print(f"Audio tokens: {run_response.metrics.audio_total_tokens}")

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    markdown=True,
    telemetry=False,
)
run_response = agent.run(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. Include an ASCII diagram of your solution.",
    stream=False,
)
pprint_run_response(run_response)
# Showing reasoning tokens metrics
print(f"Reasoning tokens: {run_response.metrics.reasoning_tokens}")

agent = Agent(model=OpenAIChat(id="gpt-4o-mini"), markdown=True, telemetry=False)
agent.run("Share a 2 sentence horror story" * 150)
run_response = agent.run("Share a 2 sentence horror story" * 150)
# Showing cached tokens metrics
print(f"Cached tokens: {run_response.metrics.cache_read_tokens}")
```

---

<a name="agents--other--agent_metricspy"></a>

### `agents/other/agent_metrics.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from agno.utils import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    markdown=True,
    session_id="test-session-metrics",
    db=PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"),
)

# Get the run response directly from the non-streaming call
run_response = agent.run("What is the stock price of NVDA")
print("Tool execution completed successfully!")

# Print metrics per message
if run_response and run_response.messages:
    for message in run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(
                    f"Message: {message.content[:100]}..."
                )  # Truncate for readability
            elif message.tool_calls:
                print(f"Tool calls: {len(message.tool_calls)} tool call(s)")
            print("---" * 5, "Message Metrics", "---" * 5)
            if message.metrics:
                pprint(message.metrics)
            else:
                print("No metrics available for this message")
            print("---" * 20)

# Print the run metrics
print("---" * 5, "Run Metrics", "---" * 5)
if run_response and run_response.metrics:
    pprint(run_response.metrics)
else:
    print("No run metrics available")

# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
try:
    session_metrics = agent.get_session_metrics()
    if session_metrics:
        pprint(session_metrics)
    else:
        print("No session metrics available")
except Exception as e:
    print(f"Error getting session metrics: {e}")
```

---

<a name="agents--other--agent_run_metadatapy"></a>

### `agents/other/agent_run_metadata.py`

```python
from datetime import datetime

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="You are a customer support agent. You help process customer inquiries efficiently.",
    markdown=True,
)

response = agent.run(
    "A customer is reporting that their premium subscription features are not working. They need urgent help as they have a presentation in 2 hours.",
    metadata={
        "ticket_id": "SUP-2024-001234",
        "priority": "high",
        "request_type": "customer_support",
        "sla_deadline": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ"),
        "escalation_level": 2,
        "customer_tier": "enterprise",
        "department": "customer_success",
        "agent_id": "support_agent_v1",
        "business_impact": "revenue_critical",
        "estimated_resolution_time_minutes": 30,
    },
    debug_mode=True,
)
```

---

<a name="agents--other--cancel_a_runpy"></a>

### `agents/other/cancel_a_run.py`

```python
"""
Example demonstrating how to cancel a running agent execution.

This example shows how to:
1. Start an agent run in a separate thread
2. Cancel the run from another thread
3. Handle the cancelled response
"""

import threading
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.agent import RunEvent
from agno.run.base import RunStatus


def long_running_task(agent: Agent, run_id_container: dict):
    """
    Simulate a long-running agent task that can be cancelled.

    Args:
        agent: The agent to run
        run_id_container: Dictionary to store the run_id for cancellation

    Returns:
        Dictionary with run results and status
    """
    try:
        # Start the agent run - this simulates a long task
        final_response = None
        content_pieces = []

        for chunk in agent.run(
            "Write a very long story about a dragon who learns to code. "
            "Make it at least 2000 words with detailed descriptions and dialogue. "
            "Take your time and be very thorough.",
            stream=True,
        ):
            if "run_id" not in run_id_container and chunk.run_id:
                run_id_container["run_id"] = chunk.run_id

            if chunk.event == RunEvent.run_content:
                print(chunk.content, end="", flush=True)
                content_pieces.append(chunk.content)
            # When the run is cancelled, a `RunEvent.run_cancelled` event is emitted
            elif chunk.event == RunEvent.run_cancelled:
                print(f"\n Run was cancelled: {chunk.run_id}")
                run_id_container["result"] = {
                    "status": "cancelled",
                    "run_id": chunk.run_id,
                    "cancelled": True,
                    "content": "".join(content_pieces)[:200] + "..."
                    if content_pieces
                    else "No content before cancellation",
                }
                return
            elif hasattr(chunk, "status") and chunk.status == RunStatus.completed:
                final_response = chunk

        # If we get here, the run completed successfully
        if final_response:
            run_id_container["result"] = {
                "status": final_response.status.value
                if final_response.status
                else "completed",
                "run_id": final_response.run_id,
                "cancelled": final_response.status == RunStatus.cancelled,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }
        else:
            run_id_container["result"] = {
                "status": "unknown",
                "run_id": run_id_container.get("run_id"),
                "cancelled": False,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }

    except Exception as e:
        print(f"\n Exception in run: {str(e)}")
        run_id_container["result"] = {
            "status": "error",
            "error": str(e),
            "run_id": run_id_container.get("run_id"),
            "cancelled": True,
            "content": "Error occurred",
        }


def cancel_after_delay(agent: Agent, run_id_container: dict, delay_seconds: int = 3):
    """
    Cancel the agent run after a specified delay.

    Args:
        agent: The agent whose run should be cancelled
        run_id_container: Dictionary containing the run_id to cancel
        delay_seconds: How long to wait before cancelling
    """
    print(f" Will cancel run in {delay_seconds} seconds...")
    time.sleep(delay_seconds)

    run_id = run_id_container.get("run_id")
    if run_id:
        print(f" Cancelling run: {run_id}")
        success = agent.cancel_run(run_id)
        if success:
            print(f" Run {run_id} marked for cancellation")
        else:
            print(
                f" Failed to cancel run {run_id} (may not exist or already completed)"
            )
    else:
        print("  No run_id found to cancel")


def main():
    """Main function demonstrating run cancellation."""

    # Initialize the agent with a model
    agent = Agent(
        name="StorytellerAgent",
        model=OpenAIChat(id="o3-mini"),  # Use a model that can generate long responses
        description="An agent that writes detailed stories",
    )

    print(" Starting agent run cancellation example...")
    print("=" * 50)

    # Container to share run_id between threads
    run_id_container = {}

    # Start the agent run in a separate thread
    agent_thread = threading.Thread(
        target=lambda: long_running_task(agent, run_id_container), name="AgentRunThread"
    )

    # Start the cancellation thread
    cancel_thread = threading.Thread(
        target=cancel_after_delay,
        args=(agent, run_id_container, 8),  # Cancel after 5 seconds
        name="CancelThread",
    )

    # Start both threads
    print(" Starting agent run thread...")
    agent_thread.start()

    print(" Starting cancellation thread...")
    cancel_thread.start()

    # Wait for both threads to complete
    print(" Waiting for threads to complete...")
    agent_thread.join()
    cancel_thread.join()

    # Print the results
    print("\n" + "=" * 50)
    print(" RESULTS:")
    print("=" * 50)

    result = run_id_container.get("result")
    if result:
        print(f"Status: {result['status']}")
        print(f"Run ID: {result['run_id']}")
        print(f"Was Cancelled: {result['cancelled']}")

        if result.get("error"):
            print(f"Error: {result['error']}")
        else:
            print(f"Content Preview: {result['content']}")

        if result["cancelled"]:
            print("\n SUCCESS: Run was successfully cancelled!")
        else:
            print("\n  WARNING: Run completed before cancellation")
    else:
        print(" No result obtained - check if cancellation happened during streaming")

    print("\n Example completed!")


if __name__ == "__main__":
    # Run the main example
    main()
```

---

<a name="agents--other--debugpy"></a>

### `agents/other/debug.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

# You can set the debug mode on the agent for all runs to have more verbose output
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    debug_mode=True,
)

agent.print_response(input="Tell me a joke.")


# You can also set the debug mode on a single run
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
)
agent.print_response(input="Tell me a joke.", debug_mode=True)
```

---

<a name="agents--other--debug_levelpy"></a>

### `agents/other/debug_level.py`

```python
"""
This example shows how to set the debug level of an agent.

The debug level is a number between 1 and 2.

1: Basic debug information
2: Detailed debug information

The default debug level is 1.
"""

from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.tools.yfinance import YFinanceTools

# Basic debug information
agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20240620"),
    tools=[YFinanceTools()],
    debug_mode=True,
    debug_level=1,
)

agent.print_response("What is the current price of Tesla?")

# Verbose debug information
agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20240620"),
    tools=[YFinanceTools()],
    debug_mode=True,
    debug_level=2,
)

agent.print_response("What is the current price of Apple?")
```

---

<a name="agents--other--intermediate_stepspy"></a>

### `agents/other/intermediate_steps.py`

```python
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    markdown=True,
)

run_stream: Iterator[RunOutputEvent] = agent.run(
    "What is the stock price of NVDA", stream=True, stream_intermediate_steps=True
)
for chunk in run_stream:
    pprint(chunk.to_dict())
    print("---" * 20)
```

---

<a name="agents--other--run_response_eventspy"></a>

### `agents/other/run_response_events.py`

```python
from typing import Iterator, List

from agno.agent import (
    Agent,
    RunContentEvent,
    RunOutputEvent,
    ToolCallCompletedEvent,
    ToolCallStartedEvent,
)
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
run_response: Iterator[RunOutputEvent] = agent.run(
    "Whats happening in USA and Canada?", stream=True
)

response: List[str] = []
for chunk in run_response:
    if isinstance(chunk, RunContentEvent):
        response.append(chunk.content)  # type: ignore
    elif isinstance(chunk, ToolCallStartedEvent):
        response.append(
            f"Tool call started: {chunk.tool.tool_name} with args: {chunk.tool.tool_args}"  # type: ignore
        )
    elif isinstance(chunk, ToolCallCompletedEvent):
        response.append(
            f"Tool call completed: {chunk.tool.tool_name} with result: {chunk.tool.result}"  # type: ignore
        )

print("\n".join(response))
```

---

<a name="agents--other--scenario_testingpy"></a>

### `agents/other/scenario_testing.py`

```python
"""
This is an example that uses the [scenario](https://github.com/langwatch/scenario) testing library to test an agent.

Prerequisites:
- Install scenario: `pip install scenario`
"""

import pytest
from scenario import Scenario, TestingAgent, scenario_cache

Scenario.configure(testing_agent=TestingAgent(model="openai/gpt-4o-mini"))


@pytest.mark.agent_test
@pytest.mark.asyncio
async def test_vegetarian_recipe_agent():
    agent = VegetarianRecipeAgent()

    def vegetarian_recipe_agent(message, context):
        # Call your agent here
        return agent.run(message)

    # Define the scenario
    scenario = Scenario(
        "User is looking for a dinner idea",
        agent=vegetarian_recipe_agent,
        success_criteria=[
            "Recipe agent generates a vegetarian recipe",
            "Recipe includes a list of ingredients",
            "Recipe includes step-by-step cooking instructions",
        ],
        failure_criteria=[
            "The recipe is not vegetarian or includes meat",
            "The agent asks more than two follow-up questions",
        ],
    )

    # Run the scenario and get results
    result = await scenario.run()

    # Assert for pytest to know whether the test passed
    assert result.success


# Example agent implementation
from agno.agent import Agent  # noqa: E402
from agno.models.openai import OpenAIChat  # noqa: E402


class VegetarianRecipeAgent:
    def __init__(self):
        self.history = []

    @scenario_cache()
    def run(self, message: str):
        self.history.append({"role": "user", "content": message})

        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            markdown=True,
            instructions="You are a vegetarian recipe agent",
        )

        response = agent.run(message)
        result = response.content
        print(result)
        self.history.append(result)

        return {"message": result}
```

---

<a name="agents--other--tool_call_limitpy"></a>

### `agents/other/tool_call_limit.py`

```python
"""
This cookbook shows how to use tool call limit to control the number of tool calls an agent can make.
"""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-5-haiku-20241022"),
    tools=[YFinanceTools(company_news=True, cache_results=True)],
    tool_call_limit=1,
)

# It should only call the first tool and fail to call the second tool.
agent.print_response(
    "Find me the current price of TSLA, then after that find me the latest news about Tesla.",
    stream=True,
)
```

---

<a name="agents--rag--agentic_rag_lancedbpy"></a>

### `agents/rag/agentic_rag_lancedb.py`

```python
"""
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno` to install the dependencies
2. Run: `python cookbook/rag/04_agentic_rag_lancedb.py` to run the agent
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `recipes` table
    vector_db=LanceDb(
        table_name="recipes",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge,
    # Add a tool to search the knowledge base which enables agentic RAG.
    # This is enabled by default when `knowledge` is provided to the Agent.
    search_knowledge=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
```

---

<a name="agents--rag--agentic_rag_pgvectorpy"></a>

### `agents/rag/agentic_rag_pgvector.py`

```python
"""
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector agno` to install the dependencies
3. Run: `python cookbook/rag/02_agentic_rag_pgvector.py` to run the agent
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
knowledge = Knowledge(
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge,
    # Add a tool to search the knowledge base which enables agentic RAG.
    # This is enabled by default when `knowledge` is provided to the Agent.
    search_knowledge=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
# agent.print_response(
#     "Hi, i want to make a 3 course meal. Can you recommend some recipes. "
#     "I'd like to start with a soup, then im thinking a thai curry for the main course and finish with a dessert",
#     stream=True,
# )
```

---

<a name="agents--rag--agentic_rag_with_rerankingpy"></a>

### `agents/rag/agentic_rag_with_reranking.py`

```python
"""
1. Run: `pip install openai agno cohere lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY and CO_API_KEY
3. Run: `python cookbook/agent_concepts/rag/agentic_rag_with_reranking.py` to run the agent
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.cohere import CohereReranker
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(
            id="text-embedding-3-small"
        ),  # Use OpenAI for embeddings
        reranker=CohereReranker(
            model="rerank-multilingual-v3.0"
        ),  # Use Cohere for reranking
    ),
)

knowledge.add_content(name="Agno Docs", url="https://docs.agno.com/introduction.md")

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge,
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, comment after first run
    agent.print_response("What are Agno's key features?")
```

---

<a name="agents--rag--local_rag_langchain_qdrantpy"></a>

### `agents/rag/local_rag_langchain_qdrant.py`

```python
"""
Prerequisites:
 - pip install langchain-qdrant langchain-text-splitters langchain-community fastembed
"""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.vectordb.langchaindb import LangChainVectorDb
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.http.exceptions import UnexpectedResponse
from qdrant_client.http.models import Distance, VectorParams

urls = [
    "https://blog.google/technology/developers/gemma-3/",
]

loader = WebBaseLoader(urls)
data = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)
chunks = text_splitter.split_documents(data)
embeddings = FastEmbedEmbeddings(model_name="thenlper/gte-large")

client = QdrantClient(path="/tmp/app")
collection_name = "agent-rag"

try:
    collection_info = client.get_collection(collection_name=collection_name)
except (UnexpectedResponse, ValueError):
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
    )

vector_store = QdrantVectorStore(
    client=client,
    collection_name=collection_name,
    embedding=embeddings,
)

vector_store.add_documents(documents=chunks)
retriever = vector_store.as_retriever()

knowledge_base = LangChainVectorDb(knowledge_retriever=retriever)

agent = Agent(
    model=Ollama(id="qwen2.5:latest"),
    knowledge=knowledge_base,
    description="Answer to the user question from the knowledge base",
    markdown=True,
    search_knowledge=True,
)

user_query = "What are the new capabilities developers can use with Gemma 3"
agent.print_response(user_query, stream=True)
```

---

<a name="agents--rag--rag_sentence_transformerpy"></a>

### `agents/rag/rag_sentence_transformer.py`

```python
"""This cookbook is an implementation of Agentic RAG using Sentence Transformer Reranker with multilingual data.

## Setup Instructions:

### 1. Install Dependencies
Run: `pip install agno sentence-transformers`

### 2. Start the Postgres Server with pgvector
Run: `sh cookbook/scripts/run_pgvector.sh`

### 3. Run the example
Run: `uv run cookbook/agent_concepts/rag/rag_sentence_transformer.py`
"""

from agno.agent import Agent
from agno.knowledge.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.sentence_transformer import SentenceTransformerReranker
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

search_results = [
    "Organic skincare for sensitive skin with aloe vera and chamomile.",
    "New makeup trends focus on bold colors and innovative techniques",
    "Bio-Hautpflege fr empfindliche Haut mit Aloe Vera und Kamille",
    "Neue Make-up-Trends setzen auf krftige Farben und innovative Techniken",
    "Cuidado de la piel orgnico para piel sensible con aloe vera y manzanilla",
    "Las nuevas tendencias de maquillaje se centran en colores vivos y tcnicas innovadoras",
    "",
    "",
    "",
    "",
]

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="sentence_transformer_rerank_docs",
        embedder=SentenceTransformerEmbedder(
            id="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ),
        reranker=SentenceTransformerReranker(model="BAAI/bge-reranker-v2-m3"),
    ),
)

for result in search_results:
    knowledge.add_content(
        text_content=result,
        metadata={
            "source": "search_results",
        },
    )

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge,
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    test_queries = [
        "What organic skincare products are good for sensitive skin?",
        "Tell me about makeup trends in different languages",
        "Compare skincare and makeup information across languages",
    ]

    for query in test_queries:
        agent.print_response(
            query,
            stream=True,
            show_full_reasoning=True,
        )
```

---

<a name="agents--rag--rag_with_lance_db_and_sqlitepy"></a>

### `agents/rag/rag_with_lance_db_and_sqlite.py`

```python
"""Run `pip install lancedb` to install dependencies."""

from agno.agent import Agent
from agno.db.sqlite.sqlite import SqliteDb
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.ollama import Ollama
from agno.vectordb.lancedb import LanceDb

# Define the database URL where the vector database will be stored
db_url = "/tmp/lancedb"

# Configure the language model
model = Ollama(id="llama3.1:8b")

# Create Ollama embedder
embedder = OllamaEmbedder(id="nomic-embed-text", dimensions=768)

# Create the vector database
vector_db = LanceDb(
    table_name="recipes",  # Table name in the vector database
    uri=db_url,  # Location to initiate/create the vector database
    embedder=embedder,  # Without using this, it will use OpenAIChat embeddings by default
)

knowledge = Knowledge(
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes", url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)


# Set up SQL storage for the agent's data
db = SqliteDb(db_file="data.db")

# Initialize the Agent with various configurations including the knowledge base and storage
agent = Agent(
    session_id="session_id",  # use any unique identifier to identify the run
    user_id="user",  # user identifier to identify the user
    model=model,
    knowledge=knowledge,
    db=db,
)

# Use the agent to generate and print a response to a query, formatted in Markdown
agent.print_response(
    "What is the first step of making Gluai Buat Chi from the knowledge base?",
    markdown=True,
)
```

---

<a name="agents--rag--traditional_rag_lancedbpy"></a>

### `agents/rag/traditional_rag_lancedb.py`

```python
"""
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno` to install the dependencies
2. Run: `python cookbook/rag/03_traditional_rag_lancedb.py` to run the agent
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `recipes` table
    vector_db=LanceDb(
        table_name="recipes",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge,
    # Enable RAG by adding references from Knowledge to the user prompt.
    add_knowledge_to_context=True,
    # Set as False because Agents default to `search_knowledge=True`
    search_knowledge=False,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
```

---

<a name="agents--rag--traditional_rag_pgvectorpy"></a>

### `agents/rag/traditional_rag_pgvector.py`

```python
"""
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector agno` to install the dependencies
3. Run: `python cookbook/rag/01_traditional_rag_pgvector.py` to run the agent
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    # Use PgVector as the vector database and store embeddings in the `ai.recipes` table
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge,
    # Enable RAG by adding context from the `knowledge` to the user prompt.
    add_knowledge_to_context=True,
    # Set as False because Agents default to `search_knowledge=True`
    search_knowledge=False,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
```

---

<a name="agents--session--01_persistent_sessionpy"></a>

### `agents/session/01_persistent_session.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="session_storage",
    add_history_to_context=True,
)

agent.print_response("Tell me a new interesting fact about space")
```

---

<a name="agents--session--02_persistent_session_historypy"></a>

### `agents/session/02_persistent_session_history.py`

```python
"""
This example shows how to use the session history to store the conversation history.
add_history_to_context flag is used to add the history to the messages.
num_history_runs is used to set the number of history runs to add to the messages.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="session_storage",
    add_history_to_context=True,
    num_history_runs=2,
)

agent.print_response("Tell me a new interesting fact about space")
```

---

<a name="agents--session--03_session_summarypy"></a>

### `agents/session/03_session_summary.py`

```python
"""
This example shows how to use the session summary to store the conversation summary.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.session.summary import SessionSummaryManager  # noqa: F401

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

# Method 1: Set enable_session_summaries to True

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    enable_session_summaries=True,
    session_id="session_summary",
)

agent.print_response("Hi my name is John and I live in New York")
agent.print_response("I like to play basketball and hike in the mountains")

# Method 2: Set session_summary_manager

# session_summary_manager = SessionSummaryManager(model=OpenAIChat(id="gpt-4o-mini"))

# agent = Agent(
#     model=OpenAIChat(id="gpt-4o-mini"),
#     db=db,
#     session_id="session_summary",
#     session_summary_manager=session_summary_manager,
# )

# agent.print_response("Hi my name is John and I live in New York")
# agent.print_response("I like to play basketball and hike in the mountains")
```

---

<a name="agents--session--04_session_summary_referencespy"></a>

### `agents/session/04_session_summary_references.py`

```python
"""
This example shows how to use the `add_session_summary_to_context` parameter in the Agent config to
add session summaries to the Agent context.

Start the postgres db locally on Docker by running: cookbook/scripts/run_pgvector.sh
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    session_id="session_summary",
    enable_session_summaries=True,
)

# This will create a new session summary
agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
)

# You can use existing session summaries from session storage without creating or updating any new ones.
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    session_id="session_summary",
    add_session_summary_to_context=True,
)

agent.print_response("I also like to play basketball.")

# Alternatively, you can create a new session summary without adding the session summary to context.

# agent = Agent(
#     model=OpenAIChat(id="gpt-4o"),
#     db=db,
#     session_id="session_summary",
#     enable_session_summaries=True,
#     add_session_summary_to_context=False,
# )

# agent.print_response("I also like to play basketball.")
```

---

<a name="agents--session--05_chat_historypy"></a>

### `agents/session/05_chat_history.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="chat_history",
    instructions="You are a helpful assistant that can answer questions about space and oceans.",
    add_history_to_context=True,
)

agent.print_response("Tell me a new interesting fact about space")
print(agent.get_chat_history())

agent.print_response("Tell me a new interesting fact about oceans")
print(agent.get_chat_history())
```

---

<a name="agents--session--06_rename_sessionpy"></a>

### `agents/session/06_rename_session.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="chat_history",
    instructions="You are a helpful assistant that can answer questions about space and oceans.",
    add_history_to_context=True,
)

agent.print_response("Tell me a new interesting fact about space")
agent.set_session_name(session_name="Interesting Space Facts")

session = agent.get_session(session_id=agent.session_id)
print(session.session_data.get("session_name"))

agent.set_session_name(autogenerate=True)

session = agent.get_session(session_id=agent.session_id)
print(session.session_data.get("session_name"))
```

---

<a name="agents--session--07_in_memory_dbpy"></a>

### `agents/session/07_in_memory_db.py`

```python
"""This example shows how to use an in-memory database.

With this you will be able to store sessions, user memories, etc. without setting up a database.
Keep in mind that in production setups it is recommended to use a database.
"""

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

# Setup the in-memory database
db = InMemoryDb()

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Use the in-memory database. All db features will be available.
    db=db,
    # Set add_history_to_context=true to add the previous chat history to the context sent to the Model.
    add_history_to_context=True,
    # Number of historical responses to add to the messages.
    num_history_runs=3,
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

# -*- Create a run
agent.print_response("Share a 2 sentence horror story", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)

# -*- Ask a follow up question that continues the conversation
agent.print_response("What was my first message?", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)
```

---

<a name="agents--session--08_cache_sessionpy"></a>

### `agents/session/08_cache_session.py`

```python
"""Example of how to cache the session in memory for faster access."""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url, session_table="xxx")

# Setup the agent
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="session_storage",
    add_history_to_context=True,
    # Activate session caching. The session will be cached in memory for faster access.
    cache_session=True,
)

# Running the Agent
agent.print_response("Tell me a new interesting fact about space")

# You can get the cached session:
session = agent.get_session()
```

---

<a name="agents--state--agentic_session_statepy"></a>

### `agents/state/agentic_session_state.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

db = SqliteDb(db_file="tmp/agents.db")
agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    db=db,
    session_state={"shopping_list": []},
    add_session_state_to_context=True,  # Required so the agent is aware of the session state
    enable_agentic_state=True,
)

agent.print_response("Add milk, eggs, and bread to the shopping list")

agent.print_response("I picked up the eggs, now what's on my list?")

print(f"Session state: {agent.get_session_state()}")
```

---

<a name="agents--state--change_state_on_runpy"></a>

### `agents/state/change_state_on_run.py`

```python
from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat

agent = Agent(
    db=InMemoryDb(),
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Users name is {user_name} and age is {age}",
    debug_mode=True,
)

# Sets the session state for the session with the id "user_1_session_1"
agent.print_response(
    "What is my name?",
    session_id="user_1_session_1",
    user_id="user_1",
    session_state={"user_name": "John", "age": 30},
)

# Will load the session state from the session with the id "user_1_session_1"
agent.print_response("How old am I?", session_id="user_1_session_1", user_id="user_1")

# Sets the session state for the session with the id "user_2_session_1"
agent.print_response(
    "What is my name?",
    session_id="user_2_session_1",
    user_id="user_2",
    session_state={"user_name": "Jane", "age": 25},
)

# Will load the session state from the session with the id "user_2_session_1"
agent.print_response("How old am I?", session_id="user_2_session_1", user_id="user_2")
```

---

<a name="agents--state--dynamic_session_statepy"></a>

### `agents/state/dynamic_session_state.py`

```python
import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.tools.toolkit import Toolkit
from agno.utils.log import log_info, log_warning


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.register(self.process_customer_request)

    def process_customer_request(
        self,
        agent: Agent,
        customer_id: str,
        action: str = "retrieve",
        name: str = "John Doe",
    ):
        log_warning("Tool called, this shouldn't happen.")
        return "This should not be seen."


def customer_management_hook(
    session_state,
    function_name: str,
    function_call: Callable,
    arguments: Dict[str, Any],
):
    action = arguments.get("action", "retrieve")
    cust_id = arguments.get("customer_id")
    name = arguments.get("name", None)

    if not cust_id:
        raise ValueError("customer_id is required.")

    if action == "create":
        session_state["customer_profiles"][cust_id] = {"name": name}
        log_info(f"Hook: UPDATED session_state for customer '{cust_id}'.")
        return f"Success! Customer {cust_id} has been created."

    if action == "retrieve":
        profile = session_state.get("customer_profiles", {}).get(cust_id)
        if profile:
            log_info(f"Hook: FOUND customer '{cust_id}' in session_state.")
            return f"Profile for {cust_id}: {json.dumps(profile)}"
        else:
            raise ValueError(f"Customer '{cust_id}' not found.")

    log_info(f"Session state: {session_state}")


def run_test():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CustomerDBTools()],
        tool_hooks=[customer_management_hook],
        session_state={"customer_profiles": {"123": {"name": "Jane Doe"}}},
        instructions="Your profiles: {customer_profiles}. Use `process_customer_request`. Use either create or retrieve as action for the tool.",
        resolve_in_context=True,
        db=InMemoryDb(),
    )

    prompt = "First, create customer 789 named 'Tom'. Then, retrieve Tom's profile. Step by step."
    log_info(f" Prompting: '{prompt}'")
    agent.print_response(prompt, stream=False)

    log_info("\n--- TEST ANALYSIS ---")
    log_info(
        "Check logs for the second tool call. The system prompt will NOT contain customer '789'."
    )


if __name__ == "__main__":
    run_test()
```

---

<a name="agents--state--last_n_session_messagespy"></a>

### `agents/state/last_n_session_messages.py`

```python
import os

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

# Remove the tmp db file before running the script
if os.path.exists("tmp/data.db"):
    os.remove("tmp/data.db")

# Create agents for different users to demonstrate user-specific session history
user_1_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    user_id="user_1",
    db=SqliteDb(db_file="tmp/data.db"),
    add_history_to_context=True,
    num_history_runs=3,
    search_session_history=True,  # allow searching previous sessions
    num_history_sessions=2,  # only include the last 2 sessions in the search to avoid context length issues
)

user_2_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    user_id="user_2",
    db=SqliteDb(db_file="tmp/data.db"),
    add_history_to_context=True,
    num_history_runs=3,
    search_session_history=True,
    num_history_sessions=2,
)

# User 1 sessions
print("=== User 1 Sessions ===")
user_1_agent.print_response(
    "What is the capital of South Africa?", session_id="user1_session_1"
)
user_1_agent.print_response(
    "What is the capital of China?", session_id="user1_session_2"
)
user_1_agent.print_response(
    "What is the capital of France?", session_id="user1_session_3"
)

# User 2 sessions
print("\n=== User 2 Sessions ===")
user_2_agent.print_response(
    "What is the population of India?", session_id="user2_session_1"
)
user_2_agent.print_response(
    "What is the currency of Japan?", session_id="user2_session_2"
)

# Now test session history search - each user should only see their own sessions
print("\n=== Testing Session History Search ===")
print(
    "User 1 asking about previous conversations (should only see capitals, not population/currency):"
)
user_1_agent.print_response(
    "What did I discuss in my previous conversations?", session_id="user1_session_4"
)

print(
    "\nUser 2 asking about previous conversations (should only see population/currency, not capitals):"
)
user_2_agent.print_response(
    "What did I discuss in my previous conversations?", session_id="user2_session_3"
)
```

---

<a name="agents--state--session_state_advancedpy"></a>

### `agents/state/session_state_advanced.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat


# Define tools to manage our shopping list
def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list and return confirmation."""
    # Add the item if it's not already in the list
    if item.lower() not in [i.lower() for i in session_state["shopping_list"]]:
        session_state["shopping_list"].append(item)  # type: ignore
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"


def remove_item(session_state, item: str) -> str:
    """Remove an item from the shopping list by name."""
    # Case-insensitive search
    for i, list_item in enumerate(session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

    return f"'{item}' was not found in the shopping list"


def list_items(session_state) -> str:
    """List all items in the shopping list."""
    shopping_list = session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty."

    items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"


# Create a Shopping List Manager Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    # Initialize the session state with an empty shopping list (default session state for all sessions)
    session_state={"shopping_list": []},
    db=SqliteDb(db_file="tmp/example.db"),
    tools=[add_item, remove_item, list_items],
    # You can use variables from the session state in the instructions
    instructions=dedent("""\
        Your job is to manage a shopping list.

        The shopping list starts empty. You can add items, remove items by name, and list all items.

        Current shopping list: {shopping_list}
    """),
    markdown=True,
)

# Example usage
agent.print_response("Add milk, eggs, and bread to the shopping list", stream=True)
print(f"Session state: {agent.get_session_state()}")

agent.print_response("I got bread", stream=True)
print(f"Session state: {agent.get_session_state()}")

agent.print_response("I need apples and oranges", stream=True)
print(f"Session state: {agent.get_session_state()}")

agent.print_response("whats on my list?", stream=True)
print(f"Session state: {agent.get_session_state()}")

agent.print_response(
    "Clear everything from my list and start over with just bananas and yogurt",
    stream=True,
)
print(f"Session state: {agent.get_session_state()}")
```

---

<a name="agents--state--session_state_basicpy"></a>

### `agents/state/session_state_basic.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat


def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list."""
    session_state["shopping_list"].append(item)  # type: ignore
    return f"The shopping list is now {session_state['shopping_list']}"  # type: ignore


# Create an Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with a counter starting at 0 (this is the default session state for all users)
    session_state={"shopping_list": []},
    db=SqliteDb(db_file="tmp/agents.db"),
    tools=[add_item],
    # You can use variables from the session state in the instructions
    instructions="Current state (shopping list) is: {shopping_list}",
    markdown=True,
)

# Example usage
agent.print_response("Add milk, eggs, and bread to the shopping list", stream=True)
print(f"Final session state: {agent.get_session_state()}")
```

---

<a name="agents--state--session_state_in_contextpy"></a>

### `agents/state/session_state_in_context.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Users name is {user_name} and age is {age}",
    db=db,
)

# Sets the session state for the session with the id "user_1_session_1"
agent.print_response(
    "What is my name?",
    session_id="user_1_session_1",
    user_id="user_1",
    session_state={"user_name": "John", "age": 30},
)

# Will load the session state from the session with the id "user_1_session_1"
agent.print_response("How old am I?", session_id="user_1_session_1", user_id="user_1")

# Sets the session state for the session with the id "user_2_session_1"
agent.print_response(
    "What is my name?",
    session_id="user_2_session_1",
    user_id="user_2",
    session_state={"user_name": "Jane", "age": 25},
)

# Will load the session state from the session with the id "user_2_session_1"
agent.print_response("How old am I?", session_id="user_2_session_1", user_id="user_2")
```

---

<a name="agents--state--session_state_in_instructionspy"></a>

### `agents/state/session_state_in_instructions.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with a variable
    session_state={"user_name": "John"},
    # You can use variables from the session state in the instructions
    instructions="Users name is {user_name}",
    markdown=True,
)

agent.print_response("What is my name?", stream=True)
```

---

<a name="agents--state--session_state_multiple_userspy"></a>

### `agents/state/session_state_multiple_users.py`

```python
"""
This example demonstrates how to maintain state for each user in a multi-user environment.

The shopping list is stored in a dictionary, organized by user ID and session ID.

Agno automatically creates the "current_user_id" and "current_session_id" variables in the session state.

You can access these variables in your functions using the `agent.get_session_state()` dictionary.
"""

import json

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

# In-memory database to store user shopping lists
# Organized by user ID and session ID
shopping_list = {}


def add_item(session_state, item: str) -> str:
    """Add an item to the current user's shopping list."""

    current_user_id = session_state["current_user_id"]
    current_session_id = session_state["current_session_id"]
    shopping_list.setdefault(current_user_id, {}).setdefault(
        current_session_id, []
    ).append(item)

    return f"Item {item} added to the shopping list"


def remove_item(session_state, item: str) -> str:
    """Remove an item from the current user's shopping list."""

    current_user_id = session_state["current_user_id"]
    current_session_id = session_state["current_session_id"]

    if (
        current_user_id not in shopping_list
        or current_session_id not in shopping_list[current_user_id]
    ):
        return f"No shopping list found for user {current_user_id} and session {current_session_id}"

    if item not in shopping_list[current_user_id][current_session_id]:
        return f"Item '{item}' not found in the shopping list for user {current_user_id} and session {current_session_id}"

    shopping_list[current_user_id][current_session_id].remove(item)
    return f"Item {item} removed from the shopping list"


def get_shopping_list(session_state) -> str:
    """Get the current user's shopping list."""

    current_user_id = session_state["current_user_id"]
    current_session_id = session_state["current_session_id"]
    return f"Shopping list for user {current_user_id} and session {current_session_id}: \n{json.dumps(shopping_list[current_user_id][current_session_id], indent=2)}"


# Create an Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=SqliteDb(db_file="tmp/data.db"),
    tools=[add_item, remove_item, get_shopping_list],
    # Reference the in-memory database
    instructions=[
        "Current User ID: {current_user_id}",
        "Current Session ID: {current_session_id}",
    ],
    markdown=True,
)

user_id_1 = "john_doe"
user_id_2 = "mark_smith"
user_id_3 = "carmen_sandiago"

# Example usage
agent.print_response(
    "Add milk, eggs, and bread to the shopping list",
    stream=True,
    user_id=user_id_1,
    session_id="user_1_session_1",
)
agent.print_response(
    "Add tacos to the shopping list",
    stream=True,
    user_id=user_id_2,
    session_id="user_2_session_1",
)
agent.print_response(
    "Add apples and grapes to the shopping list",
    stream=True,
    user_id=user_id_3,
    session_id="user_3_session_1",
)
agent.print_response(
    "Remove milk from the shopping list",
    stream=True,
    user_id=user_id_1,
    session_id="user_1_session_1",
)
agent.print_response(
    "Add minced beef to the shopping list",
    stream=True,
    user_id=user_id_2,
    session_id="user_2_session_1",
)

# What is on Mark Smith's shopping list?
agent.print_response(
    "What is on Mark Smith's shopping list?",
    stream=True,
    user_id=user_id_2,
    session_id="user_2_session_1",
)

# New session, so new shopping list
agent.print_response(
    "Add chicken and soup to my list.",
    stream=True,
    user_id=user_id_2,
    session_id="user_3_session_2",
)

print(f"Final shopping lists: \n{json.dumps(shopping_list, indent=2)}")
```

---

<a name="db--01_persistent_session_storagepy"></a>

### `db/01_persistent_session_storage.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team

# Set up Postgres database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(name="test_agent", model=OpenAIChat(id="gpt-4o-mini"))

team = Team(
    members=[agent],
    db=db,
    session_id="team_session_storage",
    add_history_to_context=True,
)

team.print_response("Tell me a new interesting fact about space")
```

---

<a name="db--02_session_summarypy"></a>

### `db/02_session_summary.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.session.summary import SessionSummaryManager  # noqa: F401

# Set up Postgres database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url, session_table="sessions")

# Method 1: Set enable_session_summaries to True

# agent = Agent(
#     model=OpenAIChat(id="gpt-4o-mini"),
#     db=db,
#     enable_session_summaries=True,
#     session_id="session_summary",
#     add_session_summary_to_context=True,
# )

# agent.print_response("Hi my name is John and I live in New York")
# agent.print_response("I like to play basketball and hike in the mountains")

# Method 2: Set session_summary_manager

session_summary_manager = SessionSummaryManager(model=OpenAIChat(id="gpt-4o-mini"))

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="session_summary",
    session_summary_manager=session_summary_manager,
)

agent.print_response("Hi my name is John and I live in New York")
agent.print_response("I like to play basketball and hike in the mountains")
```

---

<a name="db--03_chat_historypy"></a>

### `db/03_chat_history.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="chat_history",
    instructions="You are a helpful assistant that can answer questions about space and oceans.",
    add_history_to_context=True,
)

agent.print_response("Tell me a new interesting fact about space")
print(agent.get_chat_history())

agent.print_response("Tell me a new interesting fact about oceans")
print(agent.get_chat_history())
```

---

<a name="db--dynamodb--dynamo_for_agentpy"></a>

### `db/dynamodb/dynamo_for_agent.py`

```python
"""Use DynamoDb as the database for an agent.

Set the following environment variables to connect to your DynamoDb instance:
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- AWS_REGION

Run `pip install boto3` to install dependencies."""

from agno.agent import Agent
from agno.db import DynamoDb

# Setup the DynamoDB database
db = DynamoDb()

# Setup a basic agent with the DynamoDB database
agent = Agent(db=db)

# The Agent sessions and runs will now be stored in DynamoDB
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="db--dynamodb--dynamo_for_teampy"></a>

### `db/dynamodb/dynamo_for_team.py`

```python
"""Use DynamoDb as the database for a team.

Set the following environment variables to connect to your DynamoDb instance:
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- AWS_REGION

Or pass those parameters when initializing the DynamoDb instance.

Run `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.dynamo import DynamoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

# Setup the DynamoDB database
db = DynamoDb()


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--examples--multi_user_multi_sessionpy"></a>

### `db/examples/multi_user_multi_session.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

db = SqliteDb(db_file="tmp/data.db")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
    num_history_runs=3,
)

user_1_id = "user_101"
user_2_id = "user_102"

user_1_session_id = "session_101"
user_2_session_id = "session_102"

# Start the session with user 1
agent.print_response(
    "Tell me a 5 second short story about a robot.",
    user_id=user_1_id,
    session_id=user_1_session_id,
)
# Continue the session with user 1
agent.print_response(
    "Now tell me a joke.", user_id=user_1_id, session_id=user_1_session_id
)

# Start the session with user 2
agent.print_response(
    "Tell me about quantum physics.", user_id=user_2_id, session_id=user_2_session_id
)
# Continue the session with user 2
agent.print_response(
    "What is the speed of light?", user_id=user_2_id, session_id=user_2_session_id
)

# Ask the agent to give a summary of the conversation, this will use the history from the previous messages
agent.print_response(
    "Give me a summary of our conversation.",
    user_id=user_1_id,
    session_id=user_1_session_id,
)
```

---

<a name="db--examples--selecting_tablespy"></a>

### `db/examples/selecting_tables.py`

```python
"""Use SQLite as the database for an Agent, selecting custom names for the tables.

Run `pip install ddgs sqlalchemy openai` to install dependencies.
"""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb

# Setup the SQLite database
db = SqliteDb(
    db_file="tmp/data.db",
    # Selecting which tables to use
    session_table="agent_sessions",
    memory_table="agent_memories",
    metrics_table="agent_metrics",
)

# Setup a basic agent with the SQLite database
agent = Agent(
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
    add_datetime_to_context=True,
)

# The Agent sessions and runs will now be stored in SQLite
agent.print_response("How many people live in Canada?")
agent.print_response("And in Mexico?")
agent.print_response("List my messages one by one")
```

---

<a name="db--firestore--firestore_for_agentpy"></a>

### `db/firestore/firestore_for_agent.py`

```python
"""
This recipe shows how to store agent sessions in a Firestore database.

Steps:
1. Ensure your gcloud project is enabled with Firestore. Reference https://cloud.google.com/firestore/docs/create-database-server-client-library ?
2. Run: `pip install openai google-cloud-firestore agno` to install dependencies
3. Make sure your gcloud project is set up and you have the necessary permissions to access Firestore
4. Run: `python cookbook/storage/firestore_storage.py` to run the agent
"""

from agno.agent import Agent
from agno.db.firestore import FirestoreDb
from agno.tools.duckduckgo import DuckDuckGoTools

PROJECT_ID = "agno-os-test"  # Use your project ID here

# The only required argument is the collection name.
# Firestore will connect automatically using your google cloud credentials.
# The class uses the (default) database by default to allow free tier access to firestore.
# You can specify a project_id if you'd like to connect to firestore in a different GCP project

# Setup the Firestore database
db = FirestoreDb(project_id=PROJECT_ID)

agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="db--gcs--gcs_json_for_agentpy"></a>

### `db/gcs/gcs_json_for_agent.py`

```python
import uuid

import google.auth
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.gcs_json import GcsJsonDb
from agno.tools.duckduckgo import DuckDuckGoTools

DEBUG_MODE = False
# Obtain the default credentials and project id from your gcloud CLI session.
credentials, project_id = google.auth.default()

# Generate a unique bucket name using a base name and a UUID4 suffix.
base_bucket_name = "example-gcs-bucket"
unique_bucket_name = f"{base_bucket_name}-{uuid.uuid4().hex[:12]}"
print(f"Using bucket: {unique_bucket_name}")

# Initialize GCSJsonDb with explicit credentials, unique bucket name, and project.
db = GcsJsonDb(
    bucket_name=unique_bucket_name,
    prefix="agent/",
    project=project_id,
    credentials=credentials,
)

# Initialize the Agno agent1 with the new storage backend and a DuckDuckGo search tool.
agent1 = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    debug_mode=DEBUG_MODE,
)

# Execute sample queries.
agent1.print_response("How many people live in Canada?")
agent1.print_response("What is their national anthem called?")

# create a new agent and make sure it pursues the conversation
agent2 = Agent(
    db=db,
    session_id=agent1.session_id,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    debug_mode=DEBUG_MODE,
)

agent2.print_response("What's the name of the country we discussed?")
agent2.print_response("What is that country's national sport?")

# After running the agent1, print the content of the bucket: blob names and JSON content.
if DEBUG_MODE:
    print(f"\nBucket {db.bucket_name} contents:")
    sessions = db.get_sessions(session_type=SessionType.AGENT)
    for session in sessions:
        print(f"Session {session.session_id}:\n\t{session.memory}")  # type: ignore
        print("-" * 40)
```

---

<a name="db--in_memory--in_memory_storage_for_agentpy"></a>

### `db/in_memory/in_memory_storage_for_agent.py`

```python
"""Run `pip install ddgs openai` to install dependencies."""

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb

# Setup the in-memory database
db = InMemoryDb()

# Setup the agent and pass the database
agent = Agent(db=db)

# The Agent sessions will now be stored in the in-memory database
agent.print_response("Give me an easy and healthy dinner recipe")
```

---

<a name="db--in_memory--in_memory_storage_for_teampy"></a>

### `db/in_memory/in_memory_storage_for_team.py`

```python
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/in_memory_storage/in_memory_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
)

# Setup the in-memory database
db = InMemoryDb()

# Setup the team and pass the database
hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--in_memory--in_memory_storage_for_workflowpy"></a>

### `db/in_memory/in_memory_storage_for_workflow.py`

```python
"""
Use JSON files as the database for a Workflow.
Useful for simple demos where performance is not critical.

Run `pip install ddgs openai` to install dependencies.
"""

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Setup the in-memory database
db = InMemoryDb()

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=db,
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

---

<a name="db--json--json_for_agentpy"></a>

### `db/json/json_for_agent.py`

```python
"""
Use JSON files as the database for an Agent.
Useful for simple demos where performance is not critical.

Run `pip install ddgs openai` to install dependencies."""

from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the JSON database
db = JsonDb(db_path="tmp/json_db")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="db--json--json_for_teampy"></a>

### `db/json/json_for_team.py`

```python
"""
<<<<<<< HEAD:cookbook/db/json/json_for_team.py
Use JSON files as the database for a Team.
Useful for simple demos where performance is not critical.

Run `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
=======
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/yaml_storage/yaml_storage_for_team.py` to run the team
>>>>>>> 6901605678366bab6617a4cda9d874d8118bef13:cookbook/storage/yaml_storage/yaml_storage_for_team.py
"""

from typing import List

from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

# Setup the JSON database
db = JsonDb(db_path="tmp/json_db")


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--json--json_for_workflowspy"></a>

### `db/json/json_for_workflows.py`

```python
"""
Use JSON files as the database for a Workflow.
Useful for simple demos where performance is not critical.

Run `pip install ddgs openai` to install dependencies.
"""

from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Setup the JSON database
db = JsonDb(db_path="tmp/json_db")

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=db,
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

---

<a name="db--mongo--mongodb_for_agentpy"></a>

### `db/mongo/mongodb_for_agent.py`

```python
"""Use MongoDb as the database for an agent.

Run `pip install openai pymongo` to install dependencies

Run a local MongoDB server using:
```bash
docker run -d \
  --name local-mongo \
  -p 27017:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \
  -e MONGO_INITDB_ROOT_PASSWORD=secret \
  mongo
```
or use our script:
```bash
./scripts/run_mongodb.sh
```
"""

from agno.agent import Agent
from agno.db.mongo import MongoDb
from agno.tools.duckduckgo import DuckDuckGoTools

# MongoDB connection settings
db_url = "mongodb://mongoadmin:secret@localhost:27017"

db = MongoDb(db_url=db_url)

agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="db--mongo--mongodb_for_teampy"></a>

### `db/mongo/mongodb_for_team.py`

```python
"""
Use MongoDb as the database for a team.

Run `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies

Run a local MongoDB server using:
```bash
docker run -d \
  --name local-mongo \
  -p 27017:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \
  -e MONGO_INITDB_ROOT_PASSWORD=secret \
  mongo
```
or use our script:
```bash
./scripts/run_mongodb.sh
"""

from typing import List

from agno.agent import Agent
from agno.db.mongo import MongoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

# MongoDB connection settings
db_url = "mongodb://mongoadmin:secret@localhost:27017"
db = MongoDb(db_url=db_url)


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
    add_member_tools_to_context=False,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--mysql--mysql_for_agentpy"></a>

### `db/mysql/mysql_for_agent.py`

```python
"""Use MySQL as the database for an agent.

Run `pip install openai` to install dependencies."""

from agno.agent import Agent
from agno.db.mysql import MySQLDb

db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"

db = MySQLDb(db_url=db_url)

agent = Agent(
    db=db,
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="db--mysql--mysql_for_teampy"></a>

### `db/mysql/mysql_for_team.py`

```python
"""Use MySQL as the database for a team.

Run `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.mysql import MySQLDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

# MySQL connection settings
db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"
db = MySQLDb(db_url=db_url)


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
    add_member_tools_to_context=False,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--postgres--postgres_for_agentpy"></a>

### `db/postgres/postgres_for_agent.py`

```python
"""Use Postgres as the database for an agent.

Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="db--postgres--postgres_for_teampy"></a>

### `db/postgres/postgres_for_team.py`

```python
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/postgres_storage/postgres_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--postgres--postgres_for_workflowpy"></a>

### `db/postgres/postgres_for_workflow.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=PostgresDb(
            session_table="workflow_session",
            db_url=db_url,
        ),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

---

<a name="db--redis--redis_for_agentpy"></a>

### `db/redis/redis_for_agent.py`

```python
"""
Example showing how to use Redis as the database for an agent.

Run `pip install redis ddgs openai` to install dependencies.

We can start Redis locally using docker:
1. Start Redis container
docker run --name my-redis -p 6379:6379 -d redis

2. Verify container is running
docker ps
"""

from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.redis import RedisDb
from agno.tools.duckduckgo import DuckDuckGoTools

# Initialize Redis db (use the right db_url for your setup)
db = RedisDb(db_url="redis://localhost:6379")

# Create agent with Redis db
agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")

# Verify db contents
print("\nVerifying db contents...")
all_sessions = db.get_sessions(session_type=SessionType.AGENT)
print(f"Total sessions in Redis: {len(all_sessions)}")

if all_sessions:
    print("\nSession details:")
    session = all_sessions[0]
    print(f"The stored session: {session}")
```

---

<a name="db--redis--redis_for_teampy"></a>

### `db/redis/redis_for_team.py`

```python
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/json_storage/json_storage_for_team.py` to run the team

We can start Redis locally using docker:
1. Start Redis container
docker run --name my-redis -p 6379:6379 -d redis

2. Verify container is running
docker ps
"""

from typing import List

from agno.agent import Agent
from agno.db.redis import RedisDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

db = RedisDb(db_url="redis://localhost:6379")


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--redis--redis_for_workflowpy"></a>

### `db/redis/redis_for_workflow.py`

```python
from agno.agent import Agent
from agno.db.redis import RedisDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=RedisDb(
            session_table="workflow_session",
            db_url="redis://localhost:6379",
        ),
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

---

<a name="db--singlestore--singlestore_for_agentpy"></a>

### `db/singlestore/singlestore_for_agent.py`

```python
"""Use SingleStore as the database for an agent.

Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from os import getenv

from agno.agent import Agent
from agno.db.singlestore.singlestore import SingleStoreDb
from agno.tools.duckduckgo import DuckDuckGoTools

# Configure SingleStore DB connection
USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")

db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)

db = SingleStoreDb(db_url=db_url)

# Create an agent with SingleStore db
agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="db--singlestore--singlestore_for_teampy"></a>

### `db/singlestore/singlestore_for_team.py`

```python
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/singlestore_storage/singlestore_storage_for_team.py` to run the team
"""

from os import getenv
from typing import List

from agno.agent import Agent
from agno.db.singlestore.singlestore import SingleStoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

# Configure SingleStore DB connection
USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")
SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)

db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)
db = SingleStoreDb(db_url=db_url)


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--sqlite--sqlite_for_agentpy"></a>

### `db/sqlite/sqlite_for_agent.py`

```python
"""Use SQLite as the database for an Agent.

Run `pip install ddgs sqlalchemy openai` to install dependencies.
"""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the SQLite database
db = SqliteDb(db_file="tmp/data.db")

# Setup a basic agent with the SQLite database
agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    add_datetime_to_context=True,
)

# The Agent sessions and runs will now be stored in SQLite
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem?")
agent.print_response("List my messages one by one")
```

---

<a name="db--sqlite--sqlite_for_teampy"></a>

### `db/sqlite/sqlite_for_team.py`

```python
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/storage/sqlite_storage/sqlite_storage_for_team.py` to run the team
"""

from typing import List

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

db = SqliteDb(db_file="tmp/data.db", session_table="new_sessions_five")


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="db--sqlite--sqlite_for_workflowpy"></a>

### `db/sqlite/sqlite_for_workflow.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db = SqliteDb(db_file="tmp/workflow.db")

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=db,
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

---

<a name="evals--accuracy--accuracy_9_11_bigger_or_9_99py"></a>

### `evals/accuracy/accuracy_9_11_bigger_or_9_99.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Comparison Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CalculatorTools(enable_all=True)],
        instructions="You must use the calculator tools for comparisons.",
    ),
    input="9.11 and 9.9 -- which is bigger?",
    expected_output="9.9",
    additional_guidelines="Its ok for the output to include additional text or information relevant to the comparison.",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

---

<a name="evals--accuracy--accuracy_asyncpy"></a>

### `evals/accuracy/accuracy_async.py`

```python
"""This example shows how to run an Accuracy evaluation asynchronously."""

import asyncio
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CalculatorTools(enable_all=True)],
    ),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
    num_iterations=3,
)

# Run the evaluation calling the arun method.
result: Optional[AccuracyResult] = asyncio.run(evaluation.arun(print_results=True))
assert result is not None and result.avg_score >= 8
```

---

<a name="evals--accuracy--accuracy_basicpy"></a>

### `evals/accuracy/accuracy_basic.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Calculator Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CalculatorTools()],
    ),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
    num_iterations=1,
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

---

<a name="evals--accuracy--accuracy_teampy"></a>

### `evals/accuracy/accuracy_team.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.team.team import Team

# Setup a team with two members
english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="gpt-4o"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You can only answer in Spanish",
    model=OpenAIChat(id="gpt-4o"),
)

multi_language_team = Team(
    name="Multi Language Team",
    model=OpenAIChat("gpt-4o"),
    members=[english_agent, spanish_agent],
    determine_input_for_members=False,
    respond_directly=True,
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English and Spanish.",
        "Always check the language of the user's input before routing to an agent.",
    ],
)

# Evaluate the accuracy of the Team's responses
evaluation = AccuracyEval(
    name="Multi Language Team",
    model=OpenAIChat(id="o4-mini"),
    team=multi_language_team,
    input="Comment allez-vous?",
    expected_output="I can only answer in the following languages: English and Spanish.",
    num_iterations=1,
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

---

<a name="evals--accuracy--accuracy_with_given_answerpy"></a>

### `evals/accuracy/accuracy_with_given_answer.py`

```python
from typing import Optional

from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat

evaluation = AccuracyEval(
    name="Given Answer Evaluation",
    model=OpenAIChat(id="o4-mini"),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
)
result_with_given_answer: Optional[AccuracyResult] = evaluation.run_with_output(
    output="2500", print_results=True
)
assert result_with_given_answer is not None and result_with_given_answer.avg_score >= 8
```

---

<a name="evals--accuracy--accuracy_with_toolspy"></a>

### `evals/accuracy/accuracy_with_tools.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Tools Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools()],
    ),
    input="What is 10!?",
    expected_output="3628800",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

---

<a name="evals--accuracy--db_loggingpy"></a>

### `evals/accuracy/db_logging.py`

```python
"""Example showing how to store evaluation results in the database."""

from typing import Optional

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5432/ai"
db = PostgresDb(db_url=db_url, eval_table="eval_runs_cookbook")


evaluation = AccuracyEval(
    db=db,  # Pass the database to the evaluation. Results will be stored in the database.
    name="Calculator Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[CalculatorTools(enable_all=True)],
    ),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
    num_iterations=1,
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

---

<a name="evals--accuracy--evaluator_agentpy"></a>

### `evals/accuracy/evaluator_agent.py`

```python
"""Example showing how to use a custom evaluator agent to evaluate the accuracy of an Agent."""

from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyAgentResponse, AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

# Setup your evaluator Agent
evaluator_agent = Agent(
    model=OpenAIChat(id="gpt-5"),
    output_schema=AccuracyAgentResponse,  # We want the evaluator agent to return an AccuracyAgentResponse
    # You can provide any additional evaluator instructions here:
    # instructions="",
)

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(model=OpenAIChat(id="gpt-5-mini"), tools=[CalculatorTools()]),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    # Use your evaluator Agent
    evaluator_agent=evaluator_agent,
    # Further adjusting the guidelines
    additional_guidelines="Agent output should include the steps and the final answer.",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

---

<a name="evals--performance--async_functionpy"></a>

### `evals/performance/async_function.py`

```python
"""This example shows how to run a Performance evaluation on an async function."""

import asyncio

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


# Simple async function to run an Agent.
async def arun_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
    )
    response = await agent.arun("What is the capital of France?")
    return response


performance_eval = PerformanceEval(func=arun_agent, num_iterations=10)

# Because we are evaluating an async function, we use the arun method.
asyncio.run(performance_eval.arun(print_summary=True, print_results=True))
```

---

<a name="evals--performance--comparison--autogen_instantiationpy"></a>

### `evals/performance/comparison/autogen_instantiation.py`

```python
"""Run `pip install autogen-agentchat "autogen-ext[openai]"` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient


def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]


def instantiate_agent():
    return AssistantAgent(
        name="assistant",
        model_client=OpenAIChatCompletionClient(
            model="gpt-4o",
            model_info={
                "vision": False,
                "function_calling": True,
                "json_output": False,
                "family": "gpt-4o",
                "structured_output": True,
            },
        ),
        tools=tools,
    )


autogen_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    autogen_instantiation.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--comparison--crewai_instantiationpy"></a>

### `evals/performance/comparison/crewai_instantiation.py`

```python
"""Run `pip install openai crewai` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from crewai.agent import Agent
from crewai.tools import tool


@tool("Tool Name")
def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]


def instantiate_agent():
    return Agent(
        llm="gpt-4o",
        role="Test Agent",
        goal="Be concise, reply with one sentence.",
        tools=tools,
        backstory="Test",
    )


crew_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    crew_instantiation.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--comparison--langgraph_instantiationpy"></a>

### `evals/performance/comparison/langgraph_instantiation.py`

```python
"""Run `pip install langgraph langchain_openai` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent


@tool
def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]


def instantiate_agent():
    return create_react_agent(model=ChatOpenAI(model="gpt-4o"), tools=tools)


langgraph_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    langgraph_instantiation.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--comparison--openai_agents_instantiationpy"></a>

### `evals/performance/comparison/openai_agents_instantiation.py`

```python
"""Run `pip install agents` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval

try:
    from agents import Agent, function_tool
except ImportError:
    raise ImportError(
        "OpenAI agents not installed. Please install it using `pip install agents`."
    )


def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


def instantiate_agent():
    return Agent(
        name="Haiku agent",
        instructions="Always respond in haiku form",
        model="o3-mini",
        tools=[function_tool(get_weather)],
    )


openai_agents_instantiation = PerformanceEval(
    func=instantiate_agent, num_iterations=1000
)

if __name__ == "__main__":
    openai_agents_instantiation.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--comparison--pydantic_ai_instantiationpy"></a>

### `evals/performance/comparison/pydantic_ai_instantiation.py`

```python
"""Run `pip install openai pydantic-ai` to install dependencies."""

from typing import Literal

from agno.eval.performance import PerformanceEval
from pydantic_ai import Agent


def instantiate_agent():
    agent = Agent("openai:gpt-4o", system_prompt="Be concise, reply with one sentence.")

    @agent.tool_plain
    def get_weather(city: Literal["nyc", "sf"]):
        """Use this to get weather information."""
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
        else:
            raise AssertionError("Unknown city")

    return agent


pydantic_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    pydantic_instantiation.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--comparison--smolagents_instantiationpy"></a>

### `evals/performance/comparison/smolagents_instantiation.py`

```python
"""Run `pip install smolagents` to install dependencies."""

from agno.eval.performance import PerformanceEval
from smolagents import InferenceClientModel, Tool, ToolCallingAgent


class WeatherTool(Tool):
    name = "weather_tool"
    description = """
    This is a tool that tells the weather"""
    inputs = {
        "city": {
            "type": "string",
            "description": "The city to look up",
        }
    }
    output_type = "string"

    def forward(self, city: str):
        """Use this to get weather information."""
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
        else:
            raise AssertionError("Unknown city")


def instantiate_agent():
    return ToolCallingAgent(
        tools=[WeatherTool()],
        model=InferenceClientModel(model_id="meta-llama/Llama-3.3-70B-Instruct"),
    )


smolagents_instantiation = PerformanceEval(func=instantiate_agent, num_iterations=1000)

if __name__ == "__main__":
    smolagents_instantiation.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--db_loggingpy"></a>

### `evals/performance/db_logging.py`

```python
"""Example showing how to store evaluation results in the database."""

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


# Simple function to run an agent which performance we will evaluate
def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
    )
    response = agent.run("What is the capital of France?")
    print(response.content)
    return response


# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5432/ai"
db = PostgresDb(db_url=db_url, eval_table="eval_runs_cookbook")

simple_response_perf = PerformanceEval(
    db=db,  # Pass the database to the evaluation. Results will be stored in the database.
    name="Simple Performance Evaluation",
    func=run_agent,
    num_iterations=1,
    warmup_runs=0,
)

if __name__ == "__main__":
    simple_response_perf.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--instantiation_agentpy"></a>

### `evals/performance/instantiation_agent.py`

```python
"""Run `pip install agno openai` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval


def instantiate_agent():
    return Agent(system_message="Be concise, reply with one sentence.")


instantiation_perf = PerformanceEval(
    name="Instantiation Performance", func=instantiate_agent, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--instantiation_agent_with_toolpy"></a>

### `evals/performance/instantiation_agent_with_tool.py`

```python
"""Run `pip install agno openai memory_profiler` to install dependencies."""

from typing import Literal

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"


tools = [get_weather]


def instantiate_agent():
    return Agent(model=OpenAIChat(id="gpt-4o"), tools=tools)  # type: ignore


instantiation_perf = PerformanceEval(
    name="Tool Instantiation Performance", func=instantiate_agent, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--instantiation_teampy"></a>

### `evals/performance/instantiation_team.py`

```python
"""Run `pip install agno openai` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat
from agno.team.team import Team

team_member = Agent(model=OpenAIChat(id="gpt-4o"))


def instantiate_team():
    return Team(members=[team_member])


instantiation_perf = PerformanceEval(
    name="Instantiation Performance Team", func=instantiate_team, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--response_with_memory_updatespy"></a>

### `evals/performance/response_with_memory_updates.py`

```python
"""Run `pip install openai agno memory_profiler` to install dependencies."""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat

# Memory creation requires a db to be provided
db = SqliteDb(db_file="tmp/memory.db")


def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
        db=db,
        enable_user_memories=True,
    )

    response = agent.run("My name is Tom! I'm 25 years old and I live in New York.")
    print(f"Agent response: {response.content}")

    return response


response_with_memory_updates_perf = PerformanceEval(
    name="Memory Updates Performance",
    func=run_agent,
    num_iterations=5,
    warmup_runs=0,
)

if __name__ == "__main__":
    response_with_memory_updates_perf.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--response_with_storagepy"></a>

### `evals/performance/response_with_storage.py`

```python
"""Run `pip install openai agno` to install dependencies."""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat

db = SqliteDb(db_file="tmp/storage.db")


def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
        add_history_to_context=True,
    )
    response_1 = agent.run("What is the capital of France?")
    print(response_1.content)

    response_2 = agent.run("How many people live there?")
    print(response_2.content)

    return response_2.content


response_with_storage_perf = PerformanceEval(
    name="Storage Performance",
    func=run_agent,
    num_iterations=1,
    warmup_runs=0,
)

if __name__ == "__main__":
    response_with_storage_perf.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--simple_responsepy"></a>

### `evals/performance/simple_response.py`

```python
"""Run `pip install openai agno memory_profiler` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat


def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        system_message="Be concise, reply with one sentence.",
    )

    response = agent.run("What is the capital of France?")
    print(f"Agent response: {response.content}")

    return response


simple_response_perf = PerformanceEval(
    name="Simple Performance Evaluation",
    func=run_agent,
    num_iterations=1,
    warmup_runs=0,
)

if __name__ == "__main__":
    simple_response_perf.run(print_results=True, print_summary=True)
```

---

<a name="evals--performance--team_response_with_memory_and_reasoningpy"></a>

### `evals/performance/team_response_with_memory_and_reasoning.py`

```python
import asyncio
import random
import uuid

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIResponses
from agno.team.team import Team
from agno.tools.reasoning import ReasoningTools

users = [
    "abel@example.com",
    "ben@example.com",
    "charlie@example.com",
    "dave@example.com",
    "edward@example.com",
]

cities = [
    "New York",
    "Los Angeles",
    "Chicago",
    "Houston",
    "Miami",
    "San Francisco",
    "Seattle",
    "Boston",
    "Washington D.C.",
    "Atlanta",
    "Denver",
    "Las Vegas",
]


# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)


def get_weather(city: str) -> str:
    """Get detailed weather information for a city."""
    weather_conditions = {
        "New York": {
            "current": "Partly cloudy with scattered showers",
            "temperature": "72F (22C)",
            "humidity": "65%",
            "wind": "12 mph from the northwest",
            "visibility": "10 miles",
            "pressure": "30.15 inches",
            "uv_index": "Moderate (5)",
            "sunrise": "6:45 AM",
            "sunset": "7:30 PM",
            "forecast": {
                "today": "High 75F, Low 58F with afternoon thunderstorms",
                "tomorrow": "High 78F, Low 62F, mostly sunny",
                "weekend": "High 82F, Low 65F, clear skies",
            },
            "air_quality": "Good (AQI: 45)",
            "pollen_count": "Moderate",
            "marine_conditions": "Waves 2-3 feet, water temperature 68F",
        },
        "Los Angeles": {
            "current": "Sunny and clear",
            "temperature": "85F (29C)",
            "humidity": "45%",
            "wind": "8 mph from the west",
            "visibility": "15 miles",
            "pressure": "30.05 inches",
            "uv_index": "Very High (9)",
            "sunrise": "6:30 AM",
            "sunset": "7:45 PM",
            "forecast": {
                "today": "High 88F, Low 65F, sunny throughout",
                "tomorrow": "High 86F, Low 63F, morning fog then sunny",
                "weekend": "High 90F, Low 68F, clear and warm",
            },
            "air_quality": "Moderate (AQI: 78)",
            "pollen_count": "High",
            "marine_conditions": "Waves 3-4 feet, water temperature 72F",
        },
        "Chicago": {
            "current": "Overcast with light drizzle",
            "temperature": "58F (14C)",
            "humidity": "78%",
            "wind": "18 mph from the northeast",
            "visibility": "6 miles",
            "pressure": "29.85 inches",
            "uv_index": "Low (2)",
            "sunrise": "7:15 AM",
            "sunset": "6:45 PM",
            "forecast": {
                "today": "High 62F, Low 48F, rain likely",
                "tomorrow": "High 65F, Low 52F, partly cloudy",
                "weekend": "High 70F, Low 55F, sunny intervals",
            },
            "air_quality": "Good (AQI: 52)",
            "pollen_count": "Low",
            "marine_conditions": "Waves 4-6 feet, water temperature 55F",
        },
        "Houston": {
            "current": "Hot and humid with scattered clouds",
            "temperature": "92F (33C)",
            "humidity": "75%",
            "wind": "10 mph from the southeast",
            "visibility": "8 miles",
            "pressure": "30.10 inches",
            "uv_index": "Extreme (11)",
            "sunrise": "6:45 AM",
            "sunset": "8:00 PM",
            "forecast": {
                "today": "High 94F, Low 76F, chance of afternoon storms",
                "tomorrow": "High 96F, Low 78F, hot and humid",
                "weekend": "High 98F, Low 80F, isolated thunderstorms",
            },
            "air_quality": "Moderate (AQI: 85)",
            "pollen_count": "Very High",
            "marine_conditions": "Waves 1-2 feet, water temperature 82F",
        },
        "Miami": {
            "current": "Partly cloudy with high humidity",
            "temperature": "88F (31C)",
            "humidity": "82%",
            "wind": "15 mph from the east",
            "visibility": "12 miles",
            "pressure": "30.20 inches",
            "uv_index": "Very High (10)",
            "sunrise": "6:30 AM",
            "sunset": "8:15 PM",
            "forecast": {
                "today": "High 90F, Low 78F, afternoon showers likely",
                "tomorrow": "High 89F, Low 77F, partly sunny",
                "weekend": "High 92F, Low 79F, scattered thunderstorms",
            },
            "air_quality": "Good (AQI: 48)",
            "pollen_count": "Moderate",
            "marine_conditions": "Waves 2-3 feet, water temperature 85F",
        },
        "San Francisco": {
            "current": "Foggy and cool",
            "temperature": "62F (17C)",
            "humidity": "85%",
            "wind": "20 mph from the west",
            "visibility": "3 miles",
            "pressure": "30.00 inches",
            "uv_index": "Low (3)",
            "sunrise": "6:45 AM",
            "sunset": "7:30 PM",
            "forecast": {
                "today": "High 65F, Low 55F, fog clearing by afternoon",
                "tomorrow": "High 68F, Low 58F, partly cloudy",
                "weekend": "High 72F, Low 60F, sunny and mild",
            },
            "air_quality": "Good (AQI: 42)",
            "pollen_count": "Low",
            "marine_conditions": "Waves 5-7 feet, water temperature 58F",
        },
        "Seattle": {
            "current": "Light rain with overcast skies",
            "temperature": "55F (13C)",
            "humidity": "88%",
            "wind": "12 mph from the southwest",
            "visibility": "4 miles",
            "pressure": "29.95 inches",
            "uv_index": "Low (1)",
            "sunrise": "7:00 AM",
            "sunset": "6:30 PM",
            "forecast": {
                "today": "High 58F, Low 48F, rain throughout the day",
                "tomorrow": "High 62F, Low 50F, showers likely",
                "weekend": "High 65F, Low 52F, partly cloudy",
            },
            "air_quality": "Good (AQI: 38)",
            "pollen_count": "Low",
            "marine_conditions": "Waves 3-5 feet, water temperature 52F",
        },
        "Boston": {
            "current": "Clear and crisp",
            "temperature": "68F (20C)",
            "humidity": "55%",
            "wind": "14 mph from the northwest",
            "visibility": "12 miles",
            "pressure": "30.25 inches",
            "uv_index": "Moderate (6)",
            "sunrise": "6:30 AM",
            "sunset": "7:15 PM",
            "forecast": {
                "today": "High 72F, Low 58F, sunny and pleasant",
                "tomorrow": "High 75F, Low 62F, mostly sunny",
                "weekend": "High 78F, Low 65F, clear skies",
            },
            "air_quality": "Good (AQI: 55)",
            "pollen_count": "Moderate",
            "marine_conditions": "Waves 2-4 feet, water temperature 62F",
        },
        "Washington D.C.": {
            "current": "Partly sunny with mild temperatures",
            "temperature": "75F (24C)",
            "humidity": "60%",
            "wind": "10 mph from the west",
            "visibility": "10 miles",
            "pressure": "30.15 inches",
            "uv_index": "High (7)",
            "sunrise": "6:45 AM",
            "sunset": "7:30 PM",
            "forecast": {
                "today": "High 78F, Low 62F, partly cloudy",
                "tomorrow": "High 80F, Low 65F, sunny intervals",
                "weekend": "High 82F, Low 68F, clear and warm",
            },
            "air_quality": "Moderate (AQI: 72)",
            "pollen_count": "High",
            "marine_conditions": "Waves 1-2 feet, water temperature 70F",
        },
        "Atlanta": {
            "current": "Warm and humid with scattered clouds",
            "temperature": "82F (28C)",
            "humidity": "70%",
            "wind": "8 mph from the south",
            "visibility": "9 miles",
            "pressure": "30.05 inches",
            "uv_index": "High (8)",
            "sunrise": "6:30 AM",
            "sunset": "8:00 PM",
            "forecast": {
                "today": "High 85F, Low 68F, chance of afternoon storms",
                "tomorrow": "High 87F, Low 70F, hot and humid",
                "weekend": "High 90F, Low 72F, isolated thunderstorms",
            },
            "air_quality": "Moderate (AQI: 68)",
            "pollen_count": "Very High",
            "marine_conditions": "Waves 1-2 feet, water temperature 75F",
        },
        "Denver": {
            "current": "Sunny and dry",
            "temperature": "78F (26C)",
            "humidity": "25%",
            "wind": "15 mph from the west",
            "visibility": "20 miles",
            "pressure": "24.85 inches",
            "uv_index": "Very High (9)",
            "sunrise": "6:15 AM",
            "sunset": "7:45 PM",
            "forecast": {
                "today": "High 82F, Low 55F, sunny and clear",
                "tomorrow": "High 85F, Low 58F, mostly sunny",
                "weekend": "High 88F, Low 62F, clear skies",
            },
            "air_quality": "Good (AQI: 45)",
            "pollen_count": "Moderate",
            "marine_conditions": "N/A - Landlocked location",
        },
        "Las Vegas": {
            "current": "Hot and dry with clear skies",
            "temperature": "95F (35C)",
            "humidity": "15%",
            "wind": "12 mph from the southwest",
            "visibility": "25 miles",
            "pressure": "29.95 inches",
            "uv_index": "Extreme (11)",
            "sunrise": "6:00 AM",
            "sunset": "8:00 PM",
            "forecast": {
                "today": "High 98F, Low 75F, sunny and hot",
                "tomorrow": "High 100F, Low 78F, clear and very hot",
                "weekend": "High 102F, Low 80F, extreme heat",
            },
            "air_quality": "Moderate (AQI: 82)",
            "pollen_count": "Low",
            "marine_conditions": "N/A - Desert location",
        },
    }

    if city not in weather_conditions:
        return f"Weather data for {city} is not available in our database."

    weather = weather_conditions[city]

    return f"""
# Comprehensive Weather Report for {city}

## Current Conditions
- **Temperature**: {weather["temperature"]}
- **Conditions**: {weather["current"]}
- **Humidity**: {weather["humidity"]}
- **Wind**: {weather["wind"]}
- **Visibility**: {weather["visibility"]}
- **Pressure**: {weather["pressure"]}
- **UV Index**: {weather["uv_index"]}

## Daily Schedule
- **Sunrise**: {weather["sunrise"]}
- **Sunset**: {weather["sunset"]}

## Extended Forecast
- **Today**: {weather["forecast"]["today"]}
- **Tomorrow**: {weather["forecast"]["tomorrow"]}
- **Weekend**: {weather["forecast"]["weekend"]}

## Environmental Conditions
- **Air Quality**: {weather["air_quality"]}
- **Pollen Count**: {weather["pollen_count"]}
- **Marine Conditions**: {weather["marine_conditions"]}

## Weather Advisory
Based on current conditions, visitors to {city} should be prepared for {weather["current"].lower()}. The UV index of {weather["uv_index"]} indicates {"sun protection is essential" if "High" in weather["uv_index"] or "Very High" in weather["uv_index"] or "Extreme" in weather["uv_index"] else "moderate sun protection recommended"}. {"High humidity may make temperatures feel warmer than actual readings." if int(weather["humidity"].replace("%", "")) > 70 else "Comfortable humidity levels are expected."}

## Travel Recommendations
- **Best Time for Outdoor Activities**: {"Early morning or late afternoon to avoid peak heat" if int(weather["temperature"].split("")[0]) > 85 else "Any time during daylight hours"}
- **Clothing Suggestions**: {"Light, breathable clothing recommended" if int(weather["temperature"].split("")[0]) > 80 else "Comfortable clothing suitable for current temperatures"}
- **Hydration**: {"Stay well-hydrated due to high temperatures" if int(weather["temperature"].split("")[0]) > 85 else "Normal hydration levels recommended"}

This comprehensive weather report provides all the essential information needed for planning activities and ensuring comfort during your visit to {city}.
"""


def get_activities(city: str) -> str:
    """Get detailed activity information for a city."""
    city_activities = {
        "New York": {
            "outdoor": [
                "Central Park walking tours and picnics",
                "Brooklyn Bridge sunset walks",
                "High Line elevated park exploration",
                "Battery Park waterfront activities",
                "Prospect Park nature trails",
                "Governors Island weekend visits",
                "Riverside Park cycling paths",
                "Bryant Park seasonal activities",
            ],
            "cultural": [
                "Metropolitan Museum of Art comprehensive tours",
                "Museum of Modern Art (MoMA) exhibitions",
                "American Museum of Natural History dinosaur exhibits",
                "Broadway theater performances",
                "Lincoln Center performing arts",
                "Guggenheim Museum architecture and art",
                "Whitney Museum of American Art",
                "Brooklyn Museum cultural exhibits",
            ],
            "entertainment": [
                "Times Square nightlife and entertainment",
                "Empire State Building observation deck",
                "Statue of Liberty and Ellis Island tours",
                "Rockefeller Center ice skating",
                "Madison Square Garden events",
                "Radio City Music Hall shows",
                "Carnegie Hall classical concerts",
                "Comedy Cellar stand-up comedy",
            ],
            "shopping": [
                "Fifth Avenue luxury shopping district",
                "SoHo boutique shopping experience",
                "Chelsea Market food and crafts",
                "Brooklyn Flea Market vintage finds",
                "Union Square Greenmarket farmers market",
                "Century 21 discount designer shopping",
                "Bergdorf Goodman luxury department store",
                "ABC Carpet & Home home decor",
            ],
            "dining": [
                "Katz's Delicatessen pastrami sandwiches",
                "Peter Luger Steak House classic steaks",
                "Joe's Pizza authentic New York slices",
                "Russ & Daughters Jewish delicatessen",
                "Gramercy Tavern farm-to-table dining",
                "Le Bernardin seafood excellence",
                "Momofuku Noodle Bar Asian fusion",
                "Magnolia Bakery cupcakes and desserts",
            ],
        },
        "Los Angeles": {
            "outdoor": [
                "Griffith Observatory hiking and city views",
                "Venice Beach boardwalk and muscle beach",
                "Runyon Canyon Park dog-friendly hiking",
                "Santa Monica Pier and beach activities",
                "Malibu beach surfing and swimming",
                "Echo Park Lake paddle boating",
                "Griffith Park horseback riding",
                "Topanga State Park wilderness trails",
            ],
            "cultural": [
                "Getty Center art museum and gardens",
                "Los Angeles County Museum of Art (LACMA)",
                "Hollywood Walk of Fame star hunting",
                "Universal Studios Hollywood theme park",
                "Warner Bros. Studio Tour",
                "Natural History Museum dinosaur exhibits",
                "California Science Center space shuttle",
                "The Broad contemporary art museum",
            ],
            "entertainment": [
                "Disneyland Resort theme park adventure",
                "Hollywood Bowl outdoor concerts",
                "Dodger Stadium baseball games",
                "Staples Center Lakers basketball",
                "Comedy Store stand-up comedy",
                "Roxy Theatre live music venue",
                "Greek Theatre outdoor amphitheater",
                "TCL Chinese Theatre movie premieres",
            ],
            "shopping": [
                "Rodeo Drive luxury shopping experience",
                "The Grove outdoor shopping center",
                "Melrose Avenue trendy boutiques",
                "Beverly Center mall shopping",
                "Abbot Kinney Boulevard unique shops",
                "Third Street Promenade Santa Monica",
                "Glendale Galleria shopping complex",
                "Fashion District wholesale shopping",
            ],
            "dining": [
                "In-N-Out Burger classic California burgers",
                "Pink's Hot Dogs Hollywood institution",
                "Philippe the Original French dip sandwiches",
                "Musso & Frank Grill classic Hollywood dining",
                "Nobu Los Angeles celebrity sushi spot",
                "Gjelina Venice Beach farm-to-table",
                "Animal Restaurant innovative cuisine",
                "Bottega Louie Italian pastries and dining",
            ],
        },
        "Chicago": {
            "outdoor": [
                "Millennium Park Cloud Gate sculpture",
                "Navy Pier lakefront entertainment",
                "Grant Park Buckingham Fountain",
                "Lincoln Park Zoo free admission",
                "Lake Michigan beach activities",
                "Chicago Riverwalk scenic strolls",
                "Maggie Daley Park family activities",
                "606 elevated trail cycling",
            ],
            "cultural": [
                "Art Institute of Chicago world-class art",
                "Field Museum natural history exhibits",
                "Shedd Aquarium marine life displays",
                "Adler Planetarium astronomy shows",
                "Museum of Science and Industry hands-on exhibits",
                "Chicago History Museum local heritage",
                "National Museum of Mexican Art",
                "DuSable Museum of African American History",
            ],
            "entertainment": [
                "Willis Tower Skydeck observation deck",
                "Wrigley Field Cubs baseball games",
                "United Center Bulls basketball",
                "Second City comedy theater",
                "Chicago Theatre historic venue",
                "Arie Crown Theater performances",
                "House of Blues live music",
                "Blue Man Group theatrical experience",
            ],
            "shopping": [
                "Magnificent Mile luxury shopping district",
                "Water Tower Place shopping center",
                "State Street retail corridor",
                "Oak Street designer boutiques",
                "Michigan Avenue shopping experience",
                "Wicker Park trendy shops",
                "Andersonville unique stores",
                "Lincoln Square German heritage shopping",
            ],
            "dining": [
                "Giordano's deep dish pizza",
                "Portillo's Chicago-style hot dogs",
                "Lou Malnati's authentic deep dish",
                "Al's Beef Italian beef sandwiches",
                "Billy Goat Tavern historic bar",
                "Girl & the Goat innovative cuisine",
                "Alinea molecular gastronomy",
                "Au Cheval gourmet burgers",
            ],
        },
        "Houston": {
            "outdoor": [
                "Buffalo Bayou Park urban nature trails",
                "Hermann Park Japanese Garden",
                "Discovery Green downtown activities",
                "Memorial Park extensive hiking trails",
                "Houston Arboretum nature education",
                "Rice University campus walking tours",
                "Sam Houston Park historic buildings",
                "Eleanor Tinsley Park bayou views",
            ],
            "cultural": [
                "Museum of Fine Arts Houston",
                "Houston Museum of Natural Science",
                "Children's Museum of Houston",
                "Contemporary Arts Museum Houston",
                "Holocaust Museum Houston",
                "Buffalo Soldiers National Museum",
                "Asia Society Texas Center",
                "Houston Center for Photography",
            ],
            "entertainment": [
                "Space Center Houston NASA exhibits",
                "Houston Zoo animal encounters",
                "Miller Outdoor Theatre free performances",
                "Toyota Center Rockets basketball",
                "Minute Maid Park Astros baseball",
                "NRG Stadium Texans football",
                "House of Blues Houston live music",
                "Jones Hall performing arts",
            ],
            "shopping": [
                "Galleria Mall luxury shopping complex",
                "River Oaks District upscale retail",
                "Rice Village boutique shopping",
                "Memorial City Mall family shopping",
                "Katy Mills outlet shopping",
                "Houston Premium Outlets",
                "Baybrook Mall suburban shopping",
                "Willowbrook Mall northwest shopping",
            ],
            "dining": [
                "Pappas Bros. Steakhouse premium steaks",
                "Killen's Barbecue Texas BBQ",
                "Ninfa's on Navigation Tex-Mex",
                "Goode Company Seafood Gulf Coast",
                "Hugo's upscale Mexican cuisine",
                "Uchi Houston sushi excellence",
                "Underbelly Houston Southern cuisine",
                "Truth BBQ award-winning barbecue",
            ],
        },
        "Miami": {
            "outdoor": [
                "South Beach Art Deco walking tours",
                "Vizcaya Museum and Gardens",
                "Biscayne Bay water activities",
                "Crandon Park beach and tennis",
                "Fairchild Tropical Botanic Garden",
                "Matheson Hammock Park natural areas",
                "Bill Baggs Cape Florida State Park",
                "Oleta River State Park kayaking",
            ],
            "cultural": [
                "Prez Art Museum Miami contemporary art",
                "Vizcaya Museum and Gardens historic estate",
                "Frost Science Museum interactive exhibits",
                "HistoryMiami Museum local heritage",
                "Jewish Museum of Florida",
                "Coral Gables Museum architecture",
                "Lowe Art Museum University of Miami",
                "Bass Museum of Art contemporary",
            ],
            "entertainment": [
                "Wynwood Walls street art district",
                "Little Havana cultural experience",
                "Bayside Marketplace waterfront shopping",
                "American Airlines Arena Heat basketball",
                "Hard Rock Stadium Dolphins football",
                "Marlins Park baseball games",
                "Fillmore Miami Beach live music",
                "Adrienne Arsht Center performing arts",
            ],
            "shopping": [
                "Lincoln Road Mall outdoor shopping",
                "Brickell City Centre luxury retail",
                "Aventura Mall largest shopping center",
                "Bal Harbour Shops upscale boutiques",
                "Dolphin Mall outlet shopping",
                "Sawgrass Mills outlet complex",
                "Merrick Park Coral Gables shopping",
                "CocoWalk Coconut Grove retail",
            ],
            "dining": [
                "Joe's Stone Crab Miami Beach institution",
                "Versailles Restaurant Cuban cuisine",
                "Garcia's Seafood Grille fresh seafood",
                "Yardbird Southern Table & Bar",
                "Zuma Miami Japanese izakaya",
                "Nobu Miami Beach celebrity dining",
                "Prime 112 steakhouse excellence",
                "La Sandwicherie French sandwiches",
            ],
        },
        "San Francisco": {
            "outdoor": [
                "Golden Gate Bridge walking and cycling",
                "Alcatraz Island historic prison tour",
                "Fisherman's Wharf waterfront activities",
                "Golden Gate Park extensive gardens",
                "Lands End coastal hiking trails",
                "Twin Peaks panoramic city views",
                "Crissy Field beach and recreation",
                "Angel Island State Park hiking",
            ],
            "cultural": [
                "de Young Museum fine arts",
                "San Francisco Museum of Modern Art",
                "California Academy of Sciences",
                "Exploratorium interactive science",
                "Asian Art Museum comprehensive collection",
                "Legion of Honor European art",
                "Contemporary Jewish Museum",
                "Walt Disney Family Museum",
            ],
            "entertainment": [
                "Pier 39 sea lions and attractions",
                "Oracle Park Giants baseball",
                "Chase Center Warriors basketball",
                "AT&T Park waterfront stadium",
                "Fillmore Auditorium live music",
                "Warfield Theatre historic venue",
                "Great American Music Hall",
                "SFJAZZ Center jazz performances",
            ],
            "shopping": [
                "Union Square luxury shopping district",
                "Fisherman's Wharf tourist shopping",
                "Haight-Ashbury vintage clothing",
                "North Beach Italian neighborhood",
                "Chestnut Street boutique shopping",
                "Fillmore Street upscale retail",
                "Valencia Street Mission District",
                "Grant Avenue Chinatown shopping",
            ],
            "dining": [
                "Tartine Bakery artisanal breads",
                "Zuni Caf California cuisine",
                "Swan Oyster Depot seafood counter",
                "House of Prime Rib classic steaks",
                "Gary Danko fine dining experience",
                "State Bird Provisions innovative",
                "Tadich Grill historic seafood",
                "Boudin Bakery sourdough bread",
            ],
        },
        "Seattle": {
            "outdoor": [
                "Pike Place Market waterfront activities",
                "Space Needle observation deck",
                "Olympic Sculpture Park waterfront art",
                "Discovery Park extensive hiking trails",
                "Green Lake Park walking and cycling",
                "Kerry Park panoramic city views",
                "Alki Beach West Seattle activities",
                "Washington Park Arboretum gardens",
            ],
            "cultural": [
                "Seattle Art Museum comprehensive collection",
                "Museum of Pop Culture (MoPOP)",
                "Chihuly Garden and Glass blown glass art",
                "Seattle Aquarium marine life",
                "Wing Luke Museum Asian American history",
                "Museum of Flight aviation history",
                "Frye Art Museum free admission",
                "Nordic Heritage Museum Scandinavian",
            ],
            "entertainment": [
                "CenturyLink Field Seahawks football",
                "T-Mobile Park Mariners baseball",
                "Climate Pledge Arena Kraken hockey",
                "Paramount Theatre historic venue",
                "Showbox at the Market live music",
                "Neptune Theatre University District",
                "Moore Theatre downtown venue",
                "Crocodile Caf intimate music venue",
            ],
            "shopping": [
                "Pike Place Market local crafts and food",
                "Westlake Center downtown shopping",
                "University Village upscale retail",
                "Bellevue Square eastside shopping",
                "Northgate Mall north Seattle",
                "Southcenter Mall south Seattle",
                "Alderwood Mall north suburbs",
                "Redmond Town Center eastside retail",
            ],
            "dining": [
                "Pike Place Chowder award-winning chowder",
                "Canlis fine dining institution",
                "Salumi Artisan Cured Meats",
                "Tilth organic farm-to-table",
                "The Walrus and the Carpenter oysters",
                "Paseo Caribbean sandwiches",
                "Molly Moon's Homemade Ice Cream",
                "Top Pot Doughnuts hand-forged doughnuts",
            ],
        },
        "Boston": {
            "outdoor": [
                "Freedom Trail historic walking tour",
                "Boston Common and Public Garden",
                "Charles River Esplanade walking",
                "Boston Harbor Islands ferry trips",
                "Emerald Necklace park system",
                "Castle Island South Boston waterfront",
                "Arnold Arboretum Harvard University",
                "Jamaica Pond walking and boating",
            ],
            "cultural": [
                "Museum of Fine Arts Boston",
                "Isabella Stewart Gardner Museum",
                "Boston Tea Party Ships & Museum",
                "John F. Kennedy Presidential Library",
                "Museum of Science interactive exhibits",
                "New England Aquarium marine life",
                "Institute of Contemporary Art",
                "Boston Children's Museum family",
            ],
            "entertainment": [
                "Fenway Park Red Sox baseball",
                "TD Garden Celtics basketball",
                "Boston Symphony Orchestra",
                "Boston Opera House performances",
                "House of Blues Boston live music",
                "Paradise Rock Club intimate venue",
                "Orpheum Theatre historic venue",
                "Wang Theatre performing arts",
            ],
            "shopping": [
                "Faneuil Hall Marketplace historic shopping",
                "Newbury Street boutique shopping",
                "Copley Place luxury retail",
                "Prudential Center shopping complex",
                "Assembly Row outlet shopping",
                "Natick Mall suburban shopping",
                "Burlington Mall north suburbs",
                "South Shore Plaza south suburbs",
            ],
            "dining": [
                "Legal Sea Foods fresh seafood",
                "Union Oyster House historic restaurant",
                "Mike's Pastry Italian pastries",
                "Neptune Oyster fresh oysters",
                "Giacomo's Ristorante Italian cuisine",
                "Flour Bakery + Caf artisanal pastries",
                "Santarpio's Pizza East Boston",
                "Kelly's Roast Beef North Shore",
            ],
        },
        "Washington D.C.": {
            "outdoor": [
                "National Mall monuments and memorials",
                "Tidal Basin cherry blossom viewing",
                "Rock Creek Park extensive trails",
                "Georgetown Waterfront Park",
                "East Potomac Park golf and recreation",
                "Kenilworth Aquatic Gardens",
                "C&O Canal National Historical Park",
                "Great Falls Park Virginia side",
            ],
            "cultural": [
                "Smithsonian Institution museums",
                "National Gallery of Art",
                "United States Holocaust Memorial Museum",
                "National Museum of African American History",
                "Library of Congress largest library",
                "National Archives historical documents",
                "International Spy Museum",
                "Newseum journalism museum",
            ],
            "entertainment": [
                "Capitol Building guided tours",
                "White House visitor center",
                "Arlington National Cemetery",
                "Kennedy Center performing arts",
                "National Theatre historic venue",
                "9:30 Club live music venue",
                "The Anthem waterfront venue",
                "Wolf Trap performing arts center",
            ],
            "shopping": [
                "Georgetown historic shopping district",
                "Union Market food and crafts",
                "Tysons Corner Center Virginia",
                "Pentagon City Mall Arlington",
                "Potomac Mills outlet shopping",
                "National Harbor waterfront retail",
                "CityCenterDC luxury shopping",
                "Eastern Market Capitol Hill",
            ],
            "dining": [
                "Ben's Chili Bowl Washington institution",
                "Old Ebbitt Grill historic restaurant",
                "Founding Farmers farm-to-table",
                "Rasika modern Indian cuisine",
                "Le Diplomate French bistro",
                "Rose's Luxury innovative American",
                "Komi Mediterranean fine dining",
                "Toki Underground ramen noodles",
            ],
        },
        "Atlanta": {
            "outdoor": [
                "Piedmont Park extensive recreation",
                "Atlanta BeltLine walking and cycling",
                "Stone Mountain Park hiking",
                "Chattahoochee River National Recreation Area",
                "Atlanta Botanical Garden",
                "Grant Park Zoo Atlanta",
                "Centennial Olympic Park",
                "Chastain Park amphitheater and trails",
            ],
            "cultural": [
                "High Museum of Art",
                "Atlanta History Center",
                "Martin Luther King Jr. National Historical Park",
                "Fernbank Museum of Natural History",
                "Center for Civil and Human Rights",
                "Atlanta Contemporary Art Center",
                "Michael C. Carlos Museum Emory",
                "Spelman College Museum of Fine Art",
            ],
            "entertainment": [
                "World of Coca-Cola museum",
                "Georgia Aquarium marine life",
                "Mercedes-Benz Stadium Falcons football",
                "Truist Park Braves baseball",
                "State Farm Arena Hawks basketball",
                "Fox Theatre historic venue",
                "Tabernacle live music venue",
                "Variety Playhouse intimate concerts",
            ],
            "shopping": [
                "Lenox Square luxury shopping",
                "Phipps Plaza upscale retail",
                "Atlantic Station mixed-use development",
                "Ponce City Market food hall and shops",
                "Krog Street Market food and retail",
                "Buckhead Village boutique shopping",
                "Virginia-Highland unique stores",
                "Little Five Points alternative shopping",
            ],
            "dining": [
                "The Varsity classic drive-in",
                "Mary Mac's Tea Room Southern cuisine",
                "Fox Bros. Bar-B-Q Texas-style barbecue",
                "Bacchanalia fine dining experience",
                "Miller Union farm-to-table",
                "Staplehouse innovative American",
                "Gunshow creative Southern cuisine",
                "Atlanta Fish Market fresh seafood",
            ],
        },
        "Denver": {
            "outdoor": [
                "Red Rocks Park and Amphitheatre",
                "Rocky Mountain National Park hiking",
                "Denver Botanic Gardens",
                "City Park walking and cycling",
                "Washington Park recreation",
                "Cherry Creek State Park",
                "Mount Evans Scenic Byway",
                "Garden of the Gods Colorado Springs",
            ],
            "cultural": [
                "Denver Art Museum",
                "Denver Museum of Nature & Science",
                "Clyfford Still Museum",
                "Museum of Contemporary Art Denver",
                "History Colorado Center",
                "Black American West Museum",
                "Mizel Museum Jewish culture",
                "Kirkland Museum of Fine & Decorative Art",
            ],
            "entertainment": [
                "Coors Field Rockies baseball",
                "Empower Field at Mile High Broncos football",
                "Ball Arena Nuggets basketball",
                "Red Rocks Amphitheatre concerts",
                "Ogden Theatre live music",
                "Bluebird Theatre intimate venue",
                "Fillmore Auditorium historic venue",
                "Paramount Theatre performing arts",
            ],
            "shopping": [
                "Cherry Creek Shopping Center",
                "Larimer Square historic shopping",
                "16th Street Mall pedestrian shopping",
                "Park Meadows Mall south Denver",
                "Flatiron Crossing Broomfield",
                "Aspen Grove Littleton",
                "Belmar Lakewood shopping",
                "Southlands Aurora retail",
            ],
            "dining": [
                "Casa Bonita Mexican restaurant",
                "Buckhorn Exchange historic steakhouse",
                "Snooze an A.M. Eatery breakfast",
                "Linger rooftop dining",
                "Root Down farm-to-table",
                "Fruition Restaurant fine dining",
                "Acorn at The Source market hall",
                "Work & Class contemporary American",
            ],
        },
        "Las Vegas": {
            "outdoor": [
                "Red Rock Canyon National Conservation Area",
                "Valley of Fire State Park",
                "Mount Charleston hiking",
                "Lake Mead National Recreation Area",
                "Springs Preserve desert gardens",
                "Floyd Lamb Park at Tule Springs",
                "Clark County Wetlands Park",
                "Sloan Canyon National Conservation Area",
            ],
            "cultural": [
                "The Mob Museum organized crime history",
                "Neon Museum vintage signs",
                "Discovery Children's Museum",
                "Las Vegas Natural History Museum",
                "Nevada State Museum",
                "Old Las Vegas Mormon Fort",
                "Atomic Testing Museum",
                "Las Vegas Art Museum",
            ],
            "entertainment": [
                "The Strip casino and resort hopping",
                "Fremont Street Experience",
                "Bellagio Fountains water show",
                "Cirque du Soleil performances",
                "High Roller observation wheel",
                "Stratosphere Tower thrill rides",
                "Downtown Container Park",
                "Area 15 immersive experiences",
            ],
            "shopping": [
                "Fashion Show Mall",
                "Forum Shops at Caesars",
                "Grand Canal Shoppes Venetian",
                "Miracle Mile Shops Planet Hollywood",
                "Town Square Las Vegas",
                "Las Vegas Premium Outlets North",
                "Las Vegas Premium Outlets South",
                "Meadows Mall local shopping",
            ],
            "dining": [
                "In-N-Out Burger California burgers",
                "Pizza Rock gourmet pizza",
                "Lotus of Siam Thai cuisine",
                "Bacchanal Buffet Caesars Palace",
                "Gordon Ramsay Hell's Kitchen",
                "Jol Robuchon fine dining",
                "Raku Japanese izakaya",
                "Echo & Rig Butcher and Steakhouse",
            ],
        },
    }

    if city not in city_activities:
        return f"Activity information for {city} is not available in our database."

    activities = city_activities[city]

    return f"""
# Comprehensive Activity Guide for {city}

## Outdoor Adventures & Recreation
{chr(10).join([f"- {activity}" for activity in activities["outdoor"]])}

## Cultural Experiences & Museums
{chr(10).join([f"- {activity}" for activity in activities["cultural"]])}

## Entertainment & Nightlife
{chr(10).join([f"- {activity}" for activity in activities["entertainment"]])}

## Shopping Destinations
{chr(10).join([f"- {activity}" for activity in activities["shopping"]])}

## Dining & Culinary Experiences
{chr(10).join([f"- {activity}" for activity in activities["dining"]])}

## Activity Recommendations by Interest

### For Nature Enthusiasts
The outdoor activities in {city} offer incredible opportunities to connect with nature. From urban parks to wilderness trails, visitors can enjoy hiking, cycling, water activities, and scenic viewpoints that showcase the city's natural beauty and diverse landscapes.

### For Culture & History Buffs
{city} boasts an impressive collection of museums, galleries, and cultural institutions that tell the story of the city's rich heritage and artistic achievements. From world-class art collections to interactive science exhibits, there's something to engage every cultural interest.

### For Entertainment Seekers
The entertainment scene in {city} is vibrant and diverse, offering everything from professional sports and live music venues to historic theaters and modern performance spaces. Whether you're looking for high-energy nightlife or family-friendly entertainment, the city delivers memorable experiences.

### For Shopping Enthusiasts
Shopping in {city} ranges from luxury boutiques and designer stores to unique local markets and outlet centers. Each shopping district offers its own character and specialties, making it easy to find everything from high-end fashion to one-of-a-kind souvenirs.

### For Food Lovers
The culinary scene in {city} reflects the city's diverse population and cultural influences. From iconic local institutions to innovative fine dining establishments, the city offers an exceptional range of dining experiences that showcase both traditional favorites and contemporary culinary creativity.

## Planning Your Visit
When planning activities in {city}, consider the weather conditions, seasonal events, and your personal interests. Many attractions offer advance booking options, and some museums have free admission days. The city's public transportation system makes it easy to explore different neighborhoods and experience the full range of activities available.

This comprehensive guide provides a starting point for discovering all that {city} has to offer, ensuring visitors can create memorable experiences tailored to their interests and preferences.
"""


weather_agent = Agent(
    id="weather_agent",
    model=OpenAIResponses(id="gpt-4o"),
    description="You are a helpful assistant that can answer questions about the weather.",
    instructions="Be concise, reply with one sentence.",
    tools=[ReasoningTools(add_instructions=True), get_weather],
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
    read_tool_call_history=False,
    stream=True,
    stream_intermediate_steps=True,
)

activities_agent = Agent(
    id="activities_agent",
    model=OpenAIResponses(id="gpt-4o"),
    description="You are a helpful assistant that can answer questions about activities in a city.",
    instructions="Be concise, reply with one sentence.",
    tools=[ReasoningTools(add_instructions=True), get_activities],
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
    read_tool_call_history=False,
    stream=True,
    stream_intermediate_steps=True,
)

team = Team(
    model=OpenAIResponses(id="gpt-4o"),
    members=[weather_agent, activities_agent],
    tools=[ReasoningTools(add_instructions=True)],
    instructions="Be concise, reply with one sentence.",
    db=db,
    markdown=True,
    add_datetime_to_context=True,
    enable_user_memories=True,
    share_member_interactions=False,
    add_history_to_context=True,
    read_team_history=False,
    stream=True,
    stream_intermediate_steps=True,
)


async def run_team_for_user(user: str, print_responses: bool = False):
    # Make four requests to the team, to build up history
    random_city = random.choice(cities)
    session_id = f"session_{user}_{uuid.uuid4()}"

    _ = team.arun(input=f"I love {random_city}!", user_id=user, session_id=session_id)
    _ = team.arun(
        input=f"Create a report on the activities and weather in {random_city}.",
        user_id=user,
        session_id=session_id,
    )
    _ = team.arun(
        input=f"What else can you tell me about {random_city}?",
        user_id=user,
        session_id=session_id,
    )
    _ = team.arun(
        input=f"What other cities are similar to {random_city}?",
        user_id=user,
        session_id=session_id,
    )


async def run_team():
    tasks = []

    # Run all 5 users concurrently
    for user in users:
        tasks.append(run_team_for_user(user))
    await asyncio.gather(*tasks)

    return "Successfully ran team"


team_response_with_memory_impact = PerformanceEval(
    name="Team Memory Impact",
    func=run_team,
    num_iterations=5,
    warmup_runs=0,
    measure_runtime=False,
    debug_mode=True,
    memory_growth_tracking=True,
    top_n_memory_allocations=10,
)

if __name__ == "__main__":
    asyncio.run(
        team_response_with_memory_impact.arun(print_results=True, print_summary=True)
    )
```

---

<a name="evals--performance--team_response_with_memory_multi_userpy"></a>

### `evals/performance/team_response_with_memory_multi_user.py`

```python
import asyncio
import random
import uuid

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat
from agno.team.team import Team

users = [
    "abel@example.com",
    "ben@example.com",
    "charlie@example.com",
    "dave@example.com",
    "edward@example.com",
]

cities = [
    "New York",
    "Los Angeles",
    "Chicago",
    "Houston",
    "Miami",
    "San Francisco",
    "Seattle",
    "Boston",
    "Washington D.C.",
    "Atlanta",
    "Denver",
    "Las Vegas",
]


# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)


def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."


def get_activities(city: str) -> str:
    activities = [
        "hiking",
        "biking",
        "swimming",
        "kayaking",
        "museum visits",
        "shopping",
        "sightseeing",
        "cafe hopping",
        "theater",
        "picnicking",
    ]
    selected_activities = random.sample(activities, k=3)
    return f"The activities in {city} are {', '.join(selected_activities)}."


weather_agent = Agent(
    id="weather_agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    description="You are a helpful assistant that can answer questions about the weather.",
    instructions="Be concise, reply with one sentence.",
    tools=[get_weather],
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
)

activities_agent = Agent(
    id="activities_agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    description="You are a helpful assistant that can answer questions about activities in a city.",
    instructions="Be concise, reply with one sentence.",
    tools=[get_activities],
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
)

team = Team(
    members=[weather_agent, activities_agent],
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Be concise, reply with one sentence.",
    db=db,
    enable_user_memories=True,
    markdown=True,
    add_history_to_context=True,
)


async def run_team():
    async def run_team_for_user(user: str):
        random_city = random.choice(cities)
        await team.arun(
            input=f"I love {random_city}! What activities and weather can I expect in {random_city}?",
            user_id=user,
            session_id=f"session_{uuid.uuid4()}",
        )

    tasks = []

    # Run all 5 users concurrently
    for user in users:
        tasks.append(run_team_for_user(user))
    await asyncio.gather(*tasks)

    return "Successfully ran team"


team_response_with_memory_impact = PerformanceEval(
    name="Team Memory Impact",
    func=run_team,
    num_iterations=5,
    warmup_runs=0,
    measure_runtime=False,
    debug_mode=True,
    memory_growth_tracking=True,
    top_n_memory_allocations=10,
)

if __name__ == "__main__":
    asyncio.run(
        team_response_with_memory_impact.arun(print_results=True, print_summary=True)
    )
```

---

<a name="evals--performance--team_response_with_memory_simplepy"></a>

### `evals/performance/team_response_with_memory_simple.py`

```python
import asyncio
import random

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat
from agno.team.team import Team

cities = [
    "New York",
    "Los Angeles",
    "Chicago",
    "Houston",
    "Miami",
    "San Francisco",
    "Seattle",
    "Boston",
    "Washington D.C.",
    "Atlanta",
    "Denver",
    "Las Vegas",
]


# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)


def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."


weather_agent = Agent(
    id="weather_agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Weather Agent",
    description="You are a helpful assistant that can answer questions about the weather.",
    instructions="Be concise, reply with one sentence.",
    tools=[get_weather],
    db=db,
    enable_user_memories=True,
    add_history_to_context=True,
)

team = Team(
    members=[weather_agent],
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Be concise, reply with one sentence.",
    db=db,
    markdown=True,
    enable_user_memories=True,
    add_history_to_context=True,
)


async def run_team():
    random_city = random.choice(cities)
    _ = team.arun(
        input=f"I love {random_city}! What weather can I expect in {random_city}?",
        stream=True,
        stream_intermediate_steps=True,
    )

    return "Successfully ran team"


team_response_with_memory_impact = PerformanceEval(
    name="Team Memory Impact",
    func=run_team,
    num_iterations=5,
    warmup_runs=0,
    measure_runtime=False,
    debug_mode=True,
    memory_growth_tracking=True,
)

if __name__ == "__main__":
    asyncio.run(
        team_response_with_memory_impact.arun(print_results=True, print_summary=True)
    )
```

---

<a name="evals--reliability--db_loggingpy"></a>

### `evals/reliability/db_logging.py`

```python
"""Example showing how to store evaluation results in the database."""

from typing import Optional

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.tools.calculator import CalculatorTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5432/ai"
db = PostgresDb(db_url=db_url, eval_table="eval_runs")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[CalculatorTools()],
)
response: RunOutput = agent.run("What is 10!?")

evaluation = ReliabilityEval(
    db=db,  # Pass the database to the evaluation. Results will be stored in the database.
    name="Tool Call Reliability",
    agent_response=response,
    expected_tool_calls=["factorial"],
)
result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
```

---

<a name="evals--reliability--multiple_tool_calls--calculatorpy"></a>

### `evals/reliability/multiple_tool_calls/calculator.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.tools.calculator import CalculatorTools


def multiply_and_exponentiate():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools()],
    )
    response: RunOutput = agent.run(
        "What is 10*5 then to the power of 2? do it step by step"
    )
    evaluation = ReliabilityEval(
        name="Tool Calls Reliability",
        agent_response=response,
        expected_tool_calls=["multiply", "exponentiate"],
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    if result:
        result.assert_passed()


if __name__ == "__main__":
    multiply_and_exponentiate()
```

---

<a name="evals--reliability--reliability_asyncpy"></a>

### `evals/reliability/reliability_async.py`

```python
"""This example shows how to run a Reliability evaluation asynchronously."""

import asyncio
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.tools.calculator import CalculatorTools


def factorial():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools()],
    )
    response: RunOutput = agent.run("What is 10!?")
    evaluation = ReliabilityEval(
        agent_response=response,
        expected_tool_calls=["factorial"],
    )

    # Run the evaluation calling the arun method.
    result: Optional[ReliabilityResult] = asyncio.run(
        evaluation.arun(print_results=True)
    )
    if result:
        result.assert_passed()


if __name__ == "__main__":
    factorial()
```

---

<a name="evals--reliability--single_tool_calls--calculatorpy"></a>

### `evals/reliability/single_tool_calls/calculator.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.tools.calculator import CalculatorTools


def factorial():
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools()],
    )
    response: RunOutput = agent.run("What is 10! (ten factorial)?")
    evaluation = ReliabilityEval(
        name="Tool Call Reliability",
        agent_response=response,
        expected_tool_calls=["factorial"],
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)

    if result:
        result.assert_passed()


if __name__ == "__main__":
    factorial()
```

---

<a name="evals--reliability--team--ai_newspy"></a>

### `evals/reliability/team/ai_news.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.models.openai import OpenAIChat
from agno.run.team import TeamRunOutput
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

team_member = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a stock.",
    tools=[DuckDuckGoTools(enable_news=True)],
)

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-4o"),
    members=[team_member],
    markdown=True,
    show_members_responses=True,
)

expected_tool_calls = [
    "delegate_task_to_member",  # Tool call used to delegate a task to a Team member
    "duckduckgo_news",  # Tool call used to get the latest news
]


def evaluate_team_reliability():
    response: TeamRunOutput = team.run("What is the latest news on AI?")
    evaluation = ReliabilityEval(
        name="Team Reliability Evaluation",
        team_response=response,
        expected_tool_calls=expected_tool_calls,
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    if result:
        result.assert_passed()


if __name__ == "__main__":
    evaluate_team_reliability()
```

---

<a name="examples--agents--agent_teampy"></a>

### `examples/agents/agent_team.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources.",
    add_datetime_to_context=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data.",
    add_datetime_to_context=True,
)

team_leader = Team(
    name="Reasoning Finance Team Leader",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[web_agent, finance_agent],
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Use tables to display data.",
        "Only respond with the final answer, no other text.",
    ],
    markdown=True,
    show_members_responses=True,
    add_datetime_to_context=True,
)

task = """\
Analyze the semiconductor market performance focusing on:
- NVIDIA (NVDA)
- AMD (AMD)
- Intel (INTC)
- Taiwan Semiconductor (TSM)
Compare their market positions, growth metrics, and future outlook."""

team_leader.print_response(
    task,
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)
```

---

<a name="examples--agents--agent_with_instructionspy"></a>

### `examples/agents/agent_with_instructions.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[YFinanceTools()],
    instructions=[
        "Use tables to display data.",
        "Only include the table in your response. No other text.",
    ],
    markdown=True,
)
agent.print_response("What is the stock price of Apple?", stream=True)
```

---

<a name="examples--agents--agent_with_knowledgepy"></a>

### `examples/agents/agent_with_knowledge.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Load Agno documentation into Knowledge
knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        # Use OpenAI for embeddings
        embedder=OpenAIEmbedder(id="text-embedding-3-small", dimensions=1536),
    ),
)

asyncio.run(
    knowledge.add_content_async(
        name="Agno Docs", url="https://docs.agno.com/introduction/agents.md"
    )
)

agent = Agent(
    name="Agno Assist",
    model=Claude(id="claude-3-7-sonnet-latest"),
    instructions=[
        "Use tables to display data.",
        "Include sources in your response.",
        "Search your knowledge before answering the question.",
        "Only include the output in your response. No other text.",
    ],
    knowledge=knowledge,
    tools=[ReasoningTools(add_instructions=True)],
    add_datetime_to_context=True,
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response(
        "What are Agents?",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="examples--agents--agent_with_memorypy"></a>

### `examples/agents/agent_with_memory.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from rich.pretty import pprint

# Database connection
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(
    db_url=db_url,
    session_table="sessions",
    memory_table="user_memories",
)

user_id = "peter_rabbit"

# Create agent with the new memory system
agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    user_id=user_id,
    db=db,
    # Enable the Agent to dynamically create and manage user memories
    enable_user_memories=True,
    add_datetime_to_context=True,
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response("My name is Peter Rabbit and I like to eat carrots.")

    # Get memories using the agent's method
    memories = agent.get_user_memories(user_id=user_id)
    print(f"Memories about {user_id}:")
    pprint(memories)

    agent.print_response("What is my favorite food?")
    agent.print_response("My best friend is Jemima Puddleduck.")

    # Get updated memories
    memories = agent.get_user_memories(user_id=user_id)
    print(f"Memories about {user_id}:")
    pprint(memories)

    agent.print_response("Recommend a good lunch meal, who should i invite?")
    agent.print_response("What have we been talking about?")
```

---

<a name="examples--agents--agent_with_reasoningpy"></a>

### `examples/agents/agent_with_reasoning.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(),
    ],
    instructions=[
        "Use tables to display data.",
        "Include sources in your response.",
        "Only include the report in your response. No other text.",
    ],
    markdown=True,
)
agent.print_response(
    "Write a report on NVDA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="examples--agents--agent_with_storagepy"></a>

### `examples/agents/agent_with_storage.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

agent = Agent(
    # This session_id is usually auto-generated
    # But for this example, we can set it to a fixed value
    # This session will now forever continue as a very long chat
    session_id="agent_session_which_is_autogenerated_if_not_set",
    model=Claude(id="claude-3-7-sonnet-latest"),
    db=SqliteDb(session_table="agent_sessions", db_file="tmp/agents.db"),
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

if __name__ == "__main__":
    print(f"Session id: {agent.session_id}")
    agent.print_response("How many people live in Canada?")
    agent.print_response("What is their national anthem?")
    agent.print_response("List my messages one by one")

    # Print all messages in this session
    messages_in_session = agent.get_messages_for_session()
    pprint(messages_in_session)
```

---

<a name="examples--agents--agent_with_toolspy"></a>

### `examples/agents/agent_with_tools.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[YFinanceTools()],
    markdown=True,
)
agent.print_response("What is the stock price of Apple?", stream=True)
```

---

<a name="examples--agents--agno_assistpy"></a>

### `examples/agents/agno_assist.py`

```python
import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

asyncio.run(
    knowledge.add_content_async(
        name="Agno Docs", url="https://docs.agno.com/llms-full.txt"
    )
)

agno_assist = Agent(
    name="Agno Assist",
    model=OpenAIChat(id="gpt-4o"),
    description="You help answer questions about the Agno framework.",
    instructions="Search your knowledge before answering the question.",
    knowledge=knowledge,
    db=SqliteDb(session_table="agno_assist_sessions", db_file="tmp/agents.db"),
    add_history_to_context=True,
    add_datetime_to_context=True,
    markdown=True,
)

if __name__ == "__main__":
    agno_assist.print_response("What is Agno?")
```

---

<a name="examples--agents--agno_support_agentpy"></a>

### `examples/agents/agno_support_agent.py`

```python
""" Agno Support Agent - Your AI Assistant for Agno Framework!

This example shows how to create an AI support assistant that combines iterative knowledge searching
with Agno's documentation to provide comprehensive, well-researched answers about the Agno framework.

Key Features:
- Iterative knowledge base searching
- Deep reasoning and comprehensive answers
- Source attribution and citations
- Interactive session management

Example prompts to try:
- "What is Agno and what are its key features?"
- "How do I create my first agent with Agno?"
- "What's the difference between Level 0 and Level 1 agents?"
- "How can I add memory to my Agno agent?"
- "What models does Agno support?"
- "How do I implement RAG with Agno?"

Run `pip install openai lancedb tantivy pypdf ddgs inquirer agno` to install dependencies.
"""

from pathlib import Path
from textwrap import dedent
from typing import List, Optional

import inquirer
import typer
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.tools.python import PythonTools
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print
from rich.console import Console
from rich.table import Table

# ************* Setup Paths *************
# Define the current working directory and output directory for saving files
cwd = Path(__file__).parent
# Create tmp directory if it doesn't exist
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)
# *************************************


def initialize_knowledge(load_knowledge: bool = False):
    """Initialize the knowledge base with Agno documentation

    Args:
        load_knowledge (bool): Whether to load the knowledge base. Defaults to False.
    """
    agent_knowledge = Knowledge(
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="agno_assist_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    )

    # Load the knowledge
    if load_knowledge:
        print("[bold blue] Initializing Knowledge...[/bold blue]")
        print("    Loading Agno documentation")
        print("    Building vector embeddings")
        print("    Optimizing for hybrid search")
        agent_knowledge.add_content(
            name="Agno Docs", url="https://docs.agno.com/llms-full.txt"
        )
        print("[bold green] Knowledge ready![/bold green]\n")
    return agent_knowledge


def get_agent_db():
    """Return agent storage for session management"""
    return SqliteDb(session_table="agno_assist_sessions", db_file="tmp/agents.db")


def create_agent(
    session_id: Optional[str] = None, load_knowledge: bool = False
) -> Agent:
    """Create and return a configured Agno Support agent."""
    agent_knowledge = initialize_knowledge(load_knowledge)
    agent_db = get_agent_db()

    return Agent(
        name="AgnoAssist",
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are AgnoAssist, an advanced AI Agent specialized in the Agno framework.
        Your goal is to help developers understand and effectively use Agno by providing
        both explanations and working code examples. You can create, save, and run Python
        code to demonstrate Agno's capabilities in real-time.

        Your strengths include:
        - Deep understanding of Agno's architecture and capabilities
        - Access to Agno documentation and API reference, search it for relevant information
        - Creating and testing working Agno Agents
        - Building practical, runnable code examples that demonstrate concepts
        - Ability to save code to files and execute them to verify functionality\
        """),
        instructions=dedent("""\
        Your mission is to provide comprehensive, hands-on support for Agno developers
        through iterative knowledge searching, clear explanations, and working code examples.

        Follow these steps for every query:
        1. **Analysis**
            - Break down the question into key technical components
            - Identify if the query requires a knowledge search, creating an Agent or both
            - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts
            - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide
            - When the user asks for an Agent, they mean an Agno Agent.
            - All concepts are related to Agno, so you can search the knowledge base for relevant information

        After Analysis, always start the iterative search process. No need to wait for approval from the user.

        2. **Iterative Search Process**
            - Make at least 3 searches in the knowledge base using the `search_knowledge_base` tool
            - Search for related concepts and implementation details
            - Continue searching until you have found all the information you need or you have exhausted all the search terms

        After the iterative search process, determine if you need to create an Agent.
        If you do, ask the user if they want you to create the Agent and run it.

        3. **Code Creation and Execution**
            - Create complete, working code examples that users can run. For example:
            ```python
            from agno.agent import Agent
            from agno.tools.duckduckgo import DuckDuckGoTools

            agent = Agent(tools=[DuckDuckGoTools()])

            # Perform a web search and capture the response
            response = agent.run("What's happening in France?")
            ```
            - You must remember to use agent.run() and NOT agent.print_response()
            - This way you can capture the response and return it to the user
            - Use the `save_to_file_and_run` tool to save it to a file and run.
            - Make sure to return the `response` variable that tells you the result
            - Remember to:
              * Build the complete agent implementation
              * Include all necessary imports and setup
              * Add comprehensive comments explaining the implementation
              * Test the agent with example queries
              * Ensure all dependencies are listed
              * Include error handling and best practices
              * Add type hints and documentation

        4. **Response Structure**
            - Start with a relevant emoji ( general,  concepts,  code,  troubleshooting)
            - Give a brief overview
            - Provide detailed explanation with source citations
            - Show the code execution results when relevant
            - Share best practices and common pitfalls
            - Suggest related topics to explore

        5. **Quality Checks**
            - Verify technical accuracy against documentation
            - Test all code examples by running them
            - Check that all aspects of the question are addressed
            - Include relevant documentation links

        Key Agno Concepts to Emphasize:
        - Agent levels (0-3) and capabilities
        - Multimodal and streaming support
        - Model agnosticism and provider flexibility
        - Knowledge base and memory management
        - Tool integration and extensibility
        - Performance optimization techniques

        Code Example Guidelines:
        - Always provide complete, runnable examples
        - Include all necessary imports and setup
        - Add error handling and type hints
        - Follow PEP 8 style guidelines
        - Use descriptive variable names
        - Add comprehensive comments
        - Show example usage and expected output

        Remember:
        - Always verify code examples by running them
        - Be clear about source attribution
        - Support developers at all skill levels
        - Focus on Agno's core principles: Simplicity, Performance, and Agnosticism
        - Save code examples to files when they would be useful to run"""),
        knowledge=agent_knowledge,
        tools=[PythonTools(base_dir=tmp_dir.joinpath("agno_assist"), read_files=True)],
        db=agent_db,
        add_history_to_context=True,
        num_history_runs=3,
        read_chat_history=True,
        markdown=True,
    )


def get_example_topics() -> List[str]:
    """Return a list of example topics for the agent."""
    return [
        "Tell me about Agno",
        "How do I create an agent with web search capabilities?",
        "How can I build an agent which can store session history?",
        "How do I create an Agent with knowledge?",
        "How can I make an agent that can write and execute code?",
    ]


def handle_session_selection() -> Optional[str]:
    """Handle session selection and return the selected session ID."""
    agent_db = get_agent_db()

    new = typer.confirm("Do you want to start a new session?", default=True)
    if new:
        return None

    existing_sessions: List[str] = agent_db.get_sessions(session_type=SessionType.AGENT)
    if not existing_sessions:
        print("No existing sessions found. Starting a new session.")
        return None

    print("\nExisting sessions:")
    for i, session in enumerate(existing_sessions, 1):
        print(f"{i}. {session}")

    session_idx = typer.prompt(
        "Choose a session number to continue (or press Enter for most recent)",
        default=1,
    )

    try:
        return existing_sessions[int(session_idx) - 1]
    except (ValueError, IndexError):
        return existing_sessions[0]


def run_interactive_loop(agent: Agent, show_topics: bool = True):
    """Run the interactive question-answering loop.

    Args:
        agent: Agent instance to use for responses
        show_topics: Whether to show example topics or continue chat-like interaction
    """
    example_topics = get_example_topics()
    first_interaction = True

    while True:
        if show_topics and first_interaction:
            choices = [f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)]
            choices.extend(["Enter custom question...", "Exit"])

            questions = [
                inquirer.List(
                    "topic",
                    message="Select a topic or ask a different question:",
                    choices=choices,
                )
            ]
            answer = inquirer.prompt(questions)

            if answer is None or answer["topic"] == "Exit":
                break

            if answer["topic"] == "Enter custom question...":
                questions = [inquirer.Text("custom", message="Enter your question:")]
                custom_answer = inquirer.prompt(questions)
                topic = custom_answer["custom"]
            else:
                topic = example_topics[int(answer["topic"].split(".")[0]) - 1]
            first_interaction = False
        else:
            # Chat-like interaction
            question = typer.prompt("\n", prompt_suffix="> ")
            if question.lower() in ("exit", "quit", "bye"):
                break
            topic = question

        agent.print_response(topic, stream=True)


def agno_support_agent(
    load_knowledge: bool = typer.Option(
        False, "--load-knowledge", "-l", help="Load the knowledge base on startup"
    ),
):
    """Main function to run the Agno Support agent."""
    session_id = handle_session_selection()
    agent = create_agent(session_id, load_knowledge)

    # Create and display welcome table
    console = Console()
    table = Table(show_header=False, style="cyan")
    table.add_column(justify="center", min_width=40)
    table.add_row(" Welcome to [bold green]AgnoAssist[/bold green]")
    table.add_row("Your Personal Agno Expert")
    console.print(table)

    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(
                "[bold green] Started New Session: [white]{}[/white][/bold green]\n".format(
                    session_id
                )
            )
        else:
            print("[bold green] Started New Session[/bold green]\n")
        show_topics = True
    else:
        print(
            "[bold blue] Continuing Previous Session: [white]{}[/white][/bold blue]\n".format(
                session_id
            )
        )
        show_topics = False

    run_interactive_loop(agent, show_topics)


if __name__ == "__main__":
    typer.run(agno_support_agent)
```

---

<a name="examples--agents--airbnb_mcppy"></a>

### `examples/agents/airbnb_mcp.py`

```python
""" MCP Airbnb Agent - Search for Airbnb listings!

This example shows how to create an agent that uses MCP and Llama 4 to search for Airbnb listings.

1. Run: `pip install groq mcp agno` to install the dependencies
2. Export your GROQ_API_KEY
3. Run: `python cookbook/examples/agents/airbnb_mcp.py` to run the agent
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools


async def run_agent(message: str) -> None:
    async with MCPTools(
        "npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt"
    ) as mcp_tools:
        agent = Agent(
            model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
            tools=[ReasoningTools(add_instructions=True), mcp_tools],
            instructions=dedent("""\
            ## General Instructions
            - Always start by using the think tool to map out the steps needed to complete the task.
            - After receiving tool results, use the think tool as a scratchpad to validate the results for correctness
            - Before responding to the user, use the think tool to jot down final thoughts and ideas.
            - Present final outputs in well-organized tables whenever possible.
            - Always provide links to the listings in your response.
            - Show your top 10 recommendations in a table and make a case for why each is the best choice.

            ## Using the think tool
            At every step, use the think tool as a scratchpad to:
            - Restate the object in your own words to ensure full comprehension.
            - List the  specific rules that apply to the current request
            - Check if all required information is collected and is valid
            - Verify that the planned action completes the task\
            """),
            add_datetime_to_context=True,
            markdown=True,
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    task = dedent("""\
    I'm traveling to San Francisco from April 20th - May 8th. Can you find me the best deals for a 1 bedroom apartment?
    I'd like a dedicated workspace and close proximity to public transport.\
    """)
    asyncio.run(run_agent(task))
```

---

<a name="examples--agents--basic_agentpy"></a>

### `examples/agents/basic_agent.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    instructions="You are an agent focused on responding in one line. All your responses must be super concise and focused.",
    markdown=True,
)
runx = agent.run("What is the stock price of Apple?")
```

---

<a name="examples--agents--book_recommendationpy"></a>

### `examples/agents/book_recommendation.py`

```python
""" Book Recommendation Agent - Your Personal Literary Curator!

This example shows how to create an intelligent book recommendation system that provides
comprehensive literary suggestions based on your preferences. The agent combines book databases,
ratings, reviews, and upcoming releases to deliver personalized reading recommendations.

Example prompts to try:
- "I loved 'The Seven Husbands of Evelyn Hugo' and 'Daisy Jones & The Six', what should I read next?"
- "Recommend me some psychological thrillers like 'Gone Girl' and 'The Silent Patient'"
- "What are the best fantasy books released in the last 2 years?"
- "I enjoy historical fiction with strong female leads, any suggestions?"
- "Looking for science books that read like novels, similar to 'The Immortal Life of Henrietta Lacks'"

Run: `pip install openai exa_py agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

book_recommendation_agent = Agent(
    name="Shelfie",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are Shelfie, a passionate and knowledgeable literary curator with expertise in books worldwide! 

        Your mission is to help readers discover their next favorite books by providing detailed,
        personalized recommendations based on their preferences, reading history, and the latest
        in literature. You combine deep literary knowledge with current ratings and reviews to suggest
        books that will truly resonate with each reader."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:

        1. Analysis Phase 
           - Understand reader preferences from their input
           - Consider mentioned favorite books' themes and styles
           - Factor in any specific requirements (genre, length, content warnings)

        2. Search & Curate 
           - Use Exa to search for relevant books
           - Ensure diversity in recommendations
           - Verify all book data is current and accurate

        3. Detailed Information 
           - Book title and author
           - Publication year
           - Genre and subgenres
           - Goodreads/StoryGraph rating
           - Page count
           - Brief, engaging plot summary
           - Content advisories
           - Awards and recognition

        4. Extra Features 
           - Include series information if applicable
           - Suggest similar authors
           - Mention audiobook availability
           - Note any upcoming adaptations

        Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar books together
        - Add emoji indicators for genres (   )
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
        - Highlight diversity in authors and perspectives
        - Note trigger warnings when relevant"""),
    markdown=True,
    add_datetime_to_context=True,
)

# Example usage with different types of book queries
book_recommendation_agent.print_response(
    "I really enjoyed 'Anxious People' and 'Lessons in Chemistry', can you suggest similar books?",
    stream=True,
)

# More example prompts to explore:
"""
Genre-specific queries:
1. "Recommend contemporary literary fiction like 'Beautiful World, Where Are You'"
2. "What are the best fantasy series completed in the last 5 years?"
3. "Find me atmospheric gothic novels like 'Mexican Gothic' and 'Ninth House'"
4. "What are the most acclaimed debut novels from this year?"

Contemporary Issues:
1. "Suggest books about climate change that aren't too depressing"
2. "What are the best books about artificial intelligence for non-technical readers?"
3. "Recommend memoirs about immigrant experiences"
4. "Find me books about mental health with hopeful endings"

Book Club Selections:
1. "What are good book club picks that spark discussion?"
2. "Suggest literary fiction under 350 pages"
3. "Find thought-provoking novels that tackle current social issues"
4. "Recommend books with multiple perspectives/narratives"

Upcoming Releases:
1. "What are the most anticipated literary releases next month?"
2. "Show me upcoming releases from my favorite authors"
3. "What debut novels are getting buzz this season?"
4. "List upcoming books being adapted for screen"
"""
```

---

<a name="examples--agents--competitor_analysis_agentpy"></a>

### `examples/agents/competitor_analysis_agent.py`

```python
""" Competitor Analysis Agent - Your AI-Powered Market Intelligence System!

This example demonstrates how to build a sophisticated competitor analysis agent that combines powerful search and scraping capabilities with advanced reasoning tools to provide
comprehensive competitive intelligence. The agent performs deep analysis of competitors including
market positioning, product offerings, and strategic insights.

Key capabilities:
- Company discovery using Firecrawl search
- Website scraping and content analysis
- Competitive intelligence gathering
- SWOT analysis with reasoning
- Strategic recommendations
- Structured thinking and analysis

Example queries to try:
- "Analyze OpenAI's main competitors in the LLM space"
- "Compare Uber vs Lyft in the ride-sharing market"
- "Analyze Tesla's competitive position vs traditional automakers"
- "Research fintech competitors to Stripe"
- "Analyze Nike vs Adidas in the athletic apparel market"

Dependencies: `pip install openai firecrawl-py agno`
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.firecrawl import FirecrawlTools
from agno.tools.reasoning import ReasoningTools

competitor_analysis_agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    tools=[
        FirecrawlTools(
            search=True,
            crawl=True,
            mapping=True,
            formats=["markdown", "links", "html"],
            search_params={
                "limit": 2,
            },
            limit=5,
        ),
        ReasoningTools(
            add_instructions=True,
        ),
    ],
    instructions=[
        "1. Initial Research & Discovery:",
        "   - Use search tool to find information about the target company",
        "   - Search for '[company name] competitors', 'companies like [company name]'",
        "   - Search for industry reports and market analysis",
        "   - Use the think tool to plan your research approach",
        "2. Competitor Identification:",
        "   - Search for each identified competitor using Firecrawl",
        "   - Find their official websites and key information sources",
        "   - Map out the competitive landscape",
        "3. Website Analysis:",
        "   - Scrape competitor websites using Firecrawl",
        "   - Map their site structure to understand their offerings",
        "   - Extract product information, pricing, and value propositions",
        "   - Look for case studies and customer testimonials",
        "4. Deep Competitive Analysis:",
        "   - Use the analyze tool after gathering information on each competitor",
        "   - Compare features, pricing, and market positioning",
        "   - Identify patterns and competitive dynamics",
        "   - Think through the implications of your findings",
        "5. Strategic Synthesis:",
        "   - Conduct SWOT analysis for each major competitor",
        "   - Use reasoning to identify competitive advantages",
        "   - Analyze market trends and opportunities",
        "   - Develop strategic recommendations",
        "- Always use the think tool before starting major research phases",
        "- Use the analyze tool to process findings and draw insights",
        "- Search for multiple perspectives on each competitor",
        "- Verify information by checking multiple sources",
        "- Be thorough but focused in your analysis",
        "- Provide evidence-based recommendations",
    ],
    expected_output=dedent("""\
    # Competitive Analysis Report: {Target Company}

    ## Executive Summary
    {High-level overview of competitive landscape and key findings}

    ## Research Methodology
    - Search queries used
    - Websites analyzed
    - Key information sources

    ## Market Overview
    ### Industry Context
    - Market size and growth rate
    - Key trends and drivers
    - Regulatory environment

    ### Competitive Landscape
    - Major players identified
    - Market segmentation
    - Competitive dynamics

    ## Competitor Analysis

    ### Competitor 1: {Name}
    #### Company Overview
    - Website: {URL}
    - Founded: {Year}
    - Headquarters: {Location}
    - Company size: {Employees/Revenue if available}

    #### Products & Services
    - Core offerings
    - Key features and capabilities
    - Pricing model and tiers
    - Target market segments

    #### Digital Presence Analysis
    - Website structure and user experience
    - Key messaging and value propositions
    - Content strategy and resources
    - Customer proof points

    #### SWOT Analysis
    **Strengths:**
    - {Evidence-based strengths}

    **Weaknesses:**
    - {Identified weaknesses}

    **Opportunities:**
    - {Market opportunities}

    **Threats:**
    - {Competitive threats}

    ### Competitor 2: {Name}
    {Similar structure as above}

    ### Competitor 3: {Name}
    {Similar structure as above}

    ## Comparative Analysis

    ### Feature Comparison Matrix
    | Feature | {Target} | Competitor 1 | Competitor 2 | Competitor 3 |
    |---------|----------|--------------|--------------|--------------|
    | {Feature 1} | / | / | / | / |
    | {Feature 2} | / | / | / | / |

    ### Pricing Comparison
    | Company | Entry Level | Professional | Enterprise |
    |---------|-------------|--------------|------------|
    | {Pricing details extracted from websites} |

    ### Market Positioning Analysis
    {Analysis of how each competitor positions themselves}

    ## Strategic Insights

    ### Key Findings
    1. {Major insight with evidence}
    2. {Competitive dynamics observed}
    3. {Market gaps identified}

    ### Competitive Advantages
    - {Target company's advantages}
    - {Unique differentiators}

    ### Competitive Risks
    - {Main threats from competitors}
    - {Market challenges}

    ## Strategic Recommendations

    ### Immediate Actions (0-3 months)
    1. {Quick competitive responses}
    2. {Low-hanging fruit opportunities}

    ### Short-term Strategy (3-12 months)
    1. {Product/service enhancements}
    2. {Market positioning adjustments}

    ### Long-term Strategy (12+ months)
    1. {Sustainable differentiation}
    2. {Market expansion opportunities}

    ## Conclusion
    {Summary of competitive position and strategic imperatives}
    """),
    markdown=True,
    add_datetime_to_context=True,
    stream_intermediate_steps=True,
)

competitor_analysis_agent.print_response(
    """\
    Analyze the competitive landscape for Stripe in the payments industry.
    Focus on their products, pricing models, and market positioning.\
    """,
    stream=True,
    show_full_reasoning=True,
)
```

---

<a name="examples--agents--deep_knowledgepy"></a>

### `examples/agents/deep_knowledge.py`

```python
""" DeepKnowledge - An AI Agent that iteratively searches a knowledge base to answer questions

This agent performs iterative searches through its knowledge base, breaking down complex
queries into sub-questions, and synthesizing comprehensive answers. It's designed to explore
topics deeply and thoroughly by following chains of reasoning.

In this example, the agent uses the Agno documentation as a knowledge base

Key Features:
- Iteratively searches a knowledge base
- Source attribution and citations

Run `pip install openai lancedb tantivy inquirer agno` to install dependencies.
"""

from textwrap import dedent
from typing import List, Optional

import inquirer
import typer
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print


def initialize_knowledge_base():
    """Initialize the knowledge base with your preferred documentation or knowledge source
    Here we use Agno docs as an example, but you can replace with any relevant URLs
    """
    agent_knowledge = Knowledge(
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="deep_knowledge_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    )
    agent_knowledge.add_content(
        url="https://docs.agno.com/llms-full.txt",
    )
    return agent_knowledge


def get_agent_db():
    """Return agent storage"""
    return SqliteDb(session_table="deep_knowledge_sessions", db_file="tmp/agents.db")


def create_agent(session_id: Optional[str] = None) -> Agent:
    """Create and return a configured DeepKnowledge agent."""
    agent_knowledge = initialize_knowledge_base()
    agent_db = get_agent_db()
    return Agent(
        name="DeepKnowledge",
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are DeepKnowledge, an advanced reasoning agent designed to provide thorough,
        well-researched answers to any query by searching your knowledge base.

        Your strengths include:
        - Breaking down complex topics into manageable components
        - Connecting information across multiple domains
        - Providing nuanced, well-researched answers
        - Maintaining intellectual honesty and citing sources
        - Explaining complex concepts in clear, accessible terms"""),
        instructions=dedent("""\
        Your mission is to leave no stone unturned in your pursuit of the correct answer.

        To achieve this, follow these steps:
        1. **Analyze the input and break it down into key components**.
        2. **Search terms**: You must identify at least 3-5 key search terms to search for.
        3. **Initial Search:** Searching your knowledge base for relevant information. You must make atleast 3 searches to get all relevant information.
        4. **Evaluation:** If the answer from the knowledge base is incomplete, ambiguous, or insufficient - Ask the user for clarification. Do not make informed guesses.
        5. **Iterative Process:**
            - Continue searching your knowledge base till you have a comprehensive answer.
            - Reevaluate the completeness of your answer after each search iteration.
            - Repeat the search process until you are confident that every aspect of the question is addressed.
        4. **Reasoning Documentation:** Clearly document your reasoning process:
            - Note when additional searches were triggered.
            - Indicate which pieces of information came from the knowledge base and where it was sourced from.
            - Explain how you reconciled any conflicting or ambiguous information.
        5. **Final Synthesis:** Only finalize and present your answer once you have verified it through multiple search passes.
            Include all pertinent details and provide proper references.
        6. **Continuous Improvement:** If new, relevant information emerges even after presenting your answer,
            be prepared to update or expand upon your response.

        **Communication Style:**
        - Use clear and concise language.
        - Organize your response with numbered steps, bullet points, or short paragraphs as needed.
        - Be transparent about your search process and cite your sources.
        - Ensure that your final answer is comprehensive and leaves no part of the query unaddressed.

        Remember: **Do not finalize your answer until every angle of the question has been explored.**"""),
        additional_context=dedent("""\
        You should only respond with the final answer and the reasoning process.
        No need to include irrelevant information.

        - User ID: {user_id}
        - Memory: You have access to your previous search results and reasoning process.
        """),
        knowledge=agent_knowledge,
        db=agent_db,
        add_history_to_context=True,
        num_history_runs=3,
        read_chat_history=True,
        markdown=True,
    )


def get_example_topics() -> List[str]:
    """Return a list of example topics for the agent."""
    return [
        "What are AI agents and how do they work in Agno?",
        "What chunking strategies does Agno support for text processing?",
        "How can I implement custom tools in Agno?",
        "How does knowledge retrieval work in Agno?",
        "What types of embeddings does Agno support?",
    ]


def handle_session_selection() -> Optional[str]:
    """Handle session selection and return the selected session ID."""
    agent_db = get_agent_db()

    new = typer.confirm("Do you want to start a new session?", default=True)
    if new:
        return None

    existing_sessions: List[str] = agent_db.get_sessions(session_type=SessionType.AGENT)
    if not existing_sessions:
        print("No existing sessions found. Starting a new session.")
        return None

    print("\nExisting sessions:")
    for i, session in enumerate(existing_sessions, 1):
        print(f"{i}. {session}")

    session_idx = typer.prompt(
        "Choose a session number to continue (or press Enter for most recent)",
        default=1,
    )

    try:
        return existing_sessions[int(session_idx) - 1]
    except (ValueError, IndexError):
        return existing_sessions[0]


def run_interactive_loop(agent: Agent):
    """Run the interactive question-answering loop."""
    example_topics = get_example_topics()

    while True:
        choices = [f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)]
        choices.extend(["Enter custom question...", "Exit"])

        questions = [
            inquirer.List(
                "topic",
                message="Select a topic or ask a different question:",
                choices=choices,
            )
        ]
        answer = inquirer.prompt(questions)

        if answer["topic"] == "Exit":
            break

        if answer["topic"] == "Enter custom question...":
            questions = [inquirer.Text("custom", message="Enter your question:")]
            custom_answer = inquirer.prompt(questions)
            topic = custom_answer["custom"]
        else:
            topic = example_topics[int(answer["topic"].split(".")[0]) - 1]

        agent.print_response(topic, stream=True)


def deep_knowledge_agent():
    """Main function to run the DeepKnowledge agent."""

    session_id = handle_session_selection()
    agent = create_agent(session_id)

    print("\n Welcome to DeepKnowledge - Your Advanced Research Assistant! ")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"[bold green]Started New Session: {session_id}[/bold green]\n")
        else:
            print("[bold green]Started New Session[/bold green]\n")
    else:
        print(f"[bold blue]Continuing Previous Session: {session_id}[/bold blue]\n")

    run_interactive_loop(agent)


if __name__ == "__main__":
    typer.run(deep_knowledge_agent)

# Example prompts to try:
"""
Explore Agno's capabilities with these queries:
1. "What are the different types of agents in Agno?"
2. "How does Agno handle knowledge base management?"
3. "What embedding models does Agno support?"
4. "How can I implement custom tools in Agno?"
5. "What storage options are available for workflow caching?"
6. "How does Agno handle streaming responses?"
7. "What types of LLM providers does Agno support?"
8. "How can I implement custom knowledge sources?"
"""
```

---

<a name="examples--agents--deep_research_agent_exapy"></a>

### `examples/agents/deep_research_agent_exa.py`

```python
"""Example: Advanced Research Agent using Exa Research Tools

This example demonstrates how to use the Exa research tool for complex,
structured research tasks with automatic citation tracking.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ExaTools(research=True, research_model="exa-research-pro")],
    instructions=dedent("""
        You are an expert research analyst with access to advanced research tools.
        
        When you are given a schema to use, pass it to the research tool as output_schema parameter to research tool. 

        The research tool has two parameters:
        - instructions (str): The research topic/question 
        - output_schema (dict, optional): A JSON schema for structured output

        Example: If user says "Research X. Use this schema {'type': 'object', ...}", you must call research tool with the schema.

        If no schema is provided, the tool will auto-infer an appropriate schema.

        Present the findings exactly as provided by the research tool.
    """),
)

# Example 1: Basic research with simple string
agent.print_response(
    "Perform a comprehensive research on the current flagship GPUs from NVIDIA, AMD and Intel. Return a table of model name, MSRP USD, TDP watts, and launch date. Include citations for each cell."
)

# Define a JSON schema for structured research output
# research_schema = {
#     "type": "object",
#     "properties": {
#         "major_players": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "name": {"type": "string"},
#                     "role": {"type": "string"},
#                     "contributions": {"type": "string"},
#                 },
#             },
#         },
#     },
#     "required": ["major_players"],
# }

# agent.print_response(
#     f"Research the top 3 Semiconductor companies in 2024. Use this schema {research_schema}."
# )
```

---

<a name="examples--agents--fibonacci_agentpy"></a>

### `examples/agents/fibonacci_agent.py`

```python
import json

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools


def get_fibonacci_series(count: int = 5) -> str:
    """Generate a Fibonacci series up to the specified count."""
    if count <= 0:
        return "Count must be a positive integer."

    fib_series = [0, 1]
    for i in range(2, count):
        fib_series.append(fib_series[-1] + fib_series[-2])

    return json.dumps(fib_series[:count])


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        get_fibonacci_series,
        CalculatorTools(),
    ],
    markdown=True,
)

agent.print_response(
    """
    1. Get 10 elements of fibonacci series
    2. Calculate the sum of cubes of the prime numbers in the series
    3. Subtract 4
""",
    stream=True,
)
```

---

<a name="examples--agents--finance_agentpy"></a>

### `examples/agents/finance_agent.py`

```python
""" Finance Agent - Your Personal Market Analyst!

This example shows how to create a sophisticated financial analyst that provides
comprehensive market insights using real-time data. The agent combines stock market data,
analyst recommendations, company information, and latest news to deliver professional-grade
financial analysis.

Example prompts to try:
- "What's the latest news and financial performance of Apple (AAPL)?"
- "Give me a detailed analysis of Tesla's (TSLA) current market position"
- "How are Microsoft's (MSFT) financials looking? Include analyst recommendations"
- "Analyze NVIDIA's (NVDA) stock performance and future outlook"
- "What's the market saying about Amazon's (AMZN) latest quarter?"

Run: `pip install openai yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(),
    ],
    instructions=dedent("""\
        You are a seasoned Wall Street analyst with deep expertise in market analysis! 

        Follow these steps for comprehensive financial analysis:
        1. Market Overview
           - Latest stock price
           - 52-week high and low
        2. Financial Deep Dive
           - Key metrics (P/E, Market Cap, EPS)
        3. Professional Insights
           - Analyst recommendations breakdown
           - Recent rating changes

        4. Market Context
           - Industry trends and positioning
           - Competitive analysis
           - Market sentiment indicators

        Your reporting style:
        - Begin with an executive summary
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends ( )
        - Highlight key insights with bullet points
        - Compare metrics to industry averages
        - Include technical term explanations
        - End with a forward-looking analysis

        Risk Disclosure:
        - Always highlight potential risk factors
        - Note market uncertainties
        - Mention relevant regulatory concerns
    """),
    add_datetime_to_context=True,
    markdown=True,
)

# Example usage with detailed market analysis request
finance_agent.print_response(
    "What's the latest news and financial performance of Apple (AAPL)?", stream=True
)

# Semiconductor market analysis example
finance_agent.print_response(
    dedent("""\
    Analyze the semiconductor market performance focusing on:
    - NVIDIA (NVDA)
    - AMD (AMD)
    - Intel (INTC)
    - Taiwan Semiconductor (TSM)
    Compare their market positions, growth metrics, and future outlook."""),
    stream=True,
)

# Automotive market analysis example
finance_agent.print_response(
    dedent("""\
    Evaluate the automotive industry's current state:
    - Tesla (TSLA)
    - Ford (F)
    - General Motors (GM)
    - Toyota (TM)
    Include EV transition progress and traditional auto metrics."""),
    stream=True,
)

# More example prompts to explore:
"""
Advanced analysis queries:
1. "Compare Tesla's valuation metrics with traditional automakers"
2. "Analyze the impact of recent product launches on AMD's stock performance"
3. "How do Meta's financial metrics compare to its social media peers?"
4. "Evaluate Netflix's subscriber growth impact on financial metrics"
5. "Break down Amazon's revenue streams and segment performance"

Industry-specific analyses:
Semiconductor Market:
1. "How is the chip shortage affecting TSMC's market position?"
2. "Compare NVIDIA's AI chip revenue growth with competitors"
3. "Analyze Intel's foundry strategy impact on stock performance"
4. "Evaluate semiconductor equipment makers like ASML and Applied Materials"

Automotive Industry:
1. "Compare EV manufacturers' production metrics and margins"
2. "Analyze traditional automakers' EV transition progress"
3. "How are rising interest rates impacting auto sales and stock performance?"
4. "Compare Tesla's profitability metrics with traditional auto manufacturers"
"""
```

---

<a name="examples--agents--finance_agent_with_memorypy"></a>

### `examples/agents/finance_agent_with_memory.py`

```python
""" Finance Agent with Memory - Your Market Analyst that remembers your preferences

1. Create virtual environment and install dependencies:
   - Run `uv venv --python 3.12` to create a virtual environment
   - Run `source .venv/bin/activate` to activate the virtual environment
   - Run `uv pip install agno openai sqlalchemy fastapi uvicorn yfinance ddgs` to install the dependencies
   - Run `ag setup` to connect your local env to Agno
   - Export your OpenAI key: `export OPENAI_API_KEY=<your_openai_key>`
2. Run the app:
   - Run `python cookbook/examples/agents/financial_agent_with_memory.py` to start the app
3. Chat with the agent:
   - Open `https://app.agno.com/playground?endpoint=localhost%3A7777`
   - Tell the agent your name and favorite stocks
   - Ask the agent to analyze your favorite stocks
"""

from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

finance_agent_with_memory = Agent(
    name="Finance Agent with Memory",
    id="financial_agent_with_memory",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[YFinanceTools(enable_all=True), DuckDuckGoTools()],
    # Let the Agent create and manage user memories
    enable_agentic_memory=True,
    # Uncomment to always create memories from the input
    # can be used instead of enable_agentic_memory
    # enable_user_memories=True,
    db=SqliteDb(
        session_table="agent_sessions",
        db_file="tmp/agent_data.db",
        memory_table="agent_memory",
    ),
    # Add messages from the last 3 runs to the messages
    add_history_to_context=True,
    num_history_runs=3,
    # Add the current datetime to the instructions
    add_datetime_to_context=True,
    # Use markdown formatting
    markdown=True,
    instructions=dedent("""\
        You are a Wall Street analyst. Your goal is to help users with financial analysis.

        Checklist for different types of financial analysis:
        1. Market Overview: Stock price, 52-week range.
        2. Financials: P/E, Market Cap, EPS.
        3. Insights: Analyst recommendations, rating changes.
        4. Market Context: Industry trends, competitive landscape, sentiment.

        Formatting guidelines:
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends ( )
        - Highlight key insights with bullet points
    """),
)

# Initialize the AgentOS with the workflows
agent_os = AgentOS(
    description="Example OS setup",
    agents=[finance_agent_with_memory],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="financial_agent_with_memory:app", reload=True)
```

---

<a name="examples--agents--legal_consultantpy"></a>

### `examples/agents/legal_consultant.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="legal_docs", db_url=db_url),
)

asyncio.run(
    knowledge.add_content_async(
        url="https://www.justice.gov/d9/criminal-ccips/legacy/2015/01/14/ccmanual_0.pdf",
    )
)

legal_agent = Agent(
    name="LegalAdvisor",
    knowledge=knowledge,
    search_knowledge=True,
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    instructions=[
        "Provide legal information and advice based on the knowledge base.",
        "Include relevant legal citations and sources when answering questions.",
        "Always clarify that you're providing general legal information, not professional legal advice.",
        "Recommend consulting with a licensed attorney for specific legal situations.",
    ],
)

legal_agent.print_response(
    "What are the legal consequences and criminal penalties for spoofing Email Address?",
    stream=True,
)
```

---

<a name="examples--agents--media_trend_analysis_agentpy"></a>

### `examples/agents/media_trend_analysis_agent.py`

```python
"""Please install dependencies using:
pip install openai exa-py agno firecrawl
"""

from datetime import datetime, timedelta
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools
from agno.tools.firecrawl import FirecrawlTools


def calculate_start_date(days: int) -> str:
    """Calculate start date based on number of days."""
    start_date = datetime.now() - timedelta(days=days)
    return start_date.strftime("%Y-%m-%d")


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ExaTools(start_published_date=calculate_start_date(30), type="keyword"),
        FirecrawlTools(scrape=True),
    ],
    description=dedent("""\
        You are an expert media trend analyst specializing in:
        1. Identifying emerging trends across news and digital platforms
        2. Recognizing pattern changes in media coverage
        3. Providing actionable insights based on data
        4. Forecasting potential future developments
    """),
    instructions=[
        "Analyze the provided topic according to the user's specifications:",
        "1. Use keywords to perform targeted searches",
        "2. Identify key influencers and authoritative sources",
        "3. Extract main themes and recurring patterns",
        "4. Provide actionable recommendations",
        "5. if got sources less then 2, only then scrape them using firecrawl tool, dont crawl it  and use them to generate the report",
        "6. growth rate should be in percentage , and if not possible dont give growth rate",
    ],
    expected_output=dedent("""\
    # Media Trend Analysis Report

    ## Executive Summary
    {High-level overview of findings and key metrics}

    ## Trend Analysis
    ### Volume Metrics
    - Peak discussion periods: {dates}
    - Growth rate: {percentage or dont show this}

    ## Source Analysis
    ### Top Sources
    1. {Source 1}

    2. {Source 2}


    ## Actionable Insights
    1. {Insight 1}
       - Evidence: {data points}
       - Recommended action: {action}

    ## Future Predictions
    1. {Prediction 1}
       - Supporting evidence: {evidence}

    ## References
    {Detailed source list with links}
    """),
    markdown=True,
    add_datetime_to_context=True,
)

# Example usage:
analysis_prompt = """\
Analyze media trends for:
Keywords: ai agents
Sources: verge.com ,linkedin.com, x.com
"""

agent.print_response(analysis_prompt, stream=True)

# Alternative prompt example
crypto_prompt = """\
Analyze media trends for:
Keywords: cryptocurrency, bitcoin, ethereum
Sources: coindesk.com, cointelegraph.com
"""

# agent.print_response(crypto_prompt, stream=True)
```

---

<a name="examples--agents--meeting_summarizer_agentpy"></a>

### `examples/agents/meeting_summarizer_agent.py`

```python
"""Example: Meeting Summarizer & Visualizer Agent

This script uses OpenAITools (transcribe_audio, generate_image, generate_speech)
to process a meeting recording, summarize it, visualize it, and create an audio summary.

Requires: pip install openai agno
"""

import base64
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.openai import OpenAITools
from agno.tools.reasoning import ReasoningTools
from agno.utils.media import download_file, save_base64_data

input_audio_url: str = (
    "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/sample_audio.mp3"
)

local_audio_path = Path("tmp/meeting_recording.mp3")
print(f"Downloading file to local path: {local_audio_path}")
download_file(input_audio_url, local_audio_path)

meeting_agent: Agent = Agent(
    model=Gemini(id="gemini-2.0-flash"),
    tools=[OpenAITools(), ReasoningTools()],
    description=dedent("""\
        You are an efficient Meeting Assistant AI.
        Your purpose is to process audio recordings of meetings, extract key information,
        create a visual representation, and provide an audio summary.
    """),
    instructions=dedent("""\
        Follow these steps precisely:
        1. Receive the path to an audio file.
        2. Use the `transcribe_audio` tool to get the text transcription.
        3. Analyze the transcription and write a concise summary highlighting key discussion points, decisions, and action items.
        4. Based *only* on the summary created in step 3, generating important meeting points. This should be a essentially an overview of the summary's content properly ordered and formatted in the form of meeting minutes.
        5. Convert the meeting minutes into an audio summary using the `generate_speech` tool.
    """),
    markdown=True,
)

response = meeting_agent.run(
    f"Please process the meeting recording located at '{local_audio_path}'"
)
if response.audio:
    base64_audio = base64.b64encode(response.audio[0].content).decode("utf-8")
    save_base64_data(base64_audio, Path("tmp/meeting_summary.mp3"))
    print(f"Meeting summary saved to: {Path('tmp/meeting_summary.mp3')}")
```

---

<a name="examples--agents--movie_recommedationpy"></a>

### `examples/agents/movie_recommedation.py`

```python
""" Movie Recommendation Agent - Your Personal Cinema Curator!

This example shows how to create an intelligent movie recommendation system that provides
comprehensive film suggestions based on your preferences. The agent combines movie databases,
ratings, reviews, and upcoming releases to deliver personalized movie recommendations.

Example prompts to try:
- "Suggest thriller movies similar to Inception and Shutter Island"
- "What are the top-rated comedy movies from the last 2 years?"
- "Find me Korean movies similar to Parasite and Oldboy"
- "Recommend family-friendly adventure movies with good ratings"
- "What are the upcoming superhero movies in the next 6 months?"

Run: `pip install openai exa_py agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

movie_recommendation_agent = Agent(
    name="PopcornPal",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are PopcornPal, a passionate and knowledgeable film curator with expertise in cinema worldwide! 

        Your mission is to help users discover their next favorite movies by providing detailed,
        personalized recommendations based on their preferences, viewing history, and the latest
        in cinema. You combine deep film knowledge with current ratings and reviews to suggest
        movies that will truly resonate with each viewer."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:
        1. Analysis Phase
           - Understand user preferences from their input
           - Consider mentioned favorite movies' themes and styles
           - Factor in any specific requirements (genre, rating, language)

        2. Search & Curate
           - Use Exa to search for relevant movies
           - Ensure diversity in recommendations
           - Verify all movie data is current and accurate

        3. Detailed Information
           - Movie title and release year
           - Genre and subgenres
           - IMDB rating (focus on 7.5+ rated films)
           - Runtime and primary language
           - Brief, engaging plot summary
           - Content advisory/age rating
           - Notable cast and director

        4. Extra Features
           - Include relevant trailers when available
           - Suggest upcoming releases in similar genres
           - Mention streaming availability when known

        Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar movies together
        - Add emoji indicators for genres (  )
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
    """),
    markdown=True,
    add_datetime_to_context=True,
)

# Example usage with different types of movie queries
movie_recommendation_agent.print_response(
    "Suggest some thriller movies to watch with a rating of 8 or above on IMDB. "
    "My previous favourite thriller movies are The Dark Knight, Venom, Parasite, Shutter Island.",
    stream=True,
)

# More example prompts to explore:
"""
Genre-specific queries:
1. "Find me psychological thrillers similar to Black Swan and Gone Girl"
2. "What are the best animated movies from Studio Ghibli?"
3. "Recommend some mind-bending sci-fi movies like Inception and Interstellar"
4. "What are the highest-rated crime documentaries from the last 5 years?"

International Cinema:
1. "Suggest Korean movies similar to Parasite and Train to Busan"
2. "What are the must-watch French films from the last decade?"
3. "Recommend Japanese animated movies for adults"
4. "Find me award-winning European drama films"

Family & Group Watching:
1. "What are good family movies for kids aged 8-12?"
2. "Suggest comedy movies perfect for a group movie night"
3. "Find educational documentaries suitable for teenagers"
4. "Recommend adventure movies that both adults and children would enjoy"

Upcoming Releases:
1. "What are the most anticipated movies coming out next month?"
2. "Show me upcoming superhero movie releases"
3. "What horror movies are releasing this Halloween season?"
4. "List upcoming book-to-movie adaptations"
"""
```

---

<a name="examples--agents--pydantic_model_as_inputpy"></a>

### `examples/agents/pydantic_model_as_input.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

hackernews_agent.print_response(
    input=ResearchTopic(
        topic="AI",
        focus_areas=["AI", "Machine Learning"],
        target_audience="Developers",
        sources_required=5,
    )
)
```

---

<a name="examples--agents--readme_generatorpy"></a>

### `examples/agents/readme_generator.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.github import GithubTools
from agno.tools.local_file_system import LocalFileSystemTools

readme_gen_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    name="Readme Generator Agent",
    tools=[GithubTools(), LocalFileSystemTools()],
    markdown=True,
    instructions=[
        "You are readme generator agent",
        "You'll be given repository url or repository name from user."
        "You'll use the `get_repository` tool to get the repository details."
        "You have to pass the repo_name as argument to the tool. It should be in the format of owner/repo_name. If given url extract owner/repo_name from it."
        "Also call the `get_repository_languages` tool to get the languages used in the repository."
        "Write a useful README for a open source project, including how to clone and install the project, run the project etc. Also add badges for the license, size of the repo, etc"
        "Don't include the project's languages-used in the README"
        "Write the produced README to the local filesystem",
    ],
)

readme_gen_agent.print_response(
    "Get details of https://github.com/agno-agi/agno", markdown=True
)
```

---

<a name="examples--agents--reasoning_finance_agentpy"></a>

### `examples/agents/reasoning_finance_agent.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(),
    ],
    instructions=[
        "Use tables to display data.",
        "Include sources in your response.",
        "Only include the report in your response. No other text.",
    ],
    markdown=True,
)
agent.print_response(
    "Write a report on NVDA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="examples--agents--recipe_creatorpy"></a>

### `examples/agents/recipe_creator.py`

```python
""" Recipe Creator - Your Personal AI Chef!

This example shows how to create an intelligent recipe recommendation system that provides
detailed, personalized recipes based on your ingredients, dietary preferences, and time constraints.
The agent combines culinary knowledge, nutritional data, and cooking techniques to deliver
comprehensive cooking instructions.

Example prompts to try:
- "I have chicken, rice, and vegetables. What can I make in 30 minutes?"
- "Create a vegetarian pasta recipe with mushrooms and spinach"
- "Suggest healthy breakfast options with oats and fruits"
- "What can I make with leftover turkey and potatoes?"
- "Need a quick dessert recipe using chocolate and bananas"

Run: `pip install openai exa_py agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

recipe_agent = Agent(
    name="ChefGenius",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are ChefGenius, a passionate and knowledgeable culinary expert with expertise in global cuisine! 

        Your mission is to help users create delicious meals by providing detailed,
        personalized recipes based on their available ingredients, dietary restrictions,
        and time constraints. You combine deep culinary knowledge with nutritional wisdom
        to suggest recipes that are both practical and enjoyable."""),
    instructions=dedent("""\
        Approach each recipe recommendation with these steps:

        1. Analysis Phase 
           - Understand available ingredients
           - Consider dietary restrictions
           - Note time constraints
           - Factor in cooking skill level
           - Check for kitchen equipment needs

        2. Recipe Selection 
           - Use Exa to search for relevant recipes
           - Ensure ingredients match availability
           - Verify cooking times are appropriate
           - Consider seasonal ingredients
           - Check recipe ratings and reviews

        3. Detailed Information 
           - Recipe title and cuisine type
           - Preparation time and cooking time
           - Complete ingredient list with measurements
           - Step-by-step cooking instructions
           - Nutritional information per serving
           - Difficulty level
           - Serving size
           - Storage instructions

        4. Extra Features 
           - Ingredient substitution options
           - Common pitfalls to avoid
           - Plating suggestions
           - Wine pairing recommendations
           - Leftover usage tips
           - Meal prep possibilities

        Presentation Style:
        - Use clear markdown formatting
        - Present ingredients in a structured list
        - Number cooking steps clearly
        - Add emoji indicators for:
           Vegetarian
           Vegan
           Gluten-free
           Contains nuts
           Quick recipes
        - Include tips for scaling portions
        - Note allergen warnings
        - Highlight make-ahead steps
        - Suggest side dish pairings"""),
    markdown=True,
    add_datetime_to_context=True,
)

# Example usage with different types of recipe queries
recipe_agent.print_response(
    "I have chicken breast, broccoli, garlic, and rice. Need a healthy dinner recipe that takes less than 45 minutes.",
    stream=True,
)

# More example prompts to explore:
"""
Quick Meals:
1. "15-minute dinner ideas with pasta and vegetables"
2. "Quick healthy lunch recipes for meal prep"
3. "Easy breakfast recipes with eggs and avocado"
4. "No-cook dinner ideas for hot summer days"

Dietary Restrictions:
1. "Keto-friendly dinner recipes with salmon"
2. "Gluten-free breakfast options without eggs"
3. "High-protein vegetarian meals for athletes"
4. "Low-carb alternatives to pasta dishes"

Special Occasions:
1. "Impressive dinner party main course for 6 people"
2. "Romantic dinner recipes for two"
3. "Kid-friendly birthday party snacks"
4. "Holiday desserts that can be made ahead"

International Cuisine:
1. "Authentic Thai curry with available ingredients"
2. "Simple Japanese recipes for beginners"
3. "Mediterranean diet dinner ideas"
4. "Traditional Mexican recipes with modern twists"

Seasonal Cooking:
1. "Summer salad recipes with seasonal produce"
2. "Warming winter soups and stews"
3. "Fall harvest vegetable recipes"
4. "Spring picnic recipe ideas"

Batch Cooking:
1. "Freezer-friendly meal prep recipes"
2. "One-pot meals for busy weeknights"
3. "Make-ahead breakfast ideas"
4. "Bulk cooking recipes for large families"
"""
```

---

<a name="examples--agents--recipe_rag_imagepy"></a>

### `examples/agents/recipe_rag_image.py`

```python
"""Example: Multi-Modal RAG & Image Agent

An agent that uses Llama 4 for multi-modal RAG and OpenAITools to create a visual, step-by-step image manual for a recipe.

Run: `pip install openai agno groq cohere` to install the dependencies
"""

import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge

# from agno.models.groq import Groq
from agno.tools.openai import OpenAITools
from agno.utils.media import download_image
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="embed_vision_documents",
        embedder=CohereEmbedder(
            id="embed-v4.0",
        ),
    ),
)

asyncio.run(
    knowledge.add_content_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )
)

agent = Agent(
    name="EmbedVisionRAGAgent",
    # model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
    tools=[OpenAITools()],
    knowledge=knowledge,
    instructions=[
        "You are a specialized recipe assistant.",
        "When asked for a recipe:",
        "1. Search the knowledge base to retrieve the relevant recipe details.",
        "2. Analyze the retrieved recipe steps carefully.",
        "3. Use the `generate_image` tool to create a visual, step-by-step image manual for the recipe.",
        "4. Present the recipe text clearly and mention that you have generated an accompanying image manual. Add instructions while generating the image.",
    ],
    markdown=True,
)

agent.print_response(
    "What is the recipe for a Thai curry?",
)
response = agent.get_last_run_output()

if response.images:
    download_image(response.images[0].url, Path("tmp/recipe_image.png"))
```

---

<a name="examples--agents--research_agentpy"></a>

### `examples/agents/research_agent.py`

```python
""" Research Agent - Your AI Investigative Journalist!

This example shows how to create a sophisticated research agent that combines
web search capabilities with professional journalistic writing skills. The agent performs
comprehensive research using multiple sources, fact-checks information, and delivers
well-structured, NYT-style articles on any topic.

Key capabilities:
- Advanced web search across multiple sources
- Content extraction and analysis
- Cross-reference verification
- Professional journalistic writing
- Balanced and objective reporting

Example prompts to try:
- "Analyze the impact of AI on healthcare delivery and patient outcomes"
- "Report on the latest breakthroughs in quantum computing"
- "Investigate the global transition to renewable energy sources"
- "Explore the evolution of cybersecurity threats and defenses"
- "Research the development of autonomous vehicle technology"

Dependencies: `pip install openai ddgs newspaper4k lxml_html_clean agno`
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

# Initialize the research agent with advanced journalistic capabilities
research_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    description=dedent("""\
        You are an elite investigative journalist with decades of experience at the New York Times.
        Your expertise encompasses: 

        - Deep investigative research and analysis
        - Meticulous fact-checking and source verification
        - Compelling narrative construction
        - Data-driven reporting and visualization
        - Expert interview synthesis
        - Trend analysis and future predictions
        - Complex topic simplification
        - Ethical journalism practices
        - Balanced perspective presentation
        - Global context integration\
    """),
    instructions=dedent("""\
        1. Research Phase 
           - Search for 10+ authoritative sources on the topic
           - Prioritize recent publications and expert opinions
           - Identify key stakeholders and perspectives

        2. Analysis Phase 
           - Extract and verify critical information
           - Cross-reference facts across multiple sources
           - Identify emerging patterns and trends
           - Evaluate conflicting viewpoints

        3. Writing Phase 
           - Craft an attention-grabbing headline
           - Structure content in NYT style
           - Include relevant quotes and statistics
           - Maintain objectivity and balance
           - Explain complex concepts clearly

        4. Quality Control 
           - Verify all facts and attributions
           - Ensure narrative flow and readability
           - Add context where necessary
           - Include future implications
    """),
    expected_output=dedent("""\
        # {Compelling Headline} 

        ## Executive Summary
        {Concise overview of key findings and significance}

        ## Background & Context
        {Historical context and importance}
        {Current landscape overview}

        ## Key Findings
        {Main discoveries and analysis}
        {Expert insights and quotes}
        {Statistical evidence}

        ## Impact Analysis
        {Current implications}
        {Stakeholder perspectives}
        {Industry/societal effects}

        ## Future Outlook
        {Emerging trends}
        {Expert predictions}
        {Potential challenges and opportunities}

        ## Expert Insights
        {Notable quotes and analysis from industry leaders}
        {Contrasting viewpoints}

        ## Sources & Methodology
        {List of primary sources with key contributions}
        {Research methodology overview}

        ---
        Research conducted by AI Investigative Journalist
        New York Times Style Report
        Published: {current_date}
        Last Updated: {current_time}\
    """),
    markdown=True,
    add_datetime_to_context=True,
)

# Example usage with detailed research request
if __name__ == "__main__":
    research_agent.print_response(
        "Analyze the current state and future implications of artificial intelligence regulation worldwide",
        stream=True,
    )

# Advanced research topics to explore:
"""
Technology & Innovation:
1. "Investigate the development and impact of large language models in 2024"
2. "Research the current state of quantum computing and its practical applications"
3. "Analyze the evolution and future of edge computing technologies"
4. "Explore the latest advances in brain-computer interface technology"

Environmental & Sustainability:
1. "Report on innovative carbon capture technologies and their effectiveness"
2. "Investigate the global progress in renewable energy adoption"
3. "Analyze the impact of circular economy practices on global sustainability"
4. "Research the development of sustainable aviation technologies"

Healthcare & Biotechnology:
1. "Explore the latest developments in CRISPR gene editing technology"
2. "Analyze the impact of AI on drug discovery and development"
3. "Investigate the evolution of personalized medicine approaches"
4. "Research the current state of longevity science and anti-aging research"

Societal Impact:
1. "Examine the effects of social media on democratic processes"
2. "Analyze the impact of remote work on urban development"
3. "Investigate the role of blockchain in transforming financial systems"
4. "Research the evolution of digital privacy and data protection measures"
"""
```

---

<a name="examples--agents--research_agent_exapy"></a>

### `examples/agents/research_agent_exa.py`

```python
""" Research Scholar Agent - Your AI Academic Research Assistant!

This example shows how to create a sophisticated research agent that combines
academic search capabilities with scholarly writing expertise. The agent performs
thorough research using Exa's academic search, analyzes recent publications, and delivers
well-structured, academic-style reports on any topic.

Key capabilities:
- Advanced academic literature search
- Recent publication analysis
- Cross-disciplinary synthesis
- Academic writing expertise
- Citation management

Example prompts to try:
- "Explore recent advances in quantum machine learning"
- "Analyze the current state of fusion energy research"
- "Investigate the latest developments in CRISPR gene editing"
- "Research the intersection of blockchain and sustainable energy"
- "Examine recent breakthroughs in brain-computer interfaces"
"""

from datetime import datetime
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

# Initialize the academic research agent with scholarly capabilities
research_scholar = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"), type="keyword"
        )
    ],
    description=dedent("""\
        You are a distinguished research scholar with expertise in multiple disciplines.
        Your academic credentials include: 

        - Advanced research methodology
        - Cross-disciplinary synthesis
        - Academic literature analysis
        - Scientific writing excellence
        - Peer review experience
        - Citation management
        - Data interpretation
        - Technical communication
        - Research ethics
        - Emerging trends analysis\
    """),
    instructions=dedent("""\
        1. Research Methodology 
           - Conduct 3 distinct academic searches
           - Focus on peer-reviewed publications
           - Prioritize recent breakthrough findings
           - Identify key researchers and institutions

        2. Analysis Framework 
           - Synthesize findings across sources
           - Evaluate research methodologies
           - Identify consensus and controversies
           - Assess practical implications

        3. Report Structure 
           - Create an engaging academic title
           - Write a compelling abstract
           - Present methodology clearly
           - Discuss findings systematically
           - Draw evidence-based conclusions

        4. Quality Standards 
           - Ensure accurate citations
           - Maintain academic rigor
           - Present balanced perspectives
           - Highlight future research directions\
    """),
    expected_output=dedent("""\
        # {Engaging Title} 

        ## Abstract
        {Concise overview of the research and key findings}

        ## Introduction
        {Context and significance}
        {Research objectives}

        ## Methodology
        {Search strategy}
        {Selection criteria}

        ## Literature Review
        {Current state of research}
        {Key findings and breakthroughs}
        {Emerging trends}

        ## Analysis
        {Critical evaluation}
        {Cross-study comparisons}
        {Research gaps}

        ## Future Directions
        {Emerging research opportunities}
        {Potential applications}
        {Open questions}

        ## Conclusions
        {Summary of key findings}
        {Implications for the field}

        ## References
        {Properly formatted academic citations}

        ---
        Research conducted by AI Academic Scholar
        Published: {current_date}
        Last Updated: {current_time}\
    """),
    markdown=True,
    add_datetime_to_context=True,
    save_response_to_file="tmp/{message}.md",
)

# Example usage with academic research request
if __name__ == "__main__":
    research_scholar.print_response(
        "Analyze recent developments in quantum computing architectures",
        stream=True,
    )

# Advanced research topics to explore:
"""
Quantum Science & Computing:
1. "Investigate recent breakthroughs in quantum error correction"
2. "Analyze the development of topological quantum computing"
3. "Research quantum machine learning algorithms and applications"
4. "Explore advances in quantum sensing technologies"

Biotechnology & Medicine:
1. "Examine recent developments in mRNA vaccine technology"
2. "Analyze breakthroughs in organoid research"
3. "Investigate advances in precision medicine"
4. "Research developments in neurotechnology"

Materials Science:
1. "Explore recent advances in metamaterials"
2. "Analyze developments in 2D materials beyond graphene"
3. "Research progress in self-healing materials"
4. "Investigate new battery technologies"

Artificial Intelligence:
1. "Examine recent advances in foundation models"
2. "Analyze developments in AI safety research"
3. "Research progress in neuromorphic computing"
4. "Investigate advances in explainable AI"
"""
```

---

<a name="examples--agents--run_as_clipy"></a>

### `examples/agents/run_as_cli.py`

```python
""" Interactive Writing Assistant - CLI App Example

This example shows how to create an interactive CLI app with an agent.

Run `pip install openai agno duckduckgo-search` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

writing_assistant = Agent(
    name="Writing Assistant",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions=dedent("""\
        You are a friendly and professional writing assistant! 
        
        Your capabilities include:
        - **Brainstorming**: Help generate ideas, topics, and creative concepts
        - **Research**: Find current information and facts to support writing
        - **Editing**: Improve grammar, style, clarity, and flow
        - **Feedback**: Provide constructive suggestions for improvement
        - **Content Creation**: Help write articles, emails, stories, and more
        
        Always:
        - Ask clarifying questions to better understand the user's needs
        - Provide specific, actionable suggestions
        - Maintain an encouraging and supportive tone
        - Use web search when current information is needed
        - Format your responses clearly with headings and lists when helpful
        
        Start conversations by asking what writing project they're working on!
        """),
    markdown=True,
)

if __name__ == "__main__":
    print(" I can research topics, help brainstorm, edit text, and more!")
    print(" Type 'exit', 'quit', or 'bye' to end our session.\n")

    writing_assistant.cli_app(
        input="Hello! What writing project are you working on today? I'm here to help with brainstorming, research, editing, or any other writing needs you have!",
        user="Writer",
        emoji="",
        stream=True,
    )

    ###########################################################################
    # ASYNC CLI APP
    ###########################################################################
    # import asyncio

    # asyncio.run(writing_assistant.acli_app(
    #     input="Hello! What writing project are you working on today? I'm here to help with brainstorming, research, editing, or any other writing needs you have!",
    #     user="Writer",
    #     emoji="",
    #     stream=True,
    # ))
```

---

<a name="examples--agents--shopping_partnerpy"></a>

### `examples/agents/shopping_partner.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

agent = Agent(
    name="shopping partner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You are a product recommender agent specializing in finding products that match user preferences.",
        "Prioritize finding products that satisfy as many user requirements as possible, but ensure a minimum match of 50%.",
        "Search for products only from authentic and trusted e-commerce websites such as Amazon, Flipkart, Myntra, Meesho, Google Shopping, Nike, and other reputable platforms.",
        "Verify that each product recommendation is in stock and available for purchase.",
        "Avoid suggesting counterfeit or unverified products.",
        "Clearly mention the key attributes of each product (e.g., price, brand, features) in the response.",
        "Format the recommendations neatly and ensure clarity for ease of user understanding.",
    ],
    tools=[ExaTools()],
)
agent.print_response(
    "I am looking for running shoes with the following preferences: Color: Black Purpose: Comfortable for long-distance running Budget: Under Rs. 10,000"
)
```

---

<a name="examples--agents--social_media_agentpy"></a>

### `examples/agents/social_media_agent.py`

```python
"""Social Media Agent Example with Dummy Dataset

This example demonstrates how to create an agent that:
1. Analyzes a dummy dataset of tweets
2. Leverages LLM capabilities to perform sophisticated sentiment analysis
3. Provides insights about the overall sentiment around a topic
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.x import XTools

# Create the social media analysis agent
social_media_agent = Agent(
    name="Social Media Analyst",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        XTools(
            include_post_metrics=True,
            wait_on_rate_limit=True,
        )
    ],
    instructions="""
    You are a senior Brand Intelligence Analyst with a specialty in social-media listening  on the X (Twitter) platform.  
    Your job is to transform raw tweet content and engagement metrics into an executive-ready intelligence report that helps product, marketing, and support teams  make data-driven decisions.  

    
    CORE RESPONSIBILITIES
    
    1. Retrieve tweets with X tools that you have access to and analyze both the text and metrics such as likes, retweets, replies.
    2. Classify every tweet as Positive / Negative / Neutral / Mixed, capturing the reasoning (e.g., praise for feature X, complaint about bugs, etc.).
    3. Detect patterns in engagement metrics to surface:
        Viral advocacy (high likes & retweets, low replies)
        Controversy (low likes, high replies)
        Influence concentration (verified or high-reach accounts driving sentiment)
    4. Extract thematic clusters and recurring keywords covering:
        Feature praise / pain points  
        UX / performance issues  
        Customer-service interactions  
        Pricing & ROI perceptions  
        Competitor mentions & comparisons  
        Emerging use-cases & adoption barriers
    5. Produce actionable, prioritized recommendations (Immediate, Short-term, Long-term) that address the issues and pain points.
    6. Supply a response strategy: which posts to engage, suggested tone & template,    influencer outreach, and community-building ideas. 

    
    DELIVERABLE FORMAT (markdown)
    
    ### 1  Executive Snapshot
     Brand-health score (1-10)  
     Net sentiment ( % positive  % negative )  
     Top 3 positive & negative drivers  
     Red-flag issues that need urgent attention    

    ### 2  Quantitative Dashboard
    | Sentiment | #Posts | % | Avg Likes | Avg Retweets | Avg Replies | Notes |
    |-----------|-------:|---:|----------:|-------------:|------------:|------|
    ( fill table )  

    ### 3  Key Themes & Representative Quotes
    For each major theme list: description, sentiment trend, excerpted tweets (truncated),  and key metrics. 

    ### 4  Competitive & Market Signals
     Competitors referenced, sentiment vs. Agno  
     Feature gaps users mention  
     Market positioning insights   

    ### 5  Risk Analysis
     Potential crises / viral negativity  
     Churn indicators  
     Trust & security concerns 

    ### 6  Opportunity Landscape
     Features or updates that delight users  
     Advocacy moments & influencer opportunities  
     Untapped use-cases highlighted by the community   

    ### 7  Strategic Recommendations
    **Immediate (48 h)**  urgent fixes or comms  
    **Short-term (1-2 wks)**  quick wins & tests  
    **Long-term (1-3 mo)**  roadmap & positioning  

    ### 8  Response Playbook
    For high-impact posts list: tweet-id/url, suggested response, recommended responder (e. g., support, PM, exec), and goal (defuse, amplify, learn).   

    
    ASSESSMENT & REASONING GUIDELINES
    
     Weigh sentiment by engagement volume & author influence (verified == 1.5 weight).  
     Use reply-to-like ratio > 0.5 as controversy flag.  
     Highlight any coordinated or bot-like behaviour.  
     Use the tools provided to you to get the data you need.

    Remember: your insights will directly inform the product strategy, customer-experience efforts, and brand reputation.  Be objective, evidence-backed, and solution-oriented.
""",
    markdown=True,
)

social_media_agent.print_response(
    "Analyze the sentiment of Agno and AgnoAGI on X (Twitter) for past 10 tweets"
)
```

---

<a name="examples--agents--startup_analyst_agentpy"></a>

### `examples/agents/startup_analyst_agent.py`

```python
"""
Startup Intelligence Agent - Comprehensive Company Analysis

This agent acts as a startup analyst that can perform comprehensive due diligence on companies

Prerequisites:
- Set SGAI_API_KEY environment variable with your ScrapeGraph API key
- Install dependencies: pip install scrapegraph-py agno openai
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.scrapegraph import ScrapeGraphTools

startup_analyst = Agent(
    name="Startup Analyst",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ScrapeGraphTools(markdownify=True, crawl=True, searchscraper=True)],
    instructions=dedent("""
        You are an elite startup analyst providing comprehensive due diligence for investment decisions.
        
        **ANALYSIS FRAMEWORK:**
        
        1. **Foundation Analysis**: Extract company basics (name, founding, location, value proposition, team)
        2. **Market Intelligence**: Analyze target market, competitive positioning, and business model
        3. **Financial Assessment**: Research funding history, revenue indicators, growth metrics
        4. **Risk Evaluation**: Identify market, technology, team, and financial risks
        
        **DELIVERABLES:**
        
        **Executive Summary** 
        
        **Company Profile**
        - Business model and revenue streams
        - Market opportunity and customer segments  
        - Team composition and expertise
        - Technology and competitive advantages
        
        **Financial & Growth Metrics**
        - Funding history and investor quality
        - Revenue/traction indicators
        - Growth trajectory and expansion plans
        - Burn rate estimates (if available)
        
        **Risk Assessment**
        - Market and competitive threats
        - Technology and team dependencies
        - Financial and regulatory risks
        
        **Strategic Recommendations**
        - Investment thesis and partnership opportunities
        - Competitive response strategies
        - Key due diligence focus areas
        
        **TOOL USAGE:**
        - **SmartScraper**: Extract structured data from specific pages (team, products, pricing)
        - **Markdownify**: Analyze content quality and messaging from key pages
        - **Crawl**: Comprehensive site analysis across multiple pages (limit: 10 pages, depth: 3)
        - **SearchScraper**: Find external information (funding, news, executive backgrounds)
        
        **OUTPUT STANDARDS:**
        - Use clear headings and bullet points
        - Include specific metrics and evidence
        - Cite sources and confidence levels
        - Distinguish facts from analysis
        - Maintain professional, executive-level language
        - Focus on actionable insights
        
        Remember: Your analysis informs million-dollar decisions. Be thorough, accurate, and actionable.
    """),
    markdown=True,
)


startup_analyst.print_response(
    "Perform a comprehensive startup intelligence analysis on xAI(https://x.ai)"
)
```

---

<a name="examples--agents--study_partnerpy"></a>

### `examples/agents/study_partner.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools
from agno.tools.youtube import YouTubeTools

study_partner = Agent(
    name="StudyScout",  # Fixed typo in name
    model=OpenAIChat(id="gpt-4o"),
    tools=[ExaTools(), YouTubeTools()],
    markdown=True,
    description="You are a study partner who assists users in finding resources, answering questions, and providing explanations on various topics.",
    instructions=[
        "Use Exa to search for relevant information on the given topic and verify information from multiple reliable sources.",
        "Break down complex topics into digestible chunks and provide step-by-step explanations with practical examples.",
        "Share curated learning resources including documentation, tutorials, articles, research papers, and community discussions.",
        "Recommend high-quality YouTube videos and online courses that match the user's learning style and proficiency level.",
        "Suggest hands-on projects and exercises to reinforce learning, ranging from beginner to advanced difficulty.",
        "Create personalized study plans with clear milestones, deadlines, and progress tracking.",
        "Provide tips for effective learning techniques, time management, and maintaining motivation.",
        "Recommend relevant communities, forums, and study groups for peer learning and networking.",
    ],
)
study_partner.print_response(
    "I want to learn about Postgres in depth. I know the basics, have 2 weeks to learn, and can spend 3 hours daily. Please share some resources and a study plan.",
    stream=True,
)
```

---

<a name="examples--agents--thinking_finance_agentpy"></a>

### `examples/agents/thinking_finance_agent.py`

```python
""" Finance Agent - Your Personal Market Analyst!

This example shows how to create a sophisticated financial analyst that provides
comprehensive market insights using real-time data. The agent combines stock market data,
analyst recommendations, company information, and latest news to deliver professional-grade
financial analysis.

Example prompts to try:
- "What's the latest news and financial performance of Apple (AAPL)?"
- "Give me a detailed analysis of Tesla's (TSLA) current market position"
- "How are Microsoft's (MSFT) financials looking? Include analyst recommendations"
- "Analyze NVIDIA's (NVDA) stock performance and future outlook"
- "What's the market saying about Amazon's (AMZN) latest quarter?"

Run: `pip install openai yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReasoningTools(add_instructions=True), YFinanceTools(enable_all=True)],
    instructions=dedent("""\
        You are a seasoned Wall Street analyst with deep expertise in market analysis! 

        Follow these steps for comprehensive financial analysis:
        1. Market Overview
           - Latest stock price
           - 52-week high and low
        2. Financial Deep Dive
           - Key metrics (P/E, Market Cap, EPS)
        3. Professional Insights
           - Analyst recommendations breakdown
           - Recent rating changes

        4. Market Context
           - Industry trends and positioning
           - Competitive analysis
           - Market sentiment indicators

        Your reporting style:
        - Begin with an executive summary
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends ( )
        - Highlight key insights with bullet points
        - Compare metrics to industry averages
        - Include technical term explanations
        - End with a forward-looking analysis

        Risk Disclosure:
        - Always highlight potential risk factors
        - Note market uncertainties
        - Mention relevant regulatory concerns\
    """),
    add_datetime_to_context=True,
    markdown=True,
    stream_intermediate_steps=True,
)

# Example usage with detailed market analysis request
finance_agent.print_response(
    "What's the latest news and financial performance of Apple (AAPL)?", stream=True
)

# Semiconductor market analysis example
finance_agent.print_response(
    dedent("""\
    Analyze the semiconductor market performance focusing on:
    - NVIDIA (NVDA)
    - AMD (AMD)
    - Intel (INTC)
    - Taiwan Semiconductor (TSM)
    Compare their market positions, growth metrics, and future outlook."""),
    stream=True,
)

# Automotive market analysis example
finance_agent.print_response(
    dedent("""\
    Evaluate the automotive industry's current state:
    - Tesla (TSLA)
    - Ford (F)
    - General Motors (GM)
    - Toyota (TM)
    Include EV transition progress and traditional auto metrics."""),
    stream=True,
)

# More example prompts to explore:
"""
Advanced analysis queries:
1. "Compare Tesla's valuation metrics with traditional automakers"
2. "Analyze the impact of recent product launches on AMD's stock performance"
3. "How do Meta's financial metrics compare to its social media peers?"
4. "Evaluate Netflix's subscriber growth impact on financial metrics"
5. "Break down Amazon's revenue streams and segment performance"

Industry-specific analyses:
Semiconductor Market:
1. "How is the chip shortage affecting TSMC's market position?"
2. "Compare NVIDIA's AI chip revenue growth with competitors"
3. "Analyze Intel's foundry strategy impact on stock performance"
4. "Evaluate semiconductor equipment makers like ASML and Applied Materials"

Automotive Industry:
1. "Compare EV manufacturers' production metrics and margins"
2. "Analyze traditional automakers' EV transition progress"
3. "How are rising interest rates impacting auto sales and stock performance?"
4. "Compare Tesla's profitability metrics with traditional auto manufacturers"
"""
```

---

<a name="examples--agents--translation_agentpy"></a>

### `examples/agents/translation_agent.py`

```python
import base64
from textwrap import dedent

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.cartesia import CartesiaTools
from agno.utils.media import save_base64_data

agent_instructions = dedent(
    """Follow these steps SEQUENTIALLY to translate text and generate a localized voice note:
    1. Identify the text to translate and the target language from the user request.
    2. Translate the text accurately to the target language. Keep this translated text for the final audio generation step.
    3. Analyze the emotion conveyed by the *translated* text (e.g., neutral, happy, sad, angry, etc.).
    4. Determine the standard 2-letter language code for the target language (e.g., 'fr' for French, 'es' for Spanish).
    5. Call the 'list_voices' tool to get a list of available Cartesia voices. Wait for the result.
    6. Examine the list of voices from the 'list_voices' result. Select the 'id' of an *existing* voice that:
       a) Matches the target language code (from step 4).
       b) Best reflects the analyzed emotion (from step 3).
    7. Call the 'localize_voice' tool to create a new voice. Provide the following arguments:
       - 'voice_id': The 'base_voice_id' selected in step 6.
       - 'name': A suitable name for the new voice (e.g., "French Happy Female").
       - 'description': A description reflecting the language and emotion.
       - 'language': The target language code (from step 4).
       - 'original_speaker_gender': User specified gender or the selected base voice gender.
       Wait for the result of this tool call.
    8. Check the result of the 'localize_voice' tool call from step 8:
       a) If the call was successful and returned the details of the newly created voice, extract the 'id' of this **new** voice. This is the 'final_voice_id'.
    9. Call the 'text_to_speech' tool to generate the audio. Provide:
        - 'transcript': The translated text from step 2.
        - 'voice_id': The 'final_voice_id' determined in step 9.
    """
)

agent = Agent(
    name="Emotion-Aware Translator Agent",
    description="Translates text, analyzes emotion, selects a suitable voice,creates a localized voice, and generates a voice note (audio file) using Cartesia TTStools.",
    instructions=agent_instructions,
    model=Gemini(id="gemini-2.5-pro"),
    tools=[CartesiaTools()],
)

response = agent.run(
    "Convert this phrase 'hello! how are you? Tell me more about the weather in Paris?' to French and create a voice note"
)

print("\nChecking for Audio Artifacts on Agent...")
if response.audio:
    base64_audio = base64.b64encode(response.audio[0].content).decode("utf-8")
    save_base64_data(base64_data=base64_audio, output_path="tmp/greeting.mp3")
    print("Saved audio to tmp/greeting.mp3")
```

---

<a name="examples--agents--web_extraction_agentpy"></a>

### `examples/agents/web_extraction_agent.py`

```python
from textwrap import dedent
from typing import Dict, List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.firecrawl import FirecrawlTools
from pydantic import BaseModel, Field
from rich.pretty import pprint


class ContentSection(BaseModel):
    """Represents a section of content from the webpage."""

    heading: Optional[str] = Field(None, description="Section heading")
    content: str = Field(..., description="Section content text")


class PageInformation(BaseModel):
    """Structured representation of a webpage."""

    url: str = Field(..., description="URL of the page")
    title: str = Field(..., description="Title of the page")
    description: Optional[str] = Field(
        None, description="Meta description or summary of the page"
    )
    features: Optional[List[str]] = Field(None, description="Key feature list")
    content_sections: Optional[List[ContentSection]] = Field(
        None, description="Main content sections of the page"
    )
    links: Optional[Dict[str, str]] = Field(
        None, description="Important links found on the page with description"
    )
    contact_info: Optional[Dict[str, str]] = Field(
        None, description="Contact information if available"
    )
    metadata: Optional[Dict[str, str]] = Field(
        None, description="Important metadata from the page"
    )


agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    tools=[FirecrawlTools(scrape=True, crawl=True)],
    instructions=dedent("""
        You are an expert web researcher and content extractor. Extract comprehensive, structured information
        from the provided webpage. Focus on:

        1. Accurately capturing the page title, description, and key features
        2. Identifying and extracting main content sections with their headings
        3. Finding important links to related pages or resources
        4. Locating contact information if available
        5. Extracting relevant metadata that provides context about the site

        Be thorough but concise. If the page has extensive content, prioritize the most important information.
    """).strip(),
    output_schema=PageInformation,
)

result = agent.run("Extract all information from https://www.agno.com")
pprint(result.content)
```

---

<a name="examples--agents--youtube_agentpy"></a>

### `examples/agents/youtube_agent.py`

```python
""" YouTube Agent - Your Video Content Expert!

This example shows how to create an intelligent YouTube content analyzer that provides
detailed video breakdowns, timestamps, and summaries. Perfect for content creators,
researchers, and viewers who want to efficiently navigate video content.

Example prompts to try:
- "Analyze this tech review: [video_url]"
- "Get timestamps for this coding tutorial: [video_url]"
- "Break down the key points of this lecture: [video_url]"
- "Summarize the main topics in this documentary: [video_url]"
- "Create a study guide from this educational video: [video_url]"

Run: `pip install openai youtube_transcript_api agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.youtube import YouTubeTools

youtube_agent = Agent(
    name="YouTube Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YouTubeTools()],
    instructions=dedent("""\
        You are an expert YouTube content analyst with a keen eye for detail! 
        Follow these steps for comprehensive video analysis:
        1. Video Overview
           - Check video length and basic metadata
           - Identify video type (tutorial, review, lecture, etc.)
           - Note the content structure
        2. Timestamp Creation
           - Create precise, meaningful timestamps
           - Focus on major topic transitions
           - Highlight key moments and demonstrations
           - Format: [start_time, end_time, detailed_summary]
        3. Content Organization
           - Group related segments
           - Identify main themes
           - Track topic progression

        Your analysis style:
        - Begin with a video overview
        - Use clear, descriptive segment titles
        - Include relevant emojis for content types:
           Educational
           Technical
           Gaming
           Tech Review
           Creative
        - Highlight key learning points
        - Note practical demonstrations
        - Mark important references

        Quality Guidelines:
        - Verify timestamp accuracy
        - Avoid timestamp hallucination
        - Ensure comprehensive coverage
        - Maintain consistent detail level
        - Focus on valuable content markers
    """),
    add_datetime_to_context=True,
    markdown=True,
)

# Example usage with different types of videos
youtube_agent.print_response(
    "Analyze this video: https://www.youtube.com/watch?v=zjkBMFhNj_g",
    stream=True,
)

# More example prompts to explore:
"""
Tutorial Analysis:
1. "Break down this Python tutorial with focus on code examples"
2. "Create a learning path from this web development course"
3. "Extract all practical exercises from this programming guide"
4. "Identify key concepts and implementation examples"

Educational Content:
1. "Create a study guide with timestamps for this math lecture"
2. "Extract main theories and examples from this science video"
3. "Break down this historical documentary into key events"
4. "Summarize the main arguments in this academic presentation"

Tech Reviews:
1. "List all product features mentioned with timestamps"
2. "Compare pros and cons discussed in this review"
3. "Extract technical specifications and benchmarks"
4. "Identify key comparison points and conclusions"

Creative Content:
1. "Break down the techniques shown in this art tutorial"
2. "Create a timeline of project steps in this DIY video"
3. "List all tools and materials mentioned with timestamps"
4. "Extract tips and tricks with their demonstrations"
"""
```

---

<a name="examples--chainlit_apps--basic--basic_apppy"></a>

### `examples/chainlit_apps/basic/basic_app.py`

```python
import chainlit as cl
from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai.chat import OpenAIChat

# Global variables
agent = None


@cl.on_chat_start
async def on_chat_start():
    """Initialize the agent when a new chat session starts."""
    # Create a unique database per session
    db = InMemoryDb()

    agent = Agent(
        model=OpenAIChat(
            id="gpt-4o",
        ),
        db=db,
        add_history_to_context=True,
        num_history_runs=5,
        stream=True,
        markdown=True,
        telemetry=False,
    )

    # Store the agent in the session
    cl.user_session.set("agent", agent)


@cl.on_message
async def on_message(message: cl.Message):
    # Get the agent from the session
    agent = cl.user_session.get("agent")

    response_msg = cl.Message(content="")
    await response_msg.send()

    async for event in agent.arun(message.content, stream=True):
        response_msg.content += event.content
        await response_msg.update()


if __name__ == "__main__":
    from chainlit.cli import run_chainlit

    run_chainlit(__file__)
```

---

<a name="examples--streamlit_apps--agentic_rag--agentic_ragpy"></a>

### `examples/streamlit_apps/agentic_rag/agentic_rag.py`

```python
""" Agentic RAG - Your AI Knowledge Agent!
This advanced example shows how to build a sophisticated RAG (Retrieval Augmented Generation) system that
leverages vector search and Language Models to provide deep insights from any knowledge base.

The Agent can:
- Process and understand documents from multiple sources (PDFs, websites, text files)
- Build a searchable knowledge base using vector embeddings
- Maintain conversation context and memory across sessions
- Provide relevant citations and sources for its responses
- Generate summaries and extract key insights
- Answer follow-up questions and clarifications

Example Queries to Try:
- "What are the key points from this document?"
- "Can you summarize the main arguments and supporting evidence?"
- "What are the important statistics and findings?"
- "How does this relate to [topic X]?"
- "What are the limitations or gaps in this analysis?"
- "Can you explain [concept X] in more detail?"
- "What other sources support or contradict these claims?"

The Agent uses:
- Vector similarity search for relevant document retrieval
- Conversation memory for contextual responses
- Citation tracking for source attribution
- Dynamic knowledge base updates

View the README for instructions on how to run the application.
"""

from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.streamlit import get_model_from_id
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


def get_agentic_rag_agent(
    model_id: str = "openai:gpt-4o",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Get an Agentic RAG Agent with Memory"""
    contents_db = PostgresDb(
        db_url=db_url,
        knowledge_table="agentic_rag_knowledge_contents",
        db_schema="ai",
    )

    knowledge_base = Knowledge(
        name="Agentic RAG Knowledge Base",
        description="Knowledge base for agentic RAG application",
        vector_db=PgVector(
            db_url=db_url,
            table_name="agentic_rag_documents",
            schema="ai",
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
        contents_db=contents_db,
        max_results=3,  # Only return top 3 most relevant documents
    )

    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    agent = Agent(
        name="Agentic RAG Agent",
        model=get_model_from_id(model_id),
        id="agentic-rag-agent",
        user_id=user_id,
        db=db,
        enable_user_memories=True,
        knowledge=knowledge_base,
        add_history_to_context=True,
        num_history_runs=5,
        session_id=session_id,
        tools=[DuckDuckGoTools()],
        instructions=dedent("""
            1. Knowledge Base Search:
               - ALWAYS start by searching the knowledge base using search_knowledge_base tool
               - Analyze ALL returned documents thoroughly before responding
               - If multiple documents are returned, synthesize the information coherently
            2. External Search:
               - If knowledge base search yields insufficient results, use duckduckgo_search
               - Focus on reputable sources and recent information
               - Cross-reference information from multiple sources when possible
            3. Context Management:
               - Use get_chat_history tool to maintain conversation continuity
               - Reference previous interactions when relevant
               - Keep track of user preferences and prior clarifications
            4. Response Quality:
               - Provide specific citations and sources for claims
               - Structure responses with clear sections and bullet points when appropriate
               - Include relevant quotes from source materials
               - Avoid hedging phrases like 'based on my knowledge' or 'depending on the information'
            5. User Interaction:
               - Ask for clarification if the query is ambiguous
               - Break down complex questions into manageable parts
               - Proactively suggest related topics or follow-up questions
            6. Error Handling:
               - If no relevant information is found, clearly state this
               - Suggest alternative approaches or questions
               - Be transparent about limitations in available information
        """),
        markdown=True,
        debug_mode=True,
    )

    return agent
```

---

<a name="examples--streamlit_apps--agentic_rag--apppy"></a>

### `examples/streamlit_apps/agentic_rag/app.py`

```python
import tempfile
from os import unlink

import nest_asyncio
import streamlit as st
from agentic_rag import get_agentic_rag_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    knowledge_base_info_widget,
    reset_session_state,
    session_selector_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Agentic RAG",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def restart_agent(model_id: str = None):
    target_model = model_id or st.session_state.get("current_model", MODELS[0])

    new_agent = get_agentic_rag_agent(model_id=target_model, session_id=None)

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            new_model_id = selected_model
            current_model = st.session_state.get("current_model")

            if current_model and current_model != new_model_id:
                try:
                    st.session_state["is_loading_session"] = False
                    # Start new chat
                    restart_agent(model_id=new_model_id)

                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Agentic RAG</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your intelligent RAG Agent powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Initialize Agent and Session
    ####################################################################
    agentic_rag_agent = initialize_agent(selected_model, get_agentic_rag_agent)
    reset_session_state(agentic_rag_agent)

    if prompt := st.chat_input(" Ask me anything!"):
        add_message("user", prompt)

    ####################################################################
    # Document Management
    ####################################################################
    st.sidebar.markdown("####  Document Management")
    knowledge_base_info_widget(agentic_rag_agent)

    # URL input
    input_url = st.sidebar.text_input("Add URL to Knowledge Base")
    if input_url and not prompt:
        alert = st.sidebar.info("Processing URL...", icon="")
        try:
            agentic_rag_agent.knowledge.add_content(
                name=f"URL: {input_url}",
                url=input_url,
                description=f"Content from {input_url}",
            )
            st.sidebar.success("URL added to knowledge base")
        except Exception as e:
            st.sidebar.error(f"Error processing URL: {str(e)}")
        finally:
            alert.empty()

    # File upload
    uploaded_file = st.sidebar.file_uploader(
        "Add a Document (.pdf, .csv, or .txt)", key="file_upload"
    )
    if uploaded_file and not prompt:
        alert = st.sidebar.info("Processing document...", icon="")
        try:
            with tempfile.NamedTemporaryFile(
                suffix=f".{uploaded_file.name.split('.')[-1]}", delete=False
            ) as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name

            agentic_rag_agent.knowledge.add_content(
                name=uploaded_file.name,
                path=tmp_path,
                description=f"Uploaded file: {uploaded_file.name}",
            )

            unlink(tmp_path)
            st.sidebar.success(f"{uploaded_file.name} added to knowledge base")
        except Exception as e:
            st.sidebar.error(f"Error processing file: {str(e)}")
        finally:
            alert.empty()

    if st.sidebar.button("Clear Knowledge Base"):
        if agentic_rag_agent.knowledge.vector_db:
            agentic_rag_agent.knowledge.vector_db.delete()
        st.sidebar.success("Knowledge base cleared")

    ###############################################################
    # Sample Question
    ###############################################################
    st.sidebar.markdown("####  Sample Questions")
    if st.sidebar.button(" What can you do?"):
        add_message(
            "user",
            "What can you do?",
        )
    if st.sidebar.button(" Summarize"):
        add_message(
            "user",
            "Can you summarize what is currently in the knowledge base (use `search_knowledge_base` tool)?",
        )
    if st.sidebar.button(" What is Agentic RAG?"):
        add_message(
            "user",
            "What is Agentic RAG?",
        )

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id and agentic_rag_agent.get_session_name():
                filename = f"agentic_rag_chat_{agentic_rag_agent.get_session_name()}.md"
            elif session_id:
                filename = f"agentic_rag_chat_{session_id}.md"
            else:
                filename = "agentic_rag_chat_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("Agentic RAG"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        display_response(agentic_rag_agent, question)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(agentic_rag_agent, selected_model, get_agentic_rag_agent)

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Agentic RAG Assistant helps you analyze documents and web content using natural language queries."
    )


main()
```

---

<a name="examples--streamlit_apps--chess_team--agentspy"></a>

### `examples/streamlit_apps/chess_team/agents.py`

```python
""" Chess Team Battle

This example demonstrates how to build a sophisticated multi-agent chess game where different AI models
compete against each other. The system coordinates multiple specialized agents working together to play chess.

The Chess Team includes:
- White Player Agent: Strategizes and makes moves for white pieces
- Black Player Agent: Strategizes and makes moves for black pieces
- Game Master Agent: Coordinates gameplay and provides position analysis

Example Gameplay Flow:
- Game Master coordinates between White and Black agents
- Each agent analyzes the current position and legal moves
- Agents make strategic decisions based on chess principles
- python-chess validates all moves and maintains game state
- Game continues until checkmate, stalemate, or draw conditions

The Chess Team uses:
- Specialized agent roles for different game aspects
- Turn-based coordination for sequential gameplay
- Real-time move validation and board updates
- Strategic analysis and position evaluation

View the README for instructions on how to run the application.
"""

from typing import Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.team.team import Team
from agno.utils.streamlit import get_model_with_provider

db_url = "postgresql+psycopg://db_user:wc6%40YU8evhm1234@localhost:5433/ai"


def get_chess_team(
    white_model: str = "gpt-4o",
    black_model: str = "claude-4-sonnet",
    master_model: str = "gpt-4o",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Team:
    """Get a Chess Team with specialized player agents.

    Args:
        white_model: Model ID for the white player agent
        black_model: Model ID for the black player agent
        master_model: Model ID for the game master agent
        user_id: Optional user ID for session tracking
        session_id: Optional session ID for game continuity

    Returns:
        Team instance configured for chess gameplay
    """

    # Get model instances with correct provider auto-detection
    white_model_instance = get_model_with_provider(white_model)
    black_model_instance = get_model_with_provider(black_model)
    master_model_instance = get_model_with_provider(master_model)

    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    # Create specialized chess agents
    white_player_agent = Agent(
        name="White Player",
        model=white_model_instance,
        db=db,
        id="white-chess-player",
        user_id=user_id,
        session_id=session_id,
        role="White Chess Strategist",
        instructions="""
            You are a chess strategist playing as WHITE pieces.
            
            Your responsibilities:
            1. Analyze the current board position and legal moves
            2. Apply chess principles: piece development, center control, king safety
            3. Consider tactical opportunities: pins, forks, skewers, discovered attacks
            4. Plan strategic goals: pawn structure, piece coordination, endgame preparation
            5. Choose the best move from the provided legal options
            
            Response format:
            - Respond ONLY with your chosen move in UCI notation (e.g., 'e2e4')
            - Do not include any explanation or additional text
            - Ensure your move is from the provided legal moves list
            
            Chess principles to follow:
            - Control the center (e4, d4, e5, d5 squares)
            - Develop pieces before moving them twice
            - Castle early for king safety
            - Don't bring queen out too early
            - Consider piece activity and coordination
        """,
        markdown=True,
        debug_mode=True,
    )

    black_player_agent = Agent(
        name="Black Player",
        model=black_model_instance,
        db=db,
        id="black-chess-player",
        user_id=user_id,
        session_id=session_id,
        role="Black Chess Strategist",
        instructions="""
            You are a chess strategist playing as BLACK pieces.
            
            Your responsibilities:
            1. Analyze the current board position and legal moves
            2. Apply chess principles: piece development, center control, king safety
            3. Consider tactical opportunities: pins, forks, skewers, discovered attacks  
            4. Plan strategic goals: pawn structure, piece coordination, endgame preparation
            5. Choose the best move from the provided legal options
            
            Response format:
            - Respond ONLY with your chosen move in UCI notation (e.g., 'e7e5')
            - Do not include any explanation or additional text
            - Ensure your move is from the provided legal moves list
            
            Chess principles to follow:
            - Control the center (e4, d4, e5, d5 squares)
            - Develop pieces before moving them twice
            - Castle early for king safety
            - Don't bring queen out too early
            - Consider piece activity and coordination
            - React to white's opening strategy appropriately
        """,
        markdown=True,
        debug_mode=True,
    )

    # Create the chess team with game master coordination
    chess_team = Team(
        name="Chess Team",
        model=master_model_instance,
        db=db,
        id="chess-game-team",
        user_id=user_id,
        session_id=session_id,
        members=[white_player_agent, black_player_agent],
        mode="route",
        instructions="""
            You are the Chess Game Master coordinating an AI vs AI chess match.
            
            Your roles:
            1. MOVE COORDINATION: Route move requests to the appropriate player agent
            2. GAME ANALYSIS: Provide position evaluation and commentary when requested
            3. GAME STATE: Monitor game progress and detect special conditions
            
            When handling requests:
            
            FOR MOVE REQUESTS:
            - Check 'current_player' in the context/dependencies
            - If current_player is 'white_piece_agent': route to White Player
            - If current_player is 'black_piece_agent': route to Black Player
            - Return the player's move response EXACTLY without modification
            
            FOR ANALYSIS REQUESTS:
            - When no current_player is specified, provide game analysis
            - Evaluate piece activity, king safety, material balance
            - Assess tactical and strategic themes in the position
            - Comment on recent moves and future planning
            
            Important guidelines:
            - Never modify or interpret player agent responses
            - Route move requests directly to the appropriate agent
            - Only provide analysis when explicitly requested
            - Maintain game flow and coordinate smooth turn transitions
        """,
        markdown=True,
        debug_mode=True,
        show_members_responses=True,
    )

    return chess_team
```

---

<a name="examples--streamlit_apps--chess_team--apppy"></a>

### `examples/streamlit_apps/chess_team/app.py`

```python
import re
from typing import Dict, List

import chess
import nest_asyncio
import streamlit as st
from agents import get_chess_team
from agno.utils.streamlit import (
    COMMON_CSS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Chess Team Battle",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)

# Chess-specific CSS additions
st.markdown(
    """
<style>
    .chess-board {
        width: 100%;
        max-width: 500px;
        margin: 0 auto;
        border: 2px solid #555;
        border-radius: 5px;
        overflow: hidden;
    }
    
    .chess-square {
        width: 12.5%;
        aspect-ratio: 1;
        display: inline-block;
        text-align: center;
        font-size: 24px;
        line-height: 60px;
        vertical-align: middle;
    }
    
    .white-square {
        background-color: #f0d9b5;
        color: #000;
    }
    
    .black-square {
        background-color: #b58863;
        color: #000;
    }
    
    .piece {
        font-size: 32px;
        line-height: 60px;
    }
    
    .game-status {
        text-align: center;
        padding: 10px;
        margin: 10px 0;
        border-radius: 5px;
        background-color: rgba(100, 100, 100, 0.2);
    }
    
    .player-turn {
        background-color: rgba(70, 130, 180, 0.3);
        border-left: 4px solid #4682B4;
    }
    
    .game-over {
        background-color: rgba(50, 205, 50, 0.3);
        border-left: 4px solid #32CD32;
    }
</style>
""",
    unsafe_allow_html=True,
)

# Chess board constants
WHITE = "white"
BLACK = "black"

PIECE_SYMBOLS = {
    "r": "",
    "n": "",
    "b": "",
    "q": "",
    "k": "",
    "p": "",
    "R": "",
    "N": "",
    "B": "",
    "Q": "",
    "K": "",
    "P": "",
}

MODELS = [
    "gpt-4o",
    "o3-mini",
    "claude-sonnet-4-20250514",
    "claude-opus-4-1-20250805",
]


class ChessBoard:
    """Chess board wrapper for python-chess."""

    def __init__(self):
        self.board = chess.Board()

    def get_fen(self) -> str:
        """Get FEN string representation."""
        return self.board.fen

    def get_board_state(self) -> str:
        """Get text representation of the board."""
        return str(self.board)

    @property
    def current_color(self) -> str:
        """Get current player color."""
        return WHITE if self.board.turn else BLACK

    def make_move(self, move_str: str) -> tuple[bool, str]:
        """Make a move on the board."""
        try:
            move = chess.Move.from_uci(move_str)
            if move in self.board.legal_moves:
                self.board.push(move)
                return True, f"Move {move_str} played successfully"
            else:
                return False, f"Illegal move: {move_str}"
        except Exception as e:
            return False, f"Invalid move format: {str(e)}"

    def get_game_state(self) -> tuple[bool, Dict]:
        """Check if game is over and return state info."""
        if self.board.is_checkmate():
            winner = BLACK if self.board.turn else WHITE
            return True, {"result": f"{winner}_win", "reason": "checkmate"}
        elif self.board.is_stalemate():
            return True, {"result": "draw", "reason": "stalemate"}
        elif self.board.is_insufficient_material():
            return True, {"result": "draw", "reason": "insufficient material"}
        elif self.board.is_seventyfive_moves():
            return True, {"result": "draw", "reason": "75-move rule"}
        elif self.board.is_fivefold_repetition():
            return True, {"result": "draw", "reason": "fivefold repetition"}
        else:
            return False, {}

    def get_legal_moves_with_descriptions(self) -> List[Dict]:
        """Get all legal moves with descriptions."""
        legal_moves = []

        for move in self.board.legal_moves:
            from_square = chess.square_name(move.from_square)
            to_square = chess.square_name(move.to_square)

            piece = self.board.piece_at(move.from_square)
            piece_type = piece.symbol().upper() if piece else "?"

            is_capture = self.board.is_capture(move)

            # Check for special moves
            if self.board.is_kingside_castling(move):
                description = "Kingside castle (O-O)"
            elif self.board.is_queenside_castling(move):
                description = "Queenside castle (O-O-O)"
            elif move.promotion:
                promotion = chess.piece_name(move.promotion)
                description = (
                    f"Pawn {from_square} to {to_square}, promote to {promotion}"
                )
            elif is_capture:
                captured_piece = self.board.piece_at(move.to_square)
                captured_type = (
                    captured_piece.symbol().upper() if captured_piece else "?"
                )
                description = f"{piece_type} from {from_square} captures {captured_type} at {to_square}"
            else:
                description = f"{piece_type} from {from_square} to {to_square}"

            legal_moves.append(
                {
                    "uci": move.uci(),
                    "san": self.board.san(move),
                    "description": description,
                    "is_capture": is_capture,
                }
            )

        return legal_moves


def display_board(chess_board: ChessBoard):
    """Display the chess board."""
    st.markdown('<div class="chess-board">', unsafe_allow_html=True)

    # Board layout (8x8 grid)
    for rank in range(8, 0, -1):  # 8 to 1
        row_html = ""
        for file in range(8):  # a to h
            square = chess.square(file, rank - 1)
            piece = chess_board.board.piece_at(square)

            # Determine square color
            is_light = (rank + file) % 2 == 1
            square_class = "white-square" if is_light else "black-square"

            # Get piece symbol
            piece_symbol = ""
            if piece:
                piece_symbol = PIECE_SYMBOLS.get(piece.symbol(), piece.symbol())

            row_html += f'<div class="chess-square {square_class}"><span class="piece">{piece_symbol}</span></div>'

        st.markdown(row_html, unsafe_allow_html=True)

    st.markdown("</div>", unsafe_allow_html=True)


def parse_move(response_text: str) -> str:
    """Extract move from agent response."""
    if not response_text:
        return ""

    response_text = response_text.strip()

    if (
        len(response_text) >= 4
        and response_text[0] in "abcdefgh"
        and response_text[1] in "12345678"
        and response_text[2] in "abcdefgh"
        and response_text[3] in "12345678"
    ):
        return response_text

    uci_match = re.search(r"([a-h][1-8][a-h][1-8][qrbn]?)", response_text)
    if uci_match:
        return uci_match.group(1)

    return response_text


def find_move_from_san(san_move: str, legal_moves: List[Dict]) -> str:
    """Convert SAN notation to UCI by finding it in legal moves."""
    san_move = san_move.strip()

    for move in legal_moves:
        if move["san"] == san_move:
            return move["uci"]

    return ""


def restart_chess_game(
    white_model: str = None, black_model: str = None, master_model: str = None
):
    """Restart the chess game with new settings."""
    white_model = white_model or st.session_state.get("white_model", MODELS[0])
    black_model = black_model or st.session_state.get("black_model", MODELS[1])
    master_model = master_model or st.session_state.get("master_model", MODELS[0])

    new_team = get_chess_team(
        white_model=white_model,
        black_model=black_model,
        master_model=master_model,
        session_id=None,
    )

    st.session_state["agent"] = new_team
    st.session_state["session_id"] = new_team.session_id
    st.session_state["white_model"] = white_model
    st.session_state["black_model"] = black_model
    st.session_state["master_model"] = master_model
    st.session_state["chess_board"] = ChessBoard()
    st.session_state["game_started"] = True
    st.session_state["game_paused"] = False
    st.session_state["move_history"] = []
    st.session_state["is_new_session"] = True


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Chess Team Battle</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Watch AI agents compete in strategic chess matches</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selectors
    ####################################################################
    st.sidebar.markdown("####  White Player")
    selected_white = st.sidebar.selectbox(
        "Select White Player Model",
        options=MODELS,
        index=0,
        key="white_selector",
    )

    st.sidebar.markdown("####  Black Player")
    selected_black = st.sidebar.selectbox(
        "Select Black Player Model",
        options=MODELS,
        index=1 if len(MODELS) > 1 else 0,
        key="black_selector",
    )

    st.sidebar.markdown("####  Game Master")
    selected_master = st.sidebar.selectbox(
        "Select Game Master Model",
        options=MODELS,
        index=0,
        key="master_selector",
    )

    ####################################################################
    # Initialize Chess Team and Session
    ####################################################################
    if "game_started" not in st.session_state:
        st.session_state.game_started = False

    if st.session_state.game_started:
        chess_team = initialize_agent(
            selected_white,
            lambda model_id, session_id: get_chess_team(
                white_model=selected_white,
                black_model=selected_black,
                master_model=selected_master,
                session_id=session_id,
            ),
        )
        reset_session_state(chess_team)

    ####################################################################
    # Game Controls
    ####################################################################
    st.sidebar.markdown("####  Game Controls")

    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if not st.session_state.game_started:
            if st.sidebar.button(" Start Game", use_container_width=True):
                restart_chess_game(selected_white, selected_black, selected_master)
                st.rerun()
        else:
            game_over = False
            if "chess_board" in st.session_state:
                game_over, _ = st.session_state.chess_board.get_game_state()

            if not game_over:
                if st.sidebar.button(
                    " Pause"
                    if not st.session_state.get("game_paused", False)
                    else " Resume",
                    use_container_width=True,
                ):
                    st.session_state.game_paused = not st.session_state.get(
                        "game_paused", False
                    )
                    st.rerun()

    with col2:
        if st.session_state.game_started:
            if st.sidebar.button(" New Game", use_container_width=True):
                restart_chess_game(selected_white, selected_black, selected_master)
                st.rerun()

    ####################################################################
    # Sample Actions
    ####################################################################
    if st.session_state.game_started:
        st.sidebar.markdown("####  Quick Actions")
        if st.sidebar.button(" Analyze Position"):
            if "chess_board" in st.session_state:
                fen = st.session_state.chess_board.get_fen()
                board_state = st.session_state.chess_board.get_board_state()

                analysis_prompt = f"""
                Analyze this chess position:
                
                FEN: {fen}
                Board:
                {board_state}
                
                Provide analysis of:
                - Material balance
                - Piece activity
                - King safety
                - Tactical themes
                - Strategic assessment
                """
                add_message("user", analysis_prompt)

        if st.sidebar.button(" Game Summary"):
            if st.session_state.get("move_history", []):
                moves = st.session_state.move_history
                summary_prompt = f"""
                Provide a summary of this chess game:
                
                Total moves: {len(moves)}
                Recent moves: {", ".join([m["move"] for m in moves[-5:]])}
                
                Please analyze:
                - Opening played
                - Key turning points
                - Current position assessment
                - Game progression
                """
                add_message("user", summary_prompt)

    ####################################################################
    # Utility buttons
    ####################################################################
    if st.session_state.game_started:
        st.sidebar.markdown("####  Utilities")
        col1, col2 = st.sidebar.columns([1, 1])

        with col1:
            if st.sidebar.button(" New Chat", use_container_width=True):
                restart_chess_game(selected_white, selected_black, selected_master)
                st.rerun()

        with col2:
            has_moves = (
                st.session_state.get("move_history")
                and len(st.session_state.move_history) > 0
            )

            if has_moves:
                session_id = st.session_state.get("session_id")
                filename = f"chess_game_{session_id or 'new'}.md"

                if st.sidebar.download_button(
                    " Export Game",
                    export_chat_history("Chess Team Battle"),
                    file_name=filename,
                    mime="text/markdown",
                    use_container_width=True,
                    help=f"Export game with {len(st.session_state.move_history)} moves",
                ):
                    st.sidebar.success("Game exported!")
            else:
                st.sidebar.button(
                    " Export Game",
                    disabled=True,
                    use_container_width=True,
                    help="No moves to export",
                )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    if st.session_state.game_started:
        display_chat_messages()

        # Generate response for user message
        last_message = (
            st.session_state["messages"][-1]
            if st.session_state.get("messages")
            else None
        )
        if last_message and last_message.get("role") == "user":
            question = last_message["content"]
            display_response(st.session_state.agent, question)

    ####################################################################
    # Session management
    ####################################################################
    if st.session_state.game_started:
        session_selector_widget(
            st.session_state.agent,
            selected_white,
            lambda model_id, session_id: get_chess_team(
                white_model=selected_white,
                black_model=selected_black,
                master_model=selected_master,
                session_id=session_id,
            ),
        )

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Chess Team Battle showcases AI agents competing in strategic chess matches. Watch different models play against each other with real-time move analysis and game coordination."
    )

    ####################################################################
    # Main Game Display
    ####################################################################
    if st.session_state.game_started and "chess_board" in st.session_state:
        chess_board = st.session_state.chess_board
        game_over, state_info = chess_board.get_game_state()

        # Display current match-up
        st.markdown(
            f"<h3 style='text-align:center; color:#87CEEB;'>{selected_white} vs {selected_black}</h3>",
            unsafe_allow_html=True,
        )

        # Display chess board
        display_board(chess_board)

        # Game status
        if game_over:
            result = state_info.get("result", "")
            reason = state_info.get("reason", "")

            if "white_win" in result:
                st.markdown(
                    f'<div class="game-status game-over"> Game Over! White ({selected_white}) wins by {reason}!</div>',
                    unsafe_allow_html=True,
                )
            elif "black_win" in result:
                st.markdown(
                    f'<div class="game-status game-over"> Game Over! Black ({selected_black}) wins by {reason}!</div>',
                    unsafe_allow_html=True,
                )
            else:
                st.markdown(
                    f'<div class="game-status game-over"> Game Over! Draw by {reason}!</div>',
                    unsafe_allow_html=True,
                )
        else:
            current_color = chess_board.current_color
            current_model = selected_white if current_color == WHITE else selected_black
            st.markdown(
                f'<div class="game-status player-turn"> {current_color.capitalize()} to move ({current_model})</div>',
                unsafe_allow_html=True,
            )

        # Move history
        if st.session_state.get("move_history", []):
            with st.expander(" Move History", expanded=False):
                for move in st.session_state.move_history[-10:]:  # Show last 10 moves
                    st.write(
                        f"**{move['number']}.** {move['player']}: {move['move']} - {move['description']}"
                    )

        # Auto-play logic
        if not st.session_state.get("game_paused", False) and not game_over:
            current_color = chess_board.current_color
            current_agent_name = (
                "white_piece_agent" if current_color == WHITE else "black_piece_agent"
            )

            with st.spinner(f" {current_color.capitalize()} player thinking..."):
                # Get legal moves
                legal_moves = chess_board.get_legal_moves_with_descriptions()
                legal_moves_text = "\n".join(
                    [
                        f"- {move['san']} ({move['uci']}): {move['description']}"
                        for move in legal_moves
                    ]
                )

                # Create move request
                fen = chess_board.get_fen()
                board_state = chess_board.get_board_state()

                move_request = f"""Current board state (FEN): {fen}
Board visualization:
{board_state}

Legal moves available:
{legal_moves_text}

Choose your next move from the legal moves above.
Respond with ONLY your chosen move in UCI notation (e.g., 'e2e4').
Do not include any other text in your response."""

                # Get response from team
                try:
                    response = st.session_state.agent.run(
                        move_request,
                        stream=False,
                        dependencies={
                            "current_player": current_agent_name,
                            "board_state": board_state,
                            "legal_moves": legal_moves,
                        },
                    )

                    # Parse and validate move
                    move_str = parse_move(response.content if response else "")
                    legal_move_ucis = [move["uci"] for move in legal_moves]

                    if move_str in legal_move_ucis:
                        valid_uci_move = move_str
                    else:
                        valid_uci_move = find_move_from_san(move_str, legal_moves)

                    if valid_uci_move:
                        success, message = chess_board.make_move(valid_uci_move)

                        if success:
                            # Record move
                            move_description = next(
                                (
                                    move["description"]
                                    for move in legal_moves
                                    if move["uci"] == valid_uci_move
                                ),
                                valid_uci_move,
                            )

                            move_number = len(st.session_state.move_history) + 1
                            current_model = (
                                selected_white
                                if current_color == WHITE
                                else selected_black
                            )

                            st.session_state.move_history.append(
                                {
                                    "number": move_number,
                                    "player": f"{current_color.capitalize()} ({current_model})",
                                    "move": valid_uci_move,
                                    "description": move_description,
                                }
                            )

                            # Check if game is now over
                            game_over_now, _ = chess_board.get_game_state()
                            if game_over_now:
                                st.session_state.game_paused = True

                            st.rerun()
                        else:
                            st.error(f"Failed to make move {valid_uci_move}: {message}")
                    else:
                        st.error(
                            f"Invalid move returned: '{move_str}' - not found in legal moves"
                        )

                except Exception as e:
                    st.error(f"Error getting move: {str(e)}")
    else:
        st.info(" Press 'Start Game' to begin the chess match!")


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--deep_researcher--agentspy"></a>

### `examples/streamlit_apps/deep_researcher/agents.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.scrapegraph import ScrapeGraphTools
from agno.utils.log import logger
from agno.workflow import Workflow

# --- Agents Definition ---
searcher_agent = Agent(
    name="Research Searcher",
    tools=[ScrapeGraphTools()],
    model=Nebius(id="deepseek-ai/DeepSeek-V3-0324"),
    markdown=True,
    description=(
        "You are ResearchBot-X, an expert at finding and extracting high-quality, "
        "up-to-date information from the web. Your job is to gather comprehensive, "
        "reliable, and diverse sources on the given topic."
    ),
    instructions=(
        "1. Search for the most recent and authoritative sources on the topic\n"
        "2. Extract key facts, statistics, and expert opinions from multiple sources\n"
        "3. Cover different perspectives and highlight any disagreements or controversies\n"
        "4. Include relevant data points and expert insights where possible\n"
        "5. Organize findings in a clear, structured format\n"
        "6. Always mention the references and sources of the content\n"
        "7. Be comprehensive and detailed in your research\n"
        "8. Focus on credible sources like news sites, official docs, research papers"
    ),
)

analyst_agent = Agent(
    name="Research Analyst",
    model=Nebius(id="deepseek-ai/DeepSeek-V3-0324"),
    markdown=True,
    description=(
        "You are AnalystBot-X, a critical thinker who synthesizes research findings "
        "into actionable insights. Your job is to analyze, compare, and interpret the "
        "information provided by the researcher."
    ),
    instructions=(
        "1. Identify key themes, trends, and patterns in the research\n"
        "2. Highlight the most important findings and their implications\n"
        "3. Note any contradictions or areas of uncertainty\n"
        "4. Suggest areas for further investigation if gaps exist\n"
        "5. Present analysis in a structured, easy-to-read format\n"
        "6. Extract and list ONLY the reference links that were actually provided\n"
        "7. Do NOT create, invent, or hallucinate any links or sources\n"
        "8. If no references were provided, clearly state that\n"
        "9. Focus on actionable insights and practical implications"
    ),
)

writer_agent = Agent(
    name="Research Writer",
    model=Nebius(id="deepseek-ai/DeepSeek-V3-0324"),
    markdown=True,
    description=(
        "You are WriterBot-X, a professional technical writer. Your job is to craft "
        "a clear, engaging, and well-structured report based on the analyst's summary."
    ),
    instructions=(
        "1. Write an engaging introduction that sets the context\n"
        "2. Organize main findings into logical sections with clear headings\n"
        "3. Use bullet points, tables, or lists for clarity where appropriate\n"
        "4. Conclude with a summary and actionable recommendations\n"
        "5. Include a References section ONLY if actual links were provided\n"
        "6. Use ONLY the reference links that were explicitly provided by the analyst\n"
        "7. Format references as clickable markdown links when available\n"
        "8. Never add fake or made-up links - only use verified sources\n"
        "9. Ensure the report is professional, clear, and actionable"
    ),
)


# --- Main Execution Function ---
def deep_research_execution(
    session_state,
    topic: str = None,
) -> str:
    """
    Deep research workflow execution function.

    Args:
        session_state: The shared session state
        topic: Research topic
    """

    if not topic:
        return " No research topic provided. Please specify a topic."

    logger.info(f"Running deep researcher workflow for topic: {topic}")

    # Step 1: Research
    logger.info("Starting research phase")
    research_content = searcher_agent.run(topic)

    if not research_content or not research_content.content:
        return f" Failed to gather research information for topic: {topic}"

    # Step 2: Analysis
    logger.info("Starting analysis phase")
    analysis = analyst_agent.run(research_content.content)

    if not analysis or not analysis.content:
        return f" Failed to analyze research findings for topic: {topic}"

    # Step 3: Report Writing
    logger.info("Starting report writing phase")
    report = writer_agent.run(analysis.content)

    if not report or not report.content:
        return f" Failed to generate final report for topic: {topic}"

    logger.info("Deep research workflow completed successfully")
    return report.content


# --- Workflow Definition ---
def get_deep_researcher_workflow(
    model_id: str = "nebius:deepseek-ai/DeepSeek-V3-0324",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Workflow:
    """Get a Deep Researcher Workflow with multi-agent pipeline"""

    return Workflow(
        name="Deep Researcher",
        description="AI-powered research assistant with multi-agent workflow for comprehensive research, analysis, and report generation",
        steps=deep_research_execution,
        session_state={},
    )
```

---

<a name="examples--streamlit_apps--deep_researcher--apppy"></a>

### `examples/streamlit_apps/deep_researcher/app.py`

```python
import nest_asyncio
import streamlit as st
from agents import get_deep_researcher_workflow
from agno.utils.streamlit import (
    COMMON_CSS,
    about_section,
    add_message,
    display_chat_messages,
    export_chat_history,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Deep Researcher",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Deep Researcher</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your AI-powered research assistant with multi-agent workflow</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Initialize Workflow
    ####################################################################
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    if prompt := st.chat_input(" What would you like me to research?"):
        add_message("user", prompt)

    ####################################################################
    # API Configuration
    ####################################################################
    st.sidebar.markdown("####  Configuration")

    nebius_api_key = st.sidebar.text_input(
        "Nebius API Key",
        type="password",
        help="Required for powering the research agents",
        placeholder="nebius_xxxxxxxxxxxx",
    )

    scrapegraph_api_key = st.sidebar.text_input(
        "ScrapeGraph API Key",
        type="password",
        help="Required for web scraping and content extraction",
        placeholder="sgai_xxxxxxxxxxxx",
    )

    if nebius_api_key and scrapegraph_api_key:
        st.sidebar.success(" API keys configured")
    else:
        st.sidebar.warning(" Please configure your API keys to start researching")

    ###############################################################
    # Example Research Topics
    ###############################################################
    st.sidebar.markdown("####  Example Topics")

    if st.sidebar.button(" AI & ML Developments 2024"):
        add_message("user", "Latest developments in AI and machine learning in 2024")

    if st.sidebar.button(" Sustainable Energy"):
        add_message("user", "Current trends in sustainable energy technologies")

    if st.sidebar.button(" Personalized Medicine"):
        add_message(
            "user", "Recent breakthroughs in personalized medicine and genomics"
        )

    if st.sidebar.button(" Quantum Cybersecurity"):
        add_message("user", "Impact of quantum computing on cybersecurity")

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])

    with col1:
        if st.sidebar.button(" New Research", use_container_width=True):
            st.session_state["messages"] = []
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            if st.sidebar.download_button(
                " Export Report",
                export_chat_history("Deep Research Report"),
                file_name="research_report.md",
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Research report exported!")
        else:
            st.sidebar.button(
                " Export Report",
                disabled=True,
                use_container_width=True,
                help="No research to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate research response
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        if not (nebius_api_key and scrapegraph_api_key):
            st.error(
                " Please configure your API keys in the sidebar to start research."
            )
            return

        research_topic = last_message["content"]

        with st.chat_message("assistant"):
            # Create containers for different phases
            response_container = st.empty()

            try:
                # Get the workflow
                app = get_deep_researcher_workflow()

                # Execute the research workflow with status updates
                with st.status(
                    " Executing research workflow...", expanded=True
                ) as status:
                    status.write(
                        " **Phase 1: Researching** - Finding and extracting relevant information..."
                    )
                    status.write(
                        " **Phase 2: Analyzing** - Synthesizing and interpreting the research findings..."
                    )
                    status.write(
                        " **Phase 3: Writing** - Crafting the final report..."
                    )

                    result = app.run(topic=research_topic)

                    full_report = ""
                    if result and result.content:
                        full_report = result.content
                        response_container.markdown(full_report)
                    else:
                        full_report = (
                            " Failed to generate research report. Please try again."
                        )
                        response_container.markdown(full_report)

                    status.update(label=" Research completed!", state="complete")

                # Add the complete response to messages
                add_message("assistant", full_report)

            except Exception as e:
                st.error(f" Research failed: {str(e)}")
                st.info(" Please check your API keys and try again.")

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Deep Researcher uses a multi-agent workflow to conduct comprehensive research, analysis, and report generation. Built with Agno, ScrapeGraph, and Nebius AI."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--gemini_tutor--agentspy"></a>

### `examples/streamlit_apps/gemini_tutor/agents.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.utils.streamlit import get_model_from_id

# Education level configurations
EDUCATION_LEVELS = [
    "Elementary School",
    "High School",
    "College",
    "Graduate",
    "PhD",
]

# Available Gemini models
GEMINI_MODELS = [
    "gemini-2.5-pro",
    "gemini-2.0-pro",
    "gemini-1.5-pro",
]


def get_gemini_tutor_agent(
    model_id: str = "gemini-2.5-pro",
    education_level: str = "High School",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Get a Gemini Tutor Agent for educational assistance.

    Args:
        model_id: Gemini model ID to use
        education_level: Target education level for content adaptation
        user_id: Optional user ID for session tracking
        session_id: Optional session ID for learning continuity

    Returns:
        Agent instance configured for educational tutoring
    """

    # Get the appropriate Gemini model
    if not model_id.startswith("google:"):
        model_id = f"google:{model_id}"

    gemini_model = get_model_from_id(model_id)

    # Configure advanced Gemini settings for education
    if hasattr(gemini_model, "temperature"):
        gemini_model.temperature = 0.7  # Balanced creativity for education
    if hasattr(gemini_model, "top_p"):
        gemini_model.top_p = 0.9
    if hasattr(gemini_model, "top_k"):
        gemini_model.top_k = 40

    # Enable grounding for research capabilities
    if hasattr(gemini_model, "grounding"):
        gemini_model.grounding = True

    # Create the educational agent
    tutor_agent = Agent(
        name="Gemini Tutor",
        model=gemini_model,
        id="gemini-educational-tutor",
        user_id=user_id,
        session_id=session_id,
        role=f"Educational AI Tutor for {education_level} Level",
        instructions=f"""
            You are an expert educational AI tutor specializing in creating personalized learning experiences for {education_level} students.
            
            Your primary responsibilities:
            1. CONTENT ADAPTATION: Adjust complexity, vocabulary, and examples for {education_level} level
            2. STRUCTURED LEARNING: Create comprehensive learning modules with clear progression
            3. INTERACTIVE EDUCATION: Include engaging elements and practical applications
            4. ASSESSMENT INTEGRATION: Provide practice questions and knowledge validation
            5. MULTIMODAL TEACHING: Leverage text, images, and multimedia when helpful
            
            Learning Experience Creation:
            
            STRUCTURE your responses with:
            - **Introduction**: Brief overview and learning objectives
            - **Core Concepts**: Key ideas explained at appropriate level
            - **Examples & Applications**: Relevant, relatable examples
            - **Interactive Elements**: Thought experiments or practical exercises
            - **Assessment**: 2-3 questions to check understanding with answers
            - **Summary**: Key takeaways and next steps
            
            ADAPTATION for {education_level} level:
            - Use appropriate vocabulary and complexity
            - Include relevant examples and analogies
            - Adjust depth of explanation to match academic level
            - Consider prior knowledge typical for this education level
            
            INTERACTIVE ELEMENTS:
            - Include thought-provoking questions during explanations
            - Suggest practical experiments or applications
            - Create scenarios for applying the concepts
            - Encourage critical thinking and analysis
            
            ASSESSMENT GUIDELINES:
            - Create 2-3 assessment questions appropriate for the level
            - Mix question types (multiple choice, short answer, application)
            - Provide clear answers and explanations
            - Connect questions back to main learning objectives
            
            SEARCH & RESEARCH:
            - Use search capabilities to find current, accurate information
            - Cite reliable educational sources when used
            - Cross-reference information for accuracy
            - Focus on authoritative educational content
            
            Always maintain an encouraging, supportive teaching style that promotes curiosity and deep understanding.
            Focus on helping students not just learn facts, but develop critical thinking and problem-solving skills.
        """,
        add_history_to_context=True,
        num_history_runs=5,
        markdown=True,
        debug_mode=True,
    )

    return tutor_agent
```

---

<a name="examples--streamlit_apps--gemini_tutor--apppy"></a>

### `examples/streamlit_apps/gemini_tutor/app.py`

```python
import nest_asyncio
import streamlit as st
from agents import EDUCATION_LEVELS, GEMINI_MODELS, get_gemini_tutor_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Gemini Tutor",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)

# Educational-specific CSS
st.markdown(
    """
<style>
    .education-level {
        background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: bold;
        font-size: 1.1em;
    }
    
    .learning-objective {
        background-color: rgba(76, 175, 80, 0.1);
        border-left: 4px solid #4CAF50;
        padding: 15px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    .assessment-box {
        background-color: rgba(33, 150, 243, 0.1);
        border-left: 4px solid #2196F3;
        padding: 15px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    .interactive-element {
        background-color: rgba(255, 152, 0, 0.1);
        border-left: 4px solid #FF9800;
        padding: 15px;
        margin: 10px 0;
        border-radius: 5px;
    }
</style>
""",
    unsafe_allow_html=True,
)


def restart_tutor(model_id: str = None, education_level: str = None):
    """Restart the tutor with new settings."""
    target_model = model_id or st.session_state.get("current_model", GEMINI_MODELS[0])
    target_level = education_level or st.session_state.get(
        "education_level", EDUCATION_LEVELS[1]
    )

    new_agent = get_gemini_tutor_agent(
        model_id=target_model,
        education_level=target_level,
        session_id=None,
    )

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["education_level"] = target_level
    st.session_state["is_new_session"] = True


def on_education_level_change():
    """Handle education level changes."""
    selected_level = st.session_state.get("education_level_selector")
    if selected_level:
        current_level = st.session_state.get("education_level")
        if current_level and current_level != selected_level:
            try:
                st.session_state["is_loading_session"] = False
                restart_tutor(education_level=selected_level)
            except Exception as e:
                st.sidebar.error(f"Error changing education level: {str(e)}")


def on_model_change():
    """Handle model changes."""
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in GEMINI_MODELS:
            current_model = st.session_state.get("current_model")
            if current_model and current_model != selected_model:
                try:
                    st.session_state["is_loading_session"] = False
                    restart_tutor(model_id=selected_model)
                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Gemini Tutor</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your intelligent educational AI assistant powered by Google Gemini</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Sidebar - Authentication
    ####################################################################
    st.sidebar.header(" Authentication")
    google_api_key = st.sidebar.text_input(
        "Google API Key",
        type="password",
        help="Get your API key from Google AI Studio (makersuite.google.com)",
    )

    if google_api_key:
        import os

        os.environ["GOOGLE_API_KEY"] = google_api_key
        st.sidebar.success(" Google API key configured")
    else:
        st.sidebar.warning(" Google API key required for Gemini models")
        st.sidebar.info(
            " Get your free API key from [Google AI Studio](https://makersuite.google.com)"
        )

    ####################################################################
    # Model and Education Level selectors
    ####################################################################
    st.sidebar.markdown("---")
    selected_model = st.sidebar.selectbox(
        "Select Gemini Model",
        options=GEMINI_MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    selected_education_level = st.sidebar.selectbox(
        "Select Education Level",
        options=EDUCATION_LEVELS,
        index=1,  # Default to High School
        key="education_level_selector",
        on_change=on_education_level_change,
    )

    ####################################################################
    # Initialize Tutor Agent and Session
    ####################################################################
    gemini_tutor_agent = initialize_agent(
        selected_model,
        lambda model_id, session_id: get_gemini_tutor_agent(
            model_id=model_id,
            education_level=selected_education_level,
            session_id=session_id,
        ),
    )
    reset_session_state(gemini_tutor_agent)

    # Display current education level
    st.sidebar.markdown(
        f"**Current Level:** <span class='education-level'>{selected_education_level}</span>",
        unsafe_allow_html=True,
    )

    if prompt := st.chat_input(" What would you like to learn about today?"):
        add_message("user", prompt)

    ####################################################################
    # Learning Templates
    ####################################################################
    st.sidebar.markdown("####  Learning Templates")

    if st.sidebar.button(" Science Concepts"):
        add_message(
            "user",
            f"Explain a fundamental science concept appropriate for {selected_education_level} level with interactive examples and practice questions.",
        )

    if st.sidebar.button(" Math Problem Solving"):
        add_message(
            "user",
            f"Teach me a math concept with step-by-step problem solving examples suitable for {selected_education_level} students.",
        )

    if st.sidebar.button(" History & Culture"):
        add_message(
            "user",
            f"Create a learning module about a historical event or cultural topic, adapted for {selected_education_level} level.",
        )

    if st.sidebar.button(" Technology & Programming"):
        add_message(
            "user",
            f"Explain a technology or programming concept with hands-on examples for {selected_education_level} learners.",
        )

    ####################################################################
    # Sample Learning Questions
    ####################################################################
    st.sidebar.markdown("####  Sample Questions")

    if st.sidebar.button(" How does DNA work?"):
        add_message(
            "user",
            "How does DNA work? Please explain with examples and create an interactive learning experience.",
        )

    if st.sidebar.button(" Physics of Space Travel"):
        add_message(
            "user",
            "Explain the physics behind space travel with practical examples and thought experiments.",
        )

    if st.sidebar.button(" Art History Overview"):
        add_message(
            "user",
            "Give me an overview of Renaissance art with visual analysis and interactive elements.",
        )

    ####################################################################
    # Study Tools
    ####################################################################
    st.sidebar.markdown("####  Study Tools")

    if st.sidebar.button(" Create Study Guide"):
        add_message(
            "user",
            "Create a comprehensive study guide for my last learning topic with key points, practice questions, and review materials.",
        )

    if st.sidebar.button(" Practice Quiz"):
        add_message(
            "user",
            "Generate a practice quiz based on our recent learning session with different question types and detailed explanations.",
        )

    if st.sidebar.button(" Deep Dive Analysis"):
        add_message(
            "user",
            "Let's do a deep dive analysis of the most complex topic we've discussed, breaking it down into simpler components.",
        )

    ####################################################################
    # Utility buttons
    ####################################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])

    with col1:
        if st.sidebar.button(" New Learning Session", use_container_width=True):
            restart_tutor()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id and gemini_tutor_agent.get_session_name():
                filename = (
                    f"gemini_tutor_session_{gemini_tutor_agent.get_session_name()}.md"
                )
            elif session_id:
                filename = f"gemini_tutor_session_{session_id}.md"
            else:
                filename = "gemini_tutor_session_new.md"

            if st.sidebar.download_button(
                " Export Learning",
                export_chat_history("Gemini Tutor"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} learning interactions",
            ):
                st.sidebar.success("Learning session exported!")
        else:
            st.sidebar.button(
                " Export Learning",
                disabled=True,
                use_container_width=True,
                help="No learning content to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        display_response(gemini_tutor_agent, question)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(
        gemini_tutor_agent,
        selected_model,
        lambda model_id, session_id: get_gemini_tutor_agent(
            model_id=model_id,
            education_level=selected_education_level,
            session_id=session_id,
        ),
    )

    ####################################################################
    # About section
    ####################################################################
    about_section(
        f"This Gemini Tutor provides personalized educational experiences for {selected_education_level} students using Google's advanced Gemini AI models with multimodal learning capabilities."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--geobuddy--agentspy"></a>

### `examples/streamlit_apps/geobuddy/agents.py`

```python
from pathlib import Path
from typing import Optional

from agno.agent import Agent
from agno.media import Image
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.streamlit import get_model_from_id


def get_geobuddy_agent(
    model_id: str = "gemini-2.0-flash-exp",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Get a GeoBuddy Agent for geographical image analysis.

    Args:
        model_id: Model ID to use for analysis
        user_id: Optional user ID for session tracking
        session_id: Optional session ID for conversation continuity

    Returns:
        Agent instance configured for geographical analysis
    """

    model = get_model_from_id(model_id)

    # Create the geography analysis agent
    geobuddy_agent = Agent(
        name="GeoBuddy",
        model=model,
        id="geography-location-detective",
        user_id=user_id,
        session_id=session_id,
        tools=[DuckDuckGoTools()],
        role="Geography Location Detective",
        instructions="""
            You are GeoBuddy, a geography expert who helps identify locations from photos.
            
            When analyzing images, look for these clues:
            
             **Architecture & Buildings**: What style? What materials? Modern or historic?
             **Signs & Text**: Street names, store signs, billboards - any readable text
             **Landmarks**: Famous buildings, monuments, or recognizable structures  
             **Natural Features**: Mountains, coastlines, rivers, distinctive landscapes
             **Cultural Details**: Clothing, vehicles, license plates, local customs
             **Environment**: Weather, vegetation, lighting that hints at climate/region
            
            For each image, provide:
            
            **Location Guess**: Be as specific as possible (street, city, country)
            **Confidence**: How sure are you? (High/Medium/Low)
            **Key Clues**: What made you think of this location?
            **Reasoning**: Walk through your thought process
            **Other Possibilities**: If unsure, what else could it be?
            
            Keep your analysis clear and conversational. Focus on what you can actually see, not speculation.
            Use search when you need to verify landmarks or get more information.
        """,
        add_history_to_context=True,
        num_history_runs=3,
        markdown=True,
        debug_mode=True,
    )

    return geobuddy_agent


def analyze_image_location(agent: Agent, image_path: Path) -> Optional[str]:
    """Analyze an image to predict its geographical location.

    Args:
        agent: The GeoBuddy agent instance
        image_path: Path to the image file

    Returns:
        Analysis result or None if failed
    """
    try:
        prompt = """
        Please analyze this image and predict its geographical location. Use your comprehensive 
        visual analysis framework to identify the location based on all available clues.
        
        Provide a detailed analysis following your structured response format with location prediction,
        visual analysis, reasoning process, and alternative possibilities.
        """

        response = agent.run(prompt, images=[Image(filepath=image_path)])
        return response.content
    except Exception as e:
        raise RuntimeError(f"Error analyzing image location: {str(e)}")
```

---

<a name="examples--streamlit_apps--geobuddy--apppy"></a>

### `examples/streamlit_apps/geobuddy/app.py`

```python
import os
import tempfile
from pathlib import Path

import streamlit as st
from agents import analyze_image_location, get_geobuddy_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)

st.set_page_config(
    page_title="GeoBuddy",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def restart_geobuddy(model_id: str = None):
    """Restart GeoBuddy with new settings."""
    target_model = model_id or st.session_state.get("current_model", MODELS[0])

    new_agent = get_geobuddy_agent(
        model_id=target_model,
        session_id=None,
    )

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True


def on_model_change():
    """Handle model changes."""
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            current_model = st.session_state.get("current_model")
            if current_model and current_model != selected_model:
                try:
                    st.session_state["is_loading_session"] = False
                    restart_geobuddy(model_id=selected_model)
                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>GeoBuddy</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your AI-powered geography detective for location identification</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    st.sidebar.header(" Configuration")
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Sidebar - Authentication
    ####################################################################
    st.sidebar.markdown("---")
    st.sidebar.header(" Authentication")

    if "api_keys" not in st.session_state:
        st.session_state["api_keys"] = {}

    if "gpt" in selected_model.lower() or "openai" in selected_model.lower():
        api_key_label = "OpenAI API Key"
        api_key_env = "OPENAI_API_KEY"
        api_key_help = "Set your OpenAI API key"
    elif "gemini" in selected_model.lower() or "google" in selected_model.lower():
        api_key_label = "Google API Key"
        api_key_env = "GOOGLE_API_KEY"
        api_key_help = "Set your Google API key"
    else:
        api_key_label = "OpenAI API Key"  # Default to OpenAI
        api_key_env = "OPENAI_API_KEY"
        api_key_help = "Set your OpenAI API key"

    current_api_key = st.session_state["api_keys"].get(api_key_env, "")

    api_key = st.sidebar.text_input(
        api_key_label,
        value=current_api_key,
        type="password",
        help=api_key_help,
    )

    if api_key:
        st.session_state["api_keys"][api_key_env] = api_key
        os.environ[api_key_env] = api_key
        st.sidebar.success(f" {api_key_label} configured")
    else:
        st.sidebar.warning(f" {api_key_label} required")

    ####################################################################
    # Initialize GeoBuddy Agent and Session
    ####################################################################
    geobuddy_agent = initialize_agent(
        selected_model,
        lambda model_id, session_id: get_geobuddy_agent(
            model_id=model_id,
            session_id=session_id,
        ),
    )
    reset_session_state(geobuddy_agent)

    if prompt := st.chat_input(
        " Ask me anything about geography or location analysis!"
    ):
        add_message("user", prompt)

    ####################################################################
    # Image Upload Section
    ####################################################################
    st.markdown("###  Image Analysis")

    # Create a clean upload area
    with st.container():
        uploaded_file = st.file_uploader(
            "Choose an image to analyze",
            type=["jpg", "jpeg", "png", "webp"],
            help="Upload a clear image with visible landmarks, architecture, or geographical features",
        )

    if uploaded_file is not None:
        col1, col2 = st.columns([3, 2], gap="large")

        with col1:
            st.markdown("####  Uploaded Image")
            st.image(
                uploaded_file, caption="Image for Analysis", use_container_width=True
            )

        with col2:
            st.markdown("####  Controls")

            st.markdown("")
            analyze_button = st.button(
                " Analyze Location",
                type="primary",
                use_container_width=True,
                help="Click to analyze the geographical location of this image",
            )

            if analyze_button:
                if not api_key:
                    st.error(
                        f" Please provide your {api_key_label} in the sidebar first!"
                    )
                else:
                    with st.spinner(" Analyzing image for geographical clues..."):
                        try:
                            # Save uploaded file temporarily
                            with tempfile.NamedTemporaryFile(
                                delete=False,
                                suffix=f".{uploaded_file.name.split('.')[-1]}",
                            ) as tmp_file:
                                tmp_file.write(uploaded_file.getvalue())
                                tmp_path = Path(tmp_file.name)

                            # Analyze the image
                            result = analyze_image_location(geobuddy_agent, tmp_path)

                            # Clean up temporary file
                            tmp_path.unlink()

                            if result:
                                add_message(
                                    "user",
                                    "Please analyze this uploaded image for geographical location identification.",
                                )
                                add_message("assistant", result)
                                st.success(
                                    " Analysis complete! Check the results below."
                                )
                                st.rerun()
                            else:
                                st.warning(
                                    " Could not analyze the image. Please try a different image."
                                )

                        except Exception as e:
                            st.error(f" Error during analysis: {str(e)}")
                            # Clean up on error
                            if "tmp_path" in locals() and tmp_path.exists():
                                tmp_path.unlink()

    ####################################################################
    # Sample Analysis Options
    ####################################################################
    st.sidebar.markdown("####  Sample Locations")

    if st.sidebar.button(" Famous Landmarks"):
        add_message(
            "user",
            "I'd like to test GeoBuddy with famous landmarks. Can you provide tips for analyzing landmark photos?",
        )

    if st.sidebar.button(" Architectural Styles"):
        add_message(
            "user",
            "How can GeoBuddy identify locations based on architectural styles? What should I look for in buildings?",
        )

    if st.sidebar.button(" Natural Features"):
        add_message(
            "user",
            "What natural geographical features help GeoBuddy identify locations? How do you analyze landscapes?",
        )

    if st.sidebar.button(" Urban Analysis"):
        add_message(
            "user",
            "How does GeoBuddy analyze urban environments and city characteristics for location identification?",
        )

    ####################################################################
    # Utility buttons
    ####################################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])

    with col1:
        if st.sidebar.button(" New Analysis Session", use_container_width=True):
            restart_geobuddy()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id:
                try:
                    session_name = geobuddy_agent.get_session_name()
                    if session_name:
                        filename = f"geobuddy_analysis_{session_name}.md"
                    else:
                        filename = f"geobuddy_analysis_{session_id}.md"
                except Exception:
                    filename = f"geobuddy_analysis_{session_id}.md"
            else:
                filename = "geobuddy_analysis_new.md"

            if st.sidebar.download_button(
                " Export Analysis",
                export_chat_history("GeoBuddy"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} analysis results",
            ):
                st.sidebar.success("Analysis exported!")
        else:
            st.sidebar.button(
                " Export Analysis",
                disabled=True,
                use_container_width=True,
                help="No analysis to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        display_response(geobuddy_agent, question)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(
        geobuddy_agent,
        selected_model,
        lambda model_id, session_id: get_geobuddy_agent(
            model_id=model_id,
            session_id=session_id,
        ),
    )

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This GeoBuddy agent analyzes images to predict geographical locations using advanced visual analysis of landmarks, architecture, and cultural clues."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--github_mcp_agent--agentspy"></a>

### `examples/streamlit_apps/github_mcp_agent/agents.py`

```python
import os
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from agno.utils.log import logger
from agno.utils.streamlit import get_model_from_id


async def run_github_agent(message: str, model_id: str = "gpt-4o"):
    if not os.getenv("GITHUB_TOKEN"):
        return "Error: GitHub token not provided"

    try:
        # Initialize MCP toolkit
        async with MCPTools(
            command="npx -y @modelcontextprotocol/server-github"
        ) as mcp_tools:
            model = get_model_from_id(model_id)

            # Create agent
            agent = Agent(
                tools=[mcp_tools],
                model=model,
                instructions=dedent("""\
                    You are a GitHub assistant. Help users explore repositories and their activity.
                    - Provide organized, concise insights about the repository
                    - Focus on facts and data from the GitHub API
                    - Use markdown formatting for better readability
                    - Present numerical data in tables when appropriate
                    - Include links to relevant GitHub pages when helpful
                """),
                markdown=True,
            )

            # Run agent
            response = await agent.arun(message)
            return response.content
    except Exception as e:
        logger.error(f"Error running GitHub MCP agent: {e}")
        return f"Error: {str(e)}"
```

---

<a name="examples--streamlit_apps--github_mcp_agent--apppy"></a>

### `examples/streamlit_apps/github_mcp_agent/app.py`

```python
import asyncio
import os

import streamlit as st
from agents import run_github_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    export_chat_history,
)

st.set_page_config(
    page_title="GitHub MCP Agent",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def restart_agent():
    """Reset the agent session"""
    st.session_state["messages"] = []
    st.session_state["is_new_session"] = True


def on_model_change():
    """Handle model selection change"""
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            current_model = st.session_state.get("current_model")
            if current_model and current_model != selected_model:
                st.session_state["current_model"] = selected_model
                restart_agent()
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>GitHub MCP Agent</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Explore GitHub repositories with natural language using the Model Context Protocol</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Sidebar - Authentication
    ####################################################################
    st.sidebar.header(" Authentication")
    github_token = st.sidebar.text_input(
        "GitHub Token",
        type="password",
        help="Create a token with repo scope at github.com/settings/tokens",
    )

    if github_token:
        os.environ["GITHUB_TOKEN"] = github_token
        st.sidebar.success(" GitHub token configured")
    else:
        st.sidebar.warning(" GitHub token required")

    ####################################################################
    # Model selector
    ####################################################################
    st.sidebar.markdown("---")
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Repository and Query Input
    ####################################################################
    col1, col2 = st.columns([3, 1])

    with col1:
        repo = st.text_input(
            "Repository", value="agno-agi/agno", help="Format: owner/repo", key="repo"
        )

    with col2:
        st.selectbox(
            "Query Type",
            ["Issues", "Pull Requests", "Repository Activity", "Custom"],
            key="query_type",
        )

    ####################################################################
    # Sample Questions
    ####################################################################
    st.sidebar.markdown("####  Sample Questions")
    if st.sidebar.button(" Issues by label"):
        add_message("user", f"Show me issues by label in {repo}")
    if st.sidebar.button(" Recent PRs"):
        add_message("user", f"Show me recent merged PRs in {repo}")
    if st.sidebar.button(" Repository health"):
        add_message("user", f"Show repository health metrics for {repo}")

    ####################################################################
    # Utility buttons
    ####################################################################
    st.sidebar.markdown("####  Utilities")

    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )
        if has_messages:
            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("GitHub Agent"),
                file_name=f"github_mcp_chat_{repo.replace('/', '_')}.md",
                mime="text/markdown",
                use_container_width=True,
            ):
                st.sidebar.success("Chat history exported!")

    # About section
    about_section(
        "This GitHub MCP Agent helps you analyze repositories using natural language queries."
    )

    ####################################################################
    # Chat input and processing
    ####################################################################
    if prompt := st.chat_input("Ask me anything about this GitHub repository!"):
        add_message("user", prompt)

    ####################################################################
    # Process user input or button queries
    ####################################################################
    if st.session_state.get("messages"):
        last_message = st.session_state["messages"][-1]
        if last_message["role"] == "user":
            user_query = last_message["content"]

            # Ensure repo is mentioned in query
            if repo and repo not in user_query:
                full_query = f"{user_query} in {repo}"
            else:
                full_query = user_query

            with st.spinner("Analyzing GitHub repository..."):
                try:
                    result = asyncio.run(run_github_agent(full_query, selected_model))
                    add_message("assistant", result)
                except Exception as e:
                    error_msg = f"Error: {str(e)}"
                    add_message("assistant", error_msg)
            st.rerun()

    ####################################################################
    # Display chat messages
    ####################################################################
    display_chat_messages()


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--github_repo_analyzer--agentspy"></a>

### `examples/streamlit_apps/github_repo_analyzer/agents.py`

```python
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.tools.github import GithubTools
from agno.utils.streamlit import get_model_from_id


def get_github_agent(
    model_id: str = "openai:gpt-4o",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Get a GitHub Repository Analyzer Agent"""

    agent = Agent(
        name="GitHub Repository Analyzer",
        model=get_model_from_id(model_id),
        id="github-repo-analyzer",
        user_id=user_id,
        session_id=session_id,
        tools=[
            GithubTools(),
        ],
        description=dedent("""
            You are an expert Code Reviewing Agent specializing in analyzing GitHub repositories,
            with a strong focus on detailed code reviews for Pull Requests.
            Use your tools to answer questions accurately and provide insightful analysis.
        """),
        instructions=dedent("""\
        **Task:** Analyze GitHub repositories and answer user questions based on the available tools and conversation history.

        **Repository Context Management:**
        1.  **Context Persistence:** Once a target repository (owner/repo) is identified (either initially or from a user query like 'analyze owner/repo'), **MAINTAIN THAT CONTEXT** for all subsequent questions in the current conversation unless the user clearly specifies a *different* repository.
        2.  **Determining Context:** If no repository is specified in the *current* user query, **CAREFULLY REVIEW THE CONVERSATION HISTORY** to find the most recently established target repository. Use that repository context.
        3.  **Accuracy:** When extracting a repository name (owner/repo) from the query or history, **BE EXTREMELY CAREFUL WITH SPELLING AND FORMATTING**. Double-check against the user's exact input.
        4.  **Ambiguity:** If no repository context has been established in the conversation history and the current query doesn't specify one, **YOU MUST ASK THE USER** to clarify which repository (using owner/repo format) they are interested in before using tools that require a repository name.

        **How to Answer Questions:**
        *   **Identify Key Information:** Understand the user's goal and the target repository (using the context rules above).
        *   **Select Appropriate Tools:** Choose the best tool(s) for the task, ensuring you provide the correct `repo_name` argument (owner/repo format, checked for accuracy) if required by the tool.
            *   Project Overview: `get_repository`, `get_file_content` (for README.md).
            *   Libraries/Dependencies: `get_file_content` (for requirements.txt, pyproject.toml, etc.), `get_directory_content`, `search_code`.
            *   PRs/Issues: Use relevant PR/issue tools.
            *   List User Repos: `list_repositories` (no repo_name needed).
            *   Search Repos: `search_repositories` (no repo_name needed).
        *   **Execute Tools:** Run the selected tools.
        *   **Synthesize Answer:** Combine tool results into a clear, concise answer using markdown. If a tool fails (e.g., 404 error because the repo name was incorrect), state that you couldn't find the specified repository and suggest checking the name.
        *   **Cite Sources:** Mention specific files (e.g., "According to README.md...").

        **Specific Analysis Areas (Most require a specific repository):**
        *   Issues: Listing, summarizing, searching.
        *   Pull Requests (PRs): Listing, summarizing, searching, getting details/changes.
        *   Code & Files: Searching code, getting file content, listing directory contents.
        *   Repository Stats & Activity: Stars, contributors, recent activity.

        **Code Review Guidelines (Requires repository and PR):**
        *   Fetch Changes: Use `get_pull_request_changes` or `get_pull_request_with_details`.
        *   Analyze Patch: Evaluate based on functionality, best practices, style, clarity, efficiency.
        *   Present Review: Structure clearly, cite lines/code, be constructive.
        """),
        markdown=True,
        debug_mode=True,
        add_history_to_context=True,
    )

    return agent
```

---

<a name="examples--streamlit_apps--github_repo_analyzer--apppy"></a>

### `examples/streamlit_apps/github_repo_analyzer/app.py`

```python
import nest_asyncio
import streamlit as st
from agents import get_github_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="GitHub Repository Analyzer",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def restart_agent(model_id: str = None):
    target_model = model_id or st.session_state.get("current_model", MODELS[0])

    new_agent = get_github_agent(model_id=target_model, session_id=None)

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            new_model_id = selected_model
            current_model = st.session_state.get("current_model")

            if current_model and current_model != new_model_id:
                try:
                    st.session_state["is_loading_session"] = False
                    # Start new chat
                    restart_agent(model_id=new_model_id)

                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>GitHub Repository Analyzer</h1>", unsafe_allow_html=True
    )
    st.markdown(
        "<p class='subtitle'>Your intelligent GitHub analysis assistant powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Initialize Agent and Session
    ####################################################################
    github_analyzer_agent = initialize_agent(selected_model, get_github_agent)
    reset_session_state(github_analyzer_agent)

    if prompt := st.chat_input(" Ask me about GitHub repositories!"):
        add_message("user", prompt)

    ####################################################################
    # GitHub Configuration
    ####################################################################
    st.sidebar.markdown("####  Configuration")

    github_token = st.sidebar.text_input(
        "GitHub Personal Access Token",
        type="password",
        help="Optional: Provides access to private repositories and higher rate limits",
        placeholder="ghp_xxxxxxxxxxxx",
    )

    if github_token:
        st.sidebar.success(" GitHub token configured")
    else:
        st.sidebar.info(" Add your GitHub token for enhanced access")

    st.sidebar.markdown(
        "[How to create a GitHub PAT?](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic)"
    )

    ###############################################################
    # Sample Questions
    ###############################################################
    st.sidebar.markdown("####  Sample Questions")

    if st.sidebar.button(" Analyze agno-agi/agno"):
        add_message(
            "user",
            "Analyze the repository 'agno-agi/agno' - show me the structure, main languages, and recent activity",
        )

    if st.sidebar.button(" Latest Issues"):
        add_message(
            "user",
            "Show me the latest issues in 'microsoft/vscode'",
        )

    if st.sidebar.button(" Review Latest PR"):
        add_message(
            "user",
            "Find and review the latest pull request in 'facebook/react'",
        )

    if st.sidebar.button(" Repository Stats"):
        add_message(
            "user",
            "What are the repository statistics for 'tensorflow/tensorflow'?",
        )

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id:
                try:
                    session_name = github_analyzer_agent.get_session_name()
                    if session_name:
                        filename = f"github_analyzer_chat_{session_name}.md"
                    else:
                        filename = f"github_analyzer_chat_{session_id}.md"
                except Exception:
                    filename = f"github_analyzer_chat_{session_id}.md"
            else:
                filename = "github_analyzer_chat_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("GitHub Repository Analyzer"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        display_response(github_analyzer_agent, question)

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This GitHub Repository Analyzer helps you analyze code repositories, review pull requests, and understand project structures using natural language queries."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--image_generation--agentspy"></a>

### `examples/streamlit_apps/image_generation/agents.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFImageReader
from agno.tools.openai import OpenAITools
from agno.utils.streamlit import get_model_from_id
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

DEFAULT_RECIPE_URL = "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"


def get_recipe_image_agent(
    model_id: str = "openai:gpt-4o",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    local_pdf_path: Optional[str] = None,
) -> Agent:
    """Get a Recipe Image Generation Agent with Knowledge Base"""

    # Choose the appropriate knowledge base
    if local_pdf_path:
        knowledge = Knowledge(
            name="Recipe Knowledge Base",
            description="Custom uploaded recipe collection",
            vector_db=PgVector(
                db_url=db_url,
                table_name="recipe_image_documents",
                embedder=OpenAIEmbedder(id="text-embedding-3-small"),
            ),
            max_results=3,
        )
        knowledge.add_content(
            name=f"Uploaded Recipe: {local_pdf_path.split('/')[-1]}",
            path=local_pdf_path,
            reader=PDFImageReader(),
            description="Custom uploaded recipe PDF",
        )
    else:
        knowledge = Knowledge(
            name="Recipe Knowledge Base",
            description="Thai recipe collection with step-by-step instructions",
            vector_db=PgVector(
                db_url=db_url,
                table_name="recipe_image_documents",
                embedder=OpenAIEmbedder(id="text-embedding-3-small"),
            ),
            max_results=3,
        )
        knowledge.add_content(
            name="Thai Recipes Collection",
            url=DEFAULT_RECIPE_URL,
            description="Comprehensive Thai recipe book with traditional dishes",
        )

    agent = Agent(
        name="Recipe Image Generator",
        model=get_model_from_id(model_id),
        id="recipe-image-agent",
        user_id=user_id,
        knowledge=knowledge,
        add_history_to_context=True,
        num_history_runs=3,
        session_id=session_id,
        tools=[OpenAITools(image_model="gpt-image-1")],
        instructions="""
            You are a specialized recipe assistant that creates visual cooking guides.
            
            When asked for a recipe:
            1. **Search Knowledge Base**: Use the `search_knowledge_base` tool to find the most relevant recipe
            2. **Format Recipe**: Extract and present the recipe in exactly this format:
            
               ## Ingredients
               - List each ingredient with quantities using bullet points
               
               ## Directions  
               1. Step-by-step numbered instructions
               2. Be clear and concise for each cooking step
               3. Include cooking times and temperatures where relevant
               
            3. **Generate Visual Guide**: After presenting the recipe, use the `generate_image` tool with a prompt like:
               '{Dish Name}: A step-by-step visual cooking guide showing all preparation and cooking steps in one overhead view with bright natural lighting. Include all ingredients and show the progression from raw ingredients to final plated dish.'
               
            4. **Maintain Quality**: 
               - Ensure visual consistency across images
               - Include all ingredients and key steps in the image
               - Use bright, appetizing lighting and overhead perspective
               - Show the complete cooking process in one comprehensive view
               
            5. **Complete the Response**: End with 'Recipe generation complete!'
            
            Keep responses focused, clear, and visually appealing. Always search the knowledge base first before responding.
        """,
        markdown=True,
        debug_mode=True,
    )

    return agent
```

---

<a name="examples--streamlit_apps--image_generation--apppy"></a>

### `examples/streamlit_apps/image_generation/app.py`

```python
import io
import tempfile
from os import unlink

import nest_asyncio
import streamlit as st
from agents import get_recipe_image_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_tool_calls,
    export_chat_history,
    initialize_agent,
    knowledge_base_info_widget,
    reset_session_state,
    session_selector_widget,
)
from PIL import Image

nest_asyncio.apply()
st.set_page_config(
    page_title="Recipe Image Generator",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def restart_agent(model_id: str = None):
    target_model = model_id or st.session_state.get("current_model", MODELS[0])

    new_agent = get_recipe_image_agent(model_id=target_model, session_id=None)

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            new_model_id = selected_model
            current_model = st.session_state.get("current_model")

            if current_model and current_model != new_model_id:
                try:
                    st.session_state["is_loading_session"] = False
                    # Start new chat
                    restart_agent(model_id=new_model_id)

                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Recipe Image Generator</h1>", unsafe_allow_html=True
    )
    st.markdown(
        "<p class='subtitle'>Your AI cooking companion - Upload recipes or use defaults, then get visual step-by-step cooking guides!</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Initialize Agent and Session
    ####################################################################
    recipe_image_agent = initialize_agent(selected_model, get_recipe_image_agent)
    reset_session_state(recipe_image_agent)

    if prompt := st.chat_input(" Ask me for a recipe (e.g., 'Recipe for Pad Thai')"):
        add_message("user", prompt)

    ####################################################################
    # Recipe Management
    ####################################################################
    st.sidebar.markdown("####  Recipe Management")
    knowledge_base_info_widget(recipe_image_agent)

    # File upload
    uploaded_file = st.sidebar.file_uploader(
        "Upload Recipe PDF (.pdf)", type=["pdf"], key="recipe_upload"
    )
    if uploaded_file and not prompt:
        alert = st.sidebar.info("Processing recipe PDF...", icon="")
        try:
            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name

            recipe_image_agent.knowledge.add_content(
                name=f"Uploaded Recipe: {uploaded_file.name}",
                path=tmp_path,
                description=f"Custom recipe PDF: {uploaded_file.name}",
            )

            unlink(tmp_path)
            st.sidebar.success(f"{uploaded_file.name} added to recipe collection")
        except Exception as e:
            st.sidebar.error(f"Error processing recipe PDF: {str(e)}")
        finally:
            alert.empty()

    if st.sidebar.button("Clear Recipe Collection"):
        if recipe_image_agent.knowledge.vector_db:
            recipe_image_agent.knowledge.vector_db.delete()
        st.sidebar.success("Recipe collection cleared")

    ###############################################################
    # Sample Recipes
    ###############################################################
    st.sidebar.markdown("####  Sample Recipes")
    if st.sidebar.button(" Recipe for Pad Thai"):
        add_message("user", "Recipe for Pad Thai with visual steps")
    if st.sidebar.button(" Recipe for Som Tum"):
        add_message("user", "Recipe for Som Tum (Papaya Salad)")
    if st.sidebar.button(" Recipe for Tom Kha Gai"):
        add_message("user", "Recipe for Tom Kha Gai soup")

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id:
                try:
                    session_name = recipe_image_agent.get_session_name()
                    if session_name:
                        filename = f"recipe_chat_{session_name}.md"
                    else:
                        filename = f"recipe_chat_{session_id}.md"
                except Exception:
                    filename = f"recipe_chat_{session_id}.md"
            else:
                filename = "recipe_chat_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("Recipe Image Generator"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message with image handling
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]

        with st.chat_message("assistant"):
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner(" Thinking..."):
                response = ""
                try:
                    # Run the agent and stream the response
                    run_response = recipe_image_agent.run(question, stream=True)
                    for resp_chunk in run_response:
                        try:
                            # Display tool calls if available
                            if hasattr(resp_chunk, "tool") and resp_chunk.tool:
                                display_tool_calls(
                                    tool_calls_container, [resp_chunk.tool]
                                )
                        except Exception:
                            pass

                        if resp_chunk.content is not None:
                            content = str(resp_chunk.content)
                            if not (
                                content.strip().endswith("completed in")
                                or "completed in" in content
                                and "s." in content
                            ):
                                response += content
                                resp_container.markdown(response)

                        if hasattr(resp_chunk, "images") and getattr(
                            resp_chunk, "images", None
                        ):
                            captured_run_output = resp_chunk

                    # Display generated images
                    if captured_run_output and hasattr(captured_run_output, "images"):
                        for i, img in enumerate(captured_run_output.images or []):
                            try:
                                if hasattr(img, "content") and img.content:
                                    image = Image.open(io.BytesIO(img.content))
                                    st.image(
                                        image,
                                        caption=f"Step-by-step cooking guide {i + 1}",
                                        use_container_width=True,
                                    )
                                elif hasattr(img, "url") and img.url:
                                    st.image(
                                        img.url,
                                        caption=f"Step-by-step cooking guide {i + 1}",
                                        use_container_width=True,
                                    )
                            except Exception as img_error:
                                st.warning(
                                    f"Could not display image {i + 1}: {str(img_error)}"
                                )

                    # Add message with tools
                    try:
                        if captured_run_output and hasattr(
                            captured_run_output, "tools"
                        ):
                            add_message(
                                "assistant", response, captured_run_output.tools
                            )
                        else:
                            add_message("assistant", response)
                    except Exception:
                        add_message("assistant", response)

                except Exception as e:
                    error_message = f"Sorry, I encountered an error: {str(e)}"
                    add_message("assistant", error_message)
                    st.error(error_message)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(recipe_image_agent, selected_model, get_recipe_image_agent)

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Recipe Image Generator creates visual step-by-step cooking guides from recipe collections. Upload your own recipes or use the built-in Thai recipe collection."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--llama_tutor--agentspy"></a>

### `examples/streamlit_apps/llama_tutor/agents.py`

```python
import os
from datetime import datetime
from pathlib import Path
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.groq import Groq
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.file import FileTools
from agno.utils.streamlit import get_model_from_id


def get_tutor_model(model_id: str):
    """Get model for tutor - handles groq and other providers"""
    if model_id.startswith("groq:"):
        model_name = model_id.split("groq:")[1]
        groq_api_key = os.environ.get("GROQ_API_KEY")
        return Groq(id=model_name, api_key=groq_api_key)
    else:
        return get_model_from_id(model_id)


# Set up paths
current_dir = Path(__file__).parent
output_dir = current_dir / "output"
output_dir.mkdir(parents=True, exist_ok=True)

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


def get_llama_tutor_agent(
    model_id: str = "groq:llama-3.3-70b-versatile",
    education_level: str = "High School",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Get a Llama Tutor Agent with education level customization"""

    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    # Tools for educational assistance
    tools = [
        ExaTools(
            start_published_date=datetime.now().strftime("%Y-%m-%d"),
            type="keyword",
            num_results=5,
            show_results=True,
        ),
        DuckDuckGoTools(
            timeout=20,
            fixed_max_results=5,
        ),
        FileTools(base_dir=output_dir),
    ]

    description = dedent(f"""
        You are Llama Tutor, an educational AI assistant designed to teach concepts at a {education_level} level.
        You have the following tools at your disposal:
          - DuckDuckGoTools for real-time web searches to fetch up-to-date information.
          - ExaTools for structured, in-depth analysis.
          - FileTools for saving the output upon user confirmation.

        Your response should always be clear, concise, and detailed, tailored to a {education_level} student's understanding.
        Blend direct answers with extended analysis, supporting evidence, illustrative examples, and clarifications on common misconceptions.
        Engage the user with follow-up questions to check understanding and deepen learning.

        <critical>
        - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.
        - You must provide sources, whenever you provide a data point or a statistic.
        - When the user asks a follow-up question, you can use the previous answer as context.
        - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.
        </critical>
    """)

    instructions = dedent(f"""
        Here's how you should answer the user's question:

        1. Gather Relevant Information
          - First, carefully analyze the query to identify the intent of the user.
          - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.
          - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.
          - Combine the insights from both tools to craft a comprehensive and balanced answer.
          - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.
          - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.

        2. Construct Your Response
          - **Start** with a succinct, clear and direct answer that immediately addresses the user's query, tailored to a {education_level} level.
          - **Then expand** the answer by including:
               A clear explanation with context and definitions appropriate for {education_level} students.
               Supporting evidence such as statistics, real-world examples, and data points that are understandable at a {education_level} level.
               Clarifications that address common misconceptions students at this level might have.
          - Structure your response with clear headings, bullet points, and organized paragraphs to make it easy to follow.
          - Include interactive elements like questions to check understanding or mini-quizzes when appropriate.
          - Use analogies and examples that would be familiar to students at a {education_level} level.

        3. Enhance Engagement
          - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)"
          - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.
          - Suggest follow-up topics or questions that might deepen their understanding.

        4. Final Quality Check & Presentation 
          - Review your response to ensure clarity, depth, and engagement.
          - Ensure the language and concepts are appropriate for a {education_level} level.
          - Make complex ideas accessible without oversimplifying to the point of inaccuracy.

        5. In case of any uncertainties, clarify limitations and encourage follow-up queries.
    """)

    agent = Agent(
        name="Llama Tutor",
        model=get_tutor_model(model_id),
        id="llama-tutor-agent",
        user_id=user_id,
        session_id=session_id,
        db=db,
        tools=tools,
        read_chat_history=True,
        read_tool_call_history=True,
        add_history_to_context=True,
        num_history_runs=5,
        add_datetime_to_context=True,
        add_name_to_context=True,
        description=description,
        instructions=instructions,
        markdown=True,
        debug_mode=True,
    )

    return agent
```

---

<a name="examples--streamlit_apps--llama_tutor--apppy"></a>

### `examples/streamlit_apps/llama_tutor/app.py`

```python
import nest_asyncio
import streamlit as st
from agents import get_llama_tutor_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Llama Tutor",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)

# Extended models list including Groq models for Llama Tutor
TUTOR_MODELS = MODELS + [
    "groq:llama-3.3-70b-versatile",
    "groq:llama-3.1-70b-versatile",
    "groq:mixtral-8x7b-32768",
]


def restart_agent(model_id: str = None, education_level: str = None):
    target_model = model_id or st.session_state.get("current_model", TUTOR_MODELS[0])
    target_education_level = education_level or st.session_state.get(
        "education_level", "High School"
    )

    new_agent = get_llama_tutor_agent(
        model_id=target_model, education_level=target_education_level, session_id=None
    )

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["education_level"] = target_education_level
    st.session_state["is_new_session"] = True


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in TUTOR_MODELS:
            new_model_id = selected_model
            current_model = st.session_state.get("current_model")

            if current_model and current_model != new_model_id:
                try:
                    st.session_state["is_loading_session"] = False
                    # Start new chat with new model
                    restart_agent(model_id=new_model_id)

                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def on_education_level_change():
    selected_level = st.session_state.get("education_level_selector")
    current_level = st.session_state.get("education_level", "High School")

    if selected_level and selected_level != current_level:
        try:
            # Start new chat with new education level
            restart_agent(education_level=selected_level)
        except Exception as e:
            st.sidebar.error(f"Error switching to {selected_level}: {str(e)}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'>Llama Tutor</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Your intelligent educational assistant powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=TUTOR_MODELS,
        index=len(MODELS),  # Default to first Groq model
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Education level selector
    ####################################################################
    education_levels = [
        "Elementary School",
        "Middle School",
        "High School",
        "College",
        "Undergrad",
        "Graduate",
    ]

    selected_education_level = st.sidebar.selectbox(
        "Education Level",
        options=education_levels,
        index=2,  # Default to High School
        key="education_level_selector",
        on_change=on_education_level_change,
    )

    ####################################################################
    # Initialize Agent and Session
    ####################################################################
    # Store the education level in session state for agent creation
    if "education_level" not in st.session_state:
        st.session_state["education_level"] = selected_education_level

    llama_tutor_agent = initialize_agent(
        selected_model,
        lambda model_id, session_id: get_llama_tutor_agent(
            model_id=model_id,
            education_level=st.session_state.get("education_level", "High School"),
            session_id=session_id,
        ),
    )
    reset_session_state(llama_tutor_agent)

    if prompt := st.chat_input(" What would you like to learn about?"):
        add_message("user", prompt)

    ###############################################################
    # Sample Questions
    ###############################################################
    st.sidebar.markdown("####  Sample Questions")
    if st.sidebar.button(" How does photosynthesis work?"):
        add_message(
            "user",
            "How does photosynthesis work?",
        )
    if st.sidebar.button(" Explain calculus basics"):
        add_message(
            "user",
            "What is calculus and how is it used in real life?",
        )
    if st.sidebar.button(" Causes of World War I"):
        add_message(
            "user",
            "What were the main causes of World War I?",
        )
    if st.sidebar.button(" What is quantum physics?"):
        add_message(
            "user",
            "Explain quantum physics in simple terms",
        )

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id:
                try:
                    session_name = llama_tutor_agent.get_session_name()
                    if session_name:
                        filename = f"llama_tutor_analysis_{session_name}.md"
                    else:
                        filename = f"llama_tutor_analysis_{session_id}.md"
                except Exception:
                    filename = f"llama_tutor_analysis_{session_id}.md"
            else:
                filename = "llama_tutor_analysis_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("Llama Tutor"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        display_response(llama_tutor_agent, question)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(
        llama_tutor_agent,
        selected_model,
        lambda model_id, session_id: get_llama_tutor_agent(
            model_id=model_id,
            education_level=st.session_state.get("education_level", "High School"),
            session_id=session_id,
        ),
    )

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Llama Tutor provides personalized educational assistance across all subjects and education levels."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--mcp_agent--apppy"></a>

### `examples/streamlit_apps/mcp_agent/app.py`

```python
import asyncio

import nest_asyncio
import streamlit as st
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_tool_calls,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)
from mcp_client import MCPClient, MCPServerConfig

from mcp_agent import get_mcp_agent

nest_asyncio.apply()
st.set_page_config(
    page_title="Universal MCP Agent",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)

# MCP Server configurations
MCP_SERVERS = {
    "GitHub": MCPServerConfig(
        id="github", command="npx", args=["-y", "@modelcontextprotocol/server-github"]
    ),
}


async def initialize_mcp_client(server_config: MCPServerConfig):
    """Initialize MCP client and connect to server."""
    try:
        if (
            "mcp_client" not in st.session_state
            or st.session_state.get("mcp_server_id") != server_config.id
            or getattr(st.session_state.get("mcp_client", None), "session", None)
            is None
        ):
            # Initialize new MCP client
            st.session_state["mcp_client"] = MCPClient()

        mcp_client = st.session_state["mcp_client"]
        mcp_tools = await mcp_client.connect_to_server(server_config)
        st.session_state["mcp_server_id"] = server_config.id

        return mcp_tools
    except Exception as e:
        st.error(f"Failed to connect to MCP server {server_config.id}: {str(e)}")
        return None


def restart_agent(model_id: str = None, mcp_server: str = None):
    """Restart agent with new configuration."""
    target_model = model_id or st.session_state.get("current_model", MODELS[0])
    target_server = mcp_server or st.session_state.get("current_mcp_server", "GitHub")

    # Clear MCP client to force reconnection
    if "mcp_client" in st.session_state:
        del st.session_state["mcp_client"]

    st.session_state["current_model"] = target_model
    st.session_state["current_mcp_server"] = target_server
    st.session_state["messages"] = []
    st.session_state["is_new_session"] = True


def on_model_change():
    """Handle model selection change."""
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            current_model = st.session_state.get("current_model")
            if current_model and current_model != selected_model:
                try:
                    restart_agent(model_id=selected_model)
                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def on_mcp_server_change():
    """Handle MCP server selection change."""
    selected_server = st.session_state.get("mcp_server_selector")
    if selected_server:
        current_server = st.session_state.get("current_mcp_server", "GitHub")
        if current_server != selected_server:
            try:
                restart_agent(mcp_server=selected_server)
            except Exception as e:
                st.sidebar.error(f"Error switching to {selected_server}: {str(e)}")


async def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Universal MCP Agent</h1>", unsafe_allow_html=True
    )
    st.markdown(
        "<p class='subtitle'>Your intelligent interface to MCP servers powered by Agno</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # MCP Server selector
    ####################################################################
    selected_mcp_server = st.sidebar.selectbox(
        "Select MCP Server",
        options=list(MCP_SERVERS.keys()),
        index=0,
        key="mcp_server_selector",
        on_change=on_mcp_server_change,
    )

    # Get current server configuration
    current_server = st.session_state.get("current_mcp_server", selected_mcp_server)
    server_config = MCP_SERVERS[current_server]

    ####################################################################
    # Initialize MCP Client and Tools
    ####################################################################
    mcp_tools = await initialize_mcp_client(server_config)
    if not mcp_tools:
        st.error("Failed to initialize MCP server. Please check the configuration.")
        return

    ####################################################################
    # Initialize Agent
    ####################################################################
    def create_agent(model_id: str, session_id: str = None):
        return get_mcp_agent(
            model_id=model_id,
            session_id=session_id,
            mcp_tools=[mcp_tools],
            mcp_server_ids=[server_config.id],
        )

    mcp_agent = initialize_agent(selected_model, create_agent)

    # Update agent tools if they've changed
    if hasattr(mcp_agent, "tools"):
        mcp_agent.tools = [mcp_tools]

    reset_session_state(mcp_agent)

    if prompt := st.chat_input(" How can I help you with MCP?"):
        add_message("user", prompt)

    ####################################################################
    # MCP Server Information
    ####################################################################
    st.sidebar.markdown("####  MCP Server Info")
    st.sidebar.info(f"**Connected to:** {server_config.id}")
    st.sidebar.info(f"**Command:** {server_config.command}")
    if server_config.args:
        st.sidebar.info(f"**Args:** {' '.join(server_config.args)}")

    ####################################################################
    # Sample Questions
    ####################################################################
    st.sidebar.markdown("####  Sample Questions")

    if current_server == "GitHub":
        if st.sidebar.button(" Search repositories"):
            add_message("user", "Search for repositories related to machine learning")
        if st.sidebar.button(" Repository info"):
            add_message("user", "Tell me about a popular Python repository")
        if st.sidebar.button(" List issues"):
            add_message("user", "Show me recent issues in a repository")

    elif current_server == "Filesystem":
        if st.sidebar.button(" List files"):
            add_message("user", "List files in the current directory")
        if st.sidebar.button(" Read file"):
            add_message("user", "Show me the contents of a text file")
        if st.sidebar.button(" Create file"):
            add_message("user", "Create a new file with some sample content")

    if st.sidebar.button(" What is MCP?"):
        add_message("user", "What is the Model Context Protocol and how does it work?")

    ####################################################################
    # Utility buttons
    ####################################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])

    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id and mcp_agent.get_session_name():
                filename = f"mcp_agent_chat_{mcp_agent.get_session_name()}.md"
            elif session_id:
                filename = f"mcp_agent_chat_{session_id}.md"
            else:
                filename = "mcp_agent_chat_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("Universal MCP Agent"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]

        # Custom response handling for MCP agent (async)
        with st.chat_message("assistant"):
            tool_calls_container = st.empty()
            resp_container = st.empty()
            with st.spinner(" Thinking..."):
                response = ""
                try:
                    # Run the agent asynchronously and stream the response
                    async for resp_chunk in mcp_agent.arun(question, stream=True):
                        try:
                            # Display tool calls if available
                            if hasattr(resp_chunk, "tool") and resp_chunk.tool:
                                display_tool_calls(
                                    tool_calls_container, [resp_chunk.tool]
                                )
                        except Exception:
                            pass  # Continue even if tool display fails

                        if resp_chunk.content is not None:
                            content = str(resp_chunk.content)
                            if not (
                                content.strip().endswith("completed in")
                                or "completed in" in content
                                and "s." in content
                            ):
                                response += content
                                resp_container.markdown(response)

                    if resp_chunk and hasattr(resp_chunk, "tools") and resp_chunk.tools:
                        add_message("assistant", response, resp_chunk.tools)
                    else:
                        add_message("assistant", response)

                except Exception as e:
                    st.error(f"Sorry, I encountered an error: {str(e)}")

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(mcp_agent, selected_model, create_agent)

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Universal MCP Agent provides a unified interface for interacting with MCP servers, enabling seamless access to various data sources and tools."
    )


def run_app():
    """Run the async streamlit app."""
    asyncio.run(main())


if __name__ == "__main__":
    run_app()
```

---

<a name="examples--streamlit_apps--mcp_agent--mcp_agentpy"></a>

### `examples/streamlit_apps/mcp_agent/mcp_agent.py`

```python
from textwrap import dedent
from typing import List, Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.tools.mcp import MCPTools
from agno.utils.streamlit import get_model_from_id
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


def get_mcp_agent(
    model_id: str = "openai:gpt-4o",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    mcp_tools: Optional[List[MCPTools]] = None,
    mcp_server_ids: Optional[List[str]] = None,
) -> Agent:
    """Get a Universal MCP Agent."""

    # Database for sessions
    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    # Knowledge base for MCP documentation
    contents_db = PostgresDb(
        db_url=db_url,
        knowledge_table="mcp_agent_knowledge_contents",
        db_schema="ai",
    )

    knowledge_base = Knowledge(
        name="MCP Agent Knowledge Base",
        description="Knowledge base for MCP documentation and usage",
        vector_db=PgVector(
            db_url=db_url,
            table_name="mcp_agent_documents",
            schema="ai",
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
        contents_db=contents_db,
        max_results=3,
    )

    try:
        knowledge_base.add_content(
            url="https://modelcontextprotocol.io/llms-full.txt",
            name="MCP Documentation",
            description="Complete Model Context Protocol documentation",
        )
    except Exception:
        # Documentation might already be added
        pass

    description = dedent("""\
        You are UAgI, a universal MCP (Model Context Protocol) agent designed to interact with MCP servers.
        You can connect to various MCP servers to access resources and execute tools.

        As an MCP agent, you can:
        - Connect to file systems, databases, APIs, and other data sources through MCP servers
        - Execute tools provided by MCP servers to perform actions
        - Access resources exposed by MCP servers

        Note: You only have access to the MCP Servers provided below, if you need to access other MCP Servers, please ask the user to enable them.

        <critical>
        - When a user mentions a task that might require external data or tools, check if an appropriate MCP server is available
        - If an MCP server is available, use its capabilities to fulfill the user's request
        - You have a knowledge base full of MCP documentation, search it using the `search_knowledge_base` tool to answer questions about MCP and the different tools available.
        - Provide clear explanations of which MCP servers and tools you're using
        - If you encounter errors with an MCP server, explain the issue and suggest alternatives
        - Always cite sources when providing information retrieved through MCP servers
        </critical>\
    """)

    if mcp_server_ids:
        description += dedent(
            """\n
            You have access to the following MCP servers:
            {}
        """.format("\n".join([f"- {server_id}" for server_id in mcp_server_ids]))
        )

    instructions = dedent("""\
        Here's how you should fulfill a user request:

        1. Understand the user's request
        - Read the user's request carefully
        - Determine if the request requires MCP server interaction
        - Search your knowledge base using the `search_knowledge_base` tool to answer questions about MCP or to learn how to use different MCP tools.
        - To interact with an MCP server, follow these steps:
            - Identify which tools are available to you
            - Select the appropriate tool for the user's request
            - Explain to the user which tool you're using
            - Execute the tool
            - Provide clear feedback about tool execution results

        2. Error Handling
        - If an MCP tool fails, explain the issue clearly and provide details about the error.
        - Suggest alternatives when MCP capabilities are unavailable

        3. Security and Privacy
        - Be transparent about which servers and tools you're using
        - Request explicit permission before executing tools that modify data
        - Respect access limitations of connected MCP servers

        MCP Knowledge
        - You have access to a knowledge base of MCP documentation
        - To answer questions about MCP, use the knowledge base
        - If you don't know the answer or can't find the information in the knowledge base, say so\
    """)

    agent = Agent(
        name="UAgI: The Universal MCP Agent",
        model=get_model_from_id(model_id),
        id="universal-mcp-agent",
        user_id=user_id,
        session_id=session_id,
        db=db,
        knowledge=knowledge_base,
        tools=mcp_tools,
        add_history_to_context=True,
        num_history_runs=5,
        read_chat_history=True,
        read_tool_call_history=True,
        add_datetime_to_context=True,
        add_name_to_context=True,
        description=description,
        instructions=instructions,
        markdown=True,
        debug_mode=True,
    )

    return agent
```

---

<a name="examples--streamlit_apps--mcp_agent--mcp_clientpy"></a>

### `examples/streamlit_apps/mcp_agent/mcp_client.py`

```python
"""MCP Client for connecting to Model Context Protocol servers."""

from contextlib import AsyncExitStack
from typing import List, Optional

from agno.tools.mcp import MCPTools
from agno.utils.log import logger
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from pydantic import BaseModel


class MCPServerConfig(BaseModel):
    """Configuration for an MCP server."""

    id: str
    command: str
    args: Optional[List[str]] = None


class MCPClient:
    """Client for connecting to MCP servers."""

    def __init__(self):
        # Initialize session and client objects
        self.session = None
        self.exit_stack = AsyncExitStack()
        self.tools = []
        self.server_id = None

    async def connect_to_server(self, server_config: MCPServerConfig) -> MCPTools:
        """Connect to an MCP server using the provided configuration.

        Args:
            server_config: Configuration for the MCP server

        Returns:
            MCPTools instance for interacting with the server
        """
        self.server_id = server_config.id

        server_params = StdioServerParameters(
            command=server_config.command,
            args=server_config.args,
        )
        logger.info(f"Connecting to server {self.server_id}")

        # Create client session
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.stdio, self.write)
        )

        # Initialize the session
        await self.session.initialize()

        # Create MCPTools for this server
        mcp_tools = MCPTools(session=self.session)
        await mcp_tools.initialize()
        logger.info(f"Connected to server {self.server_id}")

        return mcp_tools

    async def cleanup(self):
        """Clean up resources."""
        try:
            await self.exit_stack.aclose()
        except Exception as e:
            logger.warning(f"Error during MCP client cleanup: {e}")
```

---

<a name="examples--streamlit_apps--medical_imaging--apppy"></a>

### `examples/streamlit_apps/medical_imaging/app.py`

```python
import tempfile
from os import unlink

import nest_asyncio
import streamlit as st
from agno.media import Image as AgnoImage
from agno.utils.streamlit import (
    COMMON_CSS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)
from medical_agent import get_medical_imaging_agent

nest_asyncio.apply()
st.set_page_config(
    page_title="Medical Imaging Analysis",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)

MODELS = [
    "gemini-2.0-flash-exp",
    "gpt-4o",
]


def restart_agent(model_id: str = None):
    target_model = model_id or st.session_state.get("current_model", MODELS[0])

    new_agent = get_medical_imaging_agent(model_id=target_model, session_id=None)

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            new_model_id = selected_model
            current_model = st.session_state.get("current_model")

            if current_model and current_model != new_model_id:
                try:
                    st.session_state["is_loading_session"] = False
                    # Start new chat
                    restart_agent(model_id=new_model_id)

                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Medical Imaging Analysis</h1>", unsafe_allow_html=True
    )
    st.markdown(
        "<p class='subtitle'>AI-powered medical imaging analysis with professional insights</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Medical Disclaimer
    ####################################################################
    st.warning(
        " **MEDICAL DISCLAIMER**: This tool is for educational and informational purposes only. "
        "All analyses should be reviewed by qualified healthcare professionals. "
        "Do not make medical decisions based solely on this analysis."
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Initialize Agent and Session
    ####################################################################
    medical_agent = initialize_agent(selected_model, get_medical_imaging_agent)
    reset_session_state(medical_agent)

    if prompt := st.chat_input(" Upload an image or ask me about medical imaging!"):
        add_message("user", prompt)

    ####################################################################
    # Image Upload and Analysis
    ####################################################################
    st.sidebar.markdown("####  Image Upload")
    uploaded_file = st.sidebar.file_uploader(
        "Upload Medical Image",
        type=["jpg", "jpeg", "png", "dicom", "dcm"],
        help="Supported formats: JPG, JPEG, PNG, DICOM",
        key="medical_image_upload",
    )

    additional_context = st.sidebar.text_area(
        "Additional Context",
        placeholder="Patient history, symptoms, specific areas of concern...",
        help="Provide any relevant clinical information to enhance the analysis",
    )

    if uploaded_file and not prompt:
        alert = st.sidebar.info("Processing medical image...", icon="")
        try:
            # Process the uploaded image
            with tempfile.NamedTemporaryFile(
                suffix=f".{uploaded_file.name.split('.')[-1]}", delete=False
            ) as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name

            # Read image for AgnoImage
            with open(tmp_path, "rb") as f:
                image_bytes = f.read()

            agno_image = AgnoImage(
                content=image_bytes, format=uploaded_file.name.split(".")[-1]
            )

            # Create analysis prompt
            base_prompt = (
                "Please analyze this medical image and provide comprehensive findings."
            )
            if additional_context.strip():
                analysis_prompt = (
                    f"{base_prompt}\n\nAdditional context: {additional_context.strip()}"
                )
            else:
                analysis_prompt = base_prompt

            # Add message and trigger analysis
            add_message("user", f" Medical Image Analysis: {uploaded_file.name}")

            # Store image for analysis
            st.session_state["pending_image"] = agno_image
            st.session_state["pending_prompt"] = analysis_prompt

            unlink(tmp_path)
            st.sidebar.success(f"Image {uploaded_file.name} ready for analysis")

        except Exception as e:
            st.sidebar.error(f"Error processing image: {str(e)}")
        finally:
            alert.empty()

    ###############################################################
    # Sample Questions
    ###############################################################
    st.sidebar.markdown("####  Sample Questions")
    if st.sidebar.button(" What can you analyze?"):
        add_message(
            "user",
            "What types of medical images can you analyze and what insights can you provide?",
        )
    if st.sidebar.button(" Chest X-ray Guide"):
        add_message(
            "user",
            "What should I look for when reviewing a chest X-ray?",
        )
    if st.sidebar.button(" Bone Fracture Analysis"):
        add_message(
            "user",
            "How do you identify and classify bone fractures in medical imaging?",
        )
    if st.sidebar.button(" Neuroimaging Basics"):
        add_message(
            "user",
            "What are the key structures and findings to evaluate in brain imaging?",
        )

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id:
                try:
                    session_name = medical_agent.get_session_name()
                    if session_name:
                        filename = f"medical_imaging_chat_{session_name}.md"
                    else:
                        filename = f"medical_imaging_chat_{session_id}.md"
                except Exception:
                    filename = f"medical_imaging_chat_{session_id}.md"
            else:
                filename = "medical_imaging_chat_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("Medical Imaging Analysis"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]

        # Check if we have a pending image to analyze
        pending_image = st.session_state.get("pending_image")
        pending_prompt = st.session_state.get("pending_prompt")

        if pending_image and pending_prompt:
            # Display image analysis response
            with st.chat_message("assistant"):
                with st.spinner(" Analyzing medical image... Please wait."):
                    try:
                        response = medical_agent.run(
                            pending_prompt, images=[pending_image]
                        )

                        if hasattr(response, "content"):
                            content = response.content
                        elif isinstance(response, str):
                            content = response
                        elif isinstance(response, dict) and "content" in response:
                            content = response["content"]
                        else:
                            content = str(response)

                        st.markdown(content)
                        add_message("assistant", content)

                    except Exception as e:
                        error_msg = f"Error analyzing image: {str(e)}"
                        st.error(error_msg)
                        add_message("assistant", error_msg)

            # Clear pending image data
            st.session_state.pop("pending_image", None)
            st.session_state.pop("pending_prompt", None)

        else:
            # Regular text response
            display_response(medical_agent, question)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(medical_agent, selected_model, get_medical_imaging_agent)

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Medical Imaging Analysis Assistant helps healthcare professionals and students "
        "analyze medical images using AI-powered insights while maintaining professional medical standards."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--medical_imaging--medical_agentpy"></a>

### `examples/streamlit_apps/medical_imaging/medical_agent.py`

```python
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.streamlit import get_model_with_provider

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


def get_medical_imaging_agent(
    model_id: str = "gemini-2.0-flash-exp",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Get a Medical Imaging Analysis Agent"""

    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    agent = Agent(
        name="Medical Imaging Expert",
        model=get_model_with_provider(model_id),
        db=db,
        id="medical-imaging-agent",
        user_id=user_id,
        session_id=session_id,
        tools=[DuckDuckGoTools()],
        markdown=True,
        debug_mode=True,
        instructions=dedent("""
            You are a highly skilled medical imaging expert with extensive knowledge in radiology 
            and diagnostic imaging. Your role is to provide comprehensive, accurate, and ethical 
            analysis of medical images.

            Key Responsibilities:
            1. Maintain patient privacy and confidentiality
            2. Provide objective, evidence-based analysis
            3. Highlight any urgent or critical findings
            4. Explain findings in both professional and patient-friendly terms

            For each image analysis, structure your response as follows:

            ### Technical Assessment
            - Imaging modality identification (X-ray, CT, MRI, Ultrasound, etc.)
            - Anatomical region and patient positioning evaluation
            - Image quality assessment (contrast, clarity, artifacts, technical adequacy)
            - Any technical limitations affecting interpretation

            ### Professional Analysis
            - Systematic anatomical review of visible structures
            - Primary findings with precise descriptions and measurements when applicable
            - Secondary observations and incidental findings
            - Assessment of anatomical variants vs pathology
            - Severity grading (Normal/Mild/Moderate/Severe) when appropriate

            ### Clinical Interpretation
            - Primary diagnostic impression with confidence level
            - Differential diagnoses ranked by probability
            - Supporting radiological evidence from the image
            - Any critical or urgent findings requiring immediate attention
            - Recommended additional imaging or follow-up studies if needed

            ### Patient Education
            - Clear, non-technical explanation of findings
            - Visual descriptions and simple analogies when helpful
            - Address common patient concerns and questions
            - Lifestyle or activity implications if relevant

            ### Evidence-Based Context
            Using DuckDuckGo search when relevant:
            - Recent medical literature supporting findings
            - Standard diagnostic criteria and guidelines
            - Treatment approaches and prognosis information
            - Authoritative medical references (2-3 sources maximum)

            Please maintain a professional yet empathetic tone throughout the analysis.
        """),
    )

    return agent
```

---

<a name="examples--streamlit_apps--paperpal--agentspy"></a>

### `examples/streamlit_apps/paperpal/agents.py`

```python
from pathlib import Path
from textwrap import dedent
from typing import List, Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.tools.arxiv import ArxivTools
from agno.tools.exa import ExaTools
from agno.utils.streamlit import get_model_with_provider
from pydantic import BaseModel, Field

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


# Data Models for structured outputs
class SearchTerms(BaseModel):
    terms: List[str] = Field(
        ..., description="List of search terms related to a topic."
    )


class ArxivSearchResult(BaseModel):
    title: str = Field(..., description="Title of the research paper.")
    id: str = Field(..., description="ArXiv ID of the paper.")
    authors: List[str] = Field(..., description="Authors of the paper.")
    summary: str = Field(..., description="Abstract/summary of the paper.")
    pdf_url: str = Field(..., description="URL to the PDF of the paper.")
    links: List[str] = Field(..., description="Related links to the paper.")
    reasoning: str = Field(..., description="Reasoning for selecting this paper.")


class ArxivSearchResults(BaseModel):
    results: List[ArxivSearchResult] = Field(
        ..., description="List of selected ArXiv research papers."
    )


class WebSearchResult(BaseModel):
    title: str = Field(..., description="Title of the web article.")
    summary: str = Field(..., description="Summary of the article content.")
    links: List[str] = Field(..., description="Links related to the article.")
    reasoning: str = Field(..., description="Reasoning for selecting this article.")


class WebSearchResults(BaseModel):
    results: List[WebSearchResult] = Field(
        ..., description="List of selected web search results."
    )


def get_paperpal_agents(
    model_id: str = "gpt-4o",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    arxiv_download_dir: Optional[Path] = None,
):
    """Get Paperpal research agents with tools"""

    # Set up ArXiv download directory
    if not arxiv_download_dir:
        arxiv_download_dir = Path(__file__).parent.parent.parent.parent.joinpath(
            "tmp", "arxiv_pdfs"
        )
        arxiv_download_dir.mkdir(parents=True, exist_ok=True)

    # Initialize tools
    arxiv_toolkit = ArxivTools(download_dir=arxiv_download_dir)
    exa_tools = ExaTools()

    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    # Search Term Generator Agent
    search_term_generator = Agent(
        name="Search Term Generator",
        model=get_model_with_provider(model_id),
        db=db,
        id="search-term-generator",
        user_id=user_id,
        session_id=session_id,
        output_schema=SearchTerms,
        instructions=dedent("""
            You are an expert research strategist specializing in generating strategic search terms 
            for comprehensive research coverage.

            Your task is to:
            1. Analyze the given research topic to identify key concepts and aspects
            2. Generate 2-3 specific and distinct search terms that capture different dimensions
            3. Ensure terms are optimized for both academic and web search effectiveness
                            
            Focus on terms that will help find:
            - Recent research papers and theoretical developments
            - Industry applications and real-world implementations
            - Current challenges and future directions
            - Cross-disciplinary connections and emerging trends

            Provide terms as a structured list optimized for research databases and web search.
        """),
        markdown=True,
        debug_mode=True,
    )

    # ArXiv Search Agent
    arxiv_search_agent = Agent(
        name="ArXiv Research Agent",
        model=get_model_with_provider(model_id),
        db=db,
        id="arxiv-search-agent",
        user_id=user_id,
        session_id=session_id,
        tools=[arxiv_toolkit],
        output_schema=ArxivSearchResults,
        instructions=dedent("""
            You are an expert in academic research with access to ArXiv's database.

            Your task is to:
            1. Search ArXiv for the top 10 papers related to the provided search term.
            2. Select the 3 most relevant research papers based on:
                - Direct relevance to the search term.
                - Scientific impact (e.g., citations, journal reputation).
                - Recency of publication.

            For each selected paper, the output should be in json structure have these details:
                - title
                - id
                - authors
                - a concise summary
                - the PDF link of the research paper
                - links related to the research paper
                - reasoning for why the paper was chosen

            Ensure the selected research papers directly address the topic and offer valuable insights.
        """),
        markdown=True,
        debug_mode=True,
    )

    # Web Search Agent
    exa_search_agent = Agent(
        name="Web Research Agent",
        model=get_model_with_provider(model_id),
        db=db,
        id="exa-search-agent",
        user_id=user_id,
        session_id=session_id,
        tools=[exa_tools],
        output_schema=WebSearchResults,
        instructions=dedent("""
            You are a web search expert specializing in extracting high-quality information.

            Your task is to:
            1. Given a topic, search Exa for the top 10 articles about that topic.
            2. Select the 3 most relevant articles based on:
                - Source credibility.
                - Content depth and relevance.

            For each selected article, the output should have:
                - title
                - a concise summary
                - related links to the article
                - reasoning for why the article was chosen and how it contributes to understanding the topic.

            Ensure the selected articles are credible, relevant, and provide significant insights into the topic.
        """),
        markdown=True,
        debug_mode=True,
    )

    # Research Editor Agent
    research_editor = Agent(
        name="Research Editor",
        model=get_model_with_provider(model_id),
        db=db,
        id="research-editor",
        user_id=user_id,
        session_id=session_id,
        instructions=dedent("""
            You are a senior research editor specializing in breaking complex topics and information into understandable, engaging, high-quality blogs.

            Your task is to:
            1. Create a detailed blog within 1000 words based on the given topic.
            2. The blog should be of max 7-8 paragraphs, understandable, intuitive, making things easy to understand for the reader.
            3. Highlight key findings and provide a clear, high-level overview of the topic.
            4. At the end add the supporting articles link, paper link or any findings you think is necessary to add.

            The blog should help the reader in getting a decent understanding of the topic.
            The blog should be in markdown format.
        """),
        markdown=True,
        debug_mode=True,
    )

    return {
        "search_term_generator": search_term_generator,
        "arxiv_search_agent": arxiv_search_agent,
        "exa_search_agent": exa_search_agent,
        "research_editor": research_editor,
        "arxiv_toolkit": arxiv_toolkit,
    }
```

---

<a name="examples--streamlit_apps--paperpal--apppy"></a>

### `examples/streamlit_apps/paperpal/app.py`

```python
import json

import nest_asyncio
import pandas as pd
import streamlit as st
from agents import (
    ArxivSearchResults,
    SearchTerms,
    WebSearchResults,
    get_paperpal_agents,
)
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    reset_session_state,
    session_selector_widget,
)

nest_asyncio.apply()
st.set_page_config(
    page_title="Paperpal Research Assistant",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def get_main_agent(model_id: str = None, session_id: str = None):
    """Get the main research editor agent for session management"""
    agents = get_paperpal_agents(model_id=model_id, session_id=session_id)
    return agents["research_editor"]


def restart_session(model_id: str = None):
    target_model = model_id or st.session_state.get("current_model", MODELS[0])

    # Clear all research-related session state
    keys_to_clear = [
        "research_topic",
        "search_terms",
        "arxiv_results",
        "exa_results",
        "final_blog",
        "research_agents",
        "messages",
        "session_id",
    ]
    for key in keys_to_clear:
        st.session_state.pop(key, None)

    # Initialize new agents
    st.session_state["research_agents"] = get_paperpal_agents(model_id=target_model)
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            new_model_id = selected_model
            current_model = st.session_state.get("current_model")

            if current_model and current_model != new_model_id:
                try:
                    st.session_state["is_loading_session"] = False
                    restart_session(model_id=new_model_id)
                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'>Paperpal Research Assistant</h1>",
        unsafe_allow_html=True,
    )
    st.markdown(
        "<p class='subtitle'>AI-powered research workflow for technical blog generation</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
    )

    ####################################################################
    # Initialize Research Agents
    ####################################################################
    if (
        "research_agents" not in st.session_state
        or not st.session_state["research_agents"]
    ):
        st.session_state["research_agents"] = get_paperpal_agents(
            model_id=selected_model
        )
        st.session_state["current_model"] = selected_model

    # Get main agent for session management
    main_agent = get_main_agent(selected_model)
    reset_session_state(main_agent)

    if prompt := st.chat_input(
        " Ask me anything about research or start a new research project!"
    ):
        add_message("user", prompt)

    ####################################################################
    # Research Configuration
    ####################################################################
    st.sidebar.markdown("####  Research Configuration")

    # Topic input
    research_topic = st.sidebar.text_input(
        "Research Topic",
        value=st.session_state.get("research_topic", ""),
        placeholder="Enter your research topic...",
        help="Provide a specific research topic you want to explore",
    )

    # Research options
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        enable_arxiv = st.sidebar.checkbox(
            " ArXiv Search", value=True, help="Search academic papers"
        )
    with col2:
        enable_exa = st.sidebar.checkbox(
            " Web Search", value=True, help="Search web content"
        )

    num_search_terms = st.sidebar.number_input(
        "Search Terms",
        value=2,
        min_value=2,
        max_value=3,
        help="Number of strategic search terms to generate",
    )

    # Generate research button
    if st.sidebar.button(" Start Research", type="primary", use_container_width=True):
        if research_topic.strip():
            st.session_state["research_topic"] = research_topic.strip()
            st.session_state["enable_arxiv"] = enable_arxiv
            st.session_state["enable_exa"] = enable_exa
            st.session_state["num_search_terms"] = num_search_terms
            add_message("user", f" Research Request: {research_topic}")
        else:
            st.sidebar.error("Please enter a research topic")

    ####################################################################
    # Trending Topics
    ####################################################################
    st.sidebar.markdown("####  Trending Topics")
    trending_topics = [
        "Multimodal AI in autonomous systems",
        "Quantum machine learning algorithms",
        "LLM safety and alignment research",
        "Neural symbolic reasoning frameworks",
        "Federated learning in edge computing",
    ]

    for topic in trending_topics:
        if st.sidebar.button(f" {topic}", use_container_width=True):
            st.session_state["research_topic"] = topic
            add_message("user", f" Research Request: {topic}")

    ###############################################################
    # Utility buttons
    ###############################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Research", use_container_width=True):
            restart_session()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id:
                try:
                    session_name = main_agent.get_session_name()
                    if session_name:
                        filename = f"paperpal_research_{session_name}.md"
                    else:
                        filename = f"paperpal_research_{session_id}.md"
                except Exception:
                    filename = f"paperpal_research_{session_id}.md"
            else:
                filename = "paperpal_research_new.md"

            if st.sidebar.download_button(
                " Export Research",
                export_chat_history("Paperpal Research"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Research exported!")
        else:
            st.sidebar.button(
                " Export Research",
                disabled=True,
                use_container_width=True,
                help="No research to export",
            )

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Process Research Request
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]

        # Check if this is a research request
        if question.startswith(" Research Request:") and st.session_state.get(
            "research_topic"
        ):
            process_research_workflow()
        else:
            # Regular chat interaction
            display_response(main_agent, question)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(main_agent, selected_model, get_main_agent)

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "Paperpal is an AI-powered research assistant that helps you create comprehensive technical blogs "
        "by synthesizing information from academic papers and web sources."
    )


def process_research_workflow():
    """Process the complete research workflow"""
    topic = st.session_state.get("research_topic")
    if not topic:
        return

    agents = st.session_state.get("research_agents", {})
    if not agents:
        st.error("Research agents not initialized. Please refresh the page.")
        return

    with st.chat_message("assistant"):
        # Step 1: Generate Search Terms
        if not st.session_state.get("search_terms"):
            with st.status(
                " Generating strategic search terms...", expanded=True
            ) as status:
                try:
                    search_input = {
                        "topic": topic,
                        "num_terms": st.session_state.get("num_search_terms", 2),
                    }

                    response = agents["search_term_generator"].run(
                        json.dumps(search_input)
                    )
                    if isinstance(response.content, SearchTerms):
                        st.session_state["search_terms"] = response.content
                        st.json(response.content.model_dump())
                        status.update(
                            label=" Search terms generated",
                            state="complete",
                            expanded=False,
                        )
                    else:
                        raise ValueError(
                            "Invalid response format from search term generator"
                        )

                except Exception as e:
                    st.error(f"Error generating search terms: {str(e)}")
                    status.update(
                        label=" Search term generation failed", state="error"
                    )
                    return

        search_terms = st.session_state.get("search_terms")
        if not search_terms:
            return

        # Step 2: ArXiv Search
        if st.session_state.get("enable_arxiv", True) and not st.session_state.get(
            "arxiv_results"
        ):
            with st.status(
                " Searching ArXiv for research papers...", expanded=True
            ) as status:
                try:
                    arxiv_response = agents["arxiv_search_agent"].run(
                        search_terms.model_dump_json(indent=2)
                    )
                    if isinstance(arxiv_response.content, ArxivSearchResults):
                        st.session_state["arxiv_results"] = arxiv_response.content

                        # Display results as table
                        if arxiv_response.content.results:
                            df_data = []
                            for result in arxiv_response.content.results:
                                df_data.append(
                                    {
                                        "Title": result.title[:80] + "..."
                                        if len(result.title) > 80
                                        else result.title,
                                        "Authors": ", ".join(result.authors[:3])
                                        + ("..." if len(result.authors) > 3 else ""),
                                        "ID": result.id,
                                        "Reasoning": result.reasoning[:100] + "..."
                                        if len(result.reasoning) > 100
                                        else result.reasoning,
                                    }
                                )

                            df = pd.DataFrame(df_data)
                            st.dataframe(df, use_container_width=True)
                            status.update(
                                label=" ArXiv search completed",
                                state="complete",
                                expanded=False,
                            )

                except Exception as e:
                    st.error(f"ArXiv search error: {str(e)}")
                    status.update(label=" ArXiv search failed", state="error")

        # Step 3: Web Search
        if st.session_state.get("enable_exa", True) and not st.session_state.get(
            "exa_results"
        ):
            with st.status(
                " Searching web for current insights...", expanded=True
            ) as status:
                try:
                    exa_response = agents["exa_search_agent"].run(
                        search_terms.model_dump_json(indent=2)
                    )
                    if isinstance(exa_response.content, WebSearchResults):
                        st.session_state["exa_results"] = exa_response.content

                        # Display results
                        if exa_response.content.results:
                            for i, result in enumerate(exa_response.content.results, 1):
                                st.write(f"**{i}. {result.title}**")
                                st.write(
                                    result.summary[:200] + "..."
                                    if len(result.summary) > 200
                                    else result.summary
                                )
                                st.write(f"*Reasoning:* {result.reasoning}")
                                if result.links:
                                    st.write(f" [Read more]({result.links[0]})")
                                st.write("---")

                            status.update(
                                label=" Web search completed",
                                state="complete",
                                expanded=False,
                            )

                except Exception as e:
                    st.error(f"Web search error: {str(e)}")
                    status.update(label=" Web search failed", state="error")

        # Display completed web search results
        exa_results = st.session_state.get("exa_results")
        arxiv_results = st.session_state.get("arxiv_results")

        # Step 4: Generate Final Blog
        if (arxiv_results or exa_results) and not st.session_state.get("final_blog"):
            with st.status(
                " Generating comprehensive research blog...", expanded=True
            ) as status:
                try:
                    # Prepare research content
                    research_content = f"# Research Topic: {topic}\n\n"
                    research_content += (
                        f"## Search Terms\n{search_terms.model_dump_json(indent=2)}\n\n"
                    )

                    if arxiv_results:
                        research_content += "## ArXiv Research Papers\n\n"
                        research_content += (
                            f"{arxiv_results.model_dump_json(indent=2)}\n\n"
                        )

                    if exa_results:
                        research_content += "## Web Research Content\n\n"
                        research_content += (
                            f"{exa_results.model_dump_json(indent=2)}\n\n"
                        )

                    # Generate blog
                    blog_response = agents["research_editor"].run(research_content)
                    st.session_state["final_blog"] = blog_response.content

                    status.update(
                        label=" Research blog generated",
                        state="complete",
                        expanded=False,
                    )

                except Exception as e:
                    st.error(f"Blog generation error: {str(e)}")
                    status.update(label=" Blog generation failed", state="error")


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--podcast_generator--agentspy"></a>

### `examples/streamlit_apps/podcast_generator/agents.py`

```python
import os
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.audio import write_audio_to_file
from agno.utils.streamlit import get_model_with_provider

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


def generate_podcast_agent(
    model_id: str = "openai:gpt-4o",
    voice: str = "alloy",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Create a Podcast Generator Agent"""

    os.makedirs("tmp", exist_ok=True)

    model = get_model_with_provider(model_id)

    # If using OpenAI, configure for audio output
    if model_id.startswith("openai:"):
        model = OpenAIChat(
            id=model_id.split("openai:")[1],
            modalities=["text", "audio"],
            audio={"voice": voice, "format": "wav"},
        )

    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    agent = Agent(
        name="Podcast Generator",
        model=model,
        db=db,
        id="podcast-generator",
        user_id=user_id,
        session_id=session_id,
        tools=[DuckDuckGoTools()],
        instructions=dedent("""
            You are a podcast scriptwriter specializing in concise and engaging narratives.
            Your task is to research a given topic and compose a compelling podcast script.

            ### Research Phase:
            - Use DuckDuckGo to gather the most recent and relevant information on the given topic
            - Prioritize trustworthy sources such as news sites, academic articles, or established publications
            - Identify key points, statistics, expert opinions, and interesting facts

            ### Scripting Phase:
            - Write a concise podcast script in a conversational tone
            - Begin with a strong hook to capture the listener's attention
            - Present key insights in an engaging, easy-to-follow manner
            - Include smooth transitions between ideas to maintain narrative flow
            - End with a closing remark that summarizes main takeaways

            ### Formatting Guidelines:
            - Use simple, engaging language suitable for audio
            - Keep the script under 300 words (around 2 minutes of audio)
            - Write in a natural, spoken format, avoiding overly formal or technical jargon
            - Structure: intro hook  main content  conclusion
            - No special formatting or markdown - just plain conversational text

            ### Example Output Structure:
            "Welcome to today's episode where we explore [TOPIC]. [Hook or interesting fact]
            
            [Main content with 2-3 key points, smooth transitions between ideas]
            
            [Conclusion with key takeaways and closing thoughts]
            
            Thanks for listening, and we'll see you next time!"
        """),
        markdown=True,
        debug_mode=True,
    )

    return agent


def generate_podcast(
    topic: str, voice: str = "alloy", model_id: str = "openai:gpt-4o"
) -> Optional[str]:
    """
    Generate a podcast script and convert it to audio.

    Args:
        topic (str): The topic of the podcast
        voice (str): Voice model for OpenAI TTS. Options: ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
        model_id (str): Model to use for script generation

    Returns:
        str: Path to the generated audio file, or None if generation failed
    """
    try:
        # Create the podcast generator agent
        agent = generate_podcast_agent(model_id=model_id, voice=voice)

        # Generate the podcast script
        response = agent.run(f"Write a podcast script for the topic: {topic}")

        audio_file_path = "tmp/generated_podcast.wav"

        # If the model supports audio output and audio was generated
        if hasattr(response, "response_audio") and response.response_audio is not None:
            audio_content = response.response_audio.content

            if audio_content:
                write_audio_to_file(
                    audio=audio_content,
                    filename=audio_file_path,
                )
                return audio_file_path

        return None

    except Exception as e:
        print(f"Error generating podcast: {e}")
        return None
```

---

<a name="examples--streamlit_apps--podcast_generator--apppy"></a>

### `examples/streamlit_apps/podcast_generator/app.py`

```python
import streamlit as st
from agents import generate_podcast, generate_podcast_agent
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)

st.set_page_config(
    page_title="Podcast Generator",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Add custom CSS
st.markdown(COMMON_CSS, unsafe_allow_html=True)


def restart_agent(model_id: str = None):
    target_model = model_id or st.session_state.get("current_model", "openai:gpt-4o")

    new_agent = generate_podcast_agent(model_id=target_model, session_id=None)

    st.session_state["agent"] = new_agent
    st.session_state["session_id"] = new_agent.session_id
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        new_model_id = selected_model
        current_model = st.session_state.get("current_model")

        if current_model and current_model != new_model_id:
            try:
                st.session_state["is_loading_session"] = False
                # Start new chat
                restart_agent(model_id=new_model_id)
            except Exception as e:
                st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown(
        "<h1 class='main-title'> Podcast Generator</h1>", unsafe_allow_html=True
    )
    st.markdown(
        "<p class='subtitle'>Create engaging AI podcasts on any topic</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector (filter for OpenAI models only)
    ####################################################################
    openai_models = [
        model
        for model in MODELS
        if model in ["gpt-4o", "o3-mini", "gpt-5", "gemini-2.5-pro"]
    ]
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=openai_models,
        index=0,
        key="model_selector",
        on_change=on_model_change,
        help="Only OpenAI models support audio generation",
    )

    ####################################################################
    # Initialize Agent and Session
    ####################################################################
    podcast_agent = initialize_agent(selected_model, generate_podcast_agent)
    reset_session_state(podcast_agent)

    if prompt := st.chat_input(" Ask about podcasts or request a specific topic!"):
        add_message("user", prompt)

    ####################################################################
    # Voice Selection
    ####################################################################
    st.sidebar.markdown("####  Voice Settings")
    voice_options = ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
    selected_voice = st.sidebar.selectbox(
        "Choose Voice",
        options=voice_options,
        index=0,
        help="Select the AI voice for your podcast",
    )

    ####################################################################
    # Sample Topics
    ####################################################################
    st.sidebar.markdown("####  Suggested Topics")
    sample_topics = [
        " Impact of AI on Creativity",
        " Future of Renewable Energy",
        " AI in Healthcare Revolution",
        " Space Exploration Updates",
        " Climate Change Solutions",
        " Quantum Computing Explained",
    ]

    # Handle sample topic selection
    for sample_topic in sample_topics:
        if st.sidebar.button(
            sample_topic, key=f"topic_{sample_topic}", use_container_width=True
        ):
            add_message("user", sample_topic[2:])  # Remove emoji and add to chat
            st.rerun()

    ####################################################################
    # Utility buttons
    ####################################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            if session_id:
                try:
                    session_name = podcast_agent.get_session_name()
                    if session_name:
                        filename = f"podcast_chat_{session_name}.md"
                    else:
                        filename = f"podcast_chat_{session_id}.md"
                except Exception:
                    filename = f"podcast_chat_{session_id}.md"
            else:
                filename = "podcast_chat_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("Podcast Generator"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Generate Podcast
    ####################################################################
    st.sidebar.markdown("####  Generate")

    if st.sidebar.button(" Create Podcast", type="primary", use_container_width=True):
        # Get the latest user message as the topic
        user_messages = [
            msg
            for msg in st.session_state.get("messages", [])
            if msg.get("role") == "user"
        ]
        if user_messages:
            latest_topic = user_messages[-1]["content"]
            with st.spinner(
                " Generating podcast... This may take up to 2 minutes..."
            ):
                try:
                    audio_path = generate_podcast(
                        latest_topic, selected_voice, selected_model
                    )

                    if audio_path:
                        st.success(" Podcast generated successfully!")

                        st.subheader(" Your AI Podcast")
                        st.audio(audio_path, format="audio/wav")

                        # Download button
                        with open(audio_path, "rb") as audio_file:
                            st.download_button(
                                " Download Podcast",
                                audio_file,
                                file_name=f"podcast_{latest_topic[:30].replace(' ', '_')}.wav",
                                mime="audio/wav",
                                use_container_width=True,
                            )
                    else:
                        st.error(" Failed to generate podcast. Please try again.")

                except Exception as e:
                    st.error(f" Error generating podcast: {str(e)}")
        else:
            st.sidebar.warning(" Please enter a topic in the chat first.")

    ####################################################################
    # Getting Started Guide
    ####################################################################
    if not st.session_state.get("messages"):
        st.markdown("###  How to Get Started")
        st.markdown("""
        1. **Choose a Model** - Select your preferred AI model
        2. **Pick a Voice** - Choose from 6 realistic AI voices  
        3. **Enter a Topic** - Type your podcast topic in the chat below or click a suggested topic
        4. **Generate** - Click 'Create Podcast' and wait for the magic!
        """)

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]
        display_response(podcast_agent, question)

    ####################################################################
    # Session management widgets
    ####################################################################
    session_selector_widget(podcast_agent, selected_model, generate_podcast_agent)

    ####################################################################
    # Features Section
    ####################################################################
    st.markdown("---")
    st.markdown("###  Features")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        ** AI Research**
        - Real-time topic research
        - Credible source analysis
        - Latest information gathering
        """)

    with col2:
        st.markdown("""
        ** Script Generation**
        - Engaging narratives
        - Professional structure
        - Conversational tone
        """)

    with col3:
        st.markdown("""
        ** Audio Creation**
        - 6 realistic AI voices
        - High-quality audio
        - Instant download
        """)

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Podcast Generator creates professional podcasts on any topic using AI research, "
        "script writing, and text-to-speech technology."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--streamlit_apps--vision_ai--agentspy"></a>

### `examples/streamlit_apps/vision_ai/agents.py`

```python
from textwrap import dedent
from typing import Optional

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.streamlit import get_model_with_provider

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

EXTRACTION_PROMPT = dedent("""
    Analyze this image thoroughly and provide detailed insights. Please include:

    1. **Objects & Elements**: Identify and describe all visible objects, people, animals, or items
    2. **Text Content**: Extract any readable text, signs, labels, or written content
    3. **Scene Description**: Describe the setting, environment, and overall scene
    5. **Context & Purpose**: Infer the likely purpose, context, or story behind the image
    6. **Technical Details**: Comment on image quality, style, or photographic aspects if relevant

    Provide a comprehensive analysis that would be useful for follow-up questions.
    Be specific and detailed in your observations.
""")


def get_vision_agent(
    model_id: str = "openai:gpt-4o",
    enable_search: bool = False,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
) -> Agent:
    """Get a unified Vision AI Agent for both image analysis and conversation"""

    db = PostgresDb(
        db_url=db_url,
        session_table="sessions",
        db_schema="ai",
    )

    tools = [DuckDuckGoTools()] if enable_search else []

    agent = Agent(
        name="Vision AI Agent",
        model=get_model_with_provider(model_id),
        db=db,
        id="vision-ai-agent",
        user_id=user_id,
        session_id=session_id,
        tools=tools,
        add_history_to_context=True,
        num_history_runs=5,
        instructions=dedent("""
            You are an expert Vision AI assistant that can both analyze images and engage in conversation.
            
            When provided with images:
            1. **Visual Analysis**: Identify objects, people, animals, and items
            2. **Text Content**: Extract any readable text, signs, or labels  
            3. **Scene Description**: Describe the setting, environment, and context
            4. **Purpose & Story**: Infer the likely purpose or story behind the image
            5. **Technical Details**: Comment on image quality, style, and composition
            
            For follow-up questions:
            - Reference previous image analyses in your conversation history
            - Provide specific details and insights
            - Use web search (when enabled) for additional context
            - Maintain conversation flow and suggest related questions
            
            Always provide:
            - Comprehensive and accurate responses
            - Well-structured answers with clear sections
            - Professional and helpful tone
            - Specific details rather than generic observations
        """),
        markdown=True,
        debug_mode=True,
    )

    return agent
```

---

<a name="examples--streamlit_apps--vision_ai--apppy"></a>

### `examples/streamlit_apps/vision_ai/app.py`

```python
from pathlib import Path

import streamlit as st
from agents import get_vision_agent
from agno.media import Image
from agno.utils.streamlit import (
    COMMON_CSS,
    MODELS,
    about_section,
    add_message,
    display_chat_messages,
    display_response,
    export_chat_history,
    initialize_agent,
    reset_session_state,
    session_selector_widget,
)

st.set_page_config(
    page_title="Vision AI",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown(COMMON_CSS, unsafe_allow_html=True)


def restart_agent(model_id: str = None):
    target_model = model_id or st.session_state.get("current_model", MODELS[0])

    st.session_state["agent"] = None
    st.session_state["session_id"] = None
    st.session_state["messages"] = []
    st.session_state["current_model"] = target_model
    st.session_state["is_new_session"] = True

    # Clear current image
    if "current_image" in st.session_state:
        del st.session_state["current_image"]


def on_model_change():
    selected_model = st.session_state.get("model_selector")
    if selected_model:
        if selected_model in MODELS:
            new_model_id = selected_model
            current_model = st.session_state.get("current_model")

            if current_model and current_model != new_model_id:
                try:
                    st.session_state["is_loading_session"] = False
                    restart_agent(model_id=new_model_id)

                except Exception as e:
                    st.sidebar.error(f"Error switching to {selected_model}: {str(e)}")
        else:
            st.sidebar.error(f"Unknown model: {selected_model}")


def main():
    ####################################################################
    # App header
    ####################################################################
    st.markdown("<h1 class='main-title'> Vision AI</h1>", unsafe_allow_html=True)
    st.markdown(
        "<p class='subtitle'>Smart image analysis and understanding</p>",
        unsafe_allow_html=True,
    )

    ####################################################################
    # Model selector
    ####################################################################
    selected_model = st.sidebar.selectbox(
        "Select Model",
        options=MODELS,
        index=0,
        key="model_selector",
        on_change=on_model_change,
        help="Choose the AI model for image analysis",
    )

    ####################################################################
    # Vision AI Settings
    ####################################################################
    st.sidebar.markdown("####  Analysis Settings")

    analysis_mode = st.sidebar.radio(
        "Analysis Mode",
        ["Auto", "Manual", "Hybrid"],
        index=0,
        help="""
        - **Auto**: Automatic comprehensive image analysis
        - **Manual**: Analysis based on your specific instructions  
        - **Hybrid**: Automatic analysis + your custom instructions
        """,
    )

    enable_search = st.sidebar.checkbox(
        "Enable Web Search",
        value=False,
        key="enable_search",
        help="Allow the agent to search for additional context",
    )

    ####################################################################
    # Initialize Agent and Session
    ####################################################################
    # Create unified agent with search capability
    def get_vision_agent_with_settings(model_id: str, session_id: str = None):
        return get_vision_agent(
            model_id=model_id, enable_search=enable_search, session_id=session_id
        )

    vision_agent = initialize_agent(selected_model, get_vision_agent_with_settings)
    reset_session_state(vision_agent)

    if prompt := st.chat_input(" Ask me anything!"):
        add_message("user", prompt)

    ####################################################################
    # File upload
    ####################################################################
    st.sidebar.markdown("####  Image Analysis")

    uploaded_file = st.sidebar.file_uploader(
        "Upload an Image", type=["png", "jpg", "jpeg"]
    )

    if uploaded_file:
        temp_dir = Path("tmp")
        temp_dir.mkdir(exist_ok=True)
        image_path = temp_dir / uploaded_file.name

        with open(image_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.session_state["current_image"] = {
            "path": str(image_path),
            "name": uploaded_file.name,
            "analysis_mode": analysis_mode,
        }

        st.sidebar.image(uploaded_file, caption=uploaded_file.name, width=200)
        st.sidebar.success(f"Image '{uploaded_file.name}' uploaded")

    # Analysis
    if st.session_state.get("current_image") and not prompt:
        if st.sidebar.button(
            " Analyze Image", type="primary", use_container_width=True
        ):
            image_info = st.session_state["current_image"]

            if analysis_mode == "Manual":
                custom_instructions = st.sidebar.text_area(
                    "Analysis Instructions", key="manual_instructions"
                )
                if custom_instructions:
                    add_message(
                        "user",
                        f"Analyze this image with instructions: {custom_instructions}",
                    )
                else:
                    add_message("user", f"Analyze this image: {image_info['name']}")
            elif analysis_mode == "Hybrid":
                custom_instructions = st.sidebar.text_area(
                    "Additional Instructions", key="hybrid_instructions"
                )
                if custom_instructions:
                    add_message(
                        "user",
                        f"Analyze this image with additional focus: {custom_instructions}",
                    )
                else:
                    add_message("user", f"Analyze this image: {image_info['name']}")
            else:
                add_message("user", f"Analyze this image: {image_info['name']}")

    ###############################################################
    # Sample Questions
    ###############################################################
    st.sidebar.markdown("####  Sample Questions")
    if st.sidebar.button(" What are the main objects?"):
        add_message("user", "What are the main objects?")
    if st.sidebar.button(" Is there any text to read?"):
        add_message("user", "Is there any text to read?")
    if st.sidebar.button(" Describe the colors and mood"):
        add_message("user", "Describe the colors and mood")

    ####################################################################
    # Display Chat Messages
    ####################################################################
    display_chat_messages()

    ####################################################################
    # Generate response for user message
    ####################################################################
    last_message = (
        st.session_state["messages"][-1] if st.session_state["messages"] else None
    )
    if last_message and last_message.get("role") == "user":
        question = last_message["content"]

        images_to_include = []
        if st.session_state.get("current_image"):
            image_info = st.session_state["current_image"]
            images_to_include = [Image(filepath=image_info["path"])]

        if images_to_include:
            with st.chat_message("assistant"):
                response_container = st.empty()
                with st.spinner(" Thinking..."):
                    try:
                        response = vision_agent.run(question, images=images_to_include)
                        response_container.markdown(response.content)
                        add_message("assistant", response.content)
                    except Exception as e:
                        error_message = f" Error: {str(e)}"
                        response_container.error(error_message)
                        add_message("assistant", error_message)
        else:
            # Use the same unified agent for all responses (maintains session)
            display_response(vision_agent, question)

    ####################################################################
    # Utility buttons
    ####################################################################
    st.sidebar.markdown("####  Utilities")
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.sidebar.button(" New Chat", use_container_width=True):
            restart_agent()
            st.rerun()

    with col2:
        has_messages = (
            st.session_state.get("messages") and len(st.session_state["messages"]) > 0
        )

        if has_messages:
            session_id = st.session_state.get("session_id")
            session_name = None

            try:
                if session_id and vision_agent:
                    session_name = vision_agent.get_session_name()
            except Exception:
                session_name = None

            if session_id and session_name:
                filename = f"vision_ai_chat_{session_name}.md"
            elif session_id:
                filename = f"vision_ai_chat_{session_id[:8]}.md"
            else:
                filename = "vision_ai_chat_new.md"

            if st.sidebar.download_button(
                " Export Chat",
                export_chat_history("Vision AI"),
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help=f"Export {len(st.session_state['messages'])} messages",
            ):
                st.sidebar.success("Chat history exported!")
        else:
            st.sidebar.button(
                " Export Chat",
                disabled=True,
                use_container_width=True,
                help="No messages to export",
            )

    ####################################################################
    # Session management widgets
    ####################################################################
    is_new_session = st.session_state.get("is_new_session", False)
    has_messages = (
        st.session_state.get("messages") and len(st.session_state["messages"]) > 0
    )

    if not is_new_session or has_messages:
        session_selector_widget(
            vision_agent, selected_model, get_vision_agent_with_settings
        )
        if is_new_session and has_messages:
            st.session_state["is_new_session"] = False
    else:
        st.sidebar.info(" New Chat - Start your conversation!")

    ####################################################################
    # About section
    ####################################################################
    about_section(
        "This Vision AI assistant analyzes images and answers questions about visual content using "
        "advanced vision-language models."
    )


if __name__ == "__main__":
    main()
```

---

<a name="examples--teams--collaborate_mode--collaboration_teampy"></a>

### `examples/teams/collaborate_mode/collaboration_team.py`

```python
"""Example of a Team using the `collaborate` mode.

In Collaborate Mode, all team members are given the same task and the team leader synthesizes their outputs into a cohesive response.

Run `pip install agno arxiv pypdf pycountry` to install the dependencies.
"""

import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.arxiv import ArxivTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools

arxiv_download_dir = Path(__file__).parent.joinpath("tmp", "arxiv_pdfs__{session_id}")
arxiv_download_dir.mkdir(parents=True, exist_ok=True)

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant posts on Reddit.
    """),
)

hackernews_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research a topic on HackerNews.",
    tools=[HackerNewsTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a HackerNews researcher.
    You will be given a topic to research on HackerNews.
    You will need to find the most relevant posts on HackerNews.
    """),
)

academic_paper_researcher = Agent(
    name="Academic Paper Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research academic papers and scholarly content",
    tools=[GoogleSearchTools(), ArxivTools(download_dir=arxiv_download_dir)],
    add_name_to_context=True,
    instructions=dedent("""
    You are a academic paper researcher.
    You will be given a topic to research in academic literature.
    You will need to find relevant scholarly articles, papers, and academic discussions.
    Focus on peer-reviewed content and citations from reputable sources.
    Provide brief summaries of key findings and methodologies.
    """),
)

twitter_researcher = Agent(
    name="Twitter Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Research trending discussions and real-time updates",
    tools=[DuckDuckGoTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a Twitter/X researcher.
    You will be given a topic to research on Twitter/X.
    You will need to find trending discussions, influential voices, and real-time updates.
    Focus on verified accounts and credible sources when possible.
    Track relevant hashtags and ongoing conversations.
    """),
)


agent_team = Team(
    name="Discussion Team",
    model=OpenAIChat("gpt-4o"),
    members=[
        reddit_researcher,
        hackernews_researcher,
        academic_paper_researcher,
        twitter_researcher,
    ],
    delegate_task_to_all_members=True,
    instructions=[
        "You are a discussion master.",
        "You have to stop the discussion when you think the team has reached a consensus.",
    ],
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent_team.aprint_response(
            input="Start the discussion on the topic: 'What is the best way to learn to code?'",
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="examples--teams--coordinate_mode--autonomous_startup_teampy"></a>

### `examples/teams/coordinate_mode/autonomous_startup_team.py`

```python
"""Example of a Team using the `coordinate` mode to play the role of a CEO of a Startup.

1. Run: `pip install agno exa_py slack_sdk pgvector psycopg` to install the dependencies
2. Add the following environment variables:
- `EXA_API_KEY`
- `SLACK_TOKEN`
"""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.slack import SlackTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.pgvector.pgvector import PgVector

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="autonomous_startup_team",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
)

knowledge.add_content(
    path="cookbook/teams/coordinate/data", reader=PDFReader(chunk=True)
)


support_channel = "testing"
sales_channel = "sales"


legal_compliance_agent = Agent(
    name="Legal Compliance Agent",
    role="Legal Compliance",
    model=OpenAIChat("gpt-4o"),
    tools=[ExaTools()],
    knowledge=knowledge,
    instructions=[
        "You are the Legal Compliance Agent of a startup, responsible for ensuring legal and regulatory compliance.",
        "Key Responsibilities:",
        "1. Review and validate all legal documents and contracts",
        "2. Monitor regulatory changes and update compliance policies",
        "3. Assess legal risks in business operations and product development",
        "4. Ensure data privacy and security compliance (GDPR, CCPA, etc.)",
        "5. Provide legal guidance on intellectual property protection",
        "6. Create and maintain compliance documentation",
        "7. Review marketing materials for legal compliance",
        "8. Advise on employment law and HR policies",
    ],
    add_datetime_to_context=True,
    markdown=True,
)

product_manager_agent = Agent(
    name="Product Manager Agent",
    role="Product Manager",
    model=OpenAIChat("gpt-4o"),
    knowledge=knowledge,
    instructions=[
        "You are the Product Manager of a startup, responsible for product strategy and execution.",
        "Key Responsibilities:",
        "1. Define and maintain the product roadmap",
        "2. Gather and analyze user feedback to identify needs",
        "3. Write detailed product requirements and specifications",
        "4. Prioritize features based on business impact and user value",
        "5. Collaborate with technical teams on implementation feasibility",
        "6. Monitor product metrics and KPIs",
        "7. Conduct competitive analysis",
        "8. Lead product launches and go-to-market strategies",
        "9. Balance user needs with business objectives",
    ],
    add_datetime_to_context=True,
    markdown=True,
    tools=[],
)

market_research_agent = Agent(
    name="Market Research Agent",
    role="Market Research",
    model=OpenAIChat("gpt-4o"),
    tools=[DuckDuckGoTools(), ExaTools()],
    knowledge=knowledge,
    instructions=[
        "You are the Market Research Agent of a startup, responsible for market intelligence and analysis.",
        "Key Responsibilities:",
        "1. Conduct comprehensive market analysis and size estimation",
        "2. Track and analyze competitor strategies and offerings",
        "3. Identify market trends and emerging opportunities",
        "4. Research customer segments and buyer personas",
        "5. Analyze pricing strategies in the market",
        "6. Monitor industry news and developments",
        "7. Create detailed market research reports",
        "8. Provide data-driven insights for decision making",
    ],
    add_datetime_to_context=True,
    markdown=True,
)

sales_agent = Agent(
    name="Sales Agent",
    role="Sales",
    model=OpenAIChat("gpt-4o"),
    tools=[SlackTools()],
    knowledge=knowledge,
    instructions=[
        "You are the Sales & Partnerships Agent of a startup, responsible for driving revenue growth and strategic partnerships.",
        "Key Responsibilities:",
        "1. Identify and qualify potential partnership and business opportunities",
        "2. Evaluate partnership proposals and negotiate terms",
        "3. Maintain relationships with existing partners and clients",
        "5. Collaborate with Legal Compliance Agent on contract reviews",
        "6. Work with Product Manager on feature requests from partners",
        f"7. Document and communicate all partnership details in #{sales_channel} channel",
        "",
        "Communication Guidelines:",
        "1. Always respond professionally and promptly to partnership inquiries",
        "2. Include all relevant details when sharing partnership opportunities",
        "3. Highlight potential risks and benefits in partnership proposals",
        "4. Maintain clear documentation of all discussions and agreements",
        "5. Ensure proper handoff to relevant team members when needed",
    ],
    add_datetime_to_context=True,
    markdown=True,
)


financial_analyst_agent = Agent(
    name="Financial Analyst Agent",
    role="Financial Analyst",
    model=OpenAIChat("gpt-4o"),
    knowledge=knowledge,
    tools=[YFinanceTools()],
    instructions=[
        "You are the Financial Analyst of a startup, responsible for financial planning and analysis.",
        "Key Responsibilities:",
        "1. Develop financial models and projections",
        "2. Create and analyze revenue forecasts",
        "3. Evaluate pricing strategies and unit economics",
        "4. Prepare investor reports and presentations",
        "5. Monitor cash flow and burn rate",
        "6. Analyze market conditions and financial trends",
        "7. Assess potential investment opportunities",
        "8. Track key financial metrics and KPIs",
        "9. Provide financial insights for strategic decisions",
    ],
    add_datetime_to_context=True,
    markdown=True,
)

customer_support_agent = Agent(
    name="Customer Support Agent",
    role="Customer Support",
    model=OpenAIChat("gpt-4o"),
    knowledge=knowledge,
    tools=[SlackTools()],
    instructions=[
        "You are the Customer Support Agent of a startup, responsible for handling customer inquiries and maintaining customer satisfaction.",
        f"When a user reports an issue or issue or the question you cannot answer, always send it to the #{support_channel} Slack channel with all relevant details.",
        "Always maintain a professional and helpful demeanor while ensuring proper routing of issues to the right channels.",
    ],
    add_datetime_to_context=True,
    markdown=True,
)


autonomous_startup_team = Team(
    name="CEO Agent",
    model=OpenAIChat("gpt-4o"),
    instructions=[
        "You are the CEO of a startup, responsible for overall leadership and success.",
        " Always delegate task to product manager agent so it can search the knowledge base.",
        "Instruct all agents to use the knowledge base to answer questions.",
        "Key Responsibilities:",
        "1. Set and communicate company vision and strategy",
        "2. Coordinate and prioritize team activities",
        "3. Make high-level strategic decisions",
        "4. Evaluate opportunities and risks",
        "5. Manage resource allocation",
        "6. Drive growth and innovation",
        "7. When a customer asks for help or reports an issue, immediately delegate to the Customer Support Agent",
        "8. When any partnership, sales, or business development inquiries come in, immediately delegate to the Sales Agent",
        "",
        "Team Coordination Guidelines:",
        "1. Product Development:",
        "   - Consult Product Manager for feature prioritization",
        "   - Use Market Research for validation",
        "   - Verify Legal Compliance for new features",
        "2. Market Entry:",
        "   - Combine Market Research and Sales insights",
        "   - Validate financial viability with Financial Analyst",
        "3. Strategic Planning:",
        "   - Gather input from all team members",
        "   - Prioritize based on market opportunity and resources",
        "4. Risk Management:",
        "   - Consult Legal Compliance for regulatory risks",
        "   - Review Financial Analyst's risk assessments",
        "5. Customer Support:",
        "   - Ensure all customer inquiries are handled promptly and professionally",
        "   - Maintain a positive and helpful attitude",
        "   - Escalate critical issues to the appropriate team",
        "",
        "Always maintain a balanced view of short-term execution and long-term strategy.",
    ],
    members=[
        product_manager_agent,
        market_research_agent,
        financial_analyst_agent,
        legal_compliance_agent,
        customer_support_agent,
        sales_agent,
    ],
    add_datetime_to_context=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
)

autonomous_startup_team.print_response(
    input="I want to start a startup that sells AI agents to businesses. What is the best way to do this?",
    stream=True,
    stream_intermediate_steps=True,
)


autonomous_startup_team.print_response(
    input="Give me good marketing campaign for buzzai?",
    stream=True,
    stream_intermediate_steps=True,
)

autonomous_startup_team.print_response(
    input="What is my company and what are the monetization strategies?",
    stream=True,
    stream_intermediate_steps=True,
)

# autonomous_startup_team.print_response(
#     input="Read the partnership details and give me details about the partnership with InnovateAI",
#     stream=True,
#     stream_intermediate_steps=True,
# )
```

---

<a name="examples--teams--coordinate_mode--content_teampy"></a>

### `examples/teams/coordinate_mode/content_team.py`

```python
from agno.agent import Agent
from agno.models.google.gemini import Gemini
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

# Create individual specialized agents
researcher = Agent(
    name="Researcher",
    role="Expert at finding information",
    tools=[DuckDuckGoTools()],
    model=Gemini("gemini-2.0-flash-001"),
)

writer = Agent(
    name="Writer",
    role="Expert at writing clear, engaging content",
    model=Gemini("gemini-2.0-flash-001"),
)

# Create a team with these agents
content_team = Team(
    name="Content Team",
    model=Gemini("gemini-2.5-flash"),
    # model=Gemini("gemini-2.0-flash-lite"),  # Try a small model for faster response
    members=[researcher, writer],
    instructions="You are a team of researchers and writers that work together to create high-quality content.",
    show_members_responses=True,
)

# Run the team with a task
content_team.print_response("Create a short article about quantum computing")
```

---

<a name="examples--teams--coordinate_mode--hackernews_teampy"></a>

### `examples/teams/coordinate_mode/hackernews_team.py`

```python
"""Example of a Team using the `coordinate` mode to play the role of a HackerNews Researcher.

1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/examples/teams/coordinate_mode/hackernews_team.py` to run the agent
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.newspaper4k import Newspaper4kTools
from pydantic import BaseModel


class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

article_reader = Agent(
    name="Article Reader",
    role="Reads articles from URLs.",
    tools=[Newspaper4kTools()],
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-4o"),
    members=[hn_researcher, web_searcher, article_reader],
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the article reader to read the links for the stories to get more information.",
        "Important: you must provide the article reader with the links to read.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    share_member_interactions=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

---

<a name="examples--teams--coordinate_mode--news_agency_teampy"></a>

### `examples/teams/coordinate_mode/news_agency_team.py`

```python
"""Example of a Team using the `coordinate` mode to play the role of a News Agency.

1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/examples/teams/coordinate_mode/news_agency_team.py` to run the agent
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

urls_file = Path(__file__).parent.joinpath("tmp", "urls__{session_id}.md")
urls_file.parent.mkdir(parents=True, exist_ok=True)


searcher = Agent(
    name="Searcher",
    role="Searches the top URLs for a topic",
    instructions=[
        "Given a topic, first generate a list of 3 search terms related to that topic.",
        "For each search term, search the web and analyze the results.Return the 10 most relevant URLs to the topic.",
        "You are writing for the New York Times, so the quality of the sources is important.",
    ],
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)
writer = Agent(
    name="Writer",
    role="Writes a high-quality article",
    description=(
        "You are a senior writer for the New York Times. Given a topic and a list of URLs, "
        "your goal is to write a high-quality NYT-worthy article on the topic."
    ),
    instructions=[
        "First read all urls using `read_article`."
        "Then write a high-quality NYT-worthy article on the topic."
        "The article should be well-structured, informative, engaging and catchy.",
        "Ensure the length is at least as long as a NYT cover story -- at a minimum, 15 paragraphs.",
        "Ensure you provide a nuanced and balanced opinion, quoting facts where possible.",
        "Focus on clarity, coherence, and overall quality.",
        "Never make up facts or plagiarize. Always provide proper attribution.",
        "Remember: you are writing for the New York Times, so the quality of the article is important.",
    ],
    tools=[Newspaper4kTools()],
    add_datetime_to_context=True,
)

editor = Team(
    name="Editor",
    model=OpenAIChat("gpt-4o"),
    members=[searcher, writer],
    description="You are a senior NYT editor. Given a topic, your goal is to write a NYT worthy article.",
    instructions=[
        "First ask the search journalist to search for the most relevant URLs for that topic.",
        "Then ask the writer to get an engaging draft of the article.",
        "Edit, proofread, and refine the article to ensure it meets the high standards of the New York Times.",
        "The article should be extremely articulate and well written. "
        "Focus on clarity, coherence, and overall quality.",
        "Remember: you are the final gatekeeper before the article is published, so make sure the article is perfect.",
    ],
    add_datetime_to_context=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
)
editor.print_response("Write an article about latest developments in AI.")
```

---

<a name="examples--teams--coordinate_mode--reasoning_teampy"></a>

### `examples/teams/coordinate_mode/reasoning_team.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions=["Use tables to display data"],
)

team_leader = Team(
    name="Reasoning Team Leader",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[ReasoningTools(add_instructions=True)],
    markdown=True,
    show_members_responses=True,
)

team_leader.print_response(
    "Tell me 1 company in New York, 1 in San Francisco and 1 in Chicago and the stock price of each",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)
```

---

<a name="examples--teams--coordinate_mode--skyplanner_mcp_teampy"></a>

### `examples/teams/coordinate_mode/skyplanner_mcp_team.py`

```python
"""
This example demonstrates how to use the MCP protocol to coordinate a team of agents.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API key from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com

"""

import asyncio
import os
from textwrap import dedent
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters
from pydantic import BaseModel


# Define response models
class AirbnbListing(BaseModel):
    name: str
    description: str
    address: Optional[str] = None
    price: Optional[str] = None
    dates_available: Optional[List[str]] = None
    url: Optional[str] = None


class Attraction(BaseModel):
    name: str
    description: str
    location: str
    rating: Optional[float] = None
    visit_duration: Optional[str] = None
    best_time_to_visit: Optional[str] = None


class WeatherInfo(BaseModel):
    average_temperature: str
    precipitation: str
    recommendations: str


class TravelPlan(BaseModel):
    airbnb_listings: List[AirbnbListing]
    attractions: List[Attraction]
    weather_info: Optional[WeatherInfo] = None
    suggested_itinerary: Optional[List[str]] = None


async def run_team():
    env = {
        **os.environ,
        "GOOGLE_MAPS_API_KEY": os.getenv("GOOGLE_MAPS_API_KEY"),
    }
    # Define server parameters
    airbnb_server_params = StdioServerParameters(
        command="npx",
        args=["-y", "@openbnb/mcp-server-airbnb", "--ignore-robots-txt"],
        env=env,
    )

    maps_server_params = StdioServerParameters(
        command="npx", args=["-y", "@modelcontextprotocol/server-google-maps"], env=env
    )

    # Use AsyncExitStack to manage multiple context managers
    async with (
        MCPTools(server_params=airbnb_server_params) as airbnb_tools,
        MCPTools(server_params=maps_server_params) as maps_tools,
    ):
        # Create all agents
        airbnb_agent = Agent(
            name="Airbnb",
            role="Airbnb Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[airbnb_tools],
            instructions=dedent("""\
                You are an agent that can find Airbnb listings for a given location.
            """),
            add_datetime_to_context=True,
        )

        maps_agent = Agent(
            name="Google Maps",
            role="Location Services Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[maps_tools],
            instructions=dedent("""\
                You are an agent that helps find attractions, points of interest,
                and provides directions in travel destinations. Help plan travel
                routes and find interesting places to visit for a given location and date.
            """),
            add_datetime_to_context=True,
        )

        web_search_agent = Agent(
            name="Web Search",
            role="Web Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools(cache_results=True)],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for information about a given location.
            """),
            add_datetime_to_context=True,
        )

        weather_search_agent = Agent(
            name="Weather Search",
            role="Weather Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools()],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for the weather forecast for a given location and date.
            """),
            add_datetime_to_context=True,
        )

        # Create and run the team
        team = Team(
            name="SkyPlanner",
            model=OpenAIChat("gpt-4o"),
            members=[
                airbnb_agent,
                web_search_agent,
                maps_agent,
                weather_search_agent,
            ],
            instructions=[
                "First, find the best Airbnb listings for the given location.",
                "Use the Google Maps agent to identify key neighborhoods and attractions.",
                "Use the Attractions agent to find highly-rated places to visit and restaurants.",
                "Get weather information to help with packing and planning outdoor activities.",
                "Finally, plan an itinerary for the trip.",
                "Continue asking individual team members until you have ALL the information you need.",
            ],
            output_schema=TravelPlan,
            markdown=True,
            show_members_responses=True,
            add_datetime_to_context=True,
        )

        # Execute the team's task
        await team.aprint_response(
            dedent("""\
            I want to travel to San Francisco from New York sometime in May.
            I am one person going for 2 weeks.
            Plan my travel itinerary.
            Make sure to include the best attractions, restaurants, and activities.
            Make sure to include the best flight deals.
            Make sure to include the best Airbnb listings.
            Make sure to include the weather information.\
        """)
        )


if __name__ == "__main__":
    asyncio.run(run_team())
```

---

<a name="examples--teams--coordinate_mode--tic_tac_toe_teampy"></a>

### `examples/teams/coordinate_mode/tic_tac_toe_team.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.team.team import Team

player_1 = Agent(
    name="Player 1",
    role="Play Tic Tac Toe",
    model=OpenAIChat(id="gpt-4o"),
    add_name_to_context=True,
    instructions=dedent("""
    You are a Tic Tac Toe player.
    You will be given a Tic Tac Toe board and a player to play against.
    You will need to play the game and try to win.
    """),
)

player_2 = Agent(
    name="Player 2",
    role="Play Tic Tac Toe",
    model=Gemini(id="gemini-2.0-flash"),
    add_name_to_context=True,
    instructions=dedent("""
    You are a Tic Tac Toe player.
    You will be given a Tic Tac Toe board and a player to play against.
    You will need to play the game and try to win.
    """),
)

# This is a simple team that plays Tic Tac Toe. It is not perfect and would work better with reasoning.
agent_team = Team(
    name="Tic Tac Toe Team",
    model=OpenAIChat("gpt-4o"),
    members=[player_1, player_2],
    instructions=[
        "You are a games master.",
        "Initialize the board state as an empty 3x3 grid with numbers 1-9.",
        "Ask the players to make their moves one by one and wait for their responses. Delegate the turn to the other player after each move.",
        "After each move, store the updated board state so that players have access to the board state.",
        "Don't confirm the results of the game afterwards, just report the final board state and the results.",
        "You have to stop the game when one of the players has won.",
    ],
    share_member_interactions=True,
    debug_mode=True,
    markdown=True,
    show_members_responses=True,
)

agent_team.print_response(
    input="Run a full Tic Tac Toe game. After the game, report the final board state and the results.",
    stream=True,
    stream_intermediate_steps=True,
)
```

---

<a name="examples--teams--coordinate_mode--travel_planner_mcp_teampy"></a>

### `examples/teams/coordinate_mode/travel_planner_mcp_team.py`

```python
"""
This example demonstrates how to use the MCP protocol to coordinate a team of agents.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API token from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com

- Apify:
    - Set the environment variable `APIFY_TOKEN` with your Apify API token.
    You can obtain the API token from the Apify Console:
    https://console.apify.com/settings/integrations

"""

import asyncio
import os
from textwrap import dedent
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools
from mcp import StdioServerParameters
from pydantic import BaseModel


# Define response models
class AirbnbListing(BaseModel):
    name: str
    description: str
    address: Optional[str] = None
    price: Optional[str] = None
    dates_available: Optional[List[str]] = None
    url: Optional[str] = None


class Attraction(BaseModel):
    name: str
    description: str
    location: str
    rating: Optional[float] = None
    visit_duration: Optional[str] = None
    best_time_to_visit: Optional[str] = None


class WeatherInfo(BaseModel):
    average_temperature: str
    precipitation: str
    recommendations: str


class TravelPlan(BaseModel):
    airbnb_listings: List[AirbnbListing]
    attractions: List[Attraction]
    weather_info: Optional[WeatherInfo] = None
    suggested_itinerary: Optional[List[str]] = None


async def run_team():
    env = {
        **os.environ,
        "GOOGLE_MAPS_API_KEY": os.getenv("GOOGLE_MAPS_API_KEY"),
    }
    # Define server parameters
    airbnb_server_params = StdioServerParameters(
        command="npx",
        args=["-y", "@openbnb/mcp-server-airbnb", "--ignore-robots-txt"],
        env=env,
    )

    maps_server_params = StdioServerParameters(
        command="npx", args=["-y", "@modelcontextprotocol/server-google-maps"], env=env
    )

    # Use AsyncExitStack to manage multiple context managers
    async with (
        MCPTools(server_params=airbnb_server_params) as airbnb_tools,
        MCPTools(server_params=maps_server_params) as maps_tools,
    ):
        # Create all agents
        airbnb_agent = Agent(
            name="Airbnb",
            role="Airbnb Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[airbnb_tools],
            instructions=dedent("""\
                You are an agent that can find Airbnb listings for a given location.\
            """),
            add_datetime_to_context=True,
        )

        maps_agent = Agent(
            name="Google Maps",
            role="Location Services Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[maps_tools],
            instructions=dedent("""\
                You are an agent that helps find attractions, points of interest,
                and provides directions in travel destinations. Help plan travel
                routes and find interesting places to visit for a given location and date.\
            """),
            add_datetime_to_context=True,
        )

        web_search_agent = Agent(
            name="Web Search",
            role="Web Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools(cache_results=True)],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for information about a given location.\
            """),
            add_datetime_to_context=True,
        )

        weather_search_agent = Agent(
            name="Weather Search",
            role="Weather Search Agent",
            model=OpenAIChat("gpt-4o"),
            tools=[DuckDuckGoTools()],
            instructions=dedent("""\
                You are an agent that can search the web for information.
                Search for the weather forecast for a given location and date.\
            """),
            add_datetime_to_context=True,
        )

        # Create and run the team
        team = Team(
            name="SkyPlanner",
            model=OpenAIChat("gpt-4o"),
            members=[
                airbnb_agent,
                web_search_agent,
                maps_agent,
                weather_search_agent,
            ],
            instructions=[
                "Plan a full itinerary for the trip.",
                "Continue asking individual team members until you have ALL the information you need.",
                "Think about the best way to tackle the task.",
            ],
            tools=[ReasoningTools(add_instructions=True)],
            output_schema=TravelPlan,
            markdown=True,
            debug_mode=True,
            show_members_responses=True,
            add_datetime_to_context=True,
        )

        # Execute the team's task
        await team.aprint_response(
            dedent("""\
            I want to travel to San Francisco from New York sometime in May.
            I am one person going for 2 weeks.
            Plan my travel itinerary.
            Make sure to include the best attractions, restaurants, and activities.
            Make sure to include the best Airbnb listings.
            Make sure to include the weather information.\
        """)
        )


if __name__ == "__main__":
    asyncio.run(run_team())
```

---

<a name="examples--teams--route_mode--ai_customer_support_teampy"></a>

### `examples/teams/route_mode/ai_customer_support_team.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.website_reader import WebsiteReader
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.slack import SlackTools
from agno.vectordb.pgvector.pgvector import PgVector

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="website_documents",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
)

knowledge.add_content(
    url="https://docs.agno.com/introduction",
    reader=WebsiteReader(
        # Number of links to follow from the seed URLs
        max_links=10,
    ),
)


support_channel = "testing"
feedback_channel = "testing"

doc_researcher_agent = Agent(
    name="Doc researcher Agent",
    role="Search the knowledge base for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(), ExaTools()],
    knowledge=knowledge,
    search_knowledge=True,
    instructions=[
        "You are a documentation expert for given product. Search the knowledge base thoroughly to answer user questions.",
        "Always provide accurate information based on the documentation.",
        "If the question matches an FAQ, provide the specific FAQ answer from the documentation.",
        "When relevant, include direct links to specific documentation pages that address the user's question.",
        "If you're unsure about an answer, acknowledge it and suggest where the user might find more information.",
        "Format your responses clearly with headings, bullet points, and code examples when appropriate.",
        "Always verify that your answer directly addresses the user's specific question.",
        "If you cannot find the answer in the documentation knowledge base, use the DuckDuckGoTools or ExaTools to search the web for relevant information to answer the user's question.",
    ],
)


escalation_manager_agent = Agent(
    name="Escalation Manager Agent",
    role="Escalate the issue to the slack channel",
    model=OpenAIChat(id="gpt-4o"),
    tools=[SlackTools()],
    instructions=[
        "You are an escalation manager responsible for routing critical issues to the support team.",
        f"When a user reports an issue, always send it to the #{support_channel} Slack channel with all relevant details using the send_message toolkit function.",
        "Include the user's name, contact information (if available), and a clear description of the issue.",
        "After escalating the issue, respond to the user confirming that their issue has been escalated.",
        "Your response should be professional and reassuring, letting them know the support team will address it soon.",
        "Always include a ticket or reference number if available to help the user track their issue.",
        "Never attempt to solve technical problems yourself - your role is strictly to escalate and communicate.",
    ],
)

feedback_collector_agent = Agent(
    name="Feedback Collector Agent",
    role="Collect feedback from the user",
    model=OpenAIChat(id="gpt-4o"),
    tools=[SlackTools()],
    description="You are an AI agent that can collect feedback from the user.",
    instructions=[
        "You are responsible for collecting user feedback about the product or feature requests.",
        f"When a user provides feedback or suggests a feature, use the Slack tool to send it to the #{feedback_channel} channel using the send_message toolkit function.",
        "Include all relevant details from the user's feedback in your Slack message.",
        "After sending the feedback to Slack, respond to the user professionally, thanking them for their input.",
        "Your response should acknowledge their feedback and assure them that it will be taken into consideration.",
        "Be warm and appreciative in your tone, as user feedback is valuable for improving our product.",
        "Do not promise specific timelines or guarantee that their suggestions will be implemented.",
    ],
)


customer_support_team = Team(
    name="Customer Support Team",
    model=OpenAIChat("gpt-4o"),
    members=[doc_researcher_agent, escalation_manager_agent, feedback_collector_agent],
    determine_input_for_members=False,
    respond_directly=True,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
    instructions=[
        "You are the lead customer support agent responsible for classifying and routing customer inquiries.",
        "Carefully analyze each user message and determine if it is: a question that needs documentation research, a bug report that requires escalation, or product feedback.",
        "For general questions about the product, route to the doc_researcher_agent who will search documentation for answers.",
        "If the doc_researcher_agent cannot find an answer to a question, escalate it to the escalation_manager_agent.",
        "For bug reports or technical issues, immediately route to the escalation_manager_agent.",
        "For feature requests or product feedback, route to the feedback_collector_agent.",
        "Always provide a clear explanation of why you're routing the inquiry to a specific agent.",
        "After receiving a response from the appropriate agent, relay that information back to the user in a professional and helpful manner.",
        "Ensure a seamless experience for the user by maintaining context throughout the conversation.",
    ],
)

# Add in the query and the agent redirects it to the appropriate agent
customer_support_team.print_response(
    "Hi Team, I want to build an educational platform where the models are have access to tons of study materials, How can Agno platform help me build this?",
    stream=True,
)
# customer_support_team.print_response(
#     "Support json schemas in Gemini client in addition to pydantic base model",
#     stream=True,
# )
# customer_support_team.print_response(
#     "Can you please update me on the above feature",
#     stream=True,
# )
# customer_support_team.print_response(
#     "[Bug] Async tools in team of agents not awaited properly, causing runtime errors ",
#     stream=True,
# )
```

---

<a name="examples--teams--route_mode--multi_language_teampy"></a>

### `examples/teams/route_mode/multi_language_team.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.team.team import Team

japanese_agent = Agent(
    name="Japanese Agent",
    role="You only answer in Japanese",
    model=DeepSeek(id="deepseek-chat"),
)
chinese_agent = Agent(
    name="Chinese Agent",
    role="You only answer in Chinese",
    model=DeepSeek(id="deepseek-chat"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You only answer in Spanish",
    model=OpenAIChat(id="gpt-4o"),
)
french_agent = Agent(
    name="French Agent",
    role="You only answer in French",
    model=MistralChat(id="mistral-large-latest"),
)
german_agent = Agent(
    name="German Agent",
    role="You only answer in German",
    model=Claude("claude-3-5-sonnet-20241022"),
)

multi_language_team = Team(
    name="Multi Language Team",
    respond_directly=True,
    determine_input_for_members=False,
    model=OpenAIChat("gpt-4o"),
    members=[
        spanish_agent,
        japanese_agent,
        french_agent,
        german_agent,
        chinese_agent,
    ],
    description="You are a language router that directs questions to the appropriate language agent.",
    instructions=[
        "Identify the language of the user's question and direct it to the appropriate language agent.",
        "Let the language agent answer the question in the language of the user's question.",
        "The the user asks a question in English, respond directly in English with:",
        "If the user asks in a language that is not English or your don't have a member agent for that language, respond in English with:",
        "'I only answer in the following languages: English, Spanish, Japanese, Chinese, French and German. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    # Ask "How are you?" in all supported languages
    multi_language_team.print_response("Comment allez-vous?", stream=True)  # French
    multi_language_team.print_response("How are you?", stream=True)  # English
    multi_language_team.print_response("", stream=True)  # Chinese
    multi_language_team.print_response("?", stream=True)  # Japanese
    multi_language_team.print_response("Wie geht es Ihnen?", stream=True)  # German
    multi_language_team.print_response("Hola, cmo ests?", stream=True)  # Spanish
    multi_language_team.print_response("Come stai?", stream=True)  # Italian
```

---

<a name="examples--teams--route_mode--multi_purpose_teampy"></a>

### `examples/teams/route_mode/multi_purpose_team.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.google.gemini import Gemini
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.calculator import CalculatorTools
from agno.tools.dalle import DalleTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.e2b import E2BTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions=["Use tables to display data"],
)

image_agent = Agent(
    name="Image Agent",
    role="Analyze or generate images",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description="You are an AI agent that can analyze images or create images using DALL-E.",
    instructions=[
        "When the user asks you about an image, give your best effort to analyze the image and return a description of the image.",
        "When the user asks you to create an image, use the DALL-E tool to create an image.",
        "The DALL-E tool will return an image URL.",
        "Return the image URL in your response in the following format: `![image description](image URL)`",
    ],
)

file_analysis_agent = Agent(
    name="File Analysis Agent",
    role="Analyze files",
    model=Claude(id="claude-3-7-sonnet-latest"),
    description="You are an AI agent that can analyze files.",
    instructions=[
        "You are an AI agent that can analyze files.",
        "You are given a file and you need to answer questions about the file.",
    ],
)

writer_agent = Agent(
    name="Write Agent",
    role="Write content",
    model=OpenAIChat(id="gpt-4o"),
    description="You are an AI agent that can write content.",
    instructions=[
        "You are a versatile writer who can create content on any topic.",
        "When given a topic, write engaging and informative content in the requested format and style.",
        "If you receive mathematical expressions or calculations from the calculator agent, convert them into clear written text.",
        "Ensure your writing is clear, accurate and tailored to the specific request.",
        "Maintain a natural, engaging tone while being factually precise.",
    ],
)

audio_agent = Agent(
    name="Audio Agent",
    role="Analyze audio",
    model=Gemini(id="gemini-2.0-flash-exp"),
)

calculator_agent = Agent(
    name="Calculator Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Calculate",
    tools=[CalculatorTools()],
    markdown=True,
)

calculator_writer_team = Team(
    name="Calculator Writer Team",
    model=OpenAIChat("gpt-4o"),
    members=[calculator_agent, writer_agent],
    instructions=[
        "You are a team of two agents. The calculator agent and the writer agent.",
        "The calculator agent is responsible for calculating the result of the mathematical expression.",
        "The writer agent is responsible for writing the result of the mathematical expression in a clear and engaging manner."
        "You need to coordinate the work between the two agents and give the final response to the user.",
        "You need to give the final response to the user in the requested format and style.",
    ],
    markdown=True,
    show_members_responses=True,
)

reasoning_agent = Agent(
    name="Reasoning Agent",
    role="Reasoning about Math",
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    instructions=["You are a reasoning agent that can reason about math."],
    markdown=True,
    debug_mode=True,
)

code_execution_agent = Agent(
    name="Code Execution Sandbox",
    id="e2b-sandbox",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[E2BTools()],
    markdown=True,
    instructions=[
        "You are an expert at writing and validating Python code using a secure E2B sandbox environment.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the E2B sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
        "",
    ],
)

agent_team = Team(
    name="Agent Team",
    model=Claude(id="claude-3-5-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
        image_agent,
        audio_agent,
        calculator_writer_team,
        reasoning_agent,
        file_analysis_agent,
        code_execution_agent,
    ],
    instructions=[
        "You are a team of agents that can answer questions about the web, finance, images, audio, and files.",
        "You can use your member agents to answer the questions.",
        "if you are asked about a file, use the file analysis agent to analyze the file.",
        "You can also answer directly, you don't HAVE to forward the question to a member agent.",
    ],
    respond_directly=True,
    markdown=True,
    show_members_responses=True,
)

# Use the reasoning agent to reason about the result
agent_team.print_response(
    "What is the square root of 6421123 times the square root of 9485271", stream=True
)
agent_team.print_response(
    "Calculate the sum of 10 and 20 and give write something about how you did the calculation",
    stream=True,
)

# Use web and finance agents to answer the question
agent_team.print_response(
    "Summarize analyst recommendations and share the latest news for NVDA", stream=True
)

# image_path = Path(__file__).parent.joinpath("res/sample.jpg")
# # # Use image agent to analyze the image
# agent_team.print_response(
#     "Write a 3 sentence fiction story about the image",
#     images=[Image(filepath=image_path)],
# )

# Use audio agent to analyze the audio
# url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"
# response = requests.get(url)
# audio_content = response.content
# # Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.
# agent_team.print_response(
#     "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
#     audio=[Audio(content=audio_content)],
# )

# Use image agent to generate an image
# agent_team.print_response(
#     "Generate an image of a cat", stream=True
# )

# Use the calculator writer team to calculate the result
# agent_team.print_response(
#     "What is the square root of 6421123 times the square root of 9485271", stream=True
# )

# Use the code execution agent to write and execute code
# agent_team.print_response(
#     "write a python code to calculate the square root of 6421123 times the square root of 9485271",
#     stream=True,
# )


# # Use the reasoning agent to reason about the result
# agent_team.print_response("9.11 and 9.9 -- which is bigger?", stream=True)


# pdf_path = Path(__file__).parent.joinpath("res/ThaiRecipes.pdf")

# # Download the file using the download_file function
# download_file(
#     "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
# )
# # Use file analysis agent to analyze the file
# agent_team.print_response(
#     "Summarize the contents of the attached file.",
#     files=[
#         File(
#             filepath=pdf_path,
#         ),
#     ],
# )
```

---

<a name="examples--teams--route_mode--reasoning_teampy"></a>

### `examples/teams/route_mode/reasoning_team.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions=["Use tables to display data"],
)

team_leader = Team(
    name="Reasoning Team Leader",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[ReasoningTools(add_instructions=True)],
    markdown=True,
    show_members_responses=True,
)

team_leader.print_response(
    "Hi", stream=True, stream_intermediate_steps=True, show_full_reasoning=True
)
team_leader.print_response(
    "What is the stock price of Apple?",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)
team_leader.print_response(
    "What's going on in New York?",
    stream=True,
    stream_intermediate_steps=True,
    show_full_reasoning=True,
)
```

---

<a name="examples--teams--route_mode--simplepy"></a>

### `examples/teams/route_mode/simple.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.mistral.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.team.team import Team

english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="gpt-4o"),
)
chinese_agent = Agent(
    name="Chinese Agent",
    role="You only answer in Chinese",
    model=DeepSeek(id="deepseek-chat"),
)
french_agent = Agent(
    name="French Agent",
    role="You can only answer in French",
    model=MistralChat(id="mistral-large-latest"),
)

multi_language_team = Team(
    name="Multi Language Team",
    model=OpenAIChat("gpt-4o"),
    members=[english_agent, chinese_agent, french_agent],
    markdown=True,
    description="You are a language router that directs questions to the appropriate language agent.",
    instructions=[
        "Identify the language of the user's question and direct it to the appropriate language agent.",
        "Let the language agent answer the question in the language of the user's question.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English, Chinese, French. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    respond_directly=True,
    determine_input_for_members=False,
    show_members_responses=True,
)


if __name__ == "__main__":
    # Ask "How are you?" in all supported languages
    multi_language_team.print_response("Comment allez-vous?", stream=True)  # French
    multi_language_team.print_response("How are you?", stream=True)  # English
    multi_language_team.print_response("", stream=True)  # Chinese
    multi_language_team.print_response("Come stai?", stream=True)  # Italian

    multi_language_team.print_response("What are you capable of?", stream=True)
    multi_language_team.print_response("Tell me about the history of AI?", stream=True)
```

---

<a name="examples--workflows--blog_post_generatorpy"></a>

### `examples/workflows/blog_post_generator.py`

```python
""" Blog Post Generator v2.0 - Your AI Content Creation Studio!

This advanced example demonstrates how to build a sophisticated blog post generator using
the new workflow v2.0 architecture. The workflow combines web research capabilities with
professional writing expertise using a multi-stage approach:

1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

Key capabilities:
- Advanced web research and source evaluation
- Content scraping and processing
- Professional writing with SEO optimization
- Automatic content caching for efficiency
- Source attribution and fact verification
"""

import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# --- Response Models ---
class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


# --- Agents ---
research_agent = Agent(
    name="Blog Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    description=dedent("""\
    You are BlogResearch-X, an elite research assistant specializing in discovering
    high-quality sources for compelling blog content. Your expertise includes:

    - Finding authoritative and trending sources
    - Evaluating content credibility and relevance
    - Identifying diverse perspectives and expert opinions
    - Discovering unique angles and insights
    - Ensuring comprehensive topic coverage
    """),
    instructions=dedent("""\
    1. Search Strategy 
       - Find 10-15 relevant sources and select the 5-7 best ones
       - Prioritize recent, authoritative content
       - Look for unique angles and expert insights
    2. Source Evaluation 
       - Verify source credibility and expertise
       - Check publication dates for timeliness
       - Assess content depth and uniqueness
    3. Diversity of Perspectives 
       - Include different viewpoints
       - Gather both mainstream and expert opinions
       - Find supporting data and statistics
    """),
    output_schema=SearchResults,
)

content_scraper_agent = Agent(
    name="Content Scraper Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[Newspaper4kTools()],
    description=dedent("""\
    You are ContentBot-X, a specialist in extracting and processing digital content
    for blog creation. Your expertise includes:

    - Efficient content extraction
    - Smart formatting and structuring
    - Key information identification
    - Quote and statistic preservation
    - Maintaining source attribution
    """),
    instructions=dedent("""\
    1. Content Extraction 
       - Extract content from the article
       - Preserve important quotes and statistics
       - Maintain proper attribution
       - Handle paywalls gracefully
    2. Content Processing 
       - Format text in clean markdown
       - Preserve key information
       - Structure content logically
    3. Quality Control 
       - Verify content relevance
       - Ensure accurate extraction
       - Maintain readability
    """),
    output_schema=ScrapedArticle,
)

blog_writer_agent = Agent(
    name="Blog Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are BlogMaster-X, an elite content creator combining journalistic excellence
    with digital marketing expertise. Your strengths include:

    - Crafting viral-worthy headlines
    - Writing engaging introductions
    - Structuring content for digital consumption
    - Incorporating research seamlessly
    - Optimizing for SEO while maintaining quality
    - Creating shareable conclusions
    """),
    instructions=dedent("""\
    1. Content Strategy 
       - Craft attention-grabbing headlines
       - Write compelling introductions
       - Structure content for engagement
       - Include relevant subheadings
    2. Writing Excellence 
       - Balance expertise with accessibility
       - Use clear, engaging language
       - Include relevant examples
       - Incorporate statistics naturally
    3. Source Integration 
       - Cite sources properly
       - Include expert quotes
       - Maintain factual accuracy
    4. Digital Optimization 
       - Structure for scanability
       - Include shareable takeaways
       - Optimize for SEO
       - Add engaging subheadings

    Format your blog post with this structure:
    # {Viral-Worthy Headline}

    ## Introduction
    {Engaging hook and context}

    ## {Compelling Section 1}
    {Key insights and analysis}
    {Expert quotes and statistics}

    ## {Engaging Section 2}
    {Deeper exploration}
    {Real-world examples}

    ## {Practical Section 3}
    {Actionable insights}
    {Expert recommendations}

    ## Key Takeaways
    - {Shareable insight 1}
    - {Practical takeaway 2}
    - {Notable finding 3}

    ## Sources
    {Properly attributed sources with links}
    """),
    markdown=True,
)


# --- Helper Functions ---
def get_cached_blog_post(session_state, topic: str) -> Optional[str]:
    """Get cached blog post from workflow session state"""
    logger.info("Checking if cached blog post exists")
    return session_state.get("blog_posts", {}).get(topic)


def cache_blog_post(session_state, topic: str, blog_post: str):
    """Cache blog post in workflow session state"""
    logger.info(f"Saving blog post for topic: {topic}")
    if "blog_posts" not in session_state:
        session_state["blog_posts"] = {}
    session_state["blog_posts"][topic] = blog_post


def get_cached_search_results(session_state, topic: str) -> Optional[SearchResults]:
    """Get cached search results from workflow session state"""
    logger.info("Checking if cached search results exist")
    search_results = session_state.get("search_results", {}).get(topic)
    if search_results and isinstance(search_results, dict):
        try:
            return SearchResults.model_validate(search_results)
        except Exception as e:
            logger.warning(f"Could not validate cached search results: {e}")
    return search_results if isinstance(search_results, SearchResults) else None


def cache_search_results(session_state, topic: str, search_results: SearchResults):
    """Cache search results in workflow session state"""
    logger.info(f"Saving search results for topic: {topic}")
    if "search_results" not in session_state:
        session_state["search_results"] = {}
    session_state["search_results"][topic] = search_results.model_dump()


def get_cached_scraped_articles(
    session_state, topic: str
) -> Optional[Dict[str, ScrapedArticle]]:
    """Get cached scraped articles from workflow session state"""
    logger.info("Checking if cached scraped articles exist")
    scraped_articles = session_state.get("scraped_articles", {}).get(topic)
    if scraped_articles and isinstance(scraped_articles, dict):
        try:
            return {
                url: ScrapedArticle.model_validate(article)
                for url, article in scraped_articles.items()
            }
        except Exception as e:
            logger.warning(f"Could not validate cached scraped articles: {e}")
    return scraped_articles if isinstance(scraped_articles, dict) else None


def cache_scraped_articles(
    session_state, topic: str, scraped_articles: Dict[str, ScrapedArticle]
):
    """Cache scraped articles in workflow session state"""
    logger.info(f"Saving scraped articles for topic: {topic}")
    if "scraped_articles" not in session_state:
        session_state["scraped_articles"] = {}
    session_state["scraped_articles"][topic] = {
        url: article.model_dump() for url, article in scraped_articles.items()
    }


async def get_search_results(
    session_state, topic: str, use_cache: bool = True, num_attempts: int = 3
) -> Optional[SearchResults]:
    """Get search results with caching support"""

    # Check cache first
    if use_cache:
        cached_results = get_cached_search_results(session_state, topic)
        if cached_results:
            logger.info(f"Found {len(cached_results.articles)} articles in cache.")
            return cached_results

    # Search for new results
    for attempt in range(num_attempts):
        try:
            print(
                f" Searching for articles about: {topic} (attempt {attempt + 1}/{num_attempts})"
            )
            response = await research_agent.arun(topic)

            if (
                response
                and response.content
                and isinstance(response.content, SearchResults)
            ):
                article_count = len(response.content.articles)
                logger.info(f"Found {article_count} articles on attempt {attempt + 1}")
                print(f" Found {article_count} relevant articles")

                # Cache the results
                cache_search_results(session_state, topic, response.content)
                return response.content
            else:
                logger.warning(
                    f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                )

        except Exception as e:
            logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

    logger.error(f"Failed to get search results after {num_attempts} attempts")
    return None


async def scrape_articles(
    session_state,
    topic: str,
    search_results: SearchResults,
    use_cache: bool = True,
) -> Dict[str, ScrapedArticle]:
    """Scrape articles with caching support"""

    # Check cache first
    if use_cache:
        cached_articles = get_cached_scraped_articles(session_state, topic)
        if cached_articles:
            logger.info(f"Found {len(cached_articles)} scraped articles in cache.")
            return cached_articles

    scraped_articles: Dict[str, ScrapedArticle] = {}

    print(f" Scraping {len(search_results.articles)} articles...")

    for i, article in enumerate(search_results.articles, 1):
        try:
            print(
                f" Scraping article {i}/{len(search_results.articles)}: {article.title[:50]}..."
            )
            response = await content_scraper_agent.arun(article.url)

            if (
                response
                and response.content
                and isinstance(response.content, ScrapedArticle)
            ):
                scraped_articles[response.content.url] = response.content
                logger.info(f"Scraped article: {response.content.url}")
                print(f" Successfully scraped: {response.content.title[:50]}...")
            else:
                print(f" Failed to scrape: {article.title[:50]}...")

        except Exception as e:
            logger.warning(f"Failed to scrape {article.url}: {str(e)}")
            print(f" Error scraping: {article.title[:50]}...")

    # Cache the scraped articles
    cache_scraped_articles(session_state, topic, scraped_articles)
    return scraped_articles


# --- Main Execution Function ---
async def blog_generation_execution(
    session_state,
    topic: str = None,
    use_search_cache: bool = True,
    use_scrape_cache: bool = True,
    use_blog_cache: bool = True,
) -> str:
    """
    Blog post generation workflow execution function.

    Args:
        session_state: The shared session state
        topic: Blog post topic (if not provided, uses execution_input.input)
        use_search_cache: Whether to use cached search results
        use_scrape_cache: Whether to use cached scraped articles
        use_blog_cache: Whether to use cached blog posts
    """

    blog_topic = topic

    if not blog_topic:
        return " No blog topic provided. Please specify a topic."

    print(f" Generating blog post about: {blog_topic}")
    print("=" * 60)

    # Check for cached blog post first
    if use_blog_cache:
        cached_blog = get_cached_blog_post(session_state, blog_topic)
        if cached_blog:
            print(" Found cached blog post!")
            return cached_blog

    # Phase 1: Research and gather sources
    print("\n PHASE 1: RESEARCH & SOURCE GATHERING")
    print("=" * 50)

    search_results = await get_search_results(
        session_state, blog_topic, use_search_cache
    )

    if not search_results or len(search_results.articles) == 0:
        return f" Sorry, could not find any articles on the topic: {blog_topic}"

    print(f" Found {len(search_results.articles)} relevant sources:")
    for i, article in enumerate(search_results.articles, 1):
        print(f"   {i}. {article.title[:60]}...")

    # Phase 2: Content extraction
    print("\n PHASE 2: CONTENT EXTRACTION")
    print("=" * 50)

    scraped_articles = await scrape_articles(
        session_state, blog_topic, search_results, use_scrape_cache
    )

    if not scraped_articles:
        return f" Could not extract content from any articles for topic: {blog_topic}"

    print(f" Successfully extracted content from {len(scraped_articles)} articles")

    # Phase 3: Blog post writing
    print("\n PHASE 3: BLOG POST CREATION")
    print("=" * 50)

    # Prepare input for the writer
    writer_input = {
        "topic": blog_topic,
        "articles": [article.model_dump() for article in scraped_articles.values()],
    }

    print(" AI is crafting your blog post...")
    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))

    if not writer_response or not writer_response.content:
        return f" Failed to generate blog post for topic: {blog_topic}"

    blog_post = writer_response.content

    # Cache the blog post
    cache_blog_post(session_state, blog_topic, blog_post)

    print(" Blog post generated successfully!")
    print(f" Length: {len(blog_post)} characters")
    print(f" Sources: {len(scraped_articles)} articles")

    return blog_post


# --- Workflow Definition ---
blog_generator_workflow = Workflow(
    name="Blog Post Generator",
    description="Advanced blog post generator with research and content creation capabilities",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/blog_generator.db",
    ),
    steps=blog_generation_execution,
    session_state={},  # Initialize empty session state for caching
)


if __name__ == "__main__":
    import random

    async def main():
        # Fun example topics to showcase the generator's versatility
        example_topics = [
            "The Rise of Artificial General Intelligence: Latest Breakthroughs",
            "How Quantum Computing is Revolutionizing Cybersecurity",
            "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint",
            "The Future of Work: AI and Human Collaboration",
            "Space Tourism: From Science Fiction to Reality",
            "Mindfulness and Mental Health in the Digital Age",
            "The Evolution of Electric Vehicles: Current State and Future Trends",
            "Why Cats Secretly Run the Internet",
            "The Science Behind Why Pizza Tastes Better at 2 AM",
            "How Rubber Ducks Revolutionized Software Development",
        ]

        # Test with a random topic
        topic = random.choice(example_topics)

        print(" Testing Blog Post Generator v2.0")
        print("=" * 60)
        print(f" Topic: {topic}")
        print()

        # Generate the blog post
        resp = await blog_generator_workflow.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_blog_cache=True,
        )

        pprint_run_response(resp, markdown=True, show_time=True)

    asyncio.run(main())
```

---

<a name="examples--workflows--company_analysis--agentspy"></a>

### `examples/workflows/company_analysis/agents.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.firecrawl import FirecrawlTools
from agno.tools.reasoning import ReasoningTools

company_overview_agent = Agent(
    name="Company Overview Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[FirecrawlTools(crawl=True, limit=2)],
    role="Expert in comprehensive company research and business analysis",
    instructions="""
    You are a business research analyst. You will receive structured input data containing companies to analyze, 
    category information, regional context, and other procurement details.
    
    **Input Data Structure:**
    The input contains the following data:
    - companies: List of companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers in this category
    
    **Your Task:**
    For each company in the input, provide comprehensive overviews that include:
    
    **Company Basics:**
    - Full legal name and common name
    - Industry/sector classification relevant to the procurement category
    - Founding year and key milestones
    - Public/private status
    
    **Financial Profile:**
    - Annual revenue (latest available)
    - Market capitalization (if public)
    - Employee count and growth
    - Financial health indicators
    
    **Geographic Presence:**
    - Headquarters location
    - Key operating locations in the specified region
    - Global presence and markets served
    
    **Business Model:**
    - Core products and services relevant to the category
    - Revenue streams and business lines
    - Target customer segments
    - Value proposition in the specified category
    
    **Market Position:**
    - Market share in the specified category
    - Competitive ranking in the region
    - Key differentiators relevant to procurement
    - Recent strategic initiatives related to the category
    
    **Context Integration:**
    - How the company relates to the procurement category
    - Presence in the specified region
    - Relevance to the annual spend amount provided
    - Relationship to incumbent suppliers (if any)
    
    Use web search to find current, accurate information. Present findings in a clear, structured format.
    Extract and reference the specific companies, category, region, and other details from the input data.
    """,
    markdown=True,
)

switching_barriers_agent = Agent(
    name="Switching Barriers Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in supplier switching cost analysis and procurement risk assessment",
    instructions="""
    You are a procurement analyst specializing in supplier switching barriers analysis.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers to compare against
    
    **Analysis Framework:**
    For the specified companies in the given category and region, evaluate switching barriers using a 1-9 scale (1=Low, 9=High) for each factor:
    
    1. **Switching Cost (Financial Barriers)**
       - Setup and onboarding costs specific to the category
       - Training and certification expenses
       - Technology integration costs for the category
       - Contract termination penalties with incumbent suppliers
       - Consider the annual spend amount as context for cost impact
    
    2. **Switching Risk (Operational Risks)**
       - Business continuity risks in the category
       - Quality and performance risks specific to the region
       - Supply chain disruption potential
       - Regulatory compliance risks in the specified region
    
    3. **Switching Timeline (Time Requirements)**
       - Implementation timeline for the category
       - Transition period complexity
       - Parallel running requirements
       - Go-live timeline considerations
    
    4. **Switching Effort (Resource Needs)**
       - Internal resource requirements
       - External consulting needs
       - Management attention required
       - Cross-functional coordination needed
    
    5. **Change Management (Organizational Complexity)**
       - Stakeholder buy-in requirements
       - Process change complexity for the category
       - Cultural alignment challenges
       - Communication needs
    
    **Comparison Scenarios:**
    - Compare target companies against incumbent suppliers
    - Evaluate switching between different target companies
    - Consider regional differences in switching barriers
    - Quantify differences with specific data relative to the annual spend
    
    Extract company names, category, region, spend amount, and incumbent suppliers from the input data.
    Provide detailed explanations with quantitative data where possible.
    """,
    markdown=True,
)

pestle_agent = Agent(
    name="PESTLE Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in PESTLE analysis for procurement and supply chain strategy",
    instructions="""
    You are a strategic analyst specializing in PESTLE analysis for procurement.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    
    **Analysis Framework:**
    For the specified companies in the given category and region, evaluate each factor's impact on procurement strategy using a 1-9 scale (1=Low Impact, 9=High Impact):
    
    **Political Factors:**
    - Government regulations and policies affecting the category in the region
    - Trade policies and tariffs relevant to the companies
    - Political stability and government changes in the region
    - International relations and sanctions affecting the companies
    - Government procurement policies for the category
    
    **Economic Factors:**
    - Market growth and economic conditions in the region
    - Currency exchange rates affecting the annual spend
    - Interest rates and access to capital for the companies
    - Economic cycles and recession risks
    - Commodity price volatility affecting the category
    
    **Social Factors:**
    - Consumer trends and preferences affecting the category
    - Demographics and workforce changes in the region
    - Cultural shifts and values relevant to the companies
    - Social responsibility expectations
    - Skills availability and labor costs in the region
    
    **Technological Factors:**
    - Innovation and R&D developments in the category
    - Automation and digitalization affecting the companies
    - Cybersecurity and data protection requirements
    - Technology adoption rates in the region
    - Platform and infrastructure changes
    
    **Environmental Factors:**
    - Climate change and environmental regulations in the region
    - Sustainability and ESG requirements for the category
    - Resource scarcity and circular economy impacts
    - Carbon footprint and emissions considerations
    - Environmental compliance costs
    
    **Legal Factors:**
    - Regulatory compliance requirements in the region
    - Labor laws and employment regulations
    - Intellectual property protection for the category
    - Data privacy and security laws
    - Contract and liability frameworks
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Focus on category-specific implications for procurement strategy and provide actionable insights.
    """,
    markdown=True,
)

porter_agent = Agent(
    name="Porter's Five Forces Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in Porter's Five Forces analysis for procurement and competitive strategy",
    instructions="""
    You are a strategic analyst specializing in Porter's Five Forces analysis for procurement.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for market context
    
    **Analysis Framework:**
    For the specified companies in the given category and region, evaluate each force's strength using a 1-9 scale (1=Weak Force, 9=Strong Force):
    
    **1. Competitive Rivalry (Industry Competition)**
    - Number of competitors in the category within the region
    - Industry growth rate and market maturity for the category
    - Product differentiation among the companies
    - Switching costs between the companies and incumbents
    - Competitive intensity and price wars in the category
    
    **2. Supplier Power (Bargaining Power of Suppliers)**
    - Supplier concentration in the category
    - Alternatives to the incumbent suppliers
    - Switching costs from incumbents to target companies
    - Input importance and differentiation in the category
    - Supplier profitability and margins
    
    **3. Buyer Power (Bargaining Power of Buyers)**
    - Buyer concentration considering the annual spend amount
    - Price sensitivity in the category
    - Switching costs for buyers in the region
    - Backward integration potential
    - Information availability and transparency
    
    **4. Threat of Substitutes**
    - Substitute products/services available in the category
    - Relative performance and features compared to incumbents
    - Switching costs to substitutes
    - Buyer propensity to substitute in the region
    - Price-performance trade-offs
    
    **5. Threat of New Entrants**
    - Capital requirements and barriers to entry in the category
    - Economies of scale and learning curves
    - Brand loyalty and customer switching costs
    - Regulatory barriers in the region
    - Access to distribution channels
    
    **Procurement Implications:**
    - Analyze how each force affects procurement leverage given the annual spend
    - Identify opportunities for strategic advantage with target companies
    - Recommend negotiation strategies
    - Assess long-term market dynamics in the region
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Include market data and quantitative analysis where possible.
    """,
    markdown=True,
)

kraljic_agent = Agent(
    name="Kraljic Matrix Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in Kraljic Matrix analysis for procurement portfolio management",
    instructions="""
    You are a procurement strategist specializing in Kraljic Matrix analysis.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    
    **Analysis Framework:**
    For the specified category with the given companies and region, evaluate on two dimensions using a 1-9 scale:
    
    **Supply Risk Assessment (1=Low Risk, 9=High Risk):**
    - Supplier base concentration (including incumbents vs. target companies)
    - Switching costs and barriers in the category
    - Supply market stability in the region
    - Supplier financial stability of target companies
    - Geopolitical and regulatory risks in the region
    - Technology and innovation risks for the category
    
    **Profit Impact Assessment (1=Low Impact, 9=High Impact):**
    - Percentage of total procurement spend (use annual spend amount)
    - Operational criticality of the category
    - Quality and performance requirements
    - Value creation and cost reduction potential
    - Strategic importance to business success
    
    **Matrix Positioning:**
    Based on the analysis, position the category in one of four quadrants:
    - **Routine (Low Risk + Low Impact)**: Standardize and automate
    - **Bottleneck (High Risk + Low Impact)**: Secure supply and minimize risk
    - **Leverage (Low Risk + High Impact)**: Maximize value through competition
    - **Strategic (High Risk + High Impact)**: Develop partnerships and innovation
    
    **Strategic Recommendations:**
    For each quadrant, provide specific recommendations considering:
    - Sourcing strategies for target companies vs. incumbents
    - Contract structures and terms appropriate for the annual spend
    - Risk mitigation approaches for the region
    - Performance measurement and monitoring
    - Organizational capabilities required
    
    **Company-Specific Analysis:**
    - Evaluate how each target company fits the category positioning
    - Compare target companies against incumbent suppliers
    - Consider regional variations in supply risk
    - Assess impact on the annual spend amount
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Use quantitative data and industry benchmarks where available.
    """,
    markdown=True,
)

cost_drivers_agent = Agent(
    name="Cost Drivers Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=2), ReasoningTools()],
    role="Expert in cost structure analysis and procurement cost optimization",
    instructions="""
    You are a procurement analyst specializing in cost structure analysis and cost driver identification.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for cost comparison
    
    **Analysis Framework:**
    For the specified companies in the given category and region, break down and analyze cost components with volatility assessment (1-9 scale):
    
    **Major Cost Components:**
    - Raw materials and commodities specific to the category (% of total cost)
    - Direct labor costs and wage trends in the region
    - Manufacturing and production costs for the category
    - Technology and equipment costs
    - Energy and utility costs in the region
    - Transportation and logistics costs
    - Regulatory and compliance costs
    - Overhead and administrative costs
    
    **Volatility Assessment (1=Stable, 9=Highly Volatile):**
    For each cost component, evaluate:
    - Historical price volatility and trends in the region
    - Market dynamics and supply/demand factors for the category
    - Seasonal and cyclical patterns
    - External economic factors affecting the region
    - Geopolitical influences on the category
    
    **Cost Driver Analysis:**
    - Identify primary and secondary cost drivers for the category
    - Quantify cost elasticity and sensitivity
    - Analyze cost behavior (fixed vs variable) relative to annual spend
    - Benchmark target companies against incumbent suppliers
    - Identify cost optimization opportunities
    
    **Market Intelligence:**
    - Total addressable market size for the category in the region
    - Market growth rates and trends
    - Competitive landscape and pricing among target companies
    - Technology disruption impacts on the category
    - Future cost projections considering regional factors
    
    **Company-Specific Cost Analysis:**
    - Compare cost structures between target companies and incumbents
    - Analyze regional cost variations
    - Assess impact on the annual spend amount
    - Identify cost advantages of target companies
    
    **Actionable Insights:**
    - Cost reduction opportunities with target companies
    - Value engineering possibilities for the category
    - Supplier negotiation leverage points
    - Risk mitigation strategies for cost volatility
    - Alternative sourcing options in the region
    
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Provide quantitative data and specific percentages where possible.
    """,
    markdown=True,
)

alternative_suppliers_agent = Agent(
    name="Alternative Suppliers Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=3)],
    role="Expert in supplier identification and supplier market research",
    instructions="""
    You are a procurement researcher specializing in supplier identification and market analysis.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies to analyze as potential suppliers
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    
    **Research Objectives:**
    Identify and evaluate the target companies as alternative suppliers, plus additional suppliers that can provide competitive options for the specified category in the given region.
    
    **Supplier Evaluation Framework:**
    For each target company and additional suppliers identified, provide:
    
    **Company Information:**
    - Company name and website
    - Headquarters location and presence in the specified region
    - Company size (revenue, employees)
    - Ownership structure (public/private)
    - Years in business and track record in the category
    
    **Technical Capabilities:**
    - Core products and services relevant to the category
    - Technical specifications and standards
    - Quality certifications and accreditations
    - Manufacturing capabilities and capacity for the category
    - Innovation and R&D capabilities
    
    **Market Presence:**
    - Geographic coverage in the specified region
    - Customer base and key accounts
    - Market share in the category
    - Distribution channels and partnerships
    
    **Financial Stability:**
    - Financial health indicators
    - Revenue growth and profitability
    - Credit ratings and financial stability
    - Investment and expansion plans in the region
    
    **Competitive Advantages:**
    - Key differentiators compared to incumbent suppliers
    - Pricing competitiveness for the annual spend level
    - Service levels and support in the region
    - Sustainability and ESG credentials
    - Technology and digital capabilities
    
    **Suitability Assessment:**
    - Capacity to handle the annual spend volume
    - Geographic alignment with regional requirements
    - Cultural and strategic fit
    - Risk assessment compared to incumbents
    
    **Comparison Analysis:**
    - Compare target companies against incumbent suppliers
    - Identify advantages and disadvantages
    - Assess fit for the category requirements
    - Evaluate regional presence and capabilities
    
    **Target:** Focus on the specified companies first, then identify 5-10 additional strong alternative suppliers with comprehensive profiles.
    Extract and reference the specific companies, category, region, annual spend, and incumbent suppliers from the input data.
    Focus on suppliers that can realistically serve the specified requirements.
    """,
    markdown=True,
)

report_compiler_agent = Agent(
    name="Report Compiler Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Expert in business report compilation and strategic recommendations",
    instructions="""
    You are a senior business analyst specializing in procurement strategy reports.
    
    **Input Data Usage:**
    You will receive structured input data containing:
    - companies: Target companies that were analyzed
    - category_name: The procurement category being analyzed
    - region: Regional context for the analysis
    - annual_spend: Annual procurement spend amount
    - incumbent_suppliers: Current suppliers for comparison
    - analyses_requested: List of analyses that were performed
    
    **Report Structure:**
    Create comprehensive, executive-ready reports with:
    
    **Executive Summary:**
    - Overview of the procurement category and regional context
    - Key findings for the target companies
    - Strategic recommendations overview
    - Critical success factors
    - Risk and opportunity highlights relative to annual spend
    
    **Analysis Summary:**
    - Summarize findings from each requested analysis type
    - Integrate insights across all analyses performed
    - Compare target companies against incumbent suppliers
    - Highlight regional considerations
    
    **Strategic Recommendations:**
    - Prioritized action items specific to the companies and category
    - Implementation roadmap considering regional factors
    - Resource requirements relative to annual spend
    - Expected outcomes and benefits
    
    **Key Insights Integration:**
    - Synthesize findings across all analyses
    - Identify patterns and connections between target companies
    - Highlight contradictions or conflicts
    - Provide balanced perspective on incumbents vs. alternatives
    
    **Company-Specific Recommendations:**
    - Specific recommendations for each target company
    - Comparison with incumbent suppliers
    - Regional implementation considerations
    - Cost-benefit analysis relative to annual spend
    
    **Next Steps:**
    - Immediate actions required for the category
    - Medium-term strategic initiatives
    - Long-term capability building in the region
    - Success metrics and KPIs
    
    **Formatting Standards:**
    - Clear, professional presentation
    - Logical flow and structure
    - Visual elements where appropriate
    - Actionable recommendations
    - Executive-friendly language
    
    Extract and reference the specific companies, category, region, annual spend, incumbent suppliers, and analyses performed from the input data.
    Focus on practical insights that procurement leaders can implement.
    """,
    markdown=True,
)
```

---

<a name="examples--workflows--company_analysis--modelspy"></a>

### `examples/workflows/company_analysis/models.py`

```python
from typing import List, Optional

from pydantic import BaseModel, Field


class ProcurementAnalysisRequest(BaseModel):
    companies: List[str] = Field(
        ..., min_length=1, max_length=5, description="List of 1-5 companies to analyze"
    )
    category_name: str = Field(
        ..., min_length=1, description="Category name for analysis"
    )
    analyses_requested: List[str] = Field(
        ..., min_length=1, description="List of analysis types to perform"
    )
    buyer_org_url: Optional[str] = Field(
        default=None, description="Buyer organization URL for context"
    )
    annual_spend: Optional[float] = Field(
        default=None, description="Annual spend amount for context"
    )
    region: Optional[str] = Field(default=None, description="Regional context")
    incumbent_suppliers: List[str] = Field(
        default_factory=list, description="Current/incumbent suppliers"
    )


class ProcurementAnalysisResponse(BaseModel):
    request: ProcurementAnalysisRequest
    company_overview: Optional[str] = None
    switching_barriers_analysis: Optional[str] = None
    pestle_analysis: Optional[str] = None
    porter_analysis: Optional[str] = None
    kraljic_analysis: Optional[str] = None
    cost_drivers_analysis: Optional[str] = None
    alternative_suppliers_analysis: Optional[str] = None
    final_report: Optional[str] = None
    success: bool = True
    error_message: Optional[str] = None


class AnalysisConfig(BaseModel):
    analysis_type: str = Field(..., description="Type of analysis to perform")
    max_companies: int = Field(
        default=5, description="Maximum number of companies to analyze"
    )
    include_market_data: bool = Field(
        default=True, description="Whether to include market data in analysis"
    )
    include_financial_data: bool = Field(
        default=True, description="Whether to include financial data in analysis"
    )


class CompanyProfile(BaseModel):
    name: str = Field(..., description="Company name")
    legal_name: Optional[str] = Field(default=None, description="Full legal name")
    industry: Optional[str] = Field(default=None, description="Industry/sector")
    founded_year: Optional[int] = Field(default=None, description="Year founded")
    headquarters: Optional[str] = Field(
        default=None, description="Headquarters location"
    )
    annual_revenue: Optional[float] = Field(
        default=None, description="Annual revenue in USD"
    )
    employee_count: Optional[int] = Field(
        default=None, description="Number of employees"
    )
    market_cap: Optional[float] = Field(
        default=None, description="Market capitalization in USD"
    )
    website: Optional[str] = Field(default=None, description="Company website")
    description: Optional[str] = Field(default=None, description="Company description")


class SupplierProfile(BaseModel):
    name: str = Field(..., description="Supplier name")
    website: Optional[str] = Field(default=None, description="Supplier website")
    headquarters: Optional[str] = Field(
        default=None, description="Headquarters location"
    )
    geographic_coverage: List[str] = Field(
        default_factory=list, description="Geographic coverage areas"
    )
    technical_capabilities: List[str] = Field(
        default_factory=list, description="Technical capabilities"
    )
    certifications: List[str] = Field(
        default_factory=list, description="Quality certifications"
    )
    annual_revenue: Optional[float] = Field(
        default=None, description="Annual revenue in USD"
    )
    employee_count: Optional[int] = Field(
        default=None, description="Number of employees"
    )
    key_differentiators: List[str] = Field(
        default_factory=list, description="Key competitive advantages"
    )
    financial_stability_score: Optional[int] = Field(
        default=None, ge=1, le=10, description="Financial stability score (1-10)"
    )
    suitability_score: Optional[int] = Field(
        default=None,
        ge=1,
        le=10,
        description="Suitability score for requirements (1-10)",
    )


class AnalysisResult(BaseModel):
    analysis_type: str = Field(..., description="Type of analysis performed")
    company_name: str = Field(..., description="Company analyzed")
    category_name: str = Field(..., description="Category analyzed")
    score: Optional[int] = Field(
        default=None, ge=1, le=9, description="Overall score (1-9 scale)"
    )
    summary: Optional[str] = Field(default=None, description="Analysis summary")
    detailed_findings: Optional[str] = Field(
        default=None, description="Detailed analysis findings"
    )
    recommendations: List[str] = Field(
        default_factory=list, description="Key recommendations"
    )
    risk_factors: List[str] = Field(
        default_factory=list, description="Identified risk factors"
    )
    success: bool = Field(
        default=True, description="Whether analysis completed successfully"
    )
    error_message: Optional[str] = Field(
        default=None, description="Error message if analysis failed"
    )
```

---

<a name="examples--workflows--company_analysis--run_workflowpy"></a>

### `examples/workflows/company_analysis/run_workflow.py`

```python
from agents import (
    alternative_suppliers_agent,
    company_overview_agent,
    cost_drivers_agent,
    kraljic_agent,
    pestle_agent,
    porter_agent,
    report_compiler_agent,
    switching_barriers_agent,
)
from agno.workflow import Condition, Parallel, Step, Workflow
from agno.workflow.types import StepInput
from models import ProcurementAnalysisRequest


def should_run_analysis(analysis_type: str) -> callable:
    def evaluator(step_input: StepInput) -> bool:
        request_data = step_input.input
        if isinstance(request_data, ProcurementAnalysisRequest):
            return analysis_type in request_data.analyses_requested
        return False

    return evaluator


company_overview_step = Step(
    name="Company Overview",
    agent=company_overview_agent,
    description="Research and analyze the target company",
)

switching_barriers_step = Step(
    name="Switching Barriers Analysis",
    agent=switching_barriers_agent,
    description="Analyze supplier switching barriers and costs",
)

pestle_step = Step(
    name="PESTLE Analysis",
    agent=pestle_agent,
    description="Conduct PESTLE analysis for procurement strategy",
)

porter_step = Step(
    name="Porter's Five Forces Analysis",
    agent=porter_agent,
    description="Analyze competitive forces in the supply market",
)

kraljic_step = Step(
    name="Kraljic Matrix Analysis",
    agent=kraljic_agent,
    description="Position category on Kraljic Matrix",
)

cost_drivers_step = Step(
    name="Cost Drivers Analysis",
    agent=cost_drivers_agent,
    description="Analyze cost structure and volatility",
)

alternative_suppliers_step = Step(
    name="Alternative Suppliers Research",
    agent=alternative_suppliers_agent,
    description="Identify and evaluate alternative suppliers",
)

report_compilation_step = Step(
    name="Report Compilation",
    agent=report_compiler_agent,
    description="Compile comprehensive procurement analysis report",
)

procurement_workflow = Workflow(
    name="Procurement Analysis Workflow",
    description="Comprehensive procurement intelligence using multiple strategic frameworks",
    steps=[
        company_overview_step,
        Parallel(
            Condition(
                evaluator=should_run_analysis("switching_barriers"),
                steps=[switching_barriers_step],
                name="Switching Barriers Condition",
            ),
            Condition(
                evaluator=should_run_analysis("pestle"),
                steps=[pestle_step],
                name="PESTLE Condition",
            ),
            Condition(
                evaluator=should_run_analysis("porter"),
                steps=[porter_step],
                name="Porter's Five Forces Condition",
            ),
            Condition(
                evaluator=should_run_analysis("kraljic"),
                steps=[kraljic_step],
                name="Kraljic Matrix Condition",
            ),
            Condition(
                evaluator=should_run_analysis("cost_drivers"),
                steps=[cost_drivers_step],
                name="Cost Drivers Condition",
            ),
            Condition(
                evaluator=should_run_analysis("alternative_suppliers"),
                steps=[alternative_suppliers_step],
                name="Alternative Suppliers Condition",
            ),
            name="Analysis Phase",
        ),
        report_compilation_step,
    ],
)

if __name__ == "__main__":
    analysis_details = ProcurementAnalysisRequest(
        companies=["Tesla", "Ford"],
        category_name="Electric Vehicle Components",
        analyses_requested=[
            "switching_barriers",
            "pestle",
            "porter",
            "kraljic",
            "cost_drivers",
            "alternative_suppliers",
        ],
        region="Global",
        annual_spend=50_000_000,
        incumbent_suppliers=["CATL", "Panasonic", "LG Energy Solution"],
    )
    procurement_workflow.print_response(
        input=analysis_details,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="examples--workflows--company_description--agentspy"></a>

### `examples/workflows/company_description/agents.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.firecrawl import FirecrawlTools
from agno.tools.wikipedia import WikipediaTools
from prompts import (
    COMPETITOR_INSTRUCTIONS,
    CRAWLER_INSTRUCTIONS,
    SEARCH_INSTRUCTIONS,
    SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL,
    WIKIPEDIA_INSTRUCTIONS,
)
from pydantic import BaseModel


class SupplierProfile(BaseModel):
    supplier_name: str
    supplier_homepage_url: str
    user_email: str


crawl_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[FirecrawlTools(crawl=True, limit=5)],
    instructions=CRAWLER_INSTRUCTIONS,
)

search_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=SEARCH_INSTRUCTIONS,
)

wikipedia_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[WikipediaTools()],
    instructions=WIKIPEDIA_INSTRUCTIONS,
)

competitor_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=COMPETITOR_INSTRUCTIONS,
)

profile_agent: Agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL,
)
```

---

<a name="examples--workflows--company_description--promptspy"></a>

### `examples/workflows/company_description/prompts.py`

```python
CRAWLER_INSTRUCTIONS = """
Your task is to crawl a website starting from the provided homepage URL. Follow these guidelines:

1. Initial Access: Begin by accessing the homepage URL.
2. Comprehensive Crawling: Recursively traverse the website to capture every accessible page and resource.
3. Data Extraction: Extract all available content, including text, images, metadata, and embedded resources, while preserving the original structure and context.
4. Detailed Reporting: Provide an extremely detailed and comprehensive response, including all extracted content without filtering or omissions.
5. Data Integrity: Ensure that the extracted content accurately reflects the website without any modifications.
"""

SEARCH_INSTRUCTIONS = """
You are tasked with searching the web for information about a supplier. Follow these guidelines:

1. Input: You will be provided with the name of the supplier.
2. Web Search: Perform comprehensive web searches to gather information about the supplier.
3. Latest News: Search for the most recent news and updates regarding the supplier.
4. Information Extraction: From the search results, extract all relevant details about the supplier.
5. Detailed Reporting: Provide an extremely verbose and detailed report that includes all relevant information without filtering or omissions.
"""

WIKIPEDIA_INSTRUCTIONS = """
You are tasked with searching Wikipedia for information about a supplier. Follow these guidelines:

1. Input: You will be provided with the name of the supplier.
2. Wikipedia Search: Use Wikipedia to find comprehensive information about the supplier.
3. Data Extraction: Extract all relevant details available on the supplier, including history, operations, products, and any other pertinent information.
4. Detailed Reporting: Provide an extremely verbose and detailed report that includes all extracted content without filtering or omissions.
"""

COMPETITOR_INSTRUCTIONS = """
You are tasked with finding competitors of a supplier. Follow these guidelines:

1. Input: You will be provided with the name of the supplier.
2. Competitor Search: Search the web for competitors of the supplier.
3. Data Extraction: Extract all relevant details about the competitors.
4. Detailed Reporting: Provide an extremely verbose and detailed report that includes all extracted content without filtering or omissions.
"""

SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL = """
You are a supplier profile agent. You are given a supplier name, results from the supplier homepage and search results regarding the supplier, and Wikipedia results regarding the supplier. You need to be extremely verbose in your response. Do not filter out any content.

You are tasked with generating a segment of a supplier profile. The segment will be provided to you. Make sure to format it in markdown.

General format:

Title: [Title of the segment]

[Segment]

Formatting Guidelines:
1. Ensure the profile is structured, clear, and to the point.
2. Avoid assumptionsonly include verified details.
3. Use bullet points and short paragraphs for readability.
4. Cite sources where applicable for credibility.

Objective: This supplier profile should serve as a reliable reference document for businesses evaluating potential suppliers. The details should be extracted from official sources, search results, and any other reputable databases. The profile must provide an in-depth understanding of the supplier's operational, competitive, and financial position to support informed decision-making.

"""

SUPPLIER_PROFILE_DICT = {
    "1. Supplier Overview": """Company Name: [Supplier Name]
Industry: [Industry the supplier operates in]
Headquarters: [City, Country]
Year Founded: [Year]
Key Offerings: [Brief summary of main products or services]
Business Model: [Manufacturing, Wholesale, B2B/B2C, etc.]
Notable Clients & Partnerships: [List known customers or business partners]
Company Mission & Vision: [Summary of supplier's goals and commitments]""",
    #     "2. Website Content Summary": """Extract key details from the supplier's official website:
    # Website URL: [Supplier's official website link]
    # Products & Services Overview:
    #   - [List major product categories or services]
    #   - [Highlight any specialized offerings]
    # Certifications & Compliance: (e.g., ISO, FDA, CE, etc.)
    # Manufacturing & Supply Chain Information:
    #   - Factory locations, supply chain transparency, etc.
    # Sustainability & Corporate Social Responsibility (CSR):
    #   - Environmental impact, ethical sourcing, fair labor practices
    # Customer Support & After-Sales Services:
    #   - Warranty, return policies, support channels""",
    #     "3. Search Engine Insights": """Summarize search results to provide additional context on the supplier's market standing:
    # Latest News & Updates: [Any recent developments, funding rounds, expansions]
    # Industry Mentions: [Publications, blogs, or analyst reviews mentioning the supplier]
    # Regulatory Issues or Legal Disputes: [Any lawsuits, recalls, or compliance issues]
    # Competitive Positioning: [How the supplier compares to competitors in the market]""",
    #     "4. Key Contact Information": """Include publicly available contact details for business inquiries:
    # Email: [Customer support, sales, or partnership email]
    # Phone Number: [+XX-XXX-XXX-XXXX]
    # Office Address: [Headquarters or regional office locations]
    # LinkedIn Profile: [Supplier's LinkedIn page]
    # Other Business Directories: [Crunchbase, Alibaba, etc.]""",
    #     "5. Reputation & Reviews": """Analyze customer and partner feedback from multiple sources:
    # Customer Reviews & Testimonials: [Summarized from Trustpilot, Google Reviews, etc.]
    # Third-Party Ratings: [Any industry-recognized rankings or awards]
    # Complaints & Risks: [Potential risks, delays, quality issues, or fraud warnings]
    # Social Media Presence & Engagement: [Activity on LinkedIn, Twitter, etc.]""",
    #     "6. Additional Insights": """Pricing Model: [Wholesale, subscription, per-unit pricing, etc.]
    # MOQ (Minimum Order Quantity): [If applicable]
    # Return & Refund Policies: [Key policies for buyers]
    # Logistics & Shipping: [Lead times, global shipping capabilities]""",
    #     "7. Supplier Insight": """Provide a deep-dive analysis into the supplier's market positioning and business strategy:
    # Market Trends: [How current market trends impact the supplier]
    # Strategic Advantages: [Unique selling points or competitive edge]
    # Challenges & Risks: [Any operational or market-related challenges]
    # Future Outlook: [Predicted growth or strategic initiatives]""",
    #     "8. Supplier Profiles": """Create a comparative profile if multiple suppliers are being evaluated:
    # Comparative Metrics: [Key differentiators among suppliers]
    # Strengths & Weaknesses: [Side-by-side comparison details]
    # Strategic Fit: [How each supplier aligns with potential buyer needs]""",
    #     "9. Product Portfolio": """Detail the range and depth of the supplier's offerings:
    # Major Product Lines: [Detailed listing of core products or services]
    # Innovations & Specialized Solutions: [Highlight any innovative products or custom solutions]
    # Market Segments: [Industries or consumer segments served by the products]""",
    #     "10. Competitive Intelligence": """Summarize the supplier's competitive landscape:
    # Industry Competitors: [List of main competitors]
    # Market Share: [If available, indicate the supplier's market share]
    # Competitive Strategies: [Pricing, marketing, distribution, etc.]
    # Recent Competitor Moves: [Any recent competitive actions impacting the market]""",
    #     "11. Supplier Quadrant": """Position the supplier within a competitive quadrant analysis:
    # Quadrant Position: [Leader, Challenger, Niche Player, or Visionary]
    # Analysis Criteria: [Innovativeness, operational efficiency, market impact, etc.]
    # Visual Representation: [If applicable, describe or include a link to the quadrant chart]""",
    #     "12. SWOT Analysis": """Perform a comprehensive SWOT analysis:
    # Strengths: [Internal capabilities and competitive advantages]
    # Weaknesses: [Areas for improvement or potential vulnerabilities]
    # Opportunities: [External market opportunities or expansion potentials]
    # Threats: [External risks, competitive pressures, or regulatory challenges]""",
    #     "13. Financial Risk Summary": """Evaluate the financial stability and risk factors:
    # Financial Health: [Overview of revenue, profitability, and growth metrics]
    # Risk Factors: [Credit risk, market volatility, or liquidity issues]
    # Investment Attractiveness: [Analysis for potential investors or partners]""",
    #     "14. Financial Information": """Provide detailed financial data (where publicly available):
    # Revenue Figures: [Latest annual revenue, growth trends]
    # Profitability: [Net income, EBITDA, etc.]
    # Funding & Investment: [Details of any funding rounds, investor names]
    # Financial Reports: [Links or summaries of recent financial statements]
    # Credit Ratings: [If available, include credit ratings or financial stability indicators]""",
}
```

---

<a name="examples--workflows--company_description--run_workflowpy"></a>

### `examples/workflows/company_description/run_workflow.py`

```python
import markdown
import resend
from agents import (
    SupplierProfile,
    competitor_agent,
    crawl_agent,
    search_agent,
    wikipedia_agent,
)
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.utils.log import log_error, log_info
from agno.workflow import Parallel, Step, Workflow
from agno.workflow.types import StepInput, StepOutput
from prompts import SUPPLIER_PROFILE_DICT, SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL

crawler_step = Step(
    name="Crawler",
    agent=crawl_agent,
    description="Crawl the supplier homepage for the supplier profile url",
)

search_step = Step(
    name="Search",
    agent=search_agent,
    description="Search for the supplier profile for the supplier name",
)

wikipedia_step = Step(
    name="Wikipedia",
    agent=wikipedia_agent,
    description="Search Wikipedia for the supplier profile for the supplier name",
)

competitor_step = Step(
    name="Competitor",
    agent=competitor_agent,
    description="Find competitors of the supplier name",
)


def generate_supplier_profile(step_input: StepInput) -> StepOutput:
    supplier_profile: SupplierProfile = step_input.input

    supplier_name: str = supplier_profile.supplier_name
    supplier_homepage_url: str = supplier_profile.supplier_homepage_url

    crawler_data: str = step_input.get_step_content("Gathering Information")["Crawler"]
    search_data: str = step_input.get_step_content("Gathering Information")["Search"]
    wikipedia_data: str = step_input.get_step_content("Gathering Information")[
        "Wikipedia"
    ]
    competitor_data: str = step_input.get_step_content("Gathering Information")[
        "Competitor"
    ]

    log_info(f"Crawler data: {crawler_data}")
    log_info(f"Search data: {search_data}")
    log_info(f"Wikipedia data: {wikipedia_data}")
    log_info(f"Competitor data: {competitor_data}")

    supplier_profile_prompt: str = f"Generate the supplier profile for the supplier name {supplier_name} and the supplier homepage url is {supplier_homepage_url}. The supplier homepage is {crawler_data} and the search results are {search_data} and the wikipedia results are {wikipedia_data} and the competitor results are {competitor_data}"

    supplier_profile_response: str = ""
    html_content: str = ""
    for key, value in SUPPLIER_PROFILE_DICT.items():
        agent = Agent(
            model=OpenAIChat(id="o3-mini"),
            instructions="Instructions: "
            + SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL
            + "Format to adhere to: "
            + value,
        )
        response: RunOutput = agent.run(
            "Write the response in markdown format for the title: "
            + key
            + " using the following information: "
            + supplier_profile_prompt
        )
        if response.content:
            html_content += markdown.markdown(response.content)
            supplier_profile_response += response.content

    log_info(f"Generated supplier profile for {html_content}")

    return StepOutput(
        content=html_content,
        success=True,
    )


generate_supplier_profile_step = Step(
    name="Generate Supplier Profile",
    executor=generate_supplier_profile,
    description="Generate the supplier profile for the supplier name",
)


def send_email(step_input: StepInput):
    supplier_profile: SupplierProfile = step_input.input
    supplier_name: str = supplier_profile.supplier_name
    user_email: str = supplier_profile.user_email

    html_content: str = step_input.get_step_content("Generate Supplier Profile")

    try:
        resend.Emails.send(
            {
                "from": "support@agno.com",
                "to": user_email,
                "subject": f"Supplier Profile for {supplier_name}",
                "html": html_content,
            }
        )
    except Exception as e:
        log_error(f"Error sending email: {e}")

    return StepOutput(
        content="Email sent successfully",
        success=True,
    )


send_email_step = Step(
    name="Send Email",
    executor=send_email,
    description="Send the email to the user",
)

company_description_workflow = Workflow(
    name="Company Description Workflow",
    description="A workflow to generate a company description for a supplier",
    steps=[
        Parallel(
            crawler_step,
            search_step,
            wikipedia_step,
            competitor_step,
            name="Gathering Information",
        ),
        generate_supplier_profile_step,
        send_email_step,
    ],
)

if __name__ == "__main__":
    supplier_profile_request = SupplierProfile(
        supplier_name="Agno",
        supplier_homepage_url="https://www.agno.com",
        user_email="yash@agno.com",
    )
    company_description_workflow.print_response(
        input=supplier_profile_request,
    )
```

---

<a name="examples--workflows--customer_support--agentspy"></a>

### `examples/workflows/customer_support/agents.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

triage_agent = Agent(
    name="Ticket Classifier",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""
    You are a customer support ticket classifier. Your job is to analyze customer queries and extract key information.
    
    For each customer query, provide:
    1. Category (billing, technical, account_access, product_info, bug_report, feature_request)
    2. Priority (low, medium, high, urgent)
    3. Key tags/keywords (extract 3-5 relevant terms)
    4. Brief summary of the issue
    
    Format your response as:
    Category: [category]
    Priority: [priority] 
    Tags: [tag1, tag2, tag3]
    Summary: [brief summary]
    """,
    markdown=True,
)

support_agent = Agent(
    name="Solution Developer",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="""
    You are a solution developer for customer support. Your job is to create clear, 
    step-by-step solutions for customer issues.
    
    Based on research and knowledge base information, create:
    1. Clear problem diagnosis
    2. Step-by-step solution instructions
    3. Alternative approaches if the main solution fails
    4. Prevention tips for the future
    
    Make solutions customer-friendly with numbered steps and clear language.
    Include any relevant screenshots, links, or additional resources.
    """,
    markdown=True,
)
```

---

<a name="examples--workflows--customer_support--run_workflowpy"></a>

### `examples/workflows/customer_support/run_workflow.py`

```python
from agents import (
    support_agent,
    triage_agent,
)
from agno.utils.log import log_info
from agno.workflow import Workflow


def cache_solution(session_state, query: str, solution: str):
    if "solutions" not in session_state:
        session_state["solutions"] = {}
    session_state["solutions"][query] = solution


def customer_support_execution(session_state, query: str) -> str:
    cached_solution = session_state.get("solutions", {}).get(query)
    if cached_solution:
        log_info(f"Cache hit! Returning cached solution for query: {query}")
        return cached_solution

    log_info(f"No cached solution found for query: {query}")

    classification_response = triage_agent.run(query)
    classification = classification_response.content

    solution_context = f"""
    Customer Query: {query}

    Classification: {classification}

    Please provide a clear, step-by-step solution for this customer issue.
    Make sure to format it in a customer-friendly way with clear instructions.
    """

    solution_response = support_agent.run(solution_context)
    solution = solution_response.content

    cache_solution(session_state, query, solution)

    return solution


# Create the customer support workflow
customer_support_workflow = Workflow(
    name="Customer Support Resolution Pipeline",
    description="AI-powered customer support with intelligent caching",
    steps=customer_support_execution,
    session_state={},  # Initialize empty session state
)


if __name__ == "__main__":
    test_queries = [
        "I can't log into my account, forgot my password",
        "How do I reset my password?",
        "My billing seems wrong, I was charged twice",
        "The app keeps crashing when I upload files",
        "I can't log into my account, forgot my password",  # repeat query
    ]

    for i, query in enumerate(test_queries, 1):
        response = customer_support_workflow.run(query=query)
```

---

<a name="examples--workflows--employee_recruiterpy"></a>

### `examples/workflows/employee_recruiter.py`

```python
"""
This workflow is a simple example of a recruitment workflow where in you can also pass custom prameters like job description,
candidate resume urls, etc. (**kwargs) in the workflow along with workflow execution input.
"""

import io
import random
from datetime import datetime, timedelta
from typing import Any, List

import requests
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel
from pypdf import PdfReader


# --- Response models ---
class ScreeningResult(BaseModel):
    name: str
    email: str
    score: float
    feedback: str


class ScheduledCall(BaseModel):
    name: str
    email: str
    call_time: str
    url: str


class EmailContent(BaseModel):
    subject: str
    body: str


# --- PDF utility ---
def extract_text_from_pdf(url: str) -> str:
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        reader = PdfReader(io.BytesIO(resp.content))
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        print(f"Error extracting PDF from {url}: {e}")
        return ""


# --- Simulation tools ---
def simulate_zoom_scheduling(
    agent: Agent, candidate_name: str, candidate_email: str
) -> str:
    """Simulate Zoom call scheduling"""
    # Generate a future time slot (1-7 days from now, between 10am-6pm IST)
    base_time = datetime.now() + timedelta(days=random.randint(1, 7))
    hour = random.randint(10, 17)  # 10am to 5pm
    scheduled_time = base_time.replace(hour=hour, minute=0, second=0, microsecond=0)

    # Generate fake Zoom URL
    meeting_id = random.randint(100000000, 999999999)
    zoom_url = f"https://zoom.us/j/{meeting_id}"

    result = " Zoom call scheduled successfully!\n"
    result += f" Time: {scheduled_time.strftime('%Y-%m-%d %H:%M')} IST\n"
    result += f" Meeting URL: {zoom_url}\n"
    result += f" Participant: {candidate_name} ({candidate_email})"

    return result


def simulate_email_sending(agent: Agent, to_email: str, subject: str, body: str) -> str:
    """Simulate email sending"""
    result = " Email sent successfully!\n"
    result += f" To: {to_email}\n"
    result += f" Subject: {subject}\n"
    result += f" Body length: {len(body)} characters\n"
    result += f" Sent at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

    return result


# --- Agents ---
screening_agent = Agent(
    name="Screening Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Screen candidate given resume text and job description.",
        "Provide a score from 0-10 based on how well they match the job requirements.",
        "Give specific feedback on strengths and areas of concern.",
        "Extract the candidate's name and email from the resume if available.",
    ],
    output_schema=ScreeningResult,
)

scheduler_agent = Agent(
    name="Scheduler Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        f"You are scheduling interview calls. Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} IST",
        "Schedule calls between 10am-6pm IST on weekdays.",
        "Use the simulate_zoom_scheduling tool to create the meeting.",
        "Provide realistic future dates and times.",
    ],
    tools=[simulate_zoom_scheduling],
    output_schema=ScheduledCall,
)

email_writer_agent = Agent(
    name="Email Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Write professional, friendly interview invitation emails.",
        "Include congratulations, interview details, and next steps.",
        "Keep emails concise but warm and welcoming.",
        "Sign emails as 'John Doe, Senior Software Engineer' with email john@agno.com",
    ],
    output_schema=EmailContent,
)

email_sender_agent = Agent(
    name="Email Sender Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You send emails using the simulate_email_sending tool.",
        "Always confirm successful delivery with details.",
    ],
    tools=[simulate_email_sending],
)


# --- Execution function ---
def recruitment_execution(
    session_state,
    execution_input: WorkflowExecutionInput,
    job_description: str,
    **kwargs: Any,
) -> str:
    """Execute the complete recruitment workflow"""

    # Get inputs
    message: str = execution_input.input
    jd: str = job_description
    resumes: List[str] = kwargs.get("candidate_resume_urls", [])

    if not resumes:
        return " No candidate resume URLs provided"

    if not jd:
        return " No job description provided"

    print(f" Starting recruitment process for {len(resumes)} candidates")
    print(f" Job Description: {jd[:100]}{'...' if len(jd) > 100 else ''}")

    selected_candidates: List[ScreeningResult] = []

    # Phase 1: Screening
    print("\n PHASE 1: CANDIDATE SCREENING")
    print("=" * 50)

    for i, url in enumerate(resumes, 1):
        print(f"\n Processing candidate {i}/{len(resumes)}")

        # Extract resume text (with caching)
        if url not in session_state:
            print(f" Extracting text from: {url}")
            session_state[url] = extract_text_from_pdf(url)
        else:
            print(" Using cached resume content")

        resume_text = session_state[url]

        if not resume_text:
            print(" Could not extract text from resume")
            continue

        # Screen the candidate
        screening_prompt = f"""
        {message}
        Please screen this candidate for the job position.

        RESUME:
        {resume_text}

        JOB DESCRIPTION:
        {jd}

        Evaluate how well this candidate matches the job requirements and provide a score from 0-10.
        """

        result = screening_agent.run(screening_prompt)
        candidate = result.content

        print(f" Candidate: {candidate.name}")
        print(f" Email: {candidate.email}")
        print(f" Score: {candidate.score}/10")
        print(
            f" Feedback: {candidate.feedback[:150]}{'...' if len(candidate.feedback) > 150 else ''}"
        )

        if candidate.score >= 5.0:
            selected_candidates.append(candidate)
            print(" SELECTED for interview!")
        else:
            print(" Not selected (score below 5.0)")

    # Phase 2: Interview Scheduling & Email Communication
    if selected_candidates:
        print("\n PHASE 2: INTERVIEW SCHEDULING")
        print("=" * 50)

        for i, candidate in enumerate(selected_candidates, 1):
            print(
                f"\n Scheduling interview {i}/{len(selected_candidates)} for {candidate.name}"
            )

            # Schedule interview
            schedule_prompt = f"""
            Schedule a 1-hour interview call for:
            - Candidate: {candidate.name}
            - Email: {candidate.email}
            - Interviewer: Dirk Brand (dirk@phidata.com)
            Use the simulate_zoom_scheduling tool to create the meeting.
            """

            call_result = scheduler_agent.run(schedule_prompt)
            scheduled_call = call_result.content

            print(f" Scheduled for: {scheduled_call.call_time}")
            print(f" Meeting URL: {scheduled_call.url}")

            # Write congratulatory email
            email_prompt = f"""
            Write a professional interview invitation email for:
            - Candidate: {candidate.name} ({candidate.email})
            - Interview time: {scheduled_call.call_time}
            - Meeting URL: {scheduled_call.url}
            - Congratulate them on being selected
            - Include next steps and what to expect
            """

            email_result = email_writer_agent.run(email_prompt)
            email_content = email_result.content

            print(f" Email subject: {email_content.subject}")

            # Send email
            send_prompt = f"""
            Send the interview invitation email:
            - To: {candidate.email}
            - Subject: {email_content.subject}
            - Body: {email_content.body}
            Use the simulate_email_sending tool.
            """

            _ = email_sender_agent.run(send_prompt)
            print(f" Email sent to {candidate.email}")

    # Final summary
    summary = f"""
     RECRUITMENT WORKFLOW COMPLETED!
     Summary:
     Processed: {len(resumes)} candidate resumes
     Selected: {len(selected_candidates)} candidates for interviews
     Interviews scheduled: {len(selected_candidates)}
     Emails sent: {len(selected_candidates)}
     Selected candidates:
    """

    for candidate in selected_candidates:
        summary += (
            f"\n    {candidate.name} ({candidate.email}) - Score: {candidate.score}/10"
        )

    return summary


# --- Workflow definition ---
recruitment_workflow = Workflow(
    name="Employee Recruitment Workflow (Simulated)",
    description="Automated candidate screening with simulated scheduling and email",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflows.db",
    ),
    steps=recruitment_execution,
    session_state={},  # Initialize empty workflow session state
)


if __name__ == "__main__":
    # Test with sample data
    print(" Testing Employee Recruitment Workflow with Simulated Tools")
    print("=" * 60)

    result = recruitment_workflow.print_response(
        input="Process candidates for backend engineer position",
        candidate_resume_urls=[
            "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_1.pdf",
            "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_2.pdf",
        ],
        job_description="""
        We are hiring for backend and systems engineers!
        Join our team building the future of agentic software

        Requirements:
         You know your way around Python, typescript, docker, and AWS.
         Love to build in public and contribute to open source.
         Are ok dealing with the pressure of an early-stage startup.
         Want to be a part of the biggest technological shift since the internet.
         Bonus: experience with infrastructure as code.
         Bonus: starred Agno repo.
        """,
    )
```

---

<a name="examples--workflows--employee_recruiter_async_streampy"></a>

### `examples/workflows/employee_recruiter_async_stream.py`

```python
import asyncio
import io
import random
from datetime import datetime, timedelta
from typing import Any, List

import requests
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel
from pypdf import PdfReader


# --- Response models ---
class ScreeningResult(BaseModel):
    name: str
    email: str
    score: float
    feedback: str


class ScheduledCall(BaseModel):
    name: str
    email: str
    call_time: str
    url: str


class EmailContent(BaseModel):
    subject: str
    body: str


# --- PDF utility ---
def extract_text_from_pdf(url: str) -> str:
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        reader = PdfReader(io.BytesIO(resp.content))
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        print(f"Error extracting PDF from {url}: {e}")
        return ""


# --- Simulation tools ---
def simulate_zoom_scheduling(
    agent: Agent, candidate_name: str, candidate_email: str
) -> str:
    """Simulate Zoom call scheduling"""
    # Generate a future time slot (1-7 days from now, between 10am-6pm IST)
    base_time = datetime.now() + timedelta(days=random.randint(1, 7))
    hour = random.randint(10, 17)  # 10am to 5pm
    scheduled_time = base_time.replace(hour=hour, minute=0, second=0, microsecond=0)

    # Generate fake Zoom URL
    meeting_id = random.randint(100000000, 999999999)
    zoom_url = f"https://zoom.us/j/{meeting_id}"

    result = " Zoom call scheduled successfully!\n"
    result += f" Time: {scheduled_time.strftime('%Y-%m-%d %H:%M')} IST\n"
    result += f" Meeting URL: {zoom_url}\n"
    result += f" Participant: {candidate_name} ({candidate_email})"

    return result


def simulate_email_sending(agent: Agent, to_email: str, subject: str, body: str) -> str:
    """Simulate email sending"""
    result = " Email sent successfully!\n"
    result += f" To: {to_email}\n"
    result += f" Subject: {subject}\n"
    result += f" Body length: {len(body)} characters\n"
    result += f" Sent at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

    return result


# --- Agents ---
screening_agent = Agent(
    name="Screening Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Screen candidate given resume text and job description.",
        "Provide a score from 0-10 based on how well they match the job requirements.",
        "Give specific feedback on strengths and areas of concern.",
        "Extract the candidate's name and email from the resume if available.",
    ],
    output_schema=ScreeningResult,
)

scheduler_agent = Agent(
    name="Scheduler Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        f"You are scheduling interview calls. Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} IST",
        "Schedule calls between 10am-6pm IST on weekdays.",
        "Use the simulate_zoom_scheduling tool to create the meeting.",
        "Provide realistic future dates and times.",
    ],
    tools=[simulate_zoom_scheduling],
    output_schema=ScheduledCall,
)

email_writer_agent = Agent(
    name="Email Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Write professional, friendly interview invitation emails.",
        "Include congratulations, interview details, and next steps.",
        "Keep emails concise but warm and welcoming.",
        "Sign emails as 'John Doe, Senior Software Engineer' with email john@agno.com",
    ],
    output_schema=EmailContent,
)

email_sender_agent = Agent(
    name="Email Sender Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You send emails using the simulate_email_sending tool.",
        "Always confirm successful delivery with details.",
    ],
    tools=[simulate_email_sending],
)


# --- Execution function ---
async def recruitment_execution(
    session_state,
    execution_input: WorkflowExecutionInput,
    job_description: str,
    **kwargs: Any,
):
    """Execute the complete recruitment workflow"""

    # Get inputs
    message: str = execution_input.input
    jd: str = job_description
    resumes: List[str] = kwargs.get("candidate_resume_urls", [])

    if not resumes:
        yield " No candidate resume URLs provided"

    if not jd:
        yield " No job description provided"

    print(f" Starting recruitment process for {len(resumes)} candidates")
    print(f" Job Description: {jd[:100]}{'...' if len(jd) > 100 else ''}")

    selected_candidates: List[ScreeningResult] = []

    # Phase 1: Screening
    print("\n PHASE 1: CANDIDATE SCREENING")
    print("=" * 50)

    for i, url in enumerate(resumes, 1):
        print(f"\n Processing candidate {i}/{len(resumes)}")

        # Extract resume text (with caching)
        if url not in session_state:
            print(f" Extracting text from: {url}")
            session_state[url] = extract_text_from_pdf(url)
        else:
            print(" Using cached resume content")

        resume_text = session_state[url]

        if not resume_text:
            print(" Could not extract text from resume")
            continue

        # Screen the candidate
        screening_prompt = f"""
        {message}
        Please screen this candidate for the job position.

        RESUME:
        {resume_text}

        JOB DESCRIPTION:
        {jd}

        Evaluate how well this candidate matches the job requirements and provide a score from 0-10.
        """

        async for response in screening_agent.arun(
            screening_prompt, stream=True, stream_intermediate_steps=True
        ):
            if hasattr(response, "content") and response.content:
                candidate = response.content

        print(f" Candidate: {candidate.name}")
        print(f" Email: {candidate.email}")
        print(f" Score: {candidate.score}/10")
        print(
            f" Feedback: {candidate.feedback[:150]}{'...' if len(candidate.feedback) > 150 else ''}"
        )

        if candidate.score >= 5.0:
            selected_candidates.append(candidate)
            print(" SELECTED for interview!")
        else:
            print(" Not selected (score below 5.0)")

    # Phase 2: Interview Scheduling & Email Communication
    if selected_candidates:
        print("\n PHASE 2: INTERVIEW SCHEDULING")
        print("=" * 50)

        for i, candidate in enumerate(selected_candidates, 1):
            print(
                f"\n Scheduling interview {i}/{len(selected_candidates)} for {candidate.name}"
            )

            # Schedule interview
            schedule_prompt = f"""
            Schedule a 1-hour interview call for:
            - Candidate: {candidate.name}
            - Email: {candidate.email}
            - Interviewer: Dirk Brand (dirk@phidata.com)
            Use the simulate_zoom_scheduling tool to create the meeting.
            """

            async for response in scheduler_agent.arun(
                schedule_prompt, stream=True, stream_intermediate_steps=True
            ):
                if hasattr(response, "content") and response.content:
                    scheduled_call = response.content

            print(f" Scheduled for: {scheduled_call.call_time}")
            print(f" Meeting URL: {scheduled_call.url}")

            # Write congratulatory email
            email_prompt = f"""
            Write a professional interview invitation email for:
            - Candidate: {candidate.name} ({candidate.email})
            - Interview time: {scheduled_call.call_time}
            - Meeting URL: {scheduled_call.url}
            - Congratulate them on being selected
            - Include next steps and what to expect
            """

            async for response in email_writer_agent.arun(
                email_prompt, stream=True, stream_intermediate_steps=True
            ):
                if hasattr(response, "content") and response.content:
                    email_content = response.content

            print(f" Email subject: {email_content.subject}")

            # Send email
            send_prompt = f"""
            Send the interview invitation email:
            - To: {candidate.email}
            - Subject: {email_content.subject}
            - Body: {email_content.body}
            Use the simulate_email_sending tool.
            """

            async for response in email_sender_agent.arun(
                send_prompt, stream=True, stream_intermediate_steps=True
            ):
                yield response


# --- Workflow definition ---
recruitment_workflow = Workflow(
    name="Employee Recruitment Workflow (Simulated)",
    description="Automated candidate screening with simulated scheduling and email",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflows.db",
    ),
    steps=recruitment_execution,
    session_state={},
)


if __name__ == "__main__":
    # Test with sample data
    print(" Testing Employee Recruitment Workflow with Simulated Tools")
    print("=" * 60)

    asyncio.run(
        recruitment_workflow.aprint_response(
            input="Process candidates for backend engineer position",
            candidate_resume_urls=[
                "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_1.pdf",
                "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/filters/cv_2.pdf",
            ],
            job_description="""
        We are hiring for backend and systems engineers!
        Join our team building the future of agentic software

        Requirements:
         You know your way around Python, typescript, docker, and AWS.
         Love to build in public and contribute to open source.
         Are ok dealing with the pressure of an early-stage startup.
         Want to be a part of the biggest technological shift since the internet.
         Bonus: experience with infrastructure as code.
         Bonus: starred Agno repo.
        """,
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="examples--workflows--investment_analyst--agentspy"></a>

### `examples/workflows/investment_analyst/agents.py`

```python
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools


# MCP Tool configuration - Only Supabase
def get_supabase_mcp_tools():
    """Get Supabase MCP tools for database operations"""
    token = os.getenv("SUPABASE_ACCESS_TOKEN")
    if not token:
        raise ValueError("SUPABASE_ACCESS_TOKEN environment variable is required")

    npx_cmd = "npx.cmd" if os.name == "nt" else "npx"
    return MCPTools(
        f"{npx_cmd} -y @supabase/mcp-server-supabase@latest --access-token {token}"
    )


database_setup_agent = Agent(
    name="Database Setup Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools()],
    role="Expert Supabase database architect for investment analysis",
    instructions="""
    You are an expert Supabase MCP architect for investment analysis. Follow these steps precisely:

    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials in your responses.**

    1. **Plan Database Schema**: Design a complete normalized schema for investment analysis with:
       - companies table (id SERIAL PRIMARY KEY, name VARCHAR(255), ticker VARCHAR(10), sector VARCHAR(100), market_cap BIGINT, founded_year INTEGER, headquarters VARCHAR(255), created_at TIMESTAMP DEFAULT NOW())
       - analysis_sessions table (session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(), analysis_date TIMESTAMP DEFAULT NOW(), investment_type VARCHAR(50), investment_amount DECIMAL(15,2), target_return DECIMAL(5,2), risk_tolerance VARCHAR(20))
       - financial_metrics table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), metric_type VARCHAR(100), value DECIMAL(20,4), period VARCHAR(50), currency VARCHAR(10), created_at TIMESTAMP DEFAULT NOW())
       - valuation_models table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), dcf_value DECIMAL(15,2), target_price DECIMAL(15,2), upside_potential DECIMAL(8,4), methodology VARCHAR(100), created_at TIMESTAMP DEFAULT NOW())
       - risk_assessments table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), risk_category VARCHAR(100), score INTEGER CHECK (score >= 1 AND score <= 10), explanation TEXT, created_at TIMESTAMP DEFAULT NOW())
       - investment_recommendations table (id SERIAL PRIMARY KEY, company_id INTEGER REFERENCES companies(id), recommendation VARCHAR(50), conviction_level INTEGER CHECK (conviction_level >= 1 AND conviction_level <= 10), rationale TEXT, created_at TIMESTAMP DEFAULT NOW())

    2. **Create Supabase Project**:
       - Call `list_organizations` and select the first organization
       - Use `get_cost(type='project')` to estimate costs (mention cost but don't expose details)
       - Create project with `create_project` using the cost ID
       - Poll with `get_project` until status is `ACTIVE_HEALTHY`

    3. **Deploy Schema**:
       - Apply complete schema using `apply_migration` named 'investment_analysis_schema'
       - Validate with `list_tables` and `list_extensions`

    4. **Insert Sample Data**:
       - Insert sample companies data for Apple, Microsoft, Google with realistic values:
         * Apple: ticker='AAPL', sector='Technology', market_cap=3000000000000, founded_year=1976, headquarters='Cupertino, CA'
         * Microsoft: ticker='MSFT', sector='Technology', market_cap=2800000000000, founded_year=1975, headquarters='Redmond, WA'
         * Google: ticker='GOOGL', sector='Technology', market_cap=1800000000000, founded_year=1998, headquarters='Mountain View, CA'

       - Insert analysis session record with current analysis parameters

       - Insert sample financial metrics for each company:
         * Revenue, net_income, pe_ratio, debt_to_equity, current_ratio, roe

       - Verify data insertion with SELECT queries

    5. **Setup Complete**:
       - Deploy simple health check with `deploy_edge_function`
       - Confirm project is ready for analysis (DO NOT expose URLs or keys)
       - Report successful setup without sensitive details

    Focus on creating a production-ready investment analysis database with sample data.
    **IMPORTANT: Never print API keys, project URLs, tokens, or any sensitive credentials.**
    """,
    markdown=True,
)

company_research_agent = Agent(
    name="Company Research Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in company research using Supabase database operations",
    instructions="""
    You are a senior equity research analyst who uses Supabase MCP tools to store and manage investment research data.

    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials.**

    **Your Task Using Supabase MCP Tools:**

    1. **Check database structure:**
       - Call list_tables() to see existing database structure
       - Database should already be set up by Database Setup Agent

    2. **Store company data:**
       - Extract company information from the input request
       - Insert company records into the companies table:
         * Apple Inc: ticker='AAPL', sector='Technology', market_cap=3000000000000, founded_year=1976, headquarters='Cupertino, CA'
         * Microsoft: ticker='MSFT', sector='Technology', market_cap=2800000000000, founded_year=1975, headquarters='Redmond, WA'
         * Google: ticker='GOOGL', sector='Technology', market_cap=1800000000000, founded_year=1998, headquarters='Mountain View, CA'

    3. **Store analysis session:**
       - Insert current analysis session with parameters from the investment request
       - Include investment_type, investment_amount, target_return, risk_tolerance

    4. **Insert basic company profiles:**
       - Add company descriptions and business model information
       - Store competitive advantages and key risks
       - Insert recent developments and strategic initiatives

    5. **Verify and report:**
       - Use SELECT statements to confirm data storage
       - Report successful data insertion (without exposing sensitive details)

    **Example SQL Operations:**
    ```sql
    -- Insert company data
    INSERT INTO companies (name, ticker, sector, market_cap, founded_year, headquarters)
    VALUES ('Apple Inc', 'AAPL', 'Technology', 3000000000000, 1976, 'Cupertino, CA');

    -- Insert analysis session
    INSERT INTO analysis_sessions (investment_type, investment_amount, target_return, risk_tolerance)
    VALUES ('equity', 100000000.00, 25.00, 'HIGH');

    -- Verify data
    SELECT COUNT(*) as companies_count FROM companies;
    SELECT COUNT(*) as sessions_count FROM analysis_sessions;
    ```

    Focus on actual database operations and data storage.
    **IMPORTANT: Never expose API keys, URLs, or sensitive credentials.**
    """,
    markdown=True,
)

financial_analysis_agent = Agent(
    name="Financial Analysis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in financial analysis using Supabase database operations",
    instructions="""
    You are a CFA-certified financial analyst who uses Supabase MCP tools for financial data management.

    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials.**

    **Your Task Using Supabase MCP Tools:**

    1. **Retrieve company data:**
       - Query companies table to get company IDs for analysis
       - Check existing database structure with list_tables()

    2. **Insert financial metrics:**
       - Store key financial metrics for each company in financial_metrics table
       - Insert sample financial data for analysis:
         * Apple: revenue=394328000000, net_income=99803000000, pe_ratio=28.5, debt_to_equity=1.73, current_ratio=1.0, roe=0.26
         * Microsoft: revenue=211915000000, net_income=72361000000, pe_ratio=32.1, debt_to_equity=0.35, current_ratio=1.8, roe=0.36
         * Google: revenue=307394000000, net_income=73795000000, pe_ratio=24.8, debt_to_equity=0.11, current_ratio=2.9, roe=0.21

    3. **Perform financial analysis:**
       - Calculate financial ratios and performance metrics
       - Store profitability analysis (gross, operating, net margins)
       - Insert liquidity and leverage ratios
       - Store growth metrics and trend analysis

    4. **Generate insights:**
       - Analyze financial health and performance trends
       - Compare metrics across companies
       - Store analysis conclusions in database

    **Example SQL Operations:**
    ```sql
    -- Get company IDs
    SELECT id, name FROM companies WHERE name IN ('Apple Inc', 'Microsoft Corporation', 'Alphabet Inc');

    -- Insert financial metrics
    INSERT INTO financial_metrics (company_id, metric_type, value, period, currency)
    VALUES (1, 'revenue', 394328000000, '2023', 'USD');

    -- Verify insertion
    SELECT COUNT(*) as metrics_count FROM financial_metrics;
    ```

    Focus on actual financial data insertion and analysis using database operations.
    **IMPORTANT: Never expose API keys, URLs, or sensitive credentials.**
    """,
    markdown=True,
)

valuation_agent = Agent(
    name="Valuation Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in valuation analysis using Supabase database operations",
    instructions="""
    You are a senior valuation analyst who uses Supabase MCP tools for valuation modeling and data storage.

    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to value
    - investment_type: Investment type for methodology
    - investment_amount: Investment size for context
    - target_return: Target return for valuations
    - investment_horizon: Time horizon for projections
    - comparable_companies: Comparable companies for relative valuation

    **Your Task Using Supabase MCP Tools:**

    1. **Valuation Schema Setup:**
       - Create 'dcf_models' table: company_id, year, free_cash_flow, terminal_value, wacc, dcf_value
       - Create 'comparable_multiples' table: company_id, comp_company, multiple_type, value
       - Create 'valuation_summary' table: company_id, current_price, target_price, upside_potential

    2. **DCF Model Implementation:**
       - Query financial data from existing tables
       - Calculate 5-year free cash flow projections
       - Compute WACC using current market data
       - Store DCF components and final valuation

    3. **Comparable Analysis:**
       - Query comparable companies data
       - Calculate trading multiples for peer group
       - Store P/E, P/B, EV/EBITDA multiples
       - Compute relative valuations

    4. **Valuation Integration:**
       - Use multiple methodologies to derive target prices
       - Store upside/downside scenarios in database
       - Calculate probability-weighted valuations

    **Key Actions to Take:**
    - Execute apply_migration() for valuation table creation
    - Use complex SQL queries for cash flow calculations
    - INSERT valuation model components
    - Use JOINs to combine financial and valuation data
    - Store final target prices and recommendations

    Perform actual valuation calculations and database storage.
    Use SQL for complex financial modeling operations.
    """,
    markdown=True,
)

risk_assessment_agent = Agent(
    name="Risk Assessment Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in risk analysis using Supabase database operations",
    instructions="""
    You are a senior risk analyst who uses Supabase MCP tools for comprehensive risk assessment and scoring.

    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to assess for risk
    - investment_type: Investment type for risk framework
    - investment_amount: Investment size for portfolio impact
    - risk_tolerance: Client risk tolerance level
    - investment_horizon: Time horizon for risk analysis
    - sectors: Sector exposure for concentration risk

    **Your Task Using Supabase MCP Tools:**

    1. **Risk Assessment Schema:**
       - Create 'risk_scores' table: company_id, risk_category, score, explanation, assessment_date
       - Create 'risk_factors' table: company_id, factor_type, description, severity, mitigation
       - Use apply_migration() to implement risk assessment structure

    2. **Risk Scoring Implementation:**
       - Score Financial Risk (1-10): Credit, liquidity, leverage, earnings quality
       - Score Operational Risk (1-10): Business model, execution, supply chain
       - Score Market Risk (1-10): Competition, cyclicality, customer concentration
       - Score Regulatory Risk (1-10): Compliance, legal, policy changes
       - Score ESG Risk (1-10): Environmental, social, governance factors

    3. **Risk Quantification:**
       - Calculate overall risk score as weighted average
       - Store Value at Risk (VaR) calculations
       - Use SQL to compute risk-adjusted returns
       - Store correlation analysis with portfolio holdings

    4. **Risk Mitigation Database:**
       - Store position sizing recommendations
       - Insert hedging strategies and derivatives data
       - Calculate and store stop-loss parameters

    **Key Actions to Take:**
    - Use apply_migration() for risk table creation
    - Execute INSERT statements for risk scores (1-10 scale)
    - Use SQL aggregations for overall risk calculations
    - Store detailed risk factor explanations
    - Calculate portfolio impact using database queries

    Perform actual risk calculations and store quantitative risk data.
    Use database operations for risk score computations.
    """,
    markdown=True,
)

market_analysis_agent = Agent(
    name="Market Analysis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in market analysis using Supabase database operations",
    instructions="""
    You are a senior sector analyst who uses Supabase MCP tools for market dynamics and industry analysis.

    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to analyze within market context
    - sectors: Target sectors for analysis
    - investment_type: Investment type for market positioning
    - investment_horizon: Time horizon for market outlook
    - benchmark_indices: Relevant market benchmarks

    **Your Task Using Supabase MCP Tools:**

    1. **Market Analysis Schema:**
       - Create 'sectors' table: sector_id, name, market_size, growth_rate, maturity_stage
       - Create 'market_dynamics' table: sector_id, factor_type, description, impact_score
       - Create 'competitive_landscape' table: sector_id, company_id, market_share, competitive_position

    2. **Sector Analysis Implementation:**
       - Store industry classification and market size data
       - Insert growth rates and historical trend analysis
       - Use SQL to calculate market concentration ratios
       - Store geographic distribution and regional dynamics

    3. **Market Dynamics Storage:**
       - Insert supply and demand fundamental data
       - Store pricing dynamics and margin trends
       - Use database queries for capacity utilization analysis
       - Store seasonal patterns and cyclicality data

    4. **Competitive Analysis:**
       - Query companies table and join with market data
       - Calculate and store market share distributions
       - Insert competitive positioning analysis
       - Store barriers to entry and switching costs

    **Key Actions to Take:**
    - Execute apply_migration() for market analysis tables
    - Use INSERT statements for sector and market data
    - Perform SQL JOINs between companies and market tables
    - Calculate market metrics using database aggregations
    - Store forecasting data for 1-3 year outlook

    Focus on actual market data storage and analysis using SQL.
    Use database operations for market intelligence gathering.
    """,
    markdown=True,
)

esg_analysis_agent = Agent(
    name="ESG Analysis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in ESG analysis using Supabase database operations",
    instructions="""
    You are an ESG analyst who uses Supabase MCP tools for comprehensive ESG assessment and scoring.

    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies to analyze for ESG factors
    - investment_type: Investment type for ESG materiality
    - sectors: Sectors for ESG risk assessment
    - investment_horizon: Time horizon for ESG impact

    **Your Task Using Supabase MCP Tools:**

    1. **ESG Database Schema:**
       - Create 'esg_scores' table: company_id, category, metric_name, score, rating_date
       - Create 'esg_metrics' table: company_id, metric_type, value, units, reporting_period
       - Create 'esg_initiatives' table: company_id, initiative_type, description, impact_score

    2. **Environmental Data Storage:**
       - Store carbon footprint and GHG emissions data
       - Insert energy efficiency and renewable energy metrics
       - Use SQL to calculate environmental compliance scores
       - Store climate risk assessments and TCFD data

    3. **Social Metrics Implementation:**
       - Insert diversity, equity, and inclusion (DEI) scores
       - Store labor practices and employee relations data
       - Use database queries for community impact analysis
       - Store product safety and customer satisfaction metrics

    4. **Governance Assessment:**
       - Store board composition and independence data
       - Insert executive compensation alignment metrics
       - Use SQL for business ethics compliance tracking
       - Store audit quality and transparency scores

    5. **ESG Integration Analysis:**
       - Calculate overall ESG scores using weighted averages
       - Use SQL aggregations for ESG performance trends
       - Store third-party ESG ratings and comparisons
       - Query ESG impact on financial performance

    **Key Actions to Take:**
    - Execute apply_migration() for ESG table creation
    - Use INSERT statements for ESG metrics and scores
    - Perform SQL calculations for weighted ESG scores
    - Store ESG improvement tracking data
    - Use database queries for ESG benchmarking

    Perform actual ESG data collection and scoring using database operations.
    Focus on quantitative ESG metrics and database storage.
    """,
    markdown=True,
)

investment_recommendation_agent = Agent(
    name="Investment Recommendation Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in investment recommendations using Supabase database analysis",
    instructions="""
    You are a senior portfolio manager who uses Supabase MCP tools to generate data-driven investment recommendations.

    **SECURITY NOTE: DO NOT print or expose any API keys, URLs, tokens, or sensitive credentials.**

    **Your Task Using Supabase MCP Tools:**

    1. **Retrieve analysis data:**
       - Query all analysis tables (companies, financial_metrics, valuation_models, risk_assessments)
       - Use SQL JOINs to combine analysis results for comprehensive view
       - Check data availability for each company

    2. **Generate investment recommendations:**
       - Analyze stored financial metrics, valuations, and risk assessments
       - Generate BUY/HOLD/SELL recommendations based on comprehensive data
       - Insert recommendations into investment_recommendations table:
         * Apple: recommendation='BUY', conviction_level=8, rationale='Strong financials and market position'
         * Microsoft: recommendation='BUY', conviction_level=9, rationale='Excellent cloud growth and profitability'
         * Google: recommendation='HOLD', conviction_level=7, rationale='Solid fundamentals but regulatory concerns'

    3. **Calculate investment scores:**
       - Use stored financial metrics to calculate overall investment attractiveness
       - Weight factors based on investment type and risk tolerance
       - Store calculated scores and rankings

    4. **Portfolio recommendations:**
       - Based on investment amount and risk tolerance, suggest position sizing
       - Consider diversification and correlation factors
       - Generate portfolio allocation recommendations

    **Example SQL Operations:**
    ```sql
    -- Retrieve comprehensive analysis data
    SELECT c.name, c.ticker, fm.metric_type, fm.value, vm.target_price, ra.risk_category, ra.score
    FROM companies c
    LEFT JOIN financial_metrics fm ON c.id = fm.company_id
    LEFT JOIN valuation_models vm ON c.id = vm.company_id
    LEFT JOIN risk_assessments ra ON c.id = ra.company_id;

    -- Insert investment recommendation
    INSERT INTO investment_recommendations (company_id, recommendation, conviction_level, rationale)
    VALUES (1, 'BUY', 8, 'Strong financial performance and market leadership');

    -- Verify recommendations
    SELECT COUNT(*) as recommendations_count FROM investment_recommendations;
    ```

    Generate actionable investment recommendations based on stored analysis data.
    **IMPORTANT: Never expose API keys, URLs, or sensitive credentials.**
    """,
    markdown=True,
)

report_synthesis_agent = Agent(
    name="Report Synthesis Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_supabase_mcp_tools(), ReasoningTools()],
    role="Expert in report generation using Supabase database queries",
    instructions="""
    You are a senior research director who uses Supabase MCP tools to compile comprehensive investment reports.

    **Input Data Usage:**
    You will receive structured input containing:
    - companies: Companies analyzed in the research
    - investment_type: Investment strategy and approach
    - investment_amount: Investment capital available
    - target_return: Return expectations
    - risk_tolerance: Risk parameters
    - investment_horizon: Time horizon for investment
    - analyses_requested: List of completed analyses

    **Your Task Using Supabase MCP Tools:**

    1. **Report Data Aggregation:**
       - Query all analysis tables to gather complete dataset
       - Use complex SQL JOINs to combine company, financial, valuation, risk, market, and ESG data
       - Execute aggregation queries for summary statistics
       - Retrieve investment recommendations and rationale

    2. **Executive Summary Generation:**
       - Query key findings from all analysis tables
       - Use SQL to calculate portfolio-level metrics
       - Retrieve top recommendations and conviction levels
       - Extract critical risk factors and opportunities

    3. **Detailed Analysis Compilation:**
       - Generate company profiles from companies table
       - Query financial performance trends and ratios
       - Retrieve valuation models and target prices
       - Extract risk scores and mitigation strategies
       - Compile market analysis and competitive positioning
       - Gather ESG scores and sustainability metrics

    4. **Investment Thesis Integration:**
       - Use SQL queries to validate investment recommendations
       - Calculate expected returns and risk-adjusted metrics
       - Query correlation and diversification benefits
       - Retrieve implementation timelines and monitoring frameworks

    5. **Report Structure Creation:**
       - Create 'investment_reports' table: report_id, analysis_date, executive_summary, detailed_findings
       - Store complete report content with version control
       - Insert supporting charts and data visualizations
       - Create exportable report formats

    **Key Actions to Take:**
    - Execute comprehensive SELECT queries across all tables
    - Use SQL aggregations and analytics functions
    - CREATE VIEW statements for report data compilation
    - INSERT final report content into reports table
    - Query historical analysis for trend identification

    **Report Output:**
    Generate a comprehensive investment research report with:
    - Executive summary with key recommendations
    - Detailed company analysis and financial modeling
    - Risk assessment and mitigation strategies
    - Market context and competitive analysis
    - ESG integration and sustainability factors
    - Implementation roadmap and monitoring framework

    Focus on data-driven insights from database analysis.
    Use actual query results to support all recommendations.
    """,
    markdown=True,
)
```

---

<a name="examples--workflows--investment_analyst--modelspy"></a>

### `examples/workflows/investment_analyst/models.py`

```python
from enum import Enum
from typing import Dict, List, Optional

from pydantic import BaseModel, Field


class InvestmentType(str, Enum):
    EQUITY = "equity"
    DEBT = "debt"
    HYBRID = "hybrid"
    VENTURE = "venture"
    GROWTH = "growth"
    BUYOUT = "buyout"


class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"


class InvestmentAnalysisRequest(BaseModel):
    companies: List[str] = Field(
        ..., min_length=1, max_length=3, description="List of 1-3 companies to analyze"
    )
    investment_type: InvestmentType = Field(
        ..., description="Type of investment being considered"
    )
    investment_amount: Optional[float] = Field(
        default=None, description="Investment amount in USD"
    )
    investment_horizon: Optional[str] = Field(
        default=None, description="Investment time horizon (e.g., '3-5 years')"
    )
    target_return: Optional[float] = Field(
        default=None, description="Target return percentage"
    )
    risk_tolerance: Optional[RiskLevel] = Field(
        default=None, description="Risk tolerance level"
    )
    sectors: List[str] = Field(
        default_factory=list, description="Target sectors for analysis"
    )
    analyses_requested: List[str] = Field(
        ..., min_length=1, description="List of analysis types to perform"
    )
    benchmark_indices: List[str] = Field(
        default_factory=list, description="Benchmark indices for comparison"
    )
    comparable_companies: List[str] = Field(
        default_factory=list, description="Comparable companies for analysis"
    )


class FinancialMetrics(BaseModel):
    revenue: Optional[float] = None
    revenue_growth: Optional[float] = None
    net_income: Optional[float] = None
    ebitda: Optional[float] = None
    gross_margin: Optional[float] = None
    operating_margin: Optional[float] = None
    net_margin: Optional[float] = None
    roe: Optional[float] = None
    roa: Optional[float] = None
    debt_to_equity: Optional[float] = None
    current_ratio: Optional[float] = None
    quick_ratio: Optional[float] = None
    pe_ratio: Optional[float] = None
    peg_ratio: Optional[float] = None
    price_to_book: Optional[float] = None
    price_to_sales: Optional[float] = None
    ev_to_ebitda: Optional[float] = None
    free_cash_flow: Optional[float] = None
    cash_per_share: Optional[float] = None
    market_cap: Optional[float] = None


class CompanyProfile(BaseModel):
    name: str
    ticker: Optional[str] = None
    sector: Optional[str] = None
    industry: Optional[str] = None
    market_cap: Optional[float] = None
    headquarters: Optional[str] = None
    founded_year: Optional[int] = None
    employees: Optional[int] = None
    description: Optional[str] = None
    business_model: Optional[str] = None
    competitive_advantages: List[str] = Field(default_factory=list)
    key_risks: List[str] = Field(default_factory=list)
    recent_developments: List[str] = Field(default_factory=list)


class ValuationAnalysis(BaseModel):
    company_name: str
    dcf_valuation: Optional[float] = None
    comparable_multiples: Dict[str, float] = Field(default_factory=dict)
    asset_based_valuation: Optional[float] = None
    sum_of_parts: Optional[float] = None
    current_price: Optional[float] = None
    target_price: Optional[float] = None
    upside_potential: Optional[float] = None
    valuation_summary: Optional[str] = None


class RiskAssessment(BaseModel):
    company_name: str
    financial_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    operational_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    market_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    regulatory_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    esg_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    overall_risk_score: Optional[int] = Field(default=None, ge=1, le=10)
    risk_factors: List[str] = Field(default_factory=list)
    risk_mitigation: List[str] = Field(default_factory=list)


class MarketAnalysis(BaseModel):
    sector: str
    market_size: Optional[float] = None
    growth_rate: Optional[float] = None
    market_trends: List[str] = Field(default_factory=list)
    competitive_landscape: Optional[str] = None
    barriers_to_entry: List[str] = Field(default_factory=list)
    growth_drivers: List[str] = Field(default_factory=list)
    headwinds: List[str] = Field(default_factory=list)


class InvestmentRecommendation(BaseModel):
    company_name: str
    recommendation: str = Field(
        ..., description="BUY, HOLD, SELL, or STRONG_BUY/STRONG_SELL"
    )
    target_price: Optional[float] = None
    price_target_timeframe: Optional[str] = None
    conviction_level: Optional[int] = Field(default=None, ge=1, le=10)
    key_catalysts: List[str] = Field(default_factory=list)
    key_risks: List[str] = Field(default_factory=list)
    rationale: Optional[str] = None


class InvestmentAnalysisResponse(BaseModel):
    request: InvestmentAnalysisRequest
    company_profiles: List[CompanyProfile] = Field(default_factory=list)
    financial_analysis: Optional[str] = None
    valuation_analysis: List[ValuationAnalysis] = Field(default_factory=list)
    risk_assessment: List[RiskAssessment] = Field(default_factory=list)
    market_analysis: List[MarketAnalysis] = Field(default_factory=list)
    esg_analysis: Optional[str] = None
    technical_analysis: Optional[str] = None
    peer_comparison: Optional[str] = None
    investment_recommendations: List[InvestmentRecommendation] = Field(
        default_factory=list
    )
    portfolio_fit: Optional[str] = None
    executive_summary: Optional[str] = None
    success: bool = True
    error_message: Optional[str] = None
    analysis_timestamp: Optional[str] = None
```

---

<a name="examples--workflows--investment_analyst--run_workflowpy"></a>

### `examples/workflows/investment_analyst/run_workflow.py`

```python
from agents import (
    company_research_agent,
    database_setup_agent,
    esg_analysis_agent,
    financial_analysis_agent,
    investment_recommendation_agent,
    market_analysis_agent,
    report_synthesis_agent,
    risk_assessment_agent,
    valuation_agent,
)
from agno.workflow import Condition, Loop, Parallel, Router, Step, Steps, Workflow
from agno.workflow.types import StepInput, StepOutput
from models import InvestmentAnalysisRequest, InvestmentType, RiskLevel


### Evaluators
def should_run_analysis(analysis_type: str) -> callable:
    """Create conditional evaluator for analysis types"""

    def evaluator(step_input: StepInput) -> bool:
        request_data = step_input.input
        if isinstance(request_data, InvestmentAnalysisRequest):
            return analysis_type in request_data.analyses_requested
        return False

    return evaluator


def is_high_risk_investment(step_input: StepInput) -> bool:
    """Check if this is a high-risk investment requiring additional analysis"""
    request_data = step_input.input
    if isinstance(request_data, InvestmentAnalysisRequest):
        return (
            request_data.risk_tolerance == RiskLevel.HIGH
            or request_data.investment_type
            in [InvestmentType.VENTURE, InvestmentType.GROWTH]
            or request_data.target_return
            and request_data.target_return > 20.0
        )
    return False


def is_large_investment(step_input: StepInput) -> bool:
    """Check if this is a large investment requiring additional due diligence"""
    request_data = step_input.input
    if isinstance(request_data, InvestmentAnalysisRequest):
        return (
            request_data.investment_amount
            and request_data.investment_amount > 50_000_000
        )
    return False


def requires_esg_analysis(step_input: StepInput) -> bool:
    """Check if ESG analysis is required"""
    request_data = step_input.input
    if isinstance(request_data, InvestmentAnalysisRequest):
        return "esg_analysis" in request_data.analyses_requested
    return False


def is_multi_company_analysis(step_input: StepInput) -> bool:
    """Check if analyzing multiple companies"""
    request_data = step_input.input
    if isinstance(request_data, InvestmentAnalysisRequest):
        return len(request_data.companies) > 1
    return False


### Routers
def select_valuation_approach(step_input: StepInput) -> Step:
    """Router to select appropriate valuation approach based on investment type"""
    request_data = step_input.input
    if isinstance(request_data, InvestmentAnalysisRequest):
        if request_data.investment_type in [
            InvestmentType.VENTURE,
            InvestmentType.GROWTH,
        ]:
            return Step(
                name="Venture Valuation",
                agent=valuation_agent,
                description="Specialized valuation for venture/growth investments",
            )
        elif request_data.investment_type == InvestmentType.DEBT:
            return Step(
                name="Credit Analysis",
                agent=financial_analysis_agent,
                description="Credit-focused analysis for debt investments",
            )
        else:
            return valuation_step
    return valuation_step


def select_risk_framework(step_input: StepInput) -> Step:
    """Router to select risk assessment framework"""
    request_data = step_input.input
    if isinstance(request_data, InvestmentAnalysisRequest):
        if request_data.investment_type == InvestmentType.VENTURE:
            return Step(
                name="Venture Risk Assessment",
                agent=risk_assessment_agent,
                description="Venture-specific risk assessment framework",
            )
        elif (
            request_data.investment_amount
            and request_data.investment_amount > 100_000_000
        ):
            return Step(
                name="Enterprise Risk Assessment",
                agent=risk_assessment_agent,
                description="Enterprise-level risk assessment for large investments",
            )
        else:
            return risk_assessment_step
    return risk_assessment_step


def analysis_quality_check(step_outputs: list[StepOutput]) -> bool:
    """End condition: Check if analysis quality is sufficient"""
    if not step_outputs:
        return False

    # Check if latest output indicates high confidence
    latest_output = step_outputs[-1]
    if hasattr(latest_output, "content") and latest_output.content:
        content_lower = latest_output.content.lower()
        return (
            "high confidence" in content_lower
            or "comprehensive analysis" in content_lower
            or "detailed valuation" in content_lower
        )
    return False


def risk_assessment_complete(step_outputs: list[StepOutput]) -> bool:
    """End condition: Check if risk assessment is comprehensive"""
    if len(step_outputs) < 2:
        return False

    # Check if we have both financial and operational risk scores
    has_financial_risk = any(
        "financial risk" in output.content.lower()
        for output in step_outputs
        if hasattr(output, "content")
    )
    has_operational_risk = any(
        "operational risk" in output.content.lower()
        for output in step_outputs
        if hasattr(output, "content")
    )

    return has_financial_risk and has_operational_risk


### Steps
database_setup_step = Step(
    name="Database Setup",
    agent=database_setup_agent,
    description="Create and configure Supabase database for investment analysis",
)

company_research_step = Step(
    name="Company Research",
    agent=company_research_agent,
    description="Company research and data storage using Supabase MCP",
)

financial_analysis_step = Step(
    name="Financial Analysis",
    agent=financial_analysis_agent,
    description="Financial analysis with Supabase database operations",
)

valuation_step = Step(
    name="Valuation Analysis",
    agent=valuation_agent,
    description="Valuation modeling using Supabase database storage",
)

risk_assessment_step = Step(
    name="Risk Assessment",
    agent=risk_assessment_agent,
    description="Risk analysis and scoring with Supabase database",
)

market_analysis_step = Step(
    name="Market Analysis",
    agent=market_analysis_agent,
    description="Market dynamics analysis using Supabase operations",
)

esg_analysis_step = Step(
    name="ESG Analysis",
    agent=esg_analysis_agent,
    description="ESG assessment and scoring with Supabase database",
)

investment_recommendation_step = Step(
    name="Investment Recommendation",
    agent=investment_recommendation_agent,
    description="Data-driven investment recommendations using Supabase queries",
)

report_synthesis_step = Step(
    name="Report Synthesis",
    agent=report_synthesis_agent,
    description="Comprehensive report generation from Supabase database",
)


investment_analysis_workflow = Workflow(
    name="Enhanced Investment Analysis Workflow",
    description="Comprehensive investment analysis using workflow v2 primitives with Supabase MCP tools",
    steps=[
        database_setup_step,
        company_research_step,
        # Phase 3: Multi-company analysis
        Condition(
            evaluator=is_multi_company_analysis,
            steps=[
                Steps(
                    name="Multi-Company Analysis Pipeline",
                    description="Sequential analysis pipeline for multiple companies",
                    steps=[
                        Loop(
                            name="Company Analysis Loop",
                            description="Iterative analysis for each company",
                            steps=[financial_analysis_step, valuation_step],
                            max_iterations=5,
                            end_condition=analysis_quality_check,
                        ),
                        Parallel(
                            market_analysis_step,
                            Step(
                                name="Comparative Analysis",
                                agent=financial_analysis_agent,
                                description="Cross-company comparison analysis",
                            ),
                            name="Comparative Analysis Phase",
                        ),
                    ],
                ),
            ],
            name="Multi-Company Condition",
        ),
        # Phase 4: Risk-based routing
        Router(
            name="Risk Assessment Router",
            description="Dynamic risk assessment based on investment characteristics",
            selector=select_risk_framework,
            choices=[
                risk_assessment_step,
                Step(
                    name="Enhanced Risk Assessment",
                    agent=risk_assessment_agent,
                    description="Enhanced risk assessment for complex investments",
                ),
            ],
        ),
        # Phase 5: Valuation strategy selection
        Router(
            name="Valuation Strategy Router",
            description="Select valuation approach based on investment type",
            selector=select_valuation_approach,
            choices=[
                valuation_step,
                Step(
                    name="Alternative Valuation",
                    agent=valuation_agent,
                    description="Alternative valuation methods",
                ),
            ],
        ),
        # Phase 6: High-risk investment analysis
        Condition(
            evaluator=is_high_risk_investment,
            steps=[
                Steps(
                    name="High-Risk Analysis Pipeline",
                    description="Additional analysis for high-risk investments",
                    steps=[
                        Parallel(
                            Step(
                                name="Scenario Analysis",
                                agent=financial_analysis_agent,
                                description="Monte Carlo and scenario analysis",
                            ),
                            Step(
                                name="Stress Testing",
                                agent=risk_assessment_agent,
                                description="Stress testing and sensitivity analysis",
                            ),
                            name="Risk Modeling Phase",
                        ),
                        Loop(
                            name="Risk Refinement Loop",
                            description="Iterative risk model refinement",
                            steps=[
                                Step(
                                    name="Risk Model Validation",
                                    agent=risk_assessment_agent,
                                    description="Validate and refine risk models",
                                ),
                            ],
                            max_iterations=3,
                            end_condition=risk_assessment_complete,
                        ),
                    ],
                ),
            ],
            name="High-Risk Investment Condition",
        ),
        # Phase 7: Large investment due diligence
        Condition(
            evaluator=is_large_investment,
            steps=[
                Parallel(
                    Step(
                        name="Regulatory Analysis",
                        agent=risk_assessment_agent,
                        description="Regulatory and compliance analysis",
                    ),
                    Step(
                        name="Market Impact Analysis",
                        agent=market_analysis_agent,
                        description="Market impact and liquidity analysis",
                    ),
                    Step(
                        name="Management Assessment",
                        agent=company_research_agent,
                        description="Management team and governance analysis",
                    ),
                    name="Due Diligence Phase",
                ),
            ],
            name="Large Investment Condition",
        ),
        # Phase 8: ESG analysis
        Condition(
            evaluator=requires_esg_analysis,
            steps=[
                Steps(
                    name="ESG Analysis Pipeline",
                    description="Comprehensive ESG analysis and integration",
                    steps=[
                        esg_analysis_step,
                        Step(
                            name="ESG Integration",
                            agent=investment_recommendation_agent,
                            description="Integrate ESG factors into investment decision",
                        ),
                    ],
                ),
            ],
            name="ESG Analysis Condition",
        ),
        # Phase 9: Market context analysis
        Condition(
            evaluator=should_run_analysis("market_analysis"),
            steps=[
                Parallel(
                    market_analysis_step,
                    Step(
                        name="Sector Analysis",
                        agent=market_analysis_agent,
                        description="Detailed sector and industry analysis",
                    ),
                    name="Market Context Phase",
                ),
            ],
            name="Market Analysis Condition",
        ),
        # Phase 10: Investment decision and reporting
        Steps(
            name="Investment Decision Pipeline",
            description="Final investment decision and reporting",
            steps=[
                Loop(
                    name="Investment Consensus Loop",
                    description="Iterative investment recommendation refinement",
                    steps=[
                        investment_recommendation_step,
                        Step(
                            name="Recommendation Validation",
                            agent=investment_recommendation_agent,
                            description="Validate investment recommendations",
                        ),
                    ],
                    max_iterations=2,
                    end_condition=lambda outputs: any(
                        "final recommendation" in output.content.lower()
                        for output in outputs
                        if hasattr(output, "content")
                    ),
                ),
                report_synthesis_step,
            ],
        ),
    ],
)

if __name__ == "__main__":
    request = InvestmentAnalysisRequest(
        companies=["Apple"],
        investment_type=InvestmentType.EQUITY,
        investment_amount=100_000_000,
        investment_horizon="5-7 years",
        target_return=25.0,
        risk_tolerance=RiskLevel.HIGH,
        sectors=["Technology"],
        analyses_requested=[
            "financial_analysis",
            "valuation",
            "risk_assessment",
            "market_analysis",
            "esg_analysis",
        ],
        benchmark_indices=["S&P 500", "NASDAQ"],
        comparable_companies=["Microsoft", "Google"],
    )
    response = investment_analysis_workflow.print_response(
        input=request,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="examples--workflows--investment_report_generatorpy"></a>

### `examples/workflows/investment_report_generator.py`

```python
""" Investment Report Generator - Your AI Financial Analysis Studio!

This advanced example demonstrates how to build a sophisticated investment analysis system that combines
market research, financial analysis, and portfolio management. The workflow uses a three-stage
approach:
1. Comprehensive stock analysis and market research
2. Investment potential evaluation and ranking
3. Strategic portfolio allocation recommendations

Key capabilities:
- Real-time market data analysis
- Professional financial research
- Investment risk assessment
- Portfolio allocation strategy
- Detailed investment rationale

Example companies to analyze:
- "AAPL, MSFT, GOOGL" (Tech Giants)
- "NVDA, AMD, INTC" (Semiconductor Leaders)
- "TSLA, F, GM" (Automotive Innovation)
- "JPM, BAC, GS" (Banking Sector)
- "AMZN, WMT, TGT" (Retail Competition)
- "PFE, JNJ, MRNA" (Healthcare Focus)
- "XOM, CVX, BP" (Energy Sector)

Run `pip install openai yfinance agno` to install dependencies.
"""

import asyncio
import random
from pathlib import Path
from shutil import rmtree
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel


# --- Response models ---
class StockAnalysisResult(BaseModel):
    company_symbols: str
    market_analysis: str
    financial_metrics: str
    risk_assessment: str
    recommendations: str


class InvestmentRanking(BaseModel):
    ranked_companies: str
    investment_rationale: str
    risk_evaluation: str
    growth_potential: str


class PortfolioAllocation(BaseModel):
    allocation_strategy: str
    investment_thesis: str
    risk_management: str
    final_recommendations: str


# --- File management ---
reports_dir = Path(__file__).parent.joinpath("reports", "investment")
if reports_dir.is_dir():
    rmtree(path=reports_dir, ignore_errors=True)
reports_dir.mkdir(parents=True, exist_ok=True)

stock_analyst_report = str(reports_dir.joinpath("stock_analyst_report.md"))
research_analyst_report = str(reports_dir.joinpath("research_analyst_report.md"))
investment_report = str(reports_dir.joinpath("investment_report.md"))


# --- Agents ---
stock_analyst = Agent(
    name="Stock Analyst",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        YFinanceTools(
            company_info=True, analyst_recommendations=True, company_news=True
        )
    ],
    description=dedent("""\
    You are MarketMaster-X, an elite Senior Investment Analyst at Goldman Sachs with expertise in:

    - Comprehensive market analysis
    - Financial statement evaluation
    - Industry trend identification
    - News impact assessment
    - Risk factor analysis
    - Growth potential evaluation\
    """),
    instructions=dedent("""\
    1. Market Research 
       - Analyze company fundamentals and metrics
       - Review recent market performance
       - Evaluate competitive positioning
       - Assess industry trends and dynamics
    2. Financial Analysis 
       - Examine key financial ratios
       - Review analyst recommendations
       - Analyze recent news impact
       - Identify growth catalysts
    3. Risk Assessment 
       - Evaluate market risks
       - Assess company-specific challenges
       - Consider macroeconomic factors
       - Identify potential red flags
    Note: This analysis is for educational purposes only.\
    """),
    output_schema=StockAnalysisResult,
)

research_analyst = Agent(
    name="Research Analyst",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are ValuePro-X, an elite Senior Research Analyst at Goldman Sachs specializing in:

    - Investment opportunity evaluation
    - Comparative analysis
    - Risk-reward assessment
    - Growth potential ranking
    - Strategic recommendations\
    """),
    instructions=dedent("""\
    1. Investment Analysis 
       - Evaluate each company's potential
       - Compare relative valuations
       - Assess competitive advantages
       - Consider market positioning
    2. Risk Evaluation 
       - Analyze risk factors
       - Consider market conditions
       - Evaluate growth sustainability
       - Assess management capability
    3. Company Ranking 
       - Rank based on investment potential
       - Provide detailed rationale
       - Consider risk-adjusted returns
       - Explain competitive advantages\
    """),
    output_schema=InvestmentRanking,
)

investment_lead = Agent(
    name="Investment Lead",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are PortfolioSage-X, a distinguished Senior Investment Lead at Goldman Sachs expert in:

    - Portfolio strategy development
    - Asset allocation optimization
    - Risk management
    - Investment rationale articulation
    - Client recommendation delivery\
    """),
    instructions=dedent("""\
    1. Portfolio Strategy 
       - Develop allocation strategy
       - Optimize risk-reward balance
       - Consider diversification
       - Set investment timeframes
    2. Investment Rationale 
       - Explain allocation decisions
       - Support with analysis
       - Address potential concerns
       - Highlight growth catalysts
    3. Recommendation Delivery 
       - Present clear allocations
       - Explain investment thesis
       - Provide actionable insights
       - Include risk considerations\
    """),
    output_schema=PortfolioAllocation,
)


# --- Execution function ---
async def investment_analysis_execution(
    execution_input: WorkflowExecutionInput,
    companies: str,
) -> str:
    """Execute the complete investment analysis workflow"""

    # Get inputs
    message: str = execution_input.input
    company_symbols: str = companies

    if not company_symbols:
        return " No company symbols provided"

    print(f" Starting investment analysis for companies: {company_symbols}")
    print(f" Analysis request: {message}")

    # Phase 1: Stock Analysis
    print("\n PHASE 1: COMPREHENSIVE STOCK ANALYSIS")
    print("=" * 60)

    analysis_prompt = f"""
    {message}

    Please conduct a comprehensive analysis of the following companies: {company_symbols}

    For each company, provide:
    1. Current market position and financial metrics
    2. Recent performance and analyst recommendations
    3. Industry trends and competitive landscape
    4. Risk factors and growth potential
    5. News impact and market sentiment
    Companies to analyze: {company_symbols}
    """

    print(" Analyzing market data and fundamentals...")
    stock_analysis_result = await stock_analyst.arun(analysis_prompt)
    stock_analysis = stock_analysis_result.content

    # Save to file
    with open(stock_analyst_report, "w") as f:
        f.write("# Stock Analysis Report\n\n")
        f.write(f"**Companies:** {stock_analysis.company_symbols}\n\n")
        f.write(f"## Market Analysis\n{stock_analysis.market_analysis}\n\n")
        f.write(f"## Financial Metrics\n{stock_analysis.financial_metrics}\n\n")
        f.write(f"## Risk Assessment\n{stock_analysis.risk_assessment}\n\n")
        f.write(f"## Recommendations\n{stock_analysis.recommendations}\n")

    print(f" Stock analysis completed and saved to {stock_analyst_report}")

    # Phase 2: Investment Ranking
    print("\n PHASE 2: INVESTMENT POTENTIAL RANKING")
    print("=" * 60)

    ranking_prompt = f"""
    Based on the comprehensive stock analysis below, please rank these companies by investment potential.
    STOCK ANALYSIS:
    - Market Analysis: {stock_analysis.market_analysis}
    - Financial Metrics: {stock_analysis.financial_metrics}
    - Risk Assessment: {stock_analysis.risk_assessment}
    - Initial Recommendations: {stock_analysis.recommendations}
    Please provide:
    1. Detailed ranking of companies from best to worst investment potential
    2. Investment rationale for each company
    3. Risk evaluation and mitigation strategies
    4. Growth potential assessment
    """

    print(" Ranking companies by investment potential...")
    ranking_result = await research_analyst.arun(ranking_prompt)
    ranking_analysis = ranking_result.content

    # Save to file
    with open(research_analyst_report, "w") as f:
        f.write("# Investment Ranking Report\n\n")
        f.write(f"## Company Rankings\n{ranking_analysis.ranked_companies}\n\n")
        f.write(f"## Investment Rationale\n{ranking_analysis.investment_rationale}\n\n")
        f.write(f"## Risk Evaluation\n{ranking_analysis.risk_evaluation}\n\n")
        f.write(f"## Growth Potential\n{ranking_analysis.growth_potential}\n")

    print(f" Investment ranking completed and saved to {research_analyst_report}")

    # Phase 3: Portfolio Allocation Strategy
    print("\n PHASE 3: PORTFOLIO ALLOCATION STRATEGY")
    print("=" * 60)

    portfolio_prompt = f"""
    Based on the investment ranking and analysis below, create a strategic portfolio allocation.
    INVESTMENT RANKING:
    - Company Rankings: {ranking_analysis.ranked_companies}
    - Investment Rationale: {ranking_analysis.investment_rationale}
    - Risk Evaluation: {ranking_analysis.risk_evaluation}
    - Growth Potential: {ranking_analysis.growth_potential}
    Please provide:
    1. Specific allocation percentages for each company
    2. Investment thesis and strategic rationale
    3. Risk management approach
    4. Final actionable recommendations
    """

    print(" Developing portfolio allocation strategy...")
    portfolio_result = await investment_lead.arun(portfolio_prompt)
    portfolio_strategy = portfolio_result.content

    # Save to file
    with open(investment_report, "w") as f:
        f.write("# Investment Portfolio Report\n\n")
        f.write(f"## Allocation Strategy\n{portfolio_strategy.allocation_strategy}\n\n")
        f.write(f"## Investment Thesis\n{portfolio_strategy.investment_thesis}\n\n")
        f.write(f"## Risk Management\n{portfolio_strategy.risk_management}\n\n")
        f.write(
            f"## Final Recommendations\n{portfolio_strategy.final_recommendations}\n"
        )

    print(f" Portfolio strategy completed and saved to {investment_report}")

    # Final summary
    summary = f"""
     INVESTMENT ANALYSIS WORKFLOW COMPLETED!

     Analysis Summary:
     Companies Analyzed: {company_symbols}
     Market Analysis:  Completed
     Investment Ranking:  Completed
     Portfolio Strategy:  Completed

     Reports Generated:
     Stock Analysis: {stock_analyst_report}
     Investment Ranking: {research_analyst_report}
     Portfolio Strategy: {investment_report}

     Key Insights:
    {portfolio_strategy.allocation_strategy[:200]}...

     Disclaimer: This analysis is for educational purposes only and should not be considered as financial advice.
    """

    return summary


# --- Workflow definition ---
investment_workflow = Workflow(
    name="Investment Report Generator",
    description="Automated investment analysis with market research and portfolio allocation",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflows.db",
    ),
    steps=investment_analysis_execution,
    session_state={},  # Initialize empty workflow session state
)


if __name__ == "__main__":

    async def main():
        from rich.prompt import Prompt

        # Example investment scenarios to showcase the analyzer's capabilities
        example_scenarios = [
            "AAPL, MSFT, GOOGL",  # Tech Giants
            "NVDA, AMD, INTC",  # Semiconductor Leaders
            "TSLA, F, GM",  # Automotive Innovation
            "JPM, BAC, GS",  # Banking Sector
            "AMZN, WMT, TGT",  # Retail Competition
            "PFE, JNJ, MRNA",  # Healthcare Focus
            "XOM, CVX, BP",  # Energy Sector
        ]

        # Get companies from user with example suggestion
        companies = Prompt.ask(
            "[bold]Enter company symbols (comma-separated)[/bold] "
            "(or press Enter for a suggested portfolio)\n",
            default=random.choice(example_scenarios),
        )

        print(" Testing Investment Report Generator with New Workflow Structure")
        print("=" * 70)

        result = await investment_workflow.arun(
            input="Generate comprehensive investment analysis and portfolio allocation recommendations",
            companies=companies,
        )

        pprint_run_response(result, markdown=True)

    asyncio.run(main())
```

---

<a name="examples--workflows--startup_idea_validatorpy"></a>

### `examples/workflows/startup_idea_validator.py`

```python
"""
 Startup Idea Validator - Your Personal Business Validation Assistant!

This workflow helps entrepreneurs validate their startup ideas by:
1. Clarifying and refining the core business concept
2. Evaluating originality compared to existing solutions
3. Defining clear mission and objectives
4. Conducting comprehensive market research and analysis

Why is this helpful?
--------------------------------------------------------------------------------
 Get objective feedback on your startup idea before investing resources
 Understand your total addressable market and target segments
 Validate assumptions about market opportunity and competition
 Define clear mission and objectives to guide execution

Who should use this?
--------------------------------------------------------------------------------
 Entrepreneurs and Startup Founders
 Product Managers and Business Strategists
 Innovation Teams
 Angel Investors and VCs doing initial screening

Example use cases:
--------------------------------------------------------------------------------
 New product/service validation
 Market opportunity assessment
 Competitive analysis
 Business model validation
 Target customer segmentation
 Mission/vision refinement

Quick Start:
--------------------------------------------------------------------------------
1. Install dependencies:
   pip install openai agno

2. Set environment variables:
   - OPENAI_API_KEY

3. Run:
   python startup_idea_validator.py

The workflow will guide you through validating your startup idea with AI-powered
analysis and research. Use the insights to refine your concept and business plan!
"""

import asyncio
from typing import Any

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.googlesearch import GoogleSearchTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# --- Response models ---
class IdeaClarification(BaseModel):
    originality: str = Field(..., description="Originality of the idea.")
    mission: str = Field(..., description="Mission of the company.")
    objectives: str = Field(..., description="Objectives of the company.")


class MarketResearch(BaseModel):
    total_addressable_market: str = Field(
        ..., description="Total addressable market (TAM)."
    )
    serviceable_available_market: str = Field(
        ..., description="Serviceable available market (SAM)."
    )
    serviceable_obtainable_market: str = Field(
        ..., description="Serviceable obtainable market (SOM)."
    )
    target_customer_segments: str = Field(..., description="Target customer segments.")


class CompetitorAnalysis(BaseModel):
    competitors: str = Field(..., description="List of identified competitors.")
    swot_analysis: str = Field(..., description="SWOT analysis for each competitor.")
    positioning: str = Field(
        ..., description="Startup's potential positioning relative to competitors."
    )


class ValidationReport(BaseModel):
    executive_summary: str = Field(
        ..., description="Executive summary of the validation."
    )
    idea_assessment: str = Field(..., description="Assessment of the startup idea.")
    market_opportunity: str = Field(..., description="Market opportunity analysis.")
    competitive_landscape: str = Field(
        ..., description="Competitive landscape overview."
    )
    recommendations: str = Field(..., description="Strategic recommendations.")
    next_steps: str = Field(..., description="Recommended next steps.")


# --- Agents ---
idea_clarifier_agent = Agent(
    name="Idea Clarifier",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "Given a user's startup idea, your goal is to refine that idea.",
        "Evaluate the originality of the idea by comparing it with existing concepts.",
        "Define the mission and objectives of the startup.",
        "Provide clear, actionable insights about the core business concept.",
    ],
    add_history_to_context=True,
    add_datetime_to_context=True,
    output_schema=IdeaClarification,
    debug_mode=False,
)

market_research_agent = Agent(
    name="Market Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    instructions=[
        "You are provided with a startup idea and the company's mission and objectives.",
        "Estimate the total addressable market (TAM), serviceable available market (SAM), and serviceable obtainable market (SOM).",
        "Define target customer segments and their characteristics.",
        "Search the web for resources and data to support your analysis.",
        "Provide specific market size estimates with supporting data sources.",
    ],
    add_history_to_context=True,
    add_datetime_to_context=True,
    output_schema=MarketResearch,
)

competitor_analysis_agent = Agent(
    name="Competitor Analysis Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    instructions=[
        "You are provided with a startup idea and market research data.",
        "Identify existing competitors in the market.",
        "Perform Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis for each competitor.",
        "Assess the startup's potential positioning relative to competitors.",
        "Search for recent competitor information and market positioning.",
    ],
    add_history_to_context=True,
    add_datetime_to_context=True,
    output_schema=CompetitorAnalysis,
    debug_mode=False,
)

report_agent = Agent(
    name="Report Generator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are provided with comprehensive data about a startup idea including clarification, market research, and competitor analysis.",
        "Synthesize all information into a comprehensive validation report.",
        "Provide clear executive summary, assessment, and actionable recommendations.",
        "Structure the report professionally with clear sections and insights.",
        "Include specific next steps for the entrepreneur.",
    ],
    add_history_to_context=True,
    add_datetime_to_context=True,
    output_schema=ValidationReport,
    debug_mode=False,
)


# --- Execution function ---
async def startup_validation_execution(
    workflow: Workflow,
    execution_input: WorkflowExecutionInput,
    startup_idea: str,
    **kwargs: Any,
) -> str:
    """Execute the complete startup idea validation workflow"""

    # Get inputs
    message: str = execution_input.input
    idea: str = startup_idea

    if not idea:
        return " No startup idea provided"

    print(f" Starting startup idea validation for: {idea}")
    print(f" Validation request: {message}")

    # Phase 1: Idea Clarification
    print("\n PHASE 1: IDEA CLARIFICATION & REFINEMENT")
    print("=" * 60)

    clarification_prompt = f"""
    {message}

    Please analyze and refine the following startup idea:

    STARTUP IDEA: {idea}

    Evaluate:
    1. The originality of this idea compared to existing solutions
    2. Define a clear mission statement for this startup
    3. Outline specific, measurable objectives
    Provide insights on how to strengthen and focus the core concept.
    """

    print(" Analyzing and refining the startup concept...")

    try:
        clarification_result = await idea_clarifier_agent.arun(clarification_prompt)
        idea_clarification = clarification_result.content

        print(" Idea clarification completed")
        print(f" Mission: {idea_clarification.mission[:100]}...")

    except Exception as e:
        return f" Failed to clarify idea: {str(e)}"

    # Phase 2: Market Research
    print("\n PHASE 2: MARKET RESEARCH & ANALYSIS")
    print("=" * 60)

    market_research_prompt = f"""
    Based on the refined startup idea and clarification below, conduct comprehensive market research:
    STARTUP IDEA: {idea}
    ORIGINALITY: {idea_clarification.originality}
    MISSION: {idea_clarification.mission}
    OBJECTIVES: {idea_clarification.objectives}
    Please research and provide:
    1. Total Addressable Market (TAM) - overall market size
    2. Serviceable Available Market (SAM) - portion you could serve
    3. Serviceable Obtainable Market (SOM) - realistic market share
    4. Target customer segments with detailed characteristics
    Use web search to find current market data and trends.
    """

    print(" Researching market size and customer segments...")

    try:
        market_result = await market_research_agent.arun(market_research_prompt)
        market_research = market_result.content

        print(" Market research completed")
        print(f" TAM: {market_research.total_addressable_market[:100]}...")

    except Exception as e:
        return f" Failed to complete market research: {str(e)}"

    # Phase 3: Competitor Analysis
    print("\n PHASE 3: COMPETITIVE LANDSCAPE ANALYSIS")
    print("=" * 60)

    competitor_prompt = f"""
    Based on the startup idea and market research below, analyze the competitive landscape:
    STARTUP IDEA: {idea}
    TAM: {market_research.total_addressable_market}
    SAM: {market_research.serviceable_available_market}
    SOM: {market_research.serviceable_obtainable_market}
    TARGET SEGMENTS: {market_research.target_customer_segments}
    Please research and provide:
    1. Identify direct and indirect competitors
    2. SWOT analysis for each major competitor
    3. Assessment of startup's potential competitive positioning
    4. Market gaps and opportunities
    Use web search to find current competitor information.
    """

    print(" Analyzing competitive landscape...")

    try:
        competitor_result = await competitor_analysis_agent.arun(competitor_prompt)
        competitor_analysis = competitor_result.content

        print(" Competitor analysis completed")
        print(f" Positioning: {competitor_analysis.positioning[:100]}...")

    except Exception as e:
        return f" Failed to complete competitor analysis: {str(e)}"

    # Phase 4: Final Validation Report
    print("\n PHASE 4: COMPREHENSIVE VALIDATION REPORT")
    print("=" * 60)

    report_prompt = f"""
    Synthesize all the research and analysis into a comprehensive startup validation report:

    STARTUP IDEA: {idea}

    IDEA CLARIFICATION:
    - Originality: {idea_clarification.originality}
    - Mission: {idea_clarification.mission}
    - Objectives: {idea_clarification.objectives}
    MARKET RESEARCH:
    - TAM: {market_research.total_addressable_market}
    - SAM: {market_research.serviceable_available_market}
    - SOM: {market_research.serviceable_obtainable_market}
    - Target Segments: {market_research.target_customer_segments}
    COMPETITOR ANALYSIS:
    - Competitors: {competitor_analysis.competitors}
    - SWOT: {competitor_analysis.swot_analysis}
    - Positioning: {competitor_analysis.positioning}
    Create a professional validation report with:
    1. Executive summary
    2. Idea assessment (strengths/weaknesses)
    3. Market opportunity analysis
    4. Competitive landscape overview
    5. Strategic recommendations
    6. Specific next steps for the entrepreneur
    """

    print(" Generating comprehensive validation report...")

    try:
        final_result = await report_agent.arun(report_prompt)
        validation_report = final_result.content

        print(" Validation report completed")

    except Exception as e:
        return f" Failed to generate final report: {str(e)}"

    # Final summary
    summary = f"""
     STARTUP IDEA VALIDATION COMPLETED!
     Validation Summary:
     Startup Idea: {idea}
     Idea Clarification:  Completed
     Market Research:  Completed
     Competitor Analysis:  Completed
     Final Report:  Generated

     Key Market Insights:
     TAM: {market_research.total_addressable_market[:150]}...
     Target Segments: {market_research.target_customer_segments[:150]}...

     Competitive Positioning:
    {competitor_analysis.positioning[:200]}...

     COMPREHENSIVE VALIDATION REPORT:

    ## Executive Summary
    {validation_report.executive_summary}

    ## Idea Assessment
    {validation_report.idea_assessment}

    ## Market Opportunity
    {validation_report.market_opportunity}

    ## Competitive Landscape
    {validation_report.competitive_landscape}

    ## Strategic Recommendations
    {validation_report.recommendations}

    ## Next Steps
    {validation_report.next_steps}

     Disclaimer: This validation is for informational purposes only. Conduct additional due diligence before making investment decisions.
    """

    return summary


# --- Workflow definition ---
startup_validation_workflow = Workflow(
    name="Startup Idea Validator",
    description="Comprehensive startup idea validation with market research and competitive analysis",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflows.db",
    ),
    steps=startup_validation_execution,
    session_state={},  # Initialize empty workflow session state
)


if __name__ == "__main__":

    async def main():
        from rich.prompt import Prompt

        # Get idea from user
        idea = Prompt.ask(
            "[bold]What is your startup idea?[/bold]\n",
            default="A marketplace for Christmas Ornaments made from leather",
        )

        print(" Testing Startup Idea Validator with New Workflow Structure")
        print("=" * 70)

        result = await startup_validation_workflow.arun(
            input="Please validate this startup idea with comprehensive market research and competitive analysis",
            startup_idea=idea,
        )

        pprint_run_response(result, markdown=True)

    asyncio.run(main())
```

---

<a name="getting_started--01_basic_agentpy"></a>

### `getting_started/01_basic_agent.py`

```python
""" Basic Agent Example - Creating a Quirky News Reporter

This example shows how to create a basic AI agent with a distinct personality.
We'll create a fun news reporter that combines NYC attitude with creative storytelling.
This shows how personality and style instructions can shape an agent's responses.

Run `pip install openai agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat

# Create our News Reporter with a fun personality
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! 
        Think of yourself as a mix between a witty comedian and a sharp journalist.

        Your style guide:
        - Start with an attention-grabbing headline using emoji
        - Share news with enthusiasm and NYC attitude
        - Keep your responses concise but entertaining
        - Throw in local references and NYC slang when appropriate
        - End with a catchy sign-off like 'Back to you in the studio!' or 'Reporting live from the Big Apple!'

        Remember to verify all facts while keeping that NYC energy high!\
    """),
    markdown=True,
)

# Example usage
agent.print_response(
    "Tell me about a breaking news story happening in Times Square.", stream=True
)

# More example prompts to try:
"""
Try these fun scenarios:
1. "What's the latest food trend taking over Brooklyn?"
2. "Tell me about a peculiar incident on the subway today"
3. "What's the scoop on the newest rooftop garden in Manhattan?"
4. "Report on an unusual traffic jam caused by escaped zoo animals"
5. "Cover a flash mob wedding proposal at Grand Central"
"""
```

---

<a name="getting_started--02_agent_with_toolspy"></a>

### `getting_started/02_agent_with_tools.py`

```python
""" Agent with Tools - Your AI News Buddy that can search the web

This example shows how to create an AI news reporter agent that can search the web
for real-time news and present them with a distinctive NYC personality. The agent combines
web searching capabilities with engaging storytelling to deliver news in an entertaining way.

Run `pip install openai ddgs agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

# Create a News Reporter Agent with a fun personality
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! 
        Think of yourself as a mix between a witty comedian and a sharp journalist.

        Follow these guidelines for every report:
        1. Start with an attention-grabbing headline using relevant emoji
        2. Use the search tool to find current, accurate information
        3. Present news with authentic NYC enthusiasm and local flavor
        4. Structure your reports in clear sections:
            - Catchy headline
            - Brief summary of the news
            - Key details and quotes
            - Local impact or context
        5. Keep responses concise but informative (2-3 paragraphs max)
        6. Include NYC-style commentary and local references
        7. End with a signature sign-off phrase

        Sign-off examples:
        - 'Back to you in the studio, folks!'
        - 'Reporting live from the city that never sleeps!'
        - 'This is [Your Name], live from the heart of Manhattan!'

        Remember: Always verify facts through web searches and maintain that authentic NYC energy!\
    """),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Example usage
agent.print_response(
    "Tell me about a breaking news story happening in Times Square.", stream=True
)

# More example prompts to try:
"""
Try these engaging news queries:
1. "What's the latest development in NYC's tech scene?"
2. "Tell me about any upcoming events at Madison Square Garden"
3. "What's the weather impact on NYC today?"
4. "Any updates on the NYC subway system?"
5. "What's the hottest food trend in Manhattan right now?"
"""
```

---

<a name="getting_started--03_agent_with_knowledgepy"></a>

### `getting_started/03_agent_with_knowledge.py`

```python
""" Agent with Knowledge - Your AI Cooking Assistant!

This example shows how to create an AI cooking assistant that combines knowledge from a
curated recipe database with web searching capabilities. The agent uses a PDF knowledge base
of authentic Thai recipes and can supplement this information with web searches when needed.

Run `pip install openai lancedb tantivy pypdf ddgs agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="recipe_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# Create a Recipe Expert Agent with knowledge of Thai recipes
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are a passionate and knowledgeable Thai cuisine expert! 
        Think of yourself as a combination of a warm, encouraging cooking instructor,
        a Thai food historian, and a cultural ambassador.

        Follow these steps when answering questions:
        1. If the user asks a about Thai cuisine, ALWAYS search your knowledge base for authentic Thai recipes and cooking information
        2. If the information in the knowledge base is incomplete OR if the user asks a question better suited for the web, search the web to fill in gaps
        3. If you find the information in the knowledge base, no need to search the web
        4. Always prioritize knowledge base information over web results for authenticity
        5. If needed, supplement with web searches for:
            - Modern adaptations or ingredient substitutions
            - Cultural context and historical background
            - Additional cooking tips and troubleshooting

        Communication style:
        1. Start each response with a relevant cooking emoji
        2. Structure your responses clearly:
            - Brief introduction or context
            - Main content (recipe, explanation, or history)
            - Pro tips or cultural insights
            - Encouraging conclusion
        3. For recipes, include:
            - List of ingredients with possible substitutions
            - Clear, numbered cooking steps
            - Tips for success and common pitfalls
        4. Use friendly, encouraging language

        Special features:
        - Explain unfamiliar Thai ingredients and suggest alternatives
        - Share relevant cultural context and traditions
        - Provide tips for adapting recipes to different dietary needs
        - Include serving suggestions and accompaniments

        End each response with an uplifting sign-off like:
        - 'Happy cooking!  (Enjoy your meal)!'
        - 'May your Thai cooking adventure bring joy!'
        - 'Enjoy your homemade Thai feast!'

        Remember:
        - Always verify recipe authenticity with the knowledge base
        - Clearly indicate when information comes from web sources
        - Be encouraging and supportive of home cooks at all skill levels\
    """),
    knowledge=knowledge,
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
agent.print_response("What is the history of Thai curry?", stream=True)
agent.print_response("What ingredients do I need for Pad Thai?", stream=True)

# More example prompts to try:
"""
Explore Thai cuisine with these queries:
1. "What are the essential spices and herbs in Thai cooking?"
2. "Can you explain the different types of Thai curry pastes?"
3. "How do I make mango sticky rice dessert?"
4. "What's the proper way to cook Thai jasmine rice?"
5. "Tell me about regional differences in Thai cuisine"
"""
```

---

<a name="getting_started--04_write_your_own_toolpy"></a>

### `getting_started/04_write_your_own_tool.py`

```python
""" Writing Your Own Tool - An Example Using Hacker News API

This example shows how to create and use your own custom tool with Agno.
You can replace the Hacker News functionality with any API or service you want!

Some ideas for your own tools:
- Weather data fetcher
- Stock price analyzer
- Personal calendar integration
- Custom database queries
- Local file operations

Run `pip install openai httpx agno` to install dependencies.
"""

import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to return. Defaults to 10.

    Returns:
        str: JSON string of top stories.
    """

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


# Create a Tech News Reporter Agent with a Silicon Valley personality
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are a tech-savvy Hacker News reporter with a passion for all things technology! 
        Think of yourself as a mix between a Silicon Valley insider and a tech journalist.

        Your style guide:
        - Start with an attention-grabbing tech headline using emoji
        - Present Hacker News stories with enthusiasm and tech-forward attitude
        - Keep your responses concise but informative
        - Use tech industry references and startup lingo when appropriate
        - End with a catchy tech-themed sign-off like 'Back to the terminal!' or 'Pushing to production!'

        Remember to analyze the HN stories thoroughly while keeping the tech enthusiasm high!\
    """),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

# Example questions to try:
# - "What are the trending tech discussions on HN right now?"
# - "Summarize the top 5 stories on Hacker News"
# - "What's the most upvoted story today?"
agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)
```

---

<a name="getting_started--05_structured_outputpy"></a>

### `getting_started/05_structured_output.py`

```python
""" Agent with Structured Output - Your AI Movie Script Generator

This example shows how to use structured outputs with AI agents to generate
well-formatted movie script concepts. It shows two approaches:
1. JSON Mode: Traditional JSON response parsing
2. Structured Output: Enhanced structured data handling

Run `pip install openai agno` to install dependencies.
"""

from textwrap import dedent
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ...,
        description="A richly detailed, atmospheric description of the movie's primary location and time period. Include sensory details and mood.",
    )
    ending: str = Field(
        ...,
        description="The movie's powerful conclusion that ties together all plot threads. Should deliver emotional impact and satisfaction.",
    )
    genre: str = Field(
        ...,
        description="The film's primary and secondary genres (e.g., 'Sci-fi Thriller', 'Romantic Comedy'). Should align with setting and tone.",
    )
    name: str = Field(
        ...,
        description="An attention-grabbing, memorable title that captures the essence of the story and appeals to target audience.",
    )
    characters: List[str] = Field(
        ...,
        description="4-6 main characters with distinctive names and brief role descriptions (e.g., 'Sarah Chen - brilliant quantum physicist with a dark secret').",
    )
    storyline: str = Field(
        ...,
        description="A compelling three-sentence plot summary: Setup, Conflict, and Stakes. Hook readers with intrigue and emotion.",
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are an acclaimed Hollywood screenwriter known for creating unforgettable blockbusters! 
        With the combined storytelling prowess of Christopher Nolan, Aaron Sorkin, and Quentin Tarantino,
        you craft unique stories that captivate audiences worldwide.

        Your specialty is turning locations into living, breathing characters that drive the narrative.\
    """),
    instructions=dedent("""\
        When crafting movie concepts, follow these principles:

        1. Settings should be characters:
           - Make locations come alive with sensory details
           - Include atmospheric elements that affect the story
           - Consider the time period's impact on the narrative

        2. Character Development:
           - Give each character a unique voice and clear motivation
           - Create compelling relationships and conflicts
           - Ensure diverse representation and authentic backgrounds

        3. Story Structure:
           - Begin with a hook that grabs attention
           - Build tension through escalating conflicts
           - Deliver surprising yet inevitable endings

        4. Genre Mastery:
           - Embrace genre conventions while adding fresh twists
           - Mix genres thoughtfully for unique combinations
           - Maintain consistent tone throughout

        Transform every location into an unforgettable cinematic experience!\
    """),
    output_schema=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are an acclaimed Hollywood screenwriter known for creating unforgettable blockbusters! 
        With the combined storytelling prowess of Christopher Nolan, Aaron Sorkin, and Quentin Tarantino,
        you craft unique stories that captivate audiences worldwide.

        Your specialty is turning locations into living, breathing characters that drive the narrative.\
    """),
    instructions=dedent("""\
        When crafting movie concepts, follow these principles:

        1. Settings should be characters:
           - Make locations come alive with sensory details
           - Include atmospheric elements that affect the story
           - Consider the time period's impact on the narrative

        2. Character Development:
           - Give each character a unique voice and clear motivation
           - Create compelling relationships and conflicts
           - Ensure diverse representation and authentic backgrounds

        3. Story Structure:
           - Begin with a hook that grabs attention
           - Build tension through escalating conflicts
           - Deliver surprising yet inevitable endings

        4. Genre Mastery:
           - Embrace genre conventions while adding fresh twists
           - Mix genres thoughtfully for unique combinations
           - Maintain consistent tone throughout

        Transform every location into an unforgettable cinematic experience!\
    """),
    output_schema=MovieScript,
)

# Example usage with different locations
json_mode_agent.print_response("Tokyo", stream=True)
structured_output_agent.print_response("Ancient Rome", stream=True)

# More examples to try:
"""
Creative location prompts to explore:
1. "Underwater Research Station" - For a claustrophobic sci-fi thriller
2. "Victorian London" - For a gothic mystery
3. "Dubai 2050" - For a futuristic heist movie
4. "Antarctic Research Base" - For a survival horror story
5. "Caribbean Island" - For a tropical adventure romance
"""

# To get the response in a variable:
# from rich.pretty import pprint

# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)
```

---

<a name="getting_started--06_agent_with_storagepy"></a>

### `getting_started/06_agent_with_storage.py`

```python
""" Agent with Storage - Your AI Thai Cooking Assistant!

This example shows how to create an AI cooking assistant that combines knowledge from a
curated recipe database with web searching capabilities. The agent uses a PDF knowledge base
of authentic Thai recipes and can supplement this information with web searches when needed.

Example prompts to try:
- "How do I make authentic Pad Thai?"
- "What's the difference between red and green curry?"
- "Can you explain what galangal is and possible substitutes?"
- "Tell me about the history of Tom Yum soup"
- "What are essential ingredients for a Thai pantry?"
- "How do I make Thai basil chicken (Pad Kra Pao)?"

Run `pip install openai lancedb tantivy pypdf ddgs sqlalchemy agno` to install dependencies.
"""

from textwrap import dedent
from typing import List, Optional

import typer
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.session import AgentSession
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print

agent_knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="recipe_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Add content to the knowledge
agent_knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

# Setup the database
db = SqliteDb(db_file="tmp/agents.db")


def recipe_agent(user: str = "user"):
    session_id: Optional[str] = None

    # Ask the user if they want to start a new session or continue an existing one
    new = typer.confirm("Do you want to start a new session?")

    if not new:
        existing_sessions: List[AgentSession] = db.get_sessions(  # type: ignore
            user_id=user, session_type=SessionType.AGENT
        )
        if len(existing_sessions) > 0:
            session_id = existing_sessions[0].session_id

    agent = Agent(
        user_id=user,
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        instructions=dedent("""\
            You are a passionate and knowledgeable Thai cuisine expert! 
            Think of yourself as a combination of a warm, encouraging cooking instructor,
            a Thai food historian, and a cultural ambassador.

            Follow these steps when answering questions:
            1. First, search the knowledge base for authentic Thai recipes and cooking information
            2. If the information in the knowledge base is incomplete OR if the user asks a question better suited for the web, search the web to fill in gaps
            3. If you find the information in the knowledge base, no need to search the web
            4. Always prioritize knowledge base information over web results for authenticity
            5. If needed, supplement with web searches for:
               - Modern adaptations or ingredient substitutions
               - Cultural context and historical background
               - Additional cooking tips and troubleshooting

            Communication style:
            1. Start each response with a relevant cooking emoji
            2. Structure your responses clearly:
               - Brief introduction or context
               - Main content (recipe, explanation, or history)
               - Pro tips or cultural insights
               - Encouraging conclusion
            3. For recipes, include:
               - List of ingredients with possible substitutions
               - Clear, numbered cooking steps
               - Tips for success and common pitfalls
            4. Use friendly, encouraging language

            Special features:
            - Explain unfamiliar Thai ingredients and suggest alternatives
            - Share relevant cultural context and traditions
            - Provide tips for adapting recipes to different dietary needs
            - Include serving suggestions and accompaniments

            End each response with an uplifting sign-off like:
            - 'Happy cooking!  (Enjoy your meal)!'
            - 'May your Thai cooking adventure bring joy!'
            - 'Enjoy your homemade Thai feast!'

            Remember:
            - Always verify recipe authenticity with the knowledge base
            - Clearly indicate when information comes from web sources
            - Be encouraging and supportive of home cooks at all skill levels\
        """),
        db=db,
        knowledge=agent_knowledge,
        tools=[DuckDuckGoTools()],
        # To provide the agent with the chat history
        # We can either:
        # 1. Provide the agent with a tool to read the chat history
        # 2. Automatically add the chat history to the messages sent to the model
        #
        # 1. Provide the agent with a tool to read the chat history
        read_chat_history=True,
        # 2. Automatically add the chat history to the messages sent to the model
        # add_history_to_context=True,
        # Number of historical responses to add to the messages.
        # num_history_runs=3,
        markdown=True,
    )

    print("You are about to chat with an agent!")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"Started Session: {session_id}\n")
        else:
            print("Started Session\n")
    else:
        print(f"Continuing Session: {session_id}\n")

    # Runs the agent as a command line application
    agent.cli_app(markdown=True)


if __name__ == "__main__":
    typer.run(recipe_agent)
```

---

<a name="getting_started--07_agent_statepy"></a>

### `getting_started/07_agent_state.py`

```python
"""Test session state management with a simple counter"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat


def increment_counter(session_state) -> str:
    """Increment the counter in session state."""
    # Initialize counter if it doesn't exist
    if "count" not in session_state:
        session_state["count"] = 0

    # Increment the counter
    session_state["count"] += 1

    return f"Counter incremented! Current count: {session_state['count']}"


def get_counter(session_state) -> str:
    """Get the current counter value."""
    count = session_state.get("count", 0)
    return f"Current count: {count}"


# Create an Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Initialize the session state with a counter starting at 0
    session_state={"count": 0},
    tools=[increment_counter, get_counter],
    # Use variables from the session state in the instructions
    instructions="You can increment and check a counter. Current count is: {count}",
    # Important: Resolve the state in the messages so the agent can see state changes
    resolve_in_context=True,
    markdown=True,
)

# Test the counter functionality
print("Testing counter functionality...")
agent.print_response(
    "Let's increment the counter 3 times and observe the state changes!", stream=True
)
print(f"Final session state: {agent.get_session_state()}")
```

---

<a name="getting_started--08_agent_contextpy"></a>

### `getting_started/08_agent_context.py`

```python
""" Agent with Context

This example shows how to inject external dependencies into an agent.
The context is evaluated when the agent is run, acting like dependency injection for Agents.

Run `pip install openai agno` to install dependencies.
"""

import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the context is evaluated when the agent is run,
    # think of it as dependency injection for Agents
    dependencies={"top_hackernews_stories": get_top_hackernews_stories},
    # Alternatively, you can manually add the context to the instructions. This gets resolved automatically
    instructions=dedent("""\
        You are an insightful tech trend observer! 

        Here are the top stories on HackerNews:
        {top_hackernews_stories}\
    """),
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)
```

---

<a name="getting_started--09_agent_sessionpy"></a>

### `getting_started/09_agent_session.py`

```python
""" Persistent Chat History i.e. Session Memory

This example shows how to create an agent with persistent memory stored in a SQLite database.
We set the session_id on the agent when resuming the conversation, this way the previous chat history is preserved.

Key features:
- Stores conversation history in a SQLite database
- Continues conversations across multiple sessions
- References previous context in responses

Run `pip install openai sqlalchemy agno` to install dependencies.
"""

import json
from typing import List, Optional

import typer
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.session import AgentSession
from rich import print
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel
from rich.prompt import Prompt

console = Console()


def create_agent(user: str = "user"):
    session_id: Optional[str] = None

    # Ask if user wants to start new session or continue existing one
    new = typer.confirm("Do you want to start a new session?")

    # Get existing session if user doesn't want a new one
    db = SqliteDb(db_file="tmp/agents.db")

    if not new:
        existing_sessions: List[AgentSession] = db.get_sessions(
            user_id=user, session_type=SessionType.AGENT
        )  # type: ignore
        if len(existing_sessions) > 0:
            session_id = existing_sessions[0].session_id

    agent = Agent(
        user_id=user,
        # Set the session_id on the agent to resume the conversation
        session_id=session_id,
        model=OpenAIChat(id="gpt-4o"),
        db=db,
        # Add chat history to messages
        add_history_to_context=True,
        num_history_runs=3,
        markdown=True,
    )

    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"Started Session: {session_id}\n")
        else:
            print("Started Session\n")
    else:
        print(f"Continuing Session: {session_id}\n")

    return agent


def print_messages(agent):
    """Print the current chat history in a formatted panel"""
    console.print(
        Panel(
            JSON(
                json.dumps(
                    [
                        m.model_dump(include={"role", "content"})
                        for m in agent.get_messages_for_session()
                    ]
                ),
                indent=4,
            ),
            title=f"Chat History for session_id: {agent.session_id}",
            expand=True,
        )
    )


def main(user: str = "user"):
    agent = create_agent(user)

    print("Chat with an OpenAI agent!")
    exit_on = ["exit", "quit", "bye"]
    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in exit_on:
            break

        agent.print_response(input=message, stream=True, markdown=True)
        print_messages(agent)


if __name__ == "__main__":
    typer.run(main)
```

---

<a name="getting_started--10_user_memories_and_summariespy"></a>

### `getting_started/10_user_memories_and_summaries.py`

```python
""" Long Term User Memories and Session Summaries

This example shows how to create an agent with persistent memory that stores:
1. Personalized user memories - facts and preferences learned about specific users
2. Session summaries - key points and context from conversations
3. Chat history - stored in SQLite for persistence

Key features:
- Stores user-specific memories in SQLite database
- Maintains session summaries for context
- Continues conversations across sessions with memory
- References previous context and user information in responses

Examples:
User: "My name is John and I live in NYC"
Agent: *Creates memory about John's location*

User: "What do you remember about me?"
Agent: *Recalls previous memories about John*

Run: `pip install openai sqlalchemy agno` to install dependencies
"""

import json
from textwrap import dedent
from typing import List, Optional

import typer
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.session import AgentSession
from rich.console import Console
from rich.json import JSON
from rich.panel import Panel
from rich.prompt import Prompt


def create_agent(user: str = "user"):
    session_id: Optional[str] = None

    # Ask if user wants to start new session or continue existing one
    new = typer.confirm("Do you want to start a new session?")

    # Initialize storage for both agent sessions and memories
    db = SqliteDb(db_file="tmp/agents.db")

    if not new:
        existing_sessions: List[AgentSession] = db.get_sessions(
            user_id=user, session_type=SessionType.AGENT
        )  # type: ignore
        if len(existing_sessions) > 0:
            session_id = existing_sessions[0].session_id

    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        user_id=user,
        session_id=session_id,
        enable_user_memories=True,
        enable_session_summaries=True,
        db=db,
        add_history_to_context=True,
        num_history_runs=3,
        # Enhanced system prompt for better personality and memory usage
        description=dedent("""\
        You are a helpful and friendly AI assistant with excellent memory.
        - Remember important details about users and reference them naturally
        - Maintain a warm, positive tone while being precise and helpful
        - When appropriate, refer back to previous conversations and memories
        - Always be truthful about what you remember or don't remember"""),
    )

    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"Started Session: {session_id}\n")
        else:
            print("Started Session\n")
    else:
        print(f"Continuing Session: {session_id}\n")

    return agent


def print_agent_memory(agent):
    """Print the current state of agent's memory systems"""
    console = Console()

    # Print chat history
    messages = agent.get_messages_for_session()
    console.print(
        Panel(
            JSON(
                json.dumps(
                    [m.model_dump(include={"role", "content"}) for m in messages],
                    indent=4,
                ),
            ),
            title=f"Chat History for session_id: {agent.session_id}",
            expand=True,
        )
    )

    # Print user memories
    user_memories = agent.get_user_memories(user_id=agent.user_id)
    if user_memories:
        memories_data = [memory.to_dict() for memory in user_memories]
        console.print(
            Panel(
                JSON(json.dumps(memories_data, indent=4)),
                title=f"Memories for user_id: {agent.user_id}",
                expand=True,
            )
        )

    # Print session summaries
    try:
        session_summary = agent.get_session_summary()
        if session_summary:
            console.print(
                Panel(
                    JSON(
                        json.dumps(session_summary.to_dict(), indent=4),
                    ),
                    title=f"Session Summary for session_id: {agent.session_id}",
                    expand=True,
                )
            )
        else:
            console.print(
                "Session summary: Not yet created (summaries are created after multiple interactions)"
            )
    except Exception as e:
        console.print(f"Session summary error: {e}")


def main(user: str = "user"):
    """Interactive chat loop with memory display"""
    agent = create_agent(user)

    print("Try these example inputs:")
    print("- 'My name is [name] and I live in [city]'")
    print("- 'I love [hobby/interest]'")
    print("- 'What do you remember about me?'")
    print("- 'What have we discussed so far?'\n")

    exit_on = ["exit", "quit", "bye"]
    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in exit_on:
            break

        agent.print_response(input=message, stream=True, markdown=True)
        print_agent_memory(agent)


if __name__ == "__main__":
    typer.run(main)
```

---

<a name="getting_started--11_retry_function_callpy"></a>

### `getting_started/11_retry_function_call.py`

```python
from typing import Iterator

from agno.agent import Agent
from agno.exceptions import RetryAgentRun
from agno.tools import FunctionCall, tool

num_calls = 0


def pre_hook(fc: FunctionCall):
    global num_calls

    print(f"Pre-hook: {fc.function.name}")
    print(f"Arguments: {fc.arguments}")
    num_calls += 1
    if num_calls < 2:
        raise RetryAgentRun(
            "This wasn't interesting enough, please retry with a different argument"
        )


@tool(pre_hook=pre_hook)
def print_something(something: str) -> Iterator[str]:
    print(something)
    yield f"I have printed {something}"


agent = Agent(tools=[print_something], markdown=True)
agent.print_response("Print something interesting", stream=True)
```

---

<a name="getting_started--12_human_in_the_looppy"></a>

### `getting_started/12_human_in_the_loop.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Add pre-hooks to tools for user confirmation
- Handle user input during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

console = Console()


@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)


# Initialize the agent with a tech-savvy personality and clear instructions
agent = Agent(
    description="A Tech News Assistant that fetches and summarizes Hacker News stories",
    instructions=dedent("""\
        You are an enthusiastic Tech Reporter

        Your responsibilities:
        - Present Hacker News stories in an engaging and informative way
        - Provide clear summaries of the information you gather

        Style guide:
        - Use emoji to make your responses more engaging
        - Keep your summaries concise but informative
        - End with a friendly tech-themed sign-off\
    """),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

# Example questions to try:
# - "What are the top 3 HN stories right now?"
# - "Show me the most recent story from Hacker News"
# - "Get the top 5 stories (you can try accepting and declining the confirmation)"
response = agent.run("What are the top 2 hackernews stories?")
if response.is_paused:
    for tool in response.tools:  # type: ignore
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

        if message == "n":
            break
        else:
            # We update the tools in place
            tool.confirmed = True

    run_response = agent.continue_run(run_response=response)
    pprint.pprint_run_response(run_response)
```

---

<a name="getting_started--13_image_agentpy"></a>

### `getting_started/13_image_agent.py`

```python
""" AI Image Reporter - Your Visual Analysis & News Companion!

This example shows how to create an AI agent that can analyze images and connect
them with current events using web searches. Perfect for:
1. News reporting and journalism
2. Travel and tourism content
3. Social media analysis
4. Educational presentations
5. Event coverage

Example images to try:
- Famous landmarks (Eiffel Tower, Taj Mahal, etc.)
- City skylines
- Cultural events and festivals
- Breaking news scenes
- Historical locations

Run `pip install ddgs agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
        You are a world-class visual journalist and cultural correspondent with a gift
        for bringing images to life through storytelling!  With the observational skills
        of a detective and the narrative flair of a bestselling author, you transform visual
        analysis into compelling stories that inform and captivate.\
    """),
    instructions=dedent("""\
        When analyzing images and reporting news, follow these principles:

        1. Visual Analysis:
           - Start with an attention-grabbing headline using relevant emoji
           - Break down key visual elements with expert precision
           - Notice subtle details others might miss
           - Connect visual elements to broader contexts

        2. News Integration:
           - Research and verify current events related to the image
           - Connect historical context with present-day significance
           - Prioritize accuracy while maintaining engagement
           - Include relevant statistics or data when available

        3. Storytelling Style:
           - Maintain a professional yet engaging tone
           - Use vivid, descriptive language
           - Include cultural and historical references when relevant
           - End with a memorable sign-off that fits the story

        4. Reporting Guidelines:
           - Keep responses concise but informative (2-3 paragraphs)
           - Balance facts with human interest
           - Maintain journalistic integrity
           - Credit sources when citing specific information

        Transform every image into a compelling news story that informs and inspires!\
    """),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Example usage with a famous landmark
agent.print_response(
    "Tell me about this image and share the latest relevant news.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

# More examples to try:
"""
Sample prompts to explore:
1. "What's the historical significance of this location?"
2. "How has this place changed over time?"
3. "What cultural events happen here?"
4. "What's the architectural style and influence?"
5. "What recent developments affect this area?"

Sample image URLs to analyze:
1. Eiffel Tower: "https://upload.wikimedia.org/wikipedia/commons/8/85/Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg"
2. Taj Mahal: "https://upload.wikimedia.org/wikipedia/commons/b/bd/Taj_Mahal%2C_Agra%2C_India_edit3.jpg"
3. Golden Gate Bridge: "https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
"""

# To get the response in a variable:
# from rich.pretty import pprint
# response = agent.run(
#     "Analyze this landmark's architecture and recent news.",
#     images=[Image(url="YOUR_IMAGE_URL")],
# )
# pprint(response.content)
```

---

<a name="getting_started--14_generate_imagepy"></a>

### `getting_started/14_generate_image.py`

```python
""" Image Generation with DALL-E - Creating AI Art with Agno

This example shows how to create an AI agent that generates images using DALL-E.
You can use this agent to create various types of images, from realistic photos to artistic
illustrations and creative concepts.

Example prompts to try:
- "Create a surreal painting of a floating city in the clouds at sunset"
- "Generate a photorealistic image of a cozy coffee shop interior"
- "Design a cute cartoon mascot for a tech startup"
- "Create an artistic portrait of a cyberpunk samurai"

Run `pip install openai agno` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.tools.dalle import DalleTools

# Create an Creative AI Artist Agent
image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description=dedent("""\
        You are an experienced AI artist with expertise in various artistic styles,
        from photorealism to abstract art. You have a deep understanding of composition,
        color theory, and visual storytelling.\
    """),
    instructions=dedent("""\
        As an AI artist, follow these guidelines:
        1. Analyze the user's request carefully to understand the desired style and mood
        2. Before generating, enhance the prompt with artistic details like lighting, perspective, and atmosphere
        3. Use the `create_image` tool with detailed, well-crafted prompts
        4. Provide a brief explanation of the artistic choices made
        5. If the request is unclear, ask for clarification about style preferences

        Always aim to create visually striking and meaningful images that capture the user's vision!\
    """),
    markdown=True,
    db=SqliteDb(session_table="test_agent", db_file="tmp/test.db"),
)

# Example usage
image_agent.print_response(
    "Create a magical library with floating books and glowing crystals",
)

# Retrieve and display generated images
run_response = image_agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput):
    for image_response in run_response.images:
        image_url = image_response.url
        print("image_url: ", image_url)
else:
    print("No images found or images is not a list")

# More example prompts to try:
"""
Try these creative prompts:
1. "Generate a steampunk-style robot playing a violin"
2. "Design a peaceful zen garden during cherry blossom season"
3. "Create an underwater city with bioluminescent buildings"
4. "Generate a cozy cabin in a snowy forest at night"
5. "Create a futuristic cityscape with flying cars and skyscrapers"
"""
```

---

<a name="getting_started--15_generate_videopy"></a>

### `getting_started/15_generate_video.py`

```python
""" Video Generation with ModelsLabs - Creating AI Videos with Agno

This example shows how to create an AI agent that generates videos using ModelsLabs.
You can use this agent to create various types of short videos, from animated scenes
to creative visual stories.

Example prompts to try:
- "Create a serene video of waves crashing on a beach at sunset"
- "Generate a magical video of butterflies flying in a enchanted forest"
- "Create a timelapse of a blooming flower in a garden"
- "Generate a video of northern lights dancing in the night sky"

Run `pip install openai agno` to install dependencies.
Remember to set your ModelsLabs API key in the environment variable `MODELS_LAB_API_KEY`.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models_labs import ModelsLabTools

# Create a Creative AI Video Director Agent
video_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ModelsLabTools()],
    description=dedent("""\
        You are an experienced AI video director with expertise in various video styles,
        from nature scenes to artistic animations. You have a deep understanding of motion,
        timing, and visual storytelling through video content.\
    """),
    instructions=dedent("""\
        As an AI video director, follow these guidelines:
        1. Analyze the user's request carefully to understand the desired style and mood
        2. Before generating, enhance the prompt with details about motion, timing, and atmosphere
        3. Use the `generate_media` tool with detailed, well-crafted prompts
        4. Provide a brief explanation of the creative choices made
        5. If the request is unclear, ask for clarification about style preferences

        The video will be displayed in the UI automatically below your response.
        Always aim to create captivating and meaningful videos that bring the user's vision to life!\
    """),
    markdown=True,
)

# Example usage
video_agent.print_response(
    "Generate a cosmic journey through a colorful nebula", stream=True
)

# Retrieve and display generated videos
run_response = video_agent.get_last_run_output()
if run_response and run_response.videos:
    for video in run_response.videos:
        print(f"Generated video URL: {video.url}")

# More example prompts to try:
"""
Try these creative prompts:
1. "Create a video of autumn leaves falling in a peaceful forest"
2. "Generate a video of a cat playing with a ball"
3. "Create a video of a peaceful koi pond with rippling water"
4. "Generate a video of a cozy fireplace with dancing flames"
5. "Create a video of a mystical portal opening in a magical realm"
"""
```

---

<a name="getting_started--16_audio_input_outputpy"></a>

### `getting_started/16_audio_input_output.py`

```python
""" Audio Input/Output with GPT-4 - Creating Voice Interactions with Agno

This example shows how to create an AI agent that can process audio input and generate
audio responses. You can use this agent for various voice-based interactions, from analyzing
speech content to generating natural-sounding responses.

Example audio interactions to try:
- Upload a recording of a conversation for analysis
- Have the agent respond to questions with voice output
- Process different languages and accents
- Analyze tone and emotion in speech

Run `pip install openai requests agno` to install dependencies.
"""

from textwrap import dedent

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

# Create an AI Voice Interaction Agent
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    description=dedent("""\
        You are an expert in audio processing and voice interaction, capable of understanding
        and analyzing spoken content while providing natural, engaging voice responses.
        You excel at comprehending context, emotion, and nuance in speech.\
    """),
    instructions=dedent("""\
        As a voice interaction specialist, follow these guidelines:
        1. Listen carefully to audio input to understand both content and context
        2. Provide clear, concise responses that address the main points
        3. When generating voice responses, maintain a natural, conversational tone
        4. Consider the speaker's tone and emotion in your analysis
        5. If the audio is unclear, ask for clarification

        Focus on creating engaging and helpful voice interactions!\
    """),
)

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()

# Process the audio and get a response
run_response = agent.run(
    "What's in this recording? Please analyze the content and tone.",
    audio=[Audio(content=response.content, format="wav")],
)

# Save the audio response if available
if run_response.response_audio is not None:
    write_audio_to_file(
        audio=run_response.response_audio.content, filename="tmp/response.wav"
    )

# More example interactions to try:
"""
Try these voice interaction scenarios:
1. "Can you summarize the main points discussed in this recording?"
2. "What emotions or tone do you detect in the speaker's voice?"
3. "Please provide a detailed analysis of the speech patterns and clarity"
4. "Can you identify any background noises or audio quality issues?"
5. "What is the overall context and purpose of this recording?"

Note: You can use your own audio files by converting them to base64 format.
Example for using your own audio file:

with open('your_audio.wav', 'rb') as audio_file:
    audio_data = audio_file.read()
    agent.run("Analyze this audio", audio=[Audio(content=audio_data, format="wav")])
"""
```

---

<a name="getting_started--17_agent_teampy"></a>

### `getting_started/17_agent_team.py`

```python
""" Multi-Agent Team - Your Professional News & Finance Squad!

This example shows how to create a powerful team of AI agents working together
to provide comprehensive financial analysis and news reporting. The team consists of:
1. Web Agent: Searches and analyzes latest news
2. Finance Agent: Analyzes financial data and market trends
3. Lead Editor: Coordinates and combines insights from both agents

Run: `pip install openai ddgs yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=dedent("""\
        You are an experienced web researcher and news analyst! 

        Follow these steps when searching for information:
        1. Start with the most recent and relevant sources
        2. Cross-reference information from multiple sources
        3. Prioritize reputable news outlets and official sources
        4. Always cite your sources with links
        5. Focus on market-moving news and significant developments

        Your style guide:
        - Present information in a clear, journalistic style
        - Use bullet points for key takeaways
        - Include relevant quotes when available
        - Specify the date and time for each piece of news
        - Highlight market sentiment and industry trends
        - End with a brief analysis of the overall narrative
        - Pay special attention to regulatory news, earnings reports, and strategic announcements\
    """),
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ExaTools(
            include_domains=["trendlyne.com"],
            text=False,
            highlights=False,
            show_results=True,
        )
    ],
    instructions=dedent("""\
        You are a skilled financial analyst with expertise in market data! 

        Follow these steps when analyzing financial data:
        1. Start with the latest stock price, trading volume, and daily range
        2. Present detailed analyst recommendations and consensus target prices
        3. Include key metrics: P/E ratio, market cap, 52-week range
        4. Analyze trading patterns and volume trends
        5. Compare performance against relevant sector indices

        Your style guide:
        - Use tables for structured data presentation
        - Include clear headers for each data section
        - Add brief explanations for technical terms
        - Highlight notable changes with emojis ( )
        - Use bullet points for quick insights
        - Compare current values with historical averages
        - End with a data-driven financial outlook\
    """),
    markdown=True,
)

agent_team = Team(
    members=[web_agent, finance_agent],
    model=OpenAIChat(id="gpt-4o"),
    instructions=dedent("""\
        You are the lead editor of a prestigious financial news desk! 

        Your role:
        1. Coordinate between the web researcher and financial analyst
        2. Combine their findings into a compelling narrative
        3. Ensure all information is properly sourced and verified
        4. Present a balanced view of both news and data
        5. Highlight key risks and opportunities

        Your style guide:
        - Start with an attention-grabbing headline
        - Begin with a powerful executive summary
        - Present financial data first, followed by news context
        - Use clear section breaks between different types of information
        - Include relevant charts or tables when available
        - Add 'Market Sentiment' section with current mood
        - Include a 'Key Takeaways' section at the end
        - End with 'Risk Factors' when appropriate
        - Sign off with 'Market Watch Team' and the current date\
    """),
    add_datetime_to_context=True,
    markdown=True,
    show_members_responses=False,
)

# Example usage with diverse queries
agent_team.print_response(
    input="Summarize analyst recommendations and share the latest news for NVDA",
    stream=True,
)
agent_team.print_response(
    input="What's the market outlook and financial performance of AI semiconductor companies?",
    stream=True,
)
agent_team.print_response(
    input="Analyze recent developments and financial performance of TSLA",
    stream=True,
)

# More example prompts to try:
"""
Advanced queries to explore:
1. "Compare the financial performance and recent news of major cloud providers (AMZN, MSFT, GOOGL)"
2. "What's the impact of recent Fed decisions on banking stocks? Focus on JPM and BAC"
3. "Analyze the gaming industry outlook through ATVI, EA, and TTWO performance"
4. "How are social media companies performing? Compare META and SNAP"
5. "What's the latest on AI chip manufacturers and their market position?"
"""
```

---

<a name="getting_started--18_research_agent_exapy"></a>

### `getting_started/18_research_agent_exa.py`

```python
""" AI Research Agent - Your AI Research Assistant!

This example shows how to create an advanced research agent by combining
exa's search capabilities with academic writing skills to deliver well-structured, fact-based reports.

Key features demonstrated:
- Using Exa.ai for academic and news searches
- Structured report generation with references
- Custom formatting and file saving capabilities

Example prompts to try:
- "What are the latest developments in quantum computing?"
- "Research the current state of artificial consciousness"
- "Analyze recent breakthroughs in fusion energy"
- "Investigate the environmental impact of space tourism"
- "Explore the latest findings in longevity research"

Run `pip install openai exa-py agno` to install dependencies.
"""

from datetime import datetime
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

cwd = Path(__file__).parent.resolve()
tmp = cwd.joinpath("tmp")
if not tmp.exists():
    tmp.mkdir(exist_ok=True, parents=True)

today = datetime.now().strftime("%Y-%m-%d")

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ExaTools(start_published_date=today, type="keyword")],
    description=dedent("""\
        You are Professor X-1000, a distinguished AI research scientist with expertise
        in analyzing and synthesizing complex information. Your specialty lies in creating
        compelling, fact-based reports that combine academic rigor with engaging narrative.

        Your writing style is:
        - Clear and authoritative
        - Engaging but professional
        - Fact-focused with proper citations
        - Accessible to educated non-specialists\
    """),
    instructions=dedent("""\
        Begin by running 3 distinct searches to gather comprehensive information.
        Analyze and cross-reference sources for accuracy and relevance.
        Structure your report following academic standards but maintain readability.
        Include only verifiable facts with proper citations.
        Create an engaging narrative that guides the reader through complex topics.
        End with actionable takeaways and future implications.\
    """),
    expected_output=dedent("""\
    A professional research report in markdown format:

    # {Compelling Title That Captures the Topic's Essence}

    ## Executive Summary
    {Brief overview of key findings and significance}

    ## Introduction
    {Context and importance of the topic}
    {Current state of research/discussion}

    ## Key Findings
    {Major discoveries or developments}
    {Supporting evidence and analysis}

    ## Implications
    {Impact on field/society}
    {Future directions}

    ## Key Takeaways
    - {Bullet point 1}
    - {Bullet point 2}
    - {Bullet point 3}

    ## References
    - [Source 1](link) - Key finding/quote
    - [Source 2](link) - Key finding/quote
    - [Source 3](link) - Key finding/quote

    ---
    Report generated by Professor X-1000
    Advanced Research Systems Division
    Date: {current_date}\
    """),
    markdown=True,
    add_datetime_to_context=True,
    save_response_to_file=str(tmp.joinpath("{message}.md")),
)

# Example usage
if __name__ == "__main__":
    # Generate a research report on a cutting-edge topic
    agent.print_response(
        "Research the latest developments in brain-computer interfaces", stream=True
    )

# More example prompts to try:
"""
Try these research topics:
1. "Analyze the current state of solid-state batteries"
2. "Research recent breakthroughs in CRISPR gene editing"
3. "Investigate the development of autonomous vehicles"
4. "Explore advances in quantum machine learning"
5. "Study the impact of artificial intelligence on healthcare"
"""
```

---

<a name="getting_started--19_blog_generator_workflowpy"></a>

### `getting_started/19_blog_generator_workflow.py`

```python
""" Advanced Research Workflow - Your AI Research Assistant!

This advanced example demonstrates how to build a sophisticated blog post generator using
the new workflow v2.0 architecture. The workflow combines web research capabilities with
professional writing expertise using a multi-stage approach:

1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

Key capabilities:
- Advanced web research and source evaluation
- Content scraping and processing
- Professional writing with SEO optimization
- Automatic content caching for efficiency
- Source attribution and fact verification

Run `pip install openai googlesearch-python newspaper4k lxml_html_clean sqlalchemy agno` to install dependencies.
"""

import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# --- Response Models ---
class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


# --- Agents ---
research_agent = Agent(
    name="Blog Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    description=dedent("""\
    You are BlogResearch-X, an elite research assistant specializing in discovering
    high-quality sources for compelling blog content. Your expertise includes:

    - Finding authoritative and trending sources
    - Evaluating content credibility and relevance
    - Identifying diverse perspectives and expert opinions
    - Discovering unique angles and insights
    - Ensuring comprehensive topic coverage
    """),
    instructions=dedent("""\
    1. Search Strategy 
       - Find 10-15 relevant sources and select the 5-7 best ones
       - Prioritize recent, authoritative content
       - Look for unique angles and expert insights
    2. Source Evaluation 
       - Verify source credibility and expertise
       - Check publication dates for timeliness
       - Assess content depth and uniqueness
    3. Diversity of Perspectives 
       - Include different viewpoints
       - Gather both mainstream and expert opinions
       - Find supporting data and statistics
    """),
    output_schema=SearchResults,
)

content_scraper_agent = Agent(
    name="Content Scraper Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[Newspaper4kTools()],
    description=dedent("""\
    You are ContentBot-X, a specialist in extracting and processing digital content
    for blog creation. Your expertise includes:

    - Efficient content extraction
    - Smart formatting and structuring
    - Key information identification
    - Quote and statistic preservation
    - Maintaining source attribution
    """),
    instructions=dedent("""\
    1. Content Extraction 
       - Extract content from the article
       - Preserve important quotes and statistics
       - Maintain proper attribution
       - Handle paywalls gracefully
    2. Content Processing 
       - Format text in clean markdown
       - Preserve key information
       - Structure content logically
    3. Quality Control 
       - Verify content relevance
       - Ensure accurate extraction
       - Maintain readability
    """),
    output_schema=ScrapedArticle,
)

blog_writer_agent = Agent(
    name="Blog Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    You are BlogMaster-X, an elite content creator combining journalistic excellence
    with digital marketing expertise. Your strengths include:

    - Crafting viral-worthy headlines
    - Writing engaging introductions
    - Structuring content for digital consumption
    - Incorporating research seamlessly
    - Optimizing for SEO while maintaining quality
    - Creating shareable conclusions
    """),
    instructions=dedent("""\
    1. Content Strategy 
       - Craft attention-grabbing headlines
       - Write compelling introductions
       - Structure content for engagement
       - Include relevant subheadings
    2. Writing Excellence 
       - Balance expertise with accessibility
       - Use clear, engaging language
       - Include relevant examples
       - Incorporate statistics naturally
    3. Source Integration 
       - Cite sources properly
       - Include expert quotes
       - Maintain factual accuracy
    4. Digital Optimization 
       - Structure for scanability
       - Include shareable takeaways
       - Optimize for SEO
       - Add engaging subheadings

    Format your blog post with this structure:
    # {Viral-Worthy Headline}

    ## Introduction
    {Engaging hook and context}

    ## {Compelling Section 1}
    {Key insights and analysis}
    {Expert quotes and statistics}

    ## {Engaging Section 2}
    {Deeper exploration}
    {Real-world examples}

    ## {Practical Section 3}
    {Actionable insights}
    {Expert recommendations}

    ## Key Takeaways
    - {Shareable insight 1}
    - {Practical takeaway 2}
    - {Notable finding 3}

    ## Sources
    {Properly attributed sources with links}
    """),
    markdown=True,
)


# --- Helper Functions ---
def get_cached_blog_post(session_state, topic: str) -> Optional[str]:
    """Get cached blog post from workflow session state"""
    logger.info("Checking if cached blog post exists")
    return session_state.get("blog_posts", {}).get(topic)


def cache_blog_post(session_state, topic: str, blog_post: str):
    """Cache blog post in workflow session state"""
    logger.info(f"Saving blog post for topic: {topic}")
    if "blog_posts" not in session_state:
        session_state["blog_posts"] = {}
    session_state["blog_posts"][topic] = blog_post


def get_cached_search_results(session_state, topic: str) -> Optional[SearchResults]:
    """Get cached search results from workflow session state"""
    logger.info("Checking if cached search results exist")
    search_results = session_state.get("search_results", {}).get(topic)
    if search_results and isinstance(search_results, dict):
        try:
            return SearchResults.model_validate(search_results)
        except Exception as e:
            logger.warning(f"Could not validate cached search results: {e}")
    return search_results if isinstance(search_results, SearchResults) else None


def cache_search_results(session_state, topic: str, search_results: SearchResults):
    """Cache search results in workflow session state"""
    logger.info(f"Saving search results for topic: {topic}")
    if "search_results" not in session_state:
        session_state["search_results"] = {}
    session_state["search_results"][topic] = search_results.model_dump()


def get_cached_scraped_articles(
    session_state, topic: str
) -> Optional[Dict[str, ScrapedArticle]]:
    """Get cached scraped articles from workflow session state"""
    logger.info("Checking if cached scraped articles exist")
    scraped_articles = session_state.get("scraped_articles", {}).get(topic)
    if scraped_articles and isinstance(scraped_articles, dict):
        try:
            return {
                url: ScrapedArticle.model_validate(article)
                for url, article in scraped_articles.items()
            }
        except Exception as e:
            logger.warning(f"Could not validate cached scraped articles: {e}")
    return scraped_articles if isinstance(scraped_articles, dict) else None


def cache_scraped_articles(
    session_state, topic: str, scraped_articles: Dict[str, ScrapedArticle]
):
    """Cache scraped articles in workflow session state"""
    logger.info(f"Saving scraped articles for topic: {topic}")
    if "scraped_articles" not in session_state:
        session_state["scraped_articles"] = {}
    session_state["scraped_articles"][topic] = {
        url: article.model_dump() for url, article in scraped_articles.items()
    }


async def get_search_results(
    session_state, topic: str, use_cache: bool = True, num_attempts: int = 3
) -> Optional[SearchResults]:
    """Get search results with caching support"""

    # Check cache first
    if use_cache:
        cached_results = get_cached_search_results(session_state, topic)
        if cached_results:
            logger.info(f"Found {len(cached_results.articles)} articles in cache.")
            return cached_results

    # Search for new results
    for attempt in range(num_attempts):
        try:
            print(
                f" Searching for articles about: {topic} (attempt {attempt + 1}/{num_attempts})"
            )
            response = await research_agent.arun(topic)

            if (
                response
                and response.content
                and isinstance(response.content, SearchResults)
            ):
                article_count = len(response.content.articles)
                logger.info(f"Found {article_count} articles on attempt {attempt + 1}")
                print(f" Found {article_count} relevant articles")

                # Cache the results
                cache_search_results(session_state, topic, response.content)
                return response.content
            else:
                logger.warning(
                    f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                )

        except Exception as e:
            logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

    logger.error(f"Failed to get search results after {num_attempts} attempts")
    return None


async def scrape_articles(
    session_state,
    topic: str,
    search_results: SearchResults,
    use_cache: bool = True,
) -> Dict[str, ScrapedArticle]:
    """Scrape articles with caching support"""

    # Check cache first
    if use_cache:
        cached_articles = get_cached_scraped_articles(session_state, topic)
        if cached_articles:
            logger.info(f"Found {len(cached_articles)} scraped articles in cache.")
            return cached_articles

    scraped_articles: Dict[str, ScrapedArticle] = {}

    print(f" Scraping {len(search_results.articles)} articles...")

    for i, article in enumerate(search_results.articles, 1):
        try:
            print(
                f" Scraping article {i}/{len(search_results.articles)}: {article.title[:50]}..."
            )
            response = await content_scraper_agent.arun(article.url)

            if (
                response
                and response.content
                and isinstance(response.content, ScrapedArticle)
            ):
                scraped_articles[response.content.url] = response.content
                logger.info(f"Scraped article: {response.content.url}")
                print(f" Successfully scraped: {response.content.title[:50]}...")
            else:
                print(f" Failed to scrape: {article.title[:50]}...")

        except Exception as e:
            logger.warning(f"Failed to scrape {article.url}: {str(e)}")
            print(f" Error scraping: {article.title[:50]}...")

    # Cache the scraped articles
    cache_scraped_articles(session_state, topic, scraped_articles)
    return scraped_articles


# --- Main Execution Function ---
async def blog_generation_execution(
    session_state,
    topic: str = None,
    use_search_cache: bool = True,
    use_scrape_cache: bool = True,
    use_blog_cache: bool = True,
) -> str:
    """
    Blog post generation workflow execution function.

    Args:
        session_state: The shared session state
        topic: Blog post topic (if not provided, uses execution_input.input)
        use_search_cache: Whether to use cached search results
        use_scrape_cache: Whether to use cached scraped articles
        use_blog_cache: Whether to use cached blog posts
    """

    blog_topic = topic

    if not blog_topic:
        return " No blog topic provided. Please specify a topic."

    print(f" Generating blog post about: {blog_topic}")
    print("=" * 60)

    # Check for cached blog post first
    if use_blog_cache:
        cached_blog = get_cached_blog_post(session_state, blog_topic)
        if cached_blog:
            print(" Found cached blog post!")
            return cached_blog

    # Phase 1: Research and gather sources
    print("\n PHASE 1: RESEARCH & SOURCE GATHERING")
    print("=" * 50)

    search_results = await get_search_results(
        session_state, blog_topic, use_search_cache
    )

    if not search_results or len(search_results.articles) == 0:
        return f" Sorry, could not find any articles on the topic: {blog_topic}"

    print(f" Found {len(search_results.articles)} relevant sources:")
    for i, article in enumerate(search_results.articles, 1):
        print(f"   {i}. {article.title[:60]}...")

    # Phase 2: Content extraction
    print("\n PHASE 2: CONTENT EXTRACTION")
    print("=" * 50)

    scraped_articles = await scrape_articles(
        session_state, blog_topic, search_results, use_scrape_cache
    )

    if not scraped_articles:
        return f" Could not extract content from any articles for topic: {blog_topic}"

    print(f" Successfully extracted content from {len(scraped_articles)} articles")

    # Phase 3: Blog post writing
    print("\n PHASE 3: BLOG POST CREATION")
    print("=" * 50)

    # Prepare input for the writer
    writer_input = {
        "topic": blog_topic,
        "articles": [article.model_dump() for article in scraped_articles.values()],
    }

    print(" AI is crafting your blog post...")
    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))

    if not writer_response or not writer_response.content:
        return f" Failed to generate blog post for topic: {blog_topic}"

    blog_post = writer_response.content

    # Cache the blog post
    cache_blog_post(session_state, blog_topic, blog_post)

    print(" Blog post generated successfully!")
    print(f" Length: {len(blog_post)} characters")
    print(f" Sources: {len(scraped_articles)} articles")

    return blog_post


# --- Workflow Definition ---
blog_generator_workflow = Workflow(
    name="Blog Post Generator",
    description="Advanced blog post generator with research and content creation capabilities",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/blog_generator.db",
    ),
    steps=blog_generation_execution,
    session_state={},  # Initialize empty session state for caching
)


if __name__ == "__main__":
    import random

    async def main():
        # Fun example topics to showcase the generator's versatility
        example_topics = [
            "The Rise of Artificial General Intelligence: Latest Breakthroughs",
            "How Quantum Computing is Revolutionizing Cybersecurity",
            "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint",
            "The Future of Work: AI and Human Collaboration",
            "Space Tourism: From Science Fiction to Reality",
            "Mindfulness and Mental Health in the Digital Age",
            "The Evolution of Electric Vehicles: Current State and Future Trends",
            "Why Cats Secretly Run the Internet",
            "The Science Behind Why Pizza Tastes Better at 2 AM",
            "How Rubber Ducks Revolutionized Software Development",
        ]

        # Test with a random topic
        topic = random.choice(example_topics)

        print(" Testing Blog Post Generator v2.0")
        print("=" * 60)
        print(f" Topic: {topic}")
        print()

        # Generate the blog post
        resp = await blog_generator_workflow.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_blog_cache=True,
        )

        pprint_run_response(resp, markdown=True, show_time=True)

    asyncio.run(main())
```

---

<a name="getting_started--readme_examplespy"></a>

### `getting_started/readme_examples.py`

```python
"""Readme Examples
Run `pip install openai ddgs yfinance lancedb tantivy pypdf agno` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Level 0: Agents with no tools (basic inference tasks).
level_0_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are an enthusiastic news reporter with a flair for storytelling!",
    markdown=True,
)
level_0_agent.print_response(
    "Tell me about a breaking news story from New York.", stream=True
)

# Level 1: Agents with tools for autonomous task execution.
level_1_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are an enthusiastic news reporter with a flair for storytelling!",
    tools=[DuckDuckGoTools()],
    markdown=True,
)
level_1_agent.print_response(
    "Tell me about a breaking news story from New York.", stream=True
)

# Level 2: Agents with knowledge, combining memory and reasoning.
knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="recipes",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

level_2_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are a Thai cuisine expert!",
    instructions=[
        "Search your knowledge for Thai recipes.",
        "If the question is better suited for the web, search the web to fill in gaps.",
        "Prefer the information in your knowledge over the web results.",
    ],
    knowledge=knowledge,
    tools=[DuckDuckGoTools()],
    markdown=True,
)

level_2_agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
level_2_agent.print_response("What is the history of Thai curry?", stream=True)

# Level 3: Teams of agents collaborating on complex workflows.
web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data",
    markdown=True,
)

level_3_agent_team = Team(
    members=[web_agent, finance_agent],
    model=OpenAIChat(id="gpt-4o"),
    instructions=["Always include sources", "Use tables to display data"],
    markdown=True,
)
level_3_agent_team.print_response(
    "What's the market outlook and financial performance of AI semiconductor companies?",
    stream=True,
)
```

---

<a name="integrations--a2a--basic_agent--__main__py"></a>

### `integrations/a2a/basic_agent/__main__.py`

```python
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import (
    AgentAuthentication,
    AgentCapabilities,
    AgentCard,
    AgentSkill,
)
from basic_agent import BasicAgentExecutor

if __name__ == "__main__":
    skill = AgentSkill(
        id="agno_agent",
        name="Agno Agent",
        description="Agno Agent",
        tags=["Agno agent"],
        examples=["hi", "hello"],
    )

    agent_card = AgentCard(
        name="Agno Agent",
        description="Agno Agent",
        url="http://localhost:9999/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(),
        skills=[skill],
        authentication=AgentAuthentication(schemes=["public"]),
    )

    request_handler = DefaultRequestHandler(
        agent_executor=BasicAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )

    server = A2AStarletteApplication(
        agent_card=agent_card, http_handler=request_handler
    )
    import uvicorn

    uvicorn.run(server.build(), host="0.0.0.0", port=9999, timeout_keep_alive=10)
```

---

<a name="integrations--a2a--basic_agent--basic_agentpy"></a>

### `integrations/a2a/basic_agent/basic_agent.py`

```python
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import Part, TextPart
from a2a.utils import new_agent_text_message
from agno.agent import Agent, Message, RunOutput
from agno.models.openai import OpenAIChat
from typing_extensions import override

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
)


class BasicAgentExecutor(AgentExecutor):
    """Test AgentProxy Implementation."""

    def __init__(self):
        self.agent = agent

    @override
    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        message: Message = Message(role="user", content="")
        for part in context.message.parts:
            if isinstance(part, Part):
                if isinstance(part.root, TextPart):
                    message.content = part.root.text
                    break

        result: RunOutput = await self.agent.arun(message)
        event_queue.enqueue_event(new_agent_text_message(result.content))

    @override
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        raise Exception("Cancel not supported")
```

---

<a name="integrations--a2a--basic_agent--clientpy"></a>

### `integrations/a2a/basic_agent/client.py`

```python
from typing import Any
from uuid import uuid4

import httpx
from a2a.client import A2AClient
from a2a.types import (
    MessageSendParams,
    SendMessageRequest,
    SendStreamingMessageRequest,  # noqa: F401
)


async def main() -> None:
    async with httpx.AsyncClient() as httpx_client:
        client = await A2AClient.get_client_from_agent_card_url(
            httpx_client, "http://localhost:9999"
        )
        send_message_payload: dict[str, Any] = {
            "message": {
                "role": "user",
                "parts": [
                    {
                        "type": "text",
                        "text": "Hello! What can you tell me about the weather in Tokyo?",
                    }
                ],
                "messageId": uuid4().hex,
            },
        }
        request = SendMessageRequest(params=MessageSendParams(**send_message_payload))

        response = await client.send_message(request)
        print(response.model_dump(mode="json", exclude_none=True))

        # streaming_request = SendStreamingMessageRequest(
        #     params=MessageSendParams(**send_message_payload)
        # )

        # stream_response = client.send_message_streaming(streaming_request)
        # async for chunk in stream_response:
        #     print(chunk.model_dump(mode='json', exclude_none=True))


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
```

---

<a name="integrations--discord--agent_with_mediapy"></a>

### `integrations/discord/agent_with_media.py`

```python
from agno.agent import Agent
from agno.integrations.discord import DiscordClient
from agno.models.google import Gemini

media_agent = Agent(
    name="Media Agent",
    model=Gemini(id="gemini-2.0-flash"),
    description="A Media processing agent",
    instructions="Analyze images, audios and videos sent by the user",
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

discord_agent = DiscordClient(media_agent)

if __name__ == "__main__":
    discord_agent.serve()
```

---

<a name="integrations--discord--agent_with_user_memorypy"></a>

### `integrations/discord/agent_with_user_memory.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.integrations.discord import DiscordClient
from agno.models.google import Gemini
from agno.tools.googlesearch import GoogleSearchTools

db = SqliteDb(db_file="tmp/discord_client_cookbook.db")

personal_agent = Agent(
    name="Basic Agent",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[GoogleSearchTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
    db=db,
    enable_agentic_memory=True,
    instructions=dedent("""
        You are a personal AI friend of the user, your purpose is to chat with the user about things and make them feel good.
        First introduce yourself and ask for their name then, ask about themeselves, their hobbies, what they like to do and what they like to talk about.
        Use Google Search tool to find latest infromation about things in the conversations
                        """),
    debug_mode=True,
)

discord_agent = DiscordClient(personal_agent)

if __name__ == "__main__":
    discord_agent.serve()
```

---

<a name="integrations--discord--basicpy"></a>

### `integrations/discord/basic.py`

```python
from agno.agent import Agent
from agno.integrations.discord import DiscordClient
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"),
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
)

discord_agent = DiscordClient(basic_agent)

if __name__ == "__main__":
    discord_agent.serve()
```

---

<a name="integrations--memory--mem0_integrationpy"></a>

### `integrations/memory/mem0_integration.py`

```python
from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

try:
    from mem0 import MemoryClient
except ImportError:
    raise ImportError(
        "mem0 is not installed. Please install it using `pip install mem0ai`."
    )

client = MemoryClient()

user_id = "agno"
messages = [
    {"role": "user", "content": "My name is John Billings."},
    {"role": "user", "content": "I live in NYC."},
    {"role": "user", "content": "I'm going to a concert tomorrow."},
]
# Comment out the following line after running the script once
client.add(messages, user_id=user_id)

agent = Agent(
    model=OpenAIChat(),
    dependencies={"memory": client.get_all(user_id=user_id)},
    add_dependencies_to_context=True,
)
run: RunOutput = agent.run("What do you know about me?")

pprint_run_response(run)

input = [{"role": i.role, "content": str(i.content)} for i in (run.messages or [])]
client.add(messages, user_id=user_id)
```

---

<a name="integrations--memory--zep_integrationpy"></a>

### `integrations/memory/zep_integration.py`

```python
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zep import ZepTools

# Initialize the ZepTools
zep_tools = ZepTools(user_id="agno", session_id="agno-session")

zep_tools.add_zep_message(role="user", content="My name is John Billings")
zep_tools.add_zep_message(role="user", content="I live in NYC")
zep_tools.add_zep_message(role="user", content="I'm going to a concert tomorrow")

# Allow the memories to sync with Zep database
time.sleep(10)

# Initialize the Agent
agent = Agent(
    model=OpenAIChat(),
    tools=[zep_tools],
    dependencies={"memory": zep_tools.get_zep_memory(memory_type="context")},
    add_dependencies_to_context=True,
)

# Ask the Agent about the user
agent.print_response("What do you know about me?")
```

---

<a name="integrations--observability--agent_opspy"></a>

### `integrations/observability/agent_ops.py`

```python
"""
This example shows how to use agentops to log model calls.

Steps to get started with agentops:
1. Install agentops: pip install agentops
2. Obtain an API key from https://app.agentops.ai/
3. Export environment variables like AGENTOPS_API_KEY and OPENAI_API_KEY.
4. Run the script.

You can view the logs in the AgentOps dashboard: https://app.agentops.ai/
"""

import agentops
from agno.agent import Agent
from agno.models.openai import OpenAIChat

# Initialize AgentOps
agentops.init()

# Create and run an agent
agent = Agent(model=OpenAIChat(id="gpt-4o"))
response = agent.run("Share a 2 sentence horror story")

# Print the response
print(response.content)
```

---

<a name="integrations--observability--arize_phoenix_via_openinferencepy"></a>

### `integrations/observability/arize_phoenix_via_openinference.py`

```python
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Arize Phoenix.

1. Install dependencies: pip install arize-phoenix openai openinference-instrumentation-agno opentelemetry-sdk opentelemetry-exporter-otlp
2. Setup your Arize Phoenix account and get your API key: https://phoenix.arize.com/.
3. Set your Arize Phoenix API key as an environment variable:
  - export ARIZE_PHOENIX_API_KEY=<your-key>
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from phoenix.otel import register

os.environ["PHOENIX_CLIENT_HEADERS"] = f"api_key={os.getenv('ARIZE_PHOENIX_API_KEY')}"
os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "https://app.phoenix.arize.com"
# configure the Phoenix tracer
tracer_provider = register(
    project_name="agno-stock-price-agent",  # Default is 'default'
    auto_instrument=True,  # Automatically use the installed OpenInference instrumentation
)

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")
```

---

<a name="integrations--observability--arize_phoenix_via_openinference_localpy"></a>

### `integrations/observability/arize_phoenix_via_openinference_local.py`

```python
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Arize Phoenix.

1. Install dependencies: pip install arize-phoenix openai openinference-instrumentation-agno opentelemetry-sdk opentelemetry-exporter-otlp
2. Run `phoenix serve` to start the local collector.
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from phoenix.otel import register

os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
# configure the Phoenix tracer
tracer_provider = register(
    project_name="agno-stock-price-agent",  # Default is 'default'
    auto_instrument=True,  # Automatically use the installed OpenInference instrumentation
)

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")
```

---

<a name="integrations--observability--atla_oppy"></a>

### `integrations/observability/atla_op.py`

```python
"""
This example shows how to add observability to your agno agent with Atla.

1. Install dependencies: pip install "atla-insights"
2. Sign up for an account at https://app.atla-ai.com
3. Set your Atla Insights API key as an environment variable:
  - export ATLA_API_KEY=<your-key>
"""

from os import getenv

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from atla_insights import configure, instrument_agno

configure(token=getenv("ATLA_API_KEY"))

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

# Instrument and run
with instrument_agno("openai"):
    agent.print_response("What are the latest news about the stock market?")
```

---

<a name="integrations--observability--langfuse_via_openinferencepy"></a>

### `integrations/observability/langfuse_via_openinference.py`

```python
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Langfuse.

1. Install dependencies: pip install openai langfuse opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-agno
2. Either self-host or sign up for an account at https://us.cloud.langfuse.com
3. Set your Langfuse API key as an environment variables:
  - export LANGFUSE_PUBLIC_KEY=<your-key>
  - export LANGFUSE_SECRET_KEY=<your-key>
"""

import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  #  US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" #  EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" #  Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)


agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")
```

---

<a name="integrations--observability--langfuse_via_openinference_response_modelpy"></a>

### `integrations/observability/langfuse_via_openinference_response_model.py`

```python
"""
This example shows how to instrument your agno agent with OpenInference and send traces to Langfuse,
using an Agent with a response model.

1. Install dependencies: pip install openai langfuse opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-agno
2. Either self-host or sign up for an account at https://us.cloud.langfuse.com
3. Set your Langfuse API key as an environment variables:
  - export LANGFUSE_PUBLIC_KEY=<your-key>
  - export LANGFUSE_SECRET_KEY=<your-key>
"""

import base64
import os
from enum import Enum

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from pydantic import BaseModel, Field

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  #  US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" #  EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" #  Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)


class MarketArea(Enum):
    USA = "USA"
    UK = "UK"
    EU = "EU"
    ASIA = "ASIA"


class StockPrice(BaseModel):
    price: str = Field(description="The price of the stock")
    symbol: str = Field(description="The symbol of the stock")
    date: str = Field(description="Current day")
    area: MarketArea


agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. You check and return the current price of a stock.",
    debug_mode=True,
    output_schema=StockPrice,
)

agent.print_response("What is the current price of Tesla?")
```

---

<a name="integrations--observability--langfuse_via_openlitpy"></a>

### `integrations/observability/langfuse_via_openlit.py`

```python
"""
This example shows how to use langfuse via OpenLIT to trace model calls.

1. Install dependencies: pip install openai langfuse openlit opentelemetry-sdk opentelemetry-exporter-otlp
2. Set your Langfuse API key as an environment variables:
  - export LANGFUSE_PUBLIC_KEY=<your-key>
  - export LANGFUSE_SECRET_KEY=<your-key>
"""

import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()

os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  #  US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" #  EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" #  Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"

from opentelemetry.exporter.otlp.proto.http.trace_exporter import (  # noqa: E402
    OTLPSpanExporter,
)
from opentelemetry.sdk.trace import TracerProvider  # noqa: E402
from opentelemetry.sdk.trace.export import SimpleSpanProcessor  # noqa: E402

trace_provider = TracerProvider()
trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Sets the global default tracer provider
from opentelemetry import trace  # noqa: E402

trace.set_tracer_provider(trace_provider)

# Creates a tracer from the global tracer provider
tracer = trace.get_tracer(__name__)

import openlit  # noqa: E402

# Initialize OpenLIT instrumentation. The disable_batch flag is set to true to process traces immediately.
openlit.init(tracer=tracer, disable_batch=True)

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    debug_mode=True,
)

agent.print_response("What is currently trending on Twitter?")
```

---

<a name="integrations--observability--langsmith_via_openinferencepy"></a>

### `integrations/observability/langsmith_via_openinference.py`

```python
"""
This example shows how to instrument your agno agent with OpenInference and send traces to LangSmith.

1. Create a LangSmith account and get your API key: https://smith.langchain.com/
2. Set your LangSmith API key as an environment variable:
  - export LANGSMITH_API_KEY=<your-key>
  - export LANGSMITH_TRACING=true
  - export LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com or https://api.smith.langchain.com
  - export LANGSMITH_PROJECT=<your-project-name>
3. Install dependencies: pip install openai openinference-instrumentation-agno opentelemetry-sdk opentelemetry-exporter-otlp
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

endpoint = "https://eu.api.smith.langchain.com/otel/v1/traces"
headers = {
    "x-api-key": os.getenv("LANGSMITH_API_KEY"),
    "Langsmith-Project": os.getenv("LANGSMITH_PROJECT"),
}


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(
    SimpleSpanProcessor(OTLPSpanExporter(endpoint=endpoint, headers=headers))
)

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)

agent = Agent(
    name="Stock Market Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    debug_mode=True,
)

agent.print_response("What is news on the stock market?")
```

---

<a name="integrations--observability--langtrace_oppy"></a>

### `integrations/observability/langtrace_op.py`

```python
"""
This example shows how to instrument your agno agent with Langtrace.

1. Install dependencies: pip install langtrace-python-sdk
2. Sign up for an account at https://app.langtrace.ai/
3. Set your Langtrace API key as an environment variables:
  - export LANGTRACE_API_KEY=<your-key>
"""

# Must precede other imports
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from langtrace_python_sdk import langtrace  # type: ignore

langtrace.init()

agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")
```

---

<a name="integrations--observability--langwatch_oppy"></a>

### `integrations/observability/langwatch_op.py`

```python
"""
This example shows how to instrument your agno agent and send traces to LangWatch.

1. Install dependencies: pip install openai langwatch openinference-instrumentation-agno
2. Sign up for an account at https://app.langwatch.ai/
3. Set your LangWatch API key as an environment variables:
  - export LANGWATCH_API_KEY=<your-key>
"""

import langwatch
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor

# Initialize LangWatch and instrument Agno
langwatch.setup(instrumentors=[AgnoInstrumentor()])

# Create and configure your Agno agent
agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

agent.print_response("What is the current price of Tesla?")
```

---

<a name="integrations--observability--teams--langfuse_via_openinference_async_teampy"></a>

### `integrations/observability/teams/langfuse_via_openinference_async_team.py`

```python
import asyncio
import base64
import os
from uuid import uuid4

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  #  US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" #  EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" #  Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)

# First agent for market data
market_data_agent = Agent(
    name="Market Data Agent",
    role="Fetch and analyze stock market data",
    id="market-data",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[YFinanceTools()],
    instructions=[
        "You are a market data specialist.",
        "Focus on current stock prices and key metrics.",
        "Always present data in tables.",
    ],
)

# Second agent for news and research
news_agent = Agent(
    name="News Research Agent",
    role="Research company news",
    id="news-research",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "You are a financial news analyst.",
        "Focus on recent company news and developments.",
        "Always cite your sources.",
    ],
)

# Create team with both agents
financial_team = Team(
    name="Financial Analysis Team",
    id=str(uuid4()),
    user_id=str(uuid4()),
    model=OpenAIChat(id="gpt-4.1"),
    members=[
        market_data_agent,
        news_agent,
    ],
    instructions=[
        "Coordinate between market data and news analysis.",
        "First get market data, then relevant news.",
        "Combine the information into a clear summary.",
    ],
    show_members_responses=True,
    markdown=True,
)

if __name__ == "__main__":
    asyncio.run(
        financial_team.aprint_response(
            "Analyze Tesla (TSLA) stock - provide both current market data and recent significant news.",
            stream=True,
        )
    )
```

---

<a name="integrations--observability--teams--langfuse_via_openinference_teampy"></a>

### `integrations/observability/teams/langfuse_via_openinference_team.py`

```python
import base64
import os
from uuid import uuid4

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  #  US data region
)
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" #  EU data region
# os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" #  Local deployment (>= v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"


tracer_provider = TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Start instrumenting agno
AgnoInstrumentor().instrument(tracer_provider=tracer_provider)

# First agent for market data
market_data_agent = Agent(
    name="Market Data Agent",
    role="Fetch and analyze stock market data",
    id="market-data",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[YFinanceTools()],
    instructions=[
        "You are a market data specialist.",
        "Focus on current stock prices and key metrics.",
        "Always present data in tables.",
    ],
)

# Second agent for news and research
news_agent = Agent(
    name="News Research Agent",
    role="Research company news",
    id="news-research",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "You are a financial news analyst.",
        "Focus on recent company news and developments.",
        "Always cite your sources.",
    ],
)

# Create team with both agents
financial_team = Team(
    name="Financial Analysis Team",
    id=str(uuid4()),
    user_id=str(uuid4()),
    model=OpenAIChat(id="gpt-4.1"),
    members=[
        market_data_agent,
        news_agent,
    ],
    instructions=[
        "Coordinate between market data and news analysis.",
        "First get market data, then relevant news.",
        "Combine the information into a clear summary.",
    ],
    show_members_responses=True,
    markdown=True,
)

if __name__ == "__main__":
    financial_team.print_response(
        "Analyze Tesla (TSLA) stock - provide both current market data and recent significant news.",
        stream=True,
    )
```

---

<a name="integrations--observability--weave_oppy"></a>

### `integrations/observability/weave_op.py`

```python
"""
This example shows how to use weave to log model calls.

Steps to get started with weave:
1. Install weave: pip install weave
2. Add weave.init('project-name') and weave.op() decorators to your functions
3. Authentication:
 - Go to https://wandb.ai and copy your API key from https://wandb.ai/authorize
 - Enter your API key in terminal when prompted
 Or
 - Export your API key as an environment variable:
    - export WANDB_API_KEY=<your-api-key>
"""

import weave
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True, debug_mode=True)

weave.init("agno")


@weave.op()
def run(content: str):
    return agent.run(content)


run("Share a 2 sentence horror story")
```

---

<a name="knowledge--basic_operations--01_from_pathpy"></a>

### `knowledge/basic_operations/01_from_path.py`

```python
"""This cookbook shows how to add content from a local file to the knowledge base.
1. Run: `python cookbook/agent_concepts/knowledge/01_from_path.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent  # noqa
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)

vector_db = PgVector(
    table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
)
# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=vector_db,
    contents_db=contents_db,
)

asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
    )
)

agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "What skills does Jordan Mitchell have?",
    markdown=True,
)
```

---

<a name="knowledge--basic_operations--02_from_urlpy"></a>

### `knowledge/basic_operations/02_from_url.py`

```python
"""This cookbook shows how to add content from a URL to the knowledge base.
1. Run: `python cookbook/agent_concepts/knowledge/02_from_url.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    contents_db=contents_db,
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)
# Add from URL to the knowledge base
asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"user_tag": "Recipes from website"},
    )
)

agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "What can you tell me about Thai recipes?",
    markdown=True,
)

knowledge.remove_vectors_by_name("Recipes")
```

---

<a name="knowledge--basic_operations--03_from_topicpy"></a>

### `knowledge/basic_operations/03_from_topic.py`

```python
"""This cookbook shows how to add topics from Wikipedia and Arxiv to the knowledge base.

It is important to specify the reader for the content when using topics.

1. Run: `pip install agno wikipedia arxiv` to install the dependencies
2. Run: `python cookbook/agent_concepts/knowledge/03_from_topic.py` to run the cookbook
"""

import asyncio

from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.arxiv_reader import ArxivReader
from agno.knowledge.reader.wikipedia_reader import WikipediaReader
from agno.vectordb.pgvector import PgVector

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# Add topics from Wikipedia
asyncio.run(
    knowledge.add_content_async(
        metadata={"user_tag": "Wikipedia content"},
        topics=["Manchester United"],
        reader=WikipediaReader(),
    )
)

# Add topics from Arxiv
asyncio.run(
    knowledge.add_content_async(
        metadata={"user_tag": "Arxiv content"},
        topics=["Carbon Dioxide", "Oxygen"],
        reader=ArxivReader(),
    )
)

# Using the add_contents method
asyncio.run(
    knowledge.add_contents_async(
        topics=["Carbon Dioxide", "Nitrogen"],
        reader=ArxivReader(),
    )
)
```

---

<a name="knowledge--basic_operations--04_from_multiplepy"></a>

### `knowledge/basic_operations/04_from_multiple.py`

```python
"""This cookbook shows how to add content from multiple paths and URLs to the knowledge base.
1. Run: `python cookbook/agent_concepts/knowledge/04_from_multiple.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# As a list
asyncio.run(
    knowledge.add_contents_async(
        [
            {
                "name": "CV's",
                "path": "cookbook/knowledge/testing_resources/cv_1.pdf",
                "metadata": {"user_tag": "Engineering candidates"},
            },
            {
                "name": "Docs",
                "url": "https://docs.agno.com/introduction",
                "metadata": {"user_tag": "Documents"},
            },
        ]
    )
)

# Using specifc fields
asyncio.run(
    knowledge.add_contents_async(
        urls=[
            "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
            "https://docs.agno.com/introduction",
            "https://docs.agno.com/agents/knowledge.md",
        ],
    )
)

agent = Agent(knowledge=knowledge)

agent.print_response("What can you tell me about my documents?", markdown=True)
```

---

<a name="knowledge--basic_operations--05_from_youtubepy"></a>

### `knowledge/basic_operations/05_from_youtube.py`

```python
"""This cookbook shows how to add content from a Youtube video to Knowledge.

1. Run: `python cookbook/agent_concepts/knowledge/05_from_youtube.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# Add from Youtube link to knowledge. Youtube links are automatically detected and the reader is assigned automatically.
asyncio.run(
    knowledge.add_content_async(
        name="Agents from Scratch",
        url="https://www.youtube.com/watch?v=nLkBNnnA8Ac",
        metadata={"user_tag": "Youtube video"},
    )
)


agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "What can you tell me about the building agents?",
    markdown=True,
)
```

---

<a name="knowledge--basic_operations--06_from_s3py"></a>

### `knowledge/basic_operations/06_from_s3.py`

```python
"""This cookbook shows how to add content from a S3 bucket to the knowledge base.

1. Run: `python cookbook/agent_concepts/knowledge/06_from_s3.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.remote_content.remote_content import S3Content
from agno.vectordb.pgvector import PgVector

contents_db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    contents_db=contents_db,
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# Add from S3 bucket
asyncio.run(
    knowledge.add_content_async(
        name="S3 PDF",
        remote_content=S3Content(
            bucket_name="agno-public", key="recipes/ThaiRecipes.pdf"
        ),
        metadata={"remote_content": "S3"},
    )
)

agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "What is the best way to make a Thai curry?",
    markdown=True,
)
```

---

<a name="knowledge--basic_operations--07_from_gcspy"></a>

### `knowledge/basic_operations/07_from_gcs.py`

```python
"""This cookbook shows how to add content from a GCS bucket to the knowledge base.
1. Run: `python cookbook/agent_concepts/knowledge/07_from_gcs.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.remote_content.remote_content import GCSContent
from agno.vectordb.pgvector import PgVector

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
)

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    contents_db=contents_db,
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# Add from GCS
asyncio.run(
    knowledge.add_content_async(
        name="GCS PDF",
        remote_content=GCSContent(
            bucket_name="thai-recepies", blob_name="ThaiRecipes.pdf"
        ),
        metadata={"remote_content": "GCS"},
    )
)


agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "What is the best way to make a Thai curry?",
    markdown=True,
)
```

---

<a name="knowledge--basic_operations--08_include_exclude_filespy"></a>

### `knowledge/basic_operations/08_include_exclude_files.py`

```python
"""This cookbook shows how to use include and exclude filters when adding content to Knowledge.

1. Run: `python cookbook/agent_concepts/knowledge/08_include_exclude_files.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# Add from local file to the knowledge base
asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources",
        metadata={"user_tag": "Engineering Candidates"},
        # Only include PDF files
        include=["*.pdf"],
        # Don't include files that match this pattern
        exclude=["*cv_5*"],
    )
)

agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "Who is the best candidate for the role of a software engineer?",
    markdown=True,
)

# Alex River is not in the knowledge base, so the Agent should not find any information about him
agent.print_response(
    "Do you think Alex Rivera is a good candidate?",
    markdown=True,
)
```

---

<a name="knowledge--basic_operations--09_remove_contentpy"></a>

### `knowledge/basic_operations/09_remove_content.py`

```python
"""This cookbook shows how to remove content from Knowledge when using a ContentDB.

You can remove content by id or by name.

1. Run: `python cookbook/agent_concepts/knowledge/09_remove_content.py` to run the cookbook
"""

import asyncio

from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    contents_db=PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        knowledge_table="knowledge_contents",
    ),
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
    )
)


# Remove content and vectors by id
contents, _ = knowledge.get_content()
for content in contents:
    print(content.id)
    print(" ")
    knowledge.remove_content_by_id(content.id)

# Remove all content
knowledge.remove_all_content()
```

---

<a name="knowledge--basic_operations--10_remove_vectorspy"></a>

### `knowledge/basic_operations/10_remove_vectors.py`

```python
"""This cookbook shows how to remove vectors from Knowledge.

You can remove vectors by metadata or by name.

1. Run: `python cookbook/agent_concepts/knowledge/10_remove_vectors.py` to run the cookbook
"""

import asyncio

from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
    )
)


knowledge.remove_vectors_by_metadata({"user_tag": "Engineering Candidates"})

# Add from local file to the knowledge base
asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
    )
)

knowledge.remove_vectors_by_name("CV")
```

---

<a name="knowledge--basic_operations--11_skip_if_existspy"></a>

### `knowledge/basic_operations/11_skip_if_exists.py`

```python
"""This cookbook shows how to skip content if it already exists in the knowledge base.
Existing content is skipped by default.

1. Run: `python cookbook/agent_concepts/knowledge/11_skip_if_exists.py` to run the cookbook
"""

import asyncio

from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# Add from local file to the knowledge base
asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        skip_if_exists=True,  # True by default
    )
)

# Add from local file to the knowledge base, but don't skip if it already exists
asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        skip_if_exists=False,
    )
)
```

---

<a name="knowledge--basic_operations--12_skip_if_exists_contentsdbpy"></a>

### `knowledge/basic_operations/12_skip_if_exists_contentsdb.py`

```python
"""This cookbook shows how to skip content if it already exists in the knowledge base.
It also demonstrates how contents can be added to the contents database post processing when
it is already in the vectorDB.
Existing content is skipped by default.

1. Run: `python cookbook/agent_concepts/knowledge/12_skip_if_exists_contentsdb.py` to run the cookbook
"""

import asyncio

from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)
# Add from a URL to the knowledge base
asyncio.run(
    knowledge.add_content_async(
        name="CV",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        skip_if_exists=True,  # True by default
    )
)

# Now add a contents_db to our Knowledge instance
contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)
knowledge.contents_db = contents_db

# Add from a URL to the knowledge base that already exists in the vectorDB, but adds it to the contentsDB
asyncio.run(
    knowledge.add_content_async(
        name="CV",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        skip_if_exists=True,
    )
)
```

---

<a name="knowledge--basic_operations--13_specify_readerpy"></a>

### `knowledge/basic_operations/13_specify_reader.py`

```python
"""This cookbook shows how to specify a reader for reading content.
Readers are assigned by default to the content based on the file extension.
You can specify a reader for a specific content by passing the reader to the add_content method
if you want to use a different reader for a specific content.

1. Run: `python cookbook/agent_concepts/knowledge/13_specify_reader.py` to run the cookbook
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

# Use a specific reader
asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        reader=PDFReader(),
    )
)

agent = Agent(knowledge=knowledge)

agent.print_response("What can you tell me about my documents?", markdown=True)
```

---

<a name="knowledge--basic_operations--14_syncpy"></a>

### `knowledge/basic_operations/14_sync.py`

```python
"""This cookbook shows how to add content from a local file to the knowledge base.
1. Run: `python cookbook/agent_concepts/knowledge/14_sync.py` to run the cookbook
"""

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)
# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

knowledge.add_content(
    name="CV",
    path="cookbook/knowledge/testing_resources/cv_1.pdf",
    metadata={"user_tag": "Engineering Candidates"},
)


agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "What skills does Jordan Mitchell have?",
    markdown=True,
)
```

---

<a name="knowledge--basic_operations--15_text_contentpy"></a>

### `knowledge/basic_operations/15_text_content.py`

```python
"""This cookbook shows how to add content from a local file to the knowledge base.
1. Run: `python cookbook/agent_concepts/knowledge/01_from_path.py` to run the cookbook
"""

import asyncio

from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)

vector_db = PgVector(
    table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
)
# Create Knowledge Instance
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=vector_db,
    contents_db=contents_db,
)

asyncio.run(
    knowledge.add_content_async(
        name="Text Content",
        text_content="Cats and dogs are pets.",
        metadata={"user_tag": "Animals"},
    )
)


asyncio.run(
    knowledge.add_contents_async(
        name="Text Content",
        text_contents=["Cats and dogs are pets.", "Birds and fish are not pets."],
        metadata={"user_tag": "Animals"},
    )
)
```

---

<a name="knowledge--chunking--agentic_chunkingpy"></a>

### `knowledge/chunking/agentic_chunking.py`

```python
from agno.agent import Agent
from agno.knowledge.chunking.agentic import AgenticChunking
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes_agentic_chunking", db_url=db_url),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    reader=PDFReader(
        name="Agentic Chunking Reader",
        chunking_strategy=AgenticChunking(),
    ),
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--chunking--csv_row_chunkingpy"></a>

### `knowledge/chunking/csv_row_chunking.py`

```python
from agno.agent import Agent
from agno.knowledge.chunking.row import RowChunking
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.csv_reader import CSVReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = Knowledge(
    vector_db=PgVector(table_name="imdb_movies_row_chunking", db_url=db_url),
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
    reader=CSVReader(
        chunking_strategy=RowChunking(),
    ),
)

# Initialize the Agent with the knowledge_base
agent = Agent(
    knowledge=knowledge_base,
    search_knowledge=True,
)

# Use the agent
agent.print_response("Tell me about the movie Guardians of the Galaxy", markdown=True)
```

---

<a name="knowledge--chunking--custom_strategy_examplepy"></a>

### `knowledge/chunking/custom_strategy_example.py`

```python
from typing import List

from agno.agent import Agent
from agno.knowledge.chunking.strategy import ChunkingStrategy
from agno.knowledge.document.base import Document
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector


class CustomSeparatorChunking(ChunkingStrategy):
    """
    Example implementation of a custom chunking strategy.

    This demonstrates how you can implement your own chunking strategy by:
    1. Inheriting from ChunkingStrategy
    2. Implementing the chunk() method
    3. Using the inherited clean_text() method
    4. Adding your own custom logic and parameters

    You can extend this pattern for your specific needs:
    - Different splitting logic (regex patterns, AI-based splitting, etc.)
    - Custom parameters (max_words, min_length, overlap, etc.)
    - Domain-specific chunking (code blocks, tables, sections, etc.)
    - Custom metadata and chunk enrichment
    """

    def __init__(self, separator: str = "---", **kwargs):
        """
        Initialize your custom chunking strategy.

        Args:
            separator: The string pattern to split documents on
            **kwargs: Additional parameters for your custom logic
        """
        self.separator = separator

    def chunk(self, document: Document) -> List[Document]:
        """
        Implement your custom chunking logic.

        This method receives a Document and must return a list of chunked Documents.
        You can implement any splitting logic here - this example uses simple separator splitting.
        """
        # Split by your custom separator
        chunks = document.content.split(self.separator)

        result = []
        for i, chunk_content in enumerate(chunks):
            # Use the inherited clean_text method for consistent text processing
            chunk_content = self.clean_text(chunk_content)

            if chunk_content:  # Only create non-empty chunks
                # Preserve original metadata and add chunk-specific info
                meta_data = document.meta_data.copy()
                meta_data["chunk"] = i + 1
                meta_data["separator_used"] = self.separator  # Your custom metadata
                meta_data["chunking_strategy"] = "custom_separator"

                result.append(
                    Document(
                        id=f"{document.id}_{i + 1}" if document.id else None,
                        name=document.name,
                        meta_data=meta_data,
                        content=chunk_content,
                    )
                )
        return result


# Example usage showing how to use your custom chunking strategy
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes_custom_strategy", db_url=db_url),
)

# Use your custom chunking strategy with any reader
# You can customize the separator based on your document structure:
# - "###" for markdown headers
# - "||" for data separators
# - "\n\n" for paragraph breaks
# - "---" for section dividers
# - Any custom pattern that fits your content
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    reader=PDFReader(
        name="Custom Strategy Reader",
        chunking_strategy=CustomSeparatorChunking(separator="---"),
    ),
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--chunking--document_chunkingpy"></a>

### `knowledge/chunking/document_chunking.py`

```python
from agno.agent import Agent
from agno.knowledge.chunking.document import DocumentChunking
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes_document_chunking", db_url=db_url),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    reader=PDFReader(
        name="Document Chunking Reader",
        chunking_strategy=DocumentChunking(),
    ),
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--chunking--fixed_size_chunkingpy"></a>

### `knowledge/chunking/fixed_size_chunking.py`

```python
from agno.agent import Agent
from agno.knowledge.chunking.fixed import FixedSizeChunking
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes_fixed_size_chunking", db_url=db_url),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    reader=PDFReader(
        name="Fixed Size Chunking Reader",
        chunking_strategy=FixedSizeChunking(),
    ),
)
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--chunking--recursive_chunkingpy"></a>

### `knowledge/chunking/recursive_chunking.py`

```python
from agno.agent import Agent
from agno.knowledge.chunking.recursive import RecursiveChunking
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes_recursive_chunking", db_url=db_url),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    reader=PDFReader(
        name="Recursive Chunking Reader",
        chunking_strategy=RecursiveChunking(),
    ),
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--chunking--semantic_chunkingpy"></a>

### `knowledge/chunking/semantic_chunking.py`

```python
from agno.agent import Agent
from agno.knowledge.chunking.semantic import SemanticChunking
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes_semantic_chunking", db_url=db_url),
)
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    reader=PDFReader(
        name="Semantic Chunking Reader",
        chunking_strategy=SemanticChunking(similarity_threshold=0.5),
    ),
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--custom_retriever--async_retrieverpy"></a>

### `knowledge/custom_retriever/async_retriever.py`

```python
import asyncio
from typing import Optional

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant
from qdrant_client import AsyncQdrantClient

# ---------------------------------------------------------
# This section loads the knowledge base. Skip if your knowledge base was populated elsewhere.
# Define the embedder
embedder = OpenAIEmbedder(id="text-embedding-3-small")
# Initialize vector database connection
vector_db = Qdrant(
    collection="thai-recipes", url="http://localhost:6333", embedder=embedder
)
# Load the knowledge base
knowledge = Knowledge(
    vector_db=vector_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)


# ---------------------------------------------------------
# Define the custom async knowledge retriever
# This is the function that the agent will use to retrieve documents
async def knowledge_retriever(
    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs
) -> Optional[list[dict]]:
    """
    Custom async knowledge retriever function to search the vector database for relevant documents.

    Args:
        query (str): The search query string
        agent (Agent): The agent instance making the query
        num_documents (int): Number of documents to retrieve (default: 5)
        **kwargs: Additional keyword arguments

    Returns:
        Optional[list[dict]]: List of retrieved documents or None if search fails
    """
    try:
        qdrant_client = AsyncQdrantClient(url="http://localhost:6333")
        query_embedding = embedder.get_embedding(query)
        results = await qdrant_client.query_points(
            collection_name="thai-recipes",
            query=query_embedding,
            limit=num_documents,
        )
        results_dict = results.model_dump()
        if "points" in results_dict:
            return results_dict["points"]
        else:
            return None
    except Exception as e:
        print(f"Error during vector database search: {str(e)}")
        return None


async def amain():
    """Async main function to demonstrate agent usage."""
    # Initialize agent with custom knowledge retriever
    # Remember to set search_knowledge=True to use agentic_rag or add_reference=True for traditional RAG
    # search_knowledge=True is default when you add a knowledge base but is needed here
    agent = Agent(
        knowledge_retriever=knowledge_retriever,
        search_knowledge=True,
        instructions="Search the knowledge base for information",
    )

    # Example query
    query = "List down the ingredients to make Massaman Gai"
    await agent.aprint_response(query, markdown=True)


def main():
    """Synchronous wrapper for main function"""
    asyncio.run(amain())


if __name__ == "__main__":
    main()
```

---

<a name="knowledge--custom_retriever--retrieverpy"></a>

### `knowledge/custom_retriever/retriever.py`

```python
from typing import Optional

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant
from qdrant_client import QdrantClient

# ---------------------------------------------------------
# This section loads the knowledge base. Skip if your knowledge base was populated elsewhere.
# Define the embedder
embedder = OpenAIEmbedder(id="text-embedding-3-small")
# Initialize vector database connection
vector_db = Qdrant(
    collection="thai-recipes", url="http://localhost:6333", embedder=embedder
)
# Load the knowledge base
knowledge = Knowledge(
    vector_db=vector_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# ---------------------------------------------------------


# Define the custom knowledge retriever
# This is the function that the agent will use to retrieve documents
def knowledge_retriever(
    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs
) -> Optional[list[dict]]:
    """
    Custom knowledge retriever function to search the vector database for relevant documents.

    Args:
        query (str): The search query string
        agent (Agent): The agent instance making the query
        num_documents (int): Number of documents to retrieve (default: 5)
        **kwargs: Additional keyword arguments

    Returns:
        Optional[list[dict]]: List of retrieved documents or None if search fails
    """
    try:
        qdrant_client = QdrantClient(url="http://localhost:6333")
        query_embedding = embedder.get_embedding(query)
        results = qdrant_client.query_points(
            collection_name="thai-recipes",
            query=query_embedding,
            limit=num_documents,
        )
        results_dict = results.model_dump()
        if "points" in results_dict:
            return results_dict["points"]
        else:
            return None
    except Exception as e:
        print(f"Error during vector database search: {str(e)}")
        return None


def main():
    """Main function to demonstrate agent usage."""
    # Initialize agent with custom knowledge retriever
    # Remember to set search_knowledge=True to use agentic_rag or add_reference=True for traditional RAG
    # search_knowledge=True is default when you add a knowledge base but is needed here
    agent = Agent(
        knowledge_retriever=knowledge_retriever,
        search_knowledge=True,
        instructions="Search the knowledge base for information",
    )

    # Example query
    query = "List down the ingredients to make Massaman Gai"
    agent.print_response(query, markdown=True)


if __name__ == "__main__":
    main()
```

---

<a name="knowledge--embedders--aws_bedrock_embedderpy"></a>

### `knowledge/embedders/aws_bedrock_embedder.py`

```python
from agno.knowledge.embedder.aws_bedrock import AwsBedrockEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

embeddings = AwsBedrockEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)
# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        embedder=AwsBedrockEmbedder(),
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    reader=PDFReader(
        chunk_size=2048
    ),  # Required because cohere has a fixed size of 2048
)
```

---

<a name="knowledge--embedders--azure_embedderpy"></a>

### `knowledge/embedders/azure_embedder.py`

```python
from agno.knowledge.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = AzureOpenAIEmbedder(id="text-embedding-3-small").get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="azure_openai_embeddings",
        embedder=AzureOpenAIEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--cohere_embedderpy"></a>

### `knowledge/embedders/cohere_embedder.py`

```python
import asyncio

from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = CohereEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)
# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="cohere_embeddings",
        embedder=CohereEmbedder(
            dimensions=1024,
        ),
    ),
    max_results=2,
)

asyncio.run(
    knowledge.add_content_async(
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
    )
)
```

---

<a name="knowledge--embedders--fireworks_embedderpy"></a>

### `knowledge/embedders/fireworks_embedder.py`

```python
from agno.knowledge.embedder.fireworks import FireworksEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = FireworksEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="fireworks_embeddings",
        embedder=FireworksEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--gemini_embedderpy"></a>

### `knowledge/embedders/gemini_embedder.py`

```python
from agno.knowledge.embedder.google import GeminiEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = GeminiEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="gemini_embeddings",
        embedder=GeminiEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--huggingface_embedderpy"></a>

### `knowledge/embedders/huggingface_embedder.py`

```python
from agno.knowledge.embedder.huggingface import HuggingfaceCustomEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = HuggingfaceCustomEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="huggingface_embeddings",
        embedder=HuggingfaceCustomEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--jina_embedderpy"></a>

### `knowledge/embedders/jina_embedder.py`

```python
from agno.embedder.jina import JinaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

# Basic usage - automatically loads from JINA_API_KEY environment variable
embeddings = JinaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

custom_embedder = JinaEmbedder(
    dimensions=1024,
    late_chunking=True,  # Improved processing for long documents
    timeout=30.0,  # Request timeout in seconds
)

# Get embedding with usage information
embedding, usage = custom_embedder.get_embedding_and_usage(
    "Advanced text processing with Jina embeddings and late chunking."
)
print(f"Embedding dimensions: {len(embedding)}")
if usage:
    print(f"Usage info: {usage}")

# Example usage with Knowledge
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="jina_embeddings",
        embedder=JinaEmbedder(
            late_chunking=True,  # Better handling of long documents
            timeout=30.0,  # Configure request timeout
        ),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--langdb_embedderpy"></a>

### `knowledge/embedders/langdb_embedder.py`

```python
from agno.knowledge.embedder.langdb import LangDBEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = LangDBEmbedder().get_embedding("Embed me")

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="langdb_embeddings",
        embedder=LangDBEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--mistral_embedderpy"></a>

### `knowledge/embedders/mistral_embedder.py`

```python
import asyncio

from agno.knowledge.embedder.mistral import MistralEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = MistralEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="mistral_embeddings",
        embedder=MistralEmbedder(),
    ),
    max_results=2,
)

asyncio.run(
    knowledge.add_content_async(
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
    )
)
```

---

<a name="knowledge--embedders--nebius_embedderpy"></a>

### `knowledge/embedders/nebius_embedder.py`

```python
from agno.knowledge.embedder.nebius import NebiusEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = NebiusEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="nebius_embeddings",
        embedder=NebiusEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--ollama_embedderpy"></a>

### `knowledge/embedders/ollama_embedder.py`

```python
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = OllamaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="ollama_embeddings",
        embedder=OllamaEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--openai_embedderpy"></a>

### `knowledge/embedders/openai_embedder.py`

```python
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = OpenAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="openai_embeddings",
        embedder=OpenAIEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--qdrant_fastembedpy"></a>

### `knowledge/embedders/qdrant_fastembed.py`

```python
from agno.knowledge.embedder.fastembed import FastEmbedEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = FastEmbedEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="qdrant_embeddings",
        embedder=FastEmbedEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--sentence_transformer_embedderpy"></a>

### `knowledge/embedders/sentence_transformer_embedder.py`

```python
from agno.knowledge.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = SentenceTransformerEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="sentence_transformer_embeddings",
        embedder=SentenceTransformerEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--together_embedderpy"></a>

### `knowledge/embedders/together_embedder.py`

```python
from agno.knowledge.embedder.together import TogetherEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = TogetherEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="together_embeddings",
        embedder=TogetherEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--embedders--voyageai_embedderpy"></a>

### `knowledge/embedders/voyageai_embedder.py`

```python
from agno.knowledge.embedder.voyageai import VoyageAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = VoyageAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage:
knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="voyageai_embeddings",
        embedder=VoyageAIEmbedder(),
    ),
    max_results=2,
)
```

---

<a name="knowledge--filters--agentic_filteringpy"></a>

### `knowledge/filters/agentic_filtering.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample sales files and get their paths
downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------

knowledge = Knowledge(
    name="CSV Knowledge Base",
    description="A knowledge base for CSV files",
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge.add_contents(
    [
        {
            "path": downloaded_csv_paths[0],
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "path": downloaded_csv_paths[1],
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
        {
            "path": downloaded_csv_paths[2],
            "metadata": {
                "data_type": "survey",
                "survey_type": "customer_satisfaction",
                "year": 2024,
                "target_demographic": "mixed",
            },
        },
        {
            "path": downloaded_csv_paths[3],
            "metadata": {
                "data_type": "financial",
                "sector": "technology",
                "year": 2024,
                "report_type": "quarterly_earnings",
            },
        },
    ],
    skip_if_exists=True,
)
# Step 2: Query the knowledge base with Agent using filters from query automatically
# -----------------------------------------------------------------------------------

# Enable agentic filtering
agent = Agent(
    model=OpenAIChat("gpt-5-mini"),
    knowledge=knowledge,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
    debug_mode=True,
)

agent.print_response(
    "Tell me about revenue performance and top selling products in the region north_america and data_type sales",
    markdown=True,
)
```

---

<a name="knowledge--filters--async_filteringpy"></a>

### `knowledge/filters/async_filtering.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.DOCX
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    name="Async Filtering",
    vector_db=vector_db,
)

asyncio.run(
    knowledge.add_contents_async(
        [
            {
                "path": downloaded_cv_paths[0],
                "metadata": {
                    "user_id": "jordan_mitchell",
                    "document_type": "cv",
                    "year": 2025,
                },
            },
            {
                "path": downloaded_cv_paths[1],
                "metadata": {
                    "user_id": "taylor_brooks",
                    "document_type": "cv",
                    "year": 2025,
                },
            },
            {
                "path": downloaded_cv_paths[2],
                "metadata": {
                    "user_id": "morgan_lee",
                    "document_type": "cv",
                    "year": 2025,
                },
            },
            {
                "path": downloaded_cv_paths[3],
                "metadata": {
                    "user_id": "casey_jordan",
                    "document_type": "cv",
                    "year": 2025,
                },
            },
            {
                "path": downloaded_cv_paths[4],
                "metadata": {
                    "user_id": "alex_rivera",
                    "document_type": "cv",
                    "year": 2025,
                },
            },
        ],
    )
)


# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

if __name__ == "__main__":
    # Query for Jordan Mitchell's experience and skills
    asyncio.run(
        agent.aprint_response(
            "Tell me about Jordan Mitchell's experience and skills",
            knowledge_filters={"user_id": "jordan_mitchell"},
            markdown=True,
        )
    )
```

---

<a name="knowledge--filters--filteringpy"></a>

### `knowledge/filters/filtering.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample sales documents and get their paths
downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge with documents and metadata
# -----------------------------------------------------------------------------
knowledge = Knowledge(
    name="CSV Knowledge Base",
    description="A knowledge base for CSV files",
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge.add_contents(
    [
        {
            "path": downloaded_csv_paths[0],
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "path": downloaded_csv_paths[1],
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
        {
            "path": downloaded_csv_paths[2],
            "metadata": {
                "data_type": "survey",
                "survey_type": "customer_satisfaction",
                "year": 2024,
                "target_demographic": "mixed",
            },
        },
        {
            "path": downloaded_csv_paths[3],
            "metadata": {
                "data_type": "financial",
                "sector": "technology",
                "year": 2024,
                "report_type": "quarterly_earnings",
            },
        },
    ],
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
na_sales = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

na_sales.print_response(
    "Revenue performance and top selling products",
    knowledge_filters={"region": "north_america", "data_type": "sales"},
    markdown=True,
)
```

---

<a name="knowledge--filters--filtering_on_loadpy"></a>

### `knowledge/filters/filtering_on_load.py`

```python
from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample sales files and get their paths
downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When loading the knowledge base, we can attach metadata that will be used for filtering

# Initialize Knowledge
knowledge = Knowledge(
    vector_db=vector_db,
    max_results=5,
    contents_db=PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        knowledge_table="knowledge_contents",
    ),
)

knowledge.add_content(
    path=downloaded_csv_paths[0],
    metadata={
        "data_type": "sales",
        "quarter": "Q1",
        "year": 2024,
        "region": "north_america",
        "currency": "USD",
    },
)

knowledge.add_content(
    path=downloaded_csv_paths[1],
    metadata={
        "data_type": "sales",
        "year": 2024,
        "region": "europe",
        "currency": "EUR",
    },
)

knowledge.add_content(
    path=downloaded_csv_paths[2],
    metadata={
        "data_type": "survey",
        "survey_type": "customer_satisfaction",
        "year": 2024,
        "target_demographic": "mixed",
    },
)

knowledge.add_content(
    path=downloaded_csv_paths[3],
    metadata={
        "data_type": "financial",
        "sector": "technology",
        "year": 2024,
        "report_type": "quarterly_earnings",
    },
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    knowledge_filters={"region": "north_america", "data_type": "sales"},
)
agent.print_response(
    "Revenue performance and top selling products",
    markdown=True,
)
```

---

<a name="knowledge--filters--filtering_with_invalid_keyspy"></a>

### `knowledge/filters/filtering_with_invalid_keys.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample sales documents and get their paths
downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge with documents and metadata
# -----------------------------------------------------------------------------
knowledge = Knowledge(
    name="CSV Knowledge Base",
    description="A knowledge base for CSV files",
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge.add_contents(
    [
        {
            "path": downloaded_csv_paths[0],
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "path": downloaded_csv_paths[1],
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
        {
            "path": downloaded_csv_paths[2],
            "metadata": {
                "data_type": "survey",
                "survey_type": "customer_satisfaction",
                "year": 2024,
                "target_demographic": "mixed",
            },
        },
        {
            "path": downloaded_csv_paths[3],
            "metadata": {
                "data_type": "financial",
                "sector": "technology",
                "year": 2024,
                "report_type": "quarterly_earnings",
            },
        },
    ],
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------
na_sales = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

na_sales.print_response(
    "Revenue performance and top selling products",
    # Use "location" instead of "region" and we should receive a warning that the key is invalid
    knowledge_filters={"location": "north_america", "data_type": "sales"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_chroma_dbpy"></a>

### `knowledge/filters/vector_dbs/filtering_chroma_db.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.chroma import ChromaDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize ChromaDB
vector_db = ChromaDb(collection="recipes", path="tmp/chromadb", persistent_client=True)

# Step 1: Initialize knowledge with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    name="ChromaDB Knowledge Base",
    description="A knowledge base for ChromaDB",
    vector_db=vector_db,
)

# Load all documents into the vector database
knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)
# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_lance_dbpy"></a>

### `knowledge/filters/vector_dbs/filtering_lance_db.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    name="LanceDB Knowledge Base",
    description="A knowledge base for LanceDB",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
)
# Load all documents into the vector database

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_milvuspy"></a>

### `knowledge/filters/vector_dbs/filtering_milvus.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.milvus import Milvus

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize Milvus vector db
vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    name="Milvus Knowledge Base",
    description="A knowledge base for Milvus",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

# Load all documents into the vector database

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_mongo_dbpy"></a>

### `knowledge/filters/vector_dbs/filtering_mongo_db.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.mongodb import MongoDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    name="MongoDB Knowledge Base",
    description="A knowledge base for MongoDB",
    vector_db=MongoDb(
        collection_name="filters",
        db_url=mdb_connection_string,
        search_index_name="filters",
    ),
)

# Load all documents into the vector database
knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_pgvectorpy"></a>

### `knowledge/filters/vector_dbs/filtering_pgvector.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pgvector import PgVector

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize PgVector
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="recipes", db_url=db_url)

# Step 1: Initialize knowledge with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    name="PgVector Knowledge Base",
    description="A knowledge base for PgVector",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_pineconepy"></a>

### `knowledge/filters/vector_dbs/filtering_pinecone.py`

```python
from os import getenv

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pineconedb import PineconeDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize Pinecone
api_key = getenv("PINECONE_API_KEY")
index_name = "filtering-index"

vector_db = PineconeDb(
    name=index_name,
    dimension=1536,
    metric="cosine",
    spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
    api_key=api_key,
)


# Step 1: Initialize knowledge with documents and metadata
knowledge = Knowledge(
    name="Pinecone Knowledge Base",
    description="A knowledge base for Pinecone",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_qdrant_dbpy"></a>

### `knowledge/filters/vector_dbs/filtering_qdrant_db.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.qdrant import Qdrant

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

COLLECTION_NAME = "filtering-cv"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")
# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    name="Qdrant Knowledge Base",
    description="A knowledge base for Qdrant",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_surrealdbpy"></a>

### `knowledge/filters/vector_dbs/filtering_surrealdb.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import Surreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Create a client
client = Surreal(url=SURREALDB_URL)
client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

vector_db = SurrealDb(
    client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
)


# Step 1: Initialize knowledge base with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge base, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

knowledge = Knowledge(
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

# Option 1: Filters on the Agent
# Initialize the Agent with the knowledge base and filters
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--filters--vector_dbs--filtering_weaviatepy"></a>

### `knowledge/filters/vector_dbs/filtering_weaviate.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Step 1: Initialize knowledge with documents and metadata
# ------------------------------------------------------------------------------
# When initializing the knowledge, we can attach metadata that will be used for filtering
# This metadata can include user IDs, document types, dates, or any other attributes

vector_db = Weaviate(
    collection="recipes",
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=False,  # Set to False if using Weaviate Cloud and True if using local instance
)

knowledge = Knowledge(
    name="Weaviate Knowledge Base",
    description="A knowledge base for Weaviate",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

# Step 2: Query the knowledge base with different filter combinations
# ------------------------------------------------------------------------------

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)
```

---

<a name="knowledge--knowledge_toolspy"></a>

### `knowledge/knowledge_tools.py`

```python
"""
Here is a tool with reasoning capabilities to allow agents to search and analyze information from a knowledge base.

1. Run: `pip install openai agno lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY
3. Run: `python cookbook/knowledge/knowledge_tools.py` to run the agent
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.tools.knowledge import KnowledgeTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge containing information from a URL
agno_docs = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Add content to the knowledge
agno_docs.add_content(url="https://docs.agno.com/llms-full.txt")

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    enable_think=True,
    enable_search=True,
    enable_analyze=True,
    add_few_shot=True,
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[knowledge_tools],
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response(
        "How do I build a team of agents in agno?",
        markdown=True,
        stream=True,
    )
```

---

<a name="knowledge--readers--arxiv_readerpy"></a>

### `knowledge/readers/arxiv_reader.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.arxiv_reader import ArxivReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the ArXiv documents
knowledge = Knowledge(
    # Table name: ai.arxiv_documents
    vector_db=PgVector(
        table_name="arxiv_documents",
        db_url=db_url,
    ),
)
# Load the knowledge
knowledge.add_content(
    topics=["Generative AI", "Machine Learning"],
    reader=ArxivReader(),
)

# Create an agent with the knowledge
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

# Ask the agent about the knowledge
agent.print_response("What can you tell me about Generative AI?", markdown=True)
```

---

<a name="knowledge--readers--arxiv_reader_asyncpy"></a>

### `knowledge/readers/arxiv_reader_async.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.arxiv_reader import ArxivReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    # Table name: ai.arxiv_documents
    vector_db=PgVector(
        table_name="arxiv_documents",
        db_url=db_url,
    ),
)

# Create an agent with the knowledge
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)


def main():
    # Load the knowledge
    asyncio.run(
        knowledge.add_content_async(
            topics=["Generative AI", "Machine Learning"],
            reader=ArxivReader(),
        )
    )

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What can you tell me about Generative AI?", markdown=True
        )
    )
```

---

<a name="knowledge--readers--csv_readerpy"></a>

### `knowledge/readers/csv_reader.py`

```python
from pathlib import Path

from agno.knowledge.reader.csv_reader import CSVReader

reader = CSVReader()

csv_path = Path("tmp/test.csv")


try:
    print("Starting read...")
    documents = reader.read(csv_path)

    if documents:
        for doc in documents:
            print(doc.name)
            # print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")
```

---

<a name="knowledge--readers--csv_reader_asyncpy"></a>

### `knowledge/readers/csv_reader_async.py`

```python
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
    max_results=5,  # Number of results to return on search
)

# Initialize the Agent with the knowledge
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)


if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge.add_content_async(path=Path("data/csv")))

    # Create and use the agent
    asyncio.run(agent.aprint_response("What is the csv file about", markdown=True))
```

---

<a name="knowledge--readers--csv_reader_custom_encodingspy"></a>

### `knowledge/readers/csv_reader_custom_encodings.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.csv_reader import CSVReader
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
    max_results=5,  # Number of results to return on search
)

# Initialize the Agent with the knowledge
agent = Agent(
    model=OpenAIChat(id="gpt-4.1-mini"),
    knowledge=knowledge,
    search_knowledge=True,
)


if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            url="https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
            reader=CSVReader(encoding="gb2312"),
        )
    )

    # Create and use the agent
    asyncio.run(agent.aprint_response("What is the csv file about", markdown=True))
```

---

<a name="knowledge--readers--csv_reader_url_asyncpy"></a>

### `knowledge/readers/csv_reader_url_async.py`

```python
import asyncio

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    # Table name: ai.arxiv_documents
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
    contents_db=PostgresDb(db_url=db_url),
)

# Initialize the Agent with the knowledge
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            url="https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv"
        )
    )

    # Create and use the agent
    asyncio.run(
        agent.aprint_response("What genre of movies are present here?", markdown=True)
    )
```

---

<a name="knowledge--readers--doc_kb_asyncpy"></a>

### `knowledge/readers/doc_kb_async.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

fun_facts = """
- Earth is the third planet from the Sun and the only known astronomical object to support life.
- Approximately 71% of Earth's surface is covered by water, with the Pacific Ocean being the largest.
- The Earth's atmosphere is composed mainly of nitrogen (78%) and oxygen (21%), with traces of other gases.
- Earth rotates on its axis once every 24 hours, leading to the cycle of day and night.
- The planet has one natural satellite, the Moon, which influences tides and stabilizes Earth's axial tilt.
- Earth's tectonic plates are constantly shifting, leading to geological activities like earthquakes and volcanic eruptions.
- The highest point on Earth is Mount Everest, standing at 8,848 meters (29,029 feet) above sea level.
- The deepest part of the ocean is the Mariana Trench, reaching depths of over 11,000 meters (36,000 feet).
- Earth has a diverse range of ecosystems, from rainforests and deserts to coral reefs and tundras.
- The planet's magnetic field protects life by deflecting harmful solar radiation and cosmic rays.
"""

# Database connection URL
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="documents",
        db_url=db_url,
    ),
)

# Create an agent with the knowledge
agent = Agent(
    knowledge=knowledge,
)


async def main():
    # Load the knowledge
    await knowledge.add_content_async(
        text_content=fun_facts,
    )

    # Ask the agent about the knowledge
    await agent.aprint_response("Could you tell me about the earth?", markdown=True)


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="knowledge--readers--firecrawl_readerpy"></a>

### `knowledge/readers/firecrawl_reader.py`

```python
import os

from agno.knowledge.reader.firecrawl_reader import FirecrawlReader

api_key = os.getenv("FIRECRAWL_API_KEY")

reader = FirecrawlReader(
    api_key=api_key,
    mode="scrape",
    chunk=True,
    # for crawling
    # params={
    #     'limit': 5,
    #     'scrapeOptions': {'formats': ['markdown']}
    # }
    # for scraping
    params={"formats": ["markdown"]},
)

try:
    print("Starting scrape...")
    documents = reader.read("https://github.com/agno-agi/agno")

    if documents:
        for doc in documents:
            print(doc.name)
            print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")
```

---

<a name="knowledge--readers--json_readerpy"></a>

### `knowledge/readers/json_reader.py`

```python
import json
from pathlib import Path

from agno.knowledge.reader.json_reader import JSONReader

reader = JSONReader()

json_path = Path("tmp/test.json")
test_data = {"key": "value"}
json_path.write_text(json.dumps(test_data))

try:
    print("Starting read...")
    documents = reader.read(json_path)

    if documents:
        for doc in documents:
            print(doc.name)
            print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")
```

---

<a name="knowledge--readers--markdown_reader_asyncpy"></a>

### `knowledge/readers/markdown_reader_async.py`

```python
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


knowledge = Knowledge(
    vector_db=PgVector(
        table_name="markdown_documents",
        db_url=db_url,
    ),
    max_results=5,  # Number of results to return on search
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

if __name__ == "__main__":
    asyncio.run(
        knowledge.add_content_async(
            path=Path("README.md"),
        )
    )

    asyncio.run(
        agent.aprint_response(
            "What can you tell me about Agno?",
            markdown=True,
        )
    )
```

---

<a name="knowledge--readers--pdf_reader_asyncpy"></a>

### `knowledge/readers/pdf_reader_async.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the PDFs from the data/pdfs directory
knowledge = Knowledge(
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    )
)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

if __name__ == "__main__":
    asyncio.run(
        knowledge.add_content_async(
            path="cookbook/knowledge/testing_resources/cv_1.pdf",
        )
    )
    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What skills does an applicant require to apply for the Software Engineer position?",
            markdown=True,
        )
    )
```

---

<a name="knowledge--readers--pdf_reader_passwordpy"></a>

### `knowledge/readers/pdf_reader_password.py`

```python
from agno.agent import Agent
from agno.knowledge.content import ContentAuth
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import download_file
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
download_file(
    "https://agno-public.s3.us-east-1.amazonaws.com/recipes/ThaiRecipes_protected.pdf",
    "ThaiRecipes_protected.pdf",
)

# Create a knowledge base with simplified password handling
knowledge = Knowledge(
    vector_db=PgVector(
        table_name="pdf_documents_password",
        db_url=db_url,
    ),
)

knowledge.add_content(
    path="ThaiRecipes_protected.pdf",
    auth=ContentAuth(password="ThaiRecipes"),
)

# Create an agent with the knowledge base
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("Give me the recipe for pad thai")
```

---

<a name="knowledge--readers--pdf_reader_url_passwordpy"></a>

### `knowledge/readers/pdf_reader_url_password.py`

```python
from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.content import ContentAuth
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with simplified password handling
knowledge = Knowledge(
    vector_db=PgVector(
        table_name="pdf_documents_password",
        db_url=db_url,
    ),
    contents_db=PostgresDb(db_url=db_url),
)

knowledge.add_content(
    url="https://agno-public.s3.us-east-1.amazonaws.com/recipes/ThaiRecipes_protected.pdf",
    auth=ContentAuth(password="ThaiRecipes"),
)


# Create an agent with the knowledge
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response("Give me the recipe for pad thai")
```

---

<a name="knowledge--readers--web_readerpy"></a>

### `knowledge/readers/web_reader.py`

```python
from agno.knowledge.reader.website_reader import WebsiteReader

reader = WebsiteReader(max_depth=3, max_links=10)


try:
    print("Starting read...")
    documents = reader.read("https://docs.agno.com/introduction")
    if documents:
        for doc in documents:
            print(doc.name)
            print(doc.content)
            print(f"Content length: {len(doc.content)}")
            print("-" * 80)
    else:
        print("No documents were returned")

except Exception as e:
    print(f"Error type: {type(e)}")
    print(f"Error occurred: {str(e)}")
```

---

<a name="knowledge--readers--web_search_readerpy"></a>

### `knowledge/readers/web_search_reader.py`

```python
from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.web_search_reader import WebSearchReader
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"


db = PostgresDb(id="web-search-db", db_url=db_url)

vector_db = PgVector(
    db_url=db_url,
    table_name="web_search_documents",
)
knowledge = Knowledge(
    name="Web Search Documents",
    contents_db=db,
    vector_db=vector_db,
)


# Load knowledge from web search
knowledge.add_content(
    topics=["agno"],
    reader=WebSearchReader(
        max_results=3,
        search_engine="duckduckgo",
        chunk=True,
    ),
)

# Create an agent with the knowledge
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

# Ask the agent about the knowledge
agent.print_response(
    "What are the latest AI trends according to the search results?", markdown=True
)
```

---

<a name="knowledge--readers--web_search_reader_asyncpy"></a>

### `knowledge/readers/web_search_reader_async.py`

```python
import asyncio

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.web_search_reader import WebSearchReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(id="web-search-db", db_url=db_url)

vector_db = PgVector(
    db_url=db_url,
    table_name="web_search_documents",
)
knowledge = Knowledge(
    name="Web Search Documents",
    contents_db=db,
    vector_db=vector_db,
)


# Initialize the Agent with the knowledge
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)


if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            topics=["web3 latest trends 2025"],
            reader=WebSearchReader(
                max_results=3,
                search_engine="duckduckgo",
                chunk=True,
            ),
        )
    )

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What are the latest AI trends according to the search results?",
            markdown=True,
        )
    )
```

---

<a name="knowledge--readers--website_readerpy"></a>

### `knowledge/readers/website_reader.py`

```python
from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.website_reader import WebsiteReader
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create a knowledge base with the ArXiv documents
knowledge = Knowledge(
    # Table name: ai.website_documents
    vector_db=PgVector(
        table_name="website_documents",
        db_url=db_url,
        embedder=OpenAIEmbedder(),
    ),
)
# Load the knowledge
knowledge.add_content(
    url="https://en.wikipedia.org/wiki/OpenAI",
    reader=WebsiteReader(),
)

# Create an agent with the knowledge
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    knowledge=knowledge,
    search_knowledge=True,
)

# Ask the agent about the knowledge
agent.print_response("What can you tell me about Generative AI?", markdown=True)
```

---

<a name="knowledge--search_type--hybrid_searchpy"></a>

### `knowledge/search_type/hybrid_search.py`

```python
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Load knowledge base using hybrid search
hybrid_db = PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.hybrid)
knowledge = Knowledge(
    name="Hybrid Search Knowledge Base",
    vector_db=hybrid_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# Run a hybrid search query
results = hybrid_db.search("chicken coconut soup", limit=5)
print("Hybrid Search Results:", results)
```

---

<a name="knowledge--search_type--keyword_searchpy"></a>

### `knowledge/search_type/keyword_search.py`

```python
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Load knowledge base using keyword search
keyword_db = PgVector(
    table_name="recipes", db_url=db_url, search_type=SearchType.keyword
)
knowledge = Knowledge(
    name="Keyword Search Knowledge Base",
    vector_db=keyword_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# Run a keyword-based query
results = keyword_db.search("chicken coconut soup", limit=5)
print("Keyword Search Results:", results)
```

---

<a name="knowledge--search_type--vector_searchpy"></a>

### `knowledge/search_type/vector_search.py`

```python
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Load knowledge base using vector search
vector_db = PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.vector)
knowledge = Knowledge(
    name="Vector Search Knowledge Base",
    vector_db=vector_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# Run a vector-based query
results = vector_db.search("chicken coconut soup", limit=5)
print("Vector Search Results:", results)
```

---

<a name="knowledge--vector_db--cassandra_db--async_cassandra_dbpy"></a>

### `knowledge/vector_db/cassandra_db/async_cassandra_db.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.mistral import MistralEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.mistral import MistralChat
from agno.vectordb.cassandra import Cassandra

try:
    from cassandra.cluster import Cluster  # type: ignore
except (ImportError, ModuleNotFoundError):
    raise ImportError(
        "Could not import cassandra-driver python package.Please install it with pip install cassandra-driver."
    )

cluster = Cluster()

session = cluster.connect()
session.execute(
    """
    CREATE KEYSPACE IF NOT EXISTS testkeyspace
    WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
    """
)

knowledge = Knowledge(
    vector_db=Cassandra(
        table_name="recipes",
        keyspace="testkeyspace",
        session=session,
        embedder=MistralEmbedder(),
    ),
)

agent = Agent(
    model=MistralChat(),
    knowledge=knowledge,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(url="https://docs.agno.com/introduction/agents.md")
    )

    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What is the purpose of an Agno Agent?",
            markdown=True,
        )
    )
```

---

<a name="knowledge--vector_db--cassandra_db--cassandra_dbpy"></a>

### `knowledge/vector_db/cassandra_db/cassandra_db.py`

```python
from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.cassandra import Cassandra

try:
    from cassandra.cluster import Cluster  # type: ignore
except (ImportError, ModuleNotFoundError):
    raise ImportError(
        "Could not import cassandra-driver python package.Please install it with pip install cassandra-driver."
    )

cluster = Cluster()

session = cluster.connect()
session.execute(
    """
    CREATE KEYSPACE IF NOT EXISTS testkeyspace
    WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
    """
)
vector_db = Cassandra(
    table_name="recipes",
    keyspace="testkeyspace",
    session=session,
    embedder=OpenAIEmbedder(
        dimensions=1024,
    ),
)

knowledge = Knowledge(
    name="My Cassandra Knowledge Base",
    vector_db=vector_db,
)


knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

agent = Agent(
    knowledge=knowledge,
)

agent.print_response(
    "What are the health benefits of Khao Niew Dam Piek Maphrao Awn?",
    markdown=True,
    show_full_reasoning=True,
)

vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--chroma_db--async_chroma_dbpy"></a>

### `knowledge/vector_db/chroma_db/async_chroma_db.py`

```python
# install chromadb - `pip install chromadb`

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.chroma import ChromaDb

# Initialize ChromaDB
vector_db = ChromaDb(collection="recipes", path="tmp/chromadb", persistent_client=True)

# Create knowledge base
knowledge = Knowledge(
    vector_db=vector_db,
)

# Create and use the agent
agent = Agent(knowledge=knowledge)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(url="https://docs.agno.com/introduction/agents.md")
    )

    # Create and use the agent
    asyncio.run(
        agent.aprint_response("What is the purpose of an Agno Agent?", markdown=True)
    )
```

---

<a name="knowledge--vector_db--chroma_db--chroma_dbpy"></a>

### `knowledge/vector_db/chroma_db/chroma_db.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.chroma import ChromaDb

# Create Knowledge Instance with ChromaDB
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with ChromaDB",
    vector_db=ChromaDb(
        collection="vectors", path="tmp/chromadb", persistent_client=True
    ),
)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"doc_type": "recipe_book"},
    )
)

# Create and use the agent
agent = Agent(knowledge=knowledge)


agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

# Delete operations examples
vector_db = knowledge.vector_db
vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"user_tag": "Recipes from website"})
```

---

<a name="knowledge--vector_db--clickhouse_db--async_clickhousepy"></a>

### `knowledge/vector_db/clickhouse_db/async_clickhouse.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.clickhouse import Clickhouse

agent = Agent(
    knowledge=Knowledge(
        vector_db=Clickhouse(
            table_name="recipe_documents",
            host="localhost",
            port=8123,
            username="ai",
            password="ai",
        ),
    ),
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        agent.knowledge.add_content_async(
            url="https://docs.agno.com/introduction/agents.md"
        )
    )

    # Create and use the agent
    asyncio.run(
        agent.aprint_response("What is the purpose of an Agno Agent?", markdown=True)
    )
```

---

<a name="knowledge--vector_db--clickhouse_db--clickhousepy"></a>

### `knowledge/vector_db/clickhouse_db/clickhouse.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.clickhouse import Clickhouse

vector_db = Clickhouse(
    table_name="recipe_documents",
    host="localhost",
    port=8123,
    username="ai",
    password="ai",
)

knowledge = Knowledge(
    name="My Clickhouse Knowledge Base",
    description="This is a knowledge base that uses a Clickhouse DB",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

agent = Agent(
    knowledge=knowledge,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

agent.print_response("How do I make pad thai?", markdown=True)

vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--couchbase_db--async_couchbase_dbpy"></a>

### `knowledge/vector_db/couchbase_db/async_couchbase_db.py`

```python
"""
Couchbase Vector DB Example
==========================

Setup Couchbase Cluster (Local via Docker):
-------------------------------------------
1. Run Couchbase locally:

   docker run -d --name couchbase-server \
     -p 8091-8096:8091-8096 \
     -p 11210:11210 \
     -e COUCHBASE_ADMINISTRATOR_USERNAME=Administrator \
     -e COUCHBASE_ADMINISTRATOR_PASSWORD=password \
     couchbase:latest

2. Access the Couchbase UI at: http://localhost:8091
   (Login with the username and password above)

3. Create a new cluster. You can select "Finish with defaults".

4. Create a bucket named 'recipe_bucket', a scope 'recipe_scope', and a collection 'recipes'.

Managed Couchbase (Capella):
----------------------------
- For a managed cluster, use Couchbase Capella: https://cloud.couchbase.com/
- Follow Capella's UI to create a database, bucket, scope, and collection as above.

Environment Variables (export before running):
----------------------------------------------
Create a shell script (e.g., set_couchbase_env.sh):

    export COUCHBASE_USER="Administrator"
    export COUCHBASE_PASSWORD="password"
    export COUCHBASE_CONNECTION_STRING="couchbase://localhost"
    export OPENAI_API_KEY="<your-openai-api-key>"

# For Capella, set COUCHBASE_CONNECTION_STRING to the Capella connection string.

Install couchbase-sdk:
----------------------
    pip install couchbase
"""

import asyncio
import os
import time

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.couchbase import CouchbaseSearch
from couchbase.auth import PasswordAuthenticator
from couchbase.management.search import SearchIndex
from couchbase.options import ClusterOptions, KnownConfigProfiles

# Couchbase connection settings
username = os.getenv("COUCHBASE_USER")  # Replace with your username
password = os.getenv("COUCHBASE_PASSWORD")  # Replace with your password
connection_string = os.getenv("COUCHBASE_CONNECTION_STRING")

# Create cluster options with authentication
auth = PasswordAuthenticator(username, password)
cluster_options = ClusterOptions(auth)
cluster_options.apply_profile(KnownConfigProfiles.WanDevelopment)

# Define the vector search index
search_index = SearchIndex(
    name="vector_search",
    source_type="gocbcore",
    idx_type="fulltext-index",
    source_name="recipe_bucket",
    plan_params={"index_partitions": 1, "num_replicas": 0},
    params={
        "doc_config": {
            "docid_prefix_delim": "",
            "docid_regexp": "",
            "mode": "scope.collection.type_field",
            "type_field": "type",
        },
        "mapping": {
            "default_analyzer": "standard",
            "default_datetime_parser": "dateTimeOptional",
            "index_dynamic": True,
            "store_dynamic": True,
            "default_mapping": {"dynamic": True, "enabled": False},
            "types": {
                "recipe_scope.recipes": {
                    "dynamic": False,
                    "enabled": True,
                    "properties": {
                        "content": {
                            "enabled": True,
                            "fields": [
                                {
                                    "docvalues": True,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "content",
                                    "store": True,
                                    "type": "text",
                                }
                            ],
                        },
                        "embedding": {
                            "enabled": True,
                            "dynamic": False,
                            "fields": [
                                {
                                    "vector_index_optimized_for": "recall",
                                    "docvalues": True,
                                    "dims": 3072,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "embedding",
                                    "similarity": "dot_product",
                                    "store": True,
                                    "type": "vector",
                                }
                            ],
                        },
                        "meta": {
                            "dynamic": True,
                            "enabled": True,
                            "properties": {
                                "name": {
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "docvalues": True,
                                            "include_in_all": False,
                                            "include_term_vectors": False,
                                            "index": True,
                                            "name": "name",
                                            "store": True,
                                            "analyzer": "keyword",
                                            "type": "text",
                                        }
                                    ],
                                }
                            },
                        },
                    },
                }
            },
        },
    },
)

knowledge_base = Knowledge(
    vector_db=CouchbaseSearch(
        bucket_name="recipe_bucket",
        scope_name="recipe_scope",
        collection_name="recipes",
        couchbase_connection_string=connection_string,
        cluster_options=cluster_options,
        search_index=search_index,
        embedder=OpenAIEmbedder(
            id="text-embedding-3-large",
            dimensions=3072,
            api_key=os.getenv("OPENAI_API_KEY"),
        ),
        wait_until_index_ready=60,
        overwrite=True,
    ),
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base)


async def run_agent():
    await knowledge_base.async_add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    time.sleep(5)  # wait for the vector index to be sync with kv
    await agent.aprint_response("How to make Thai curry?", markdown=True)


if __name__ == "__main__":
    # Comment out after the first run
    asyncio.run(run_agent())
```

---

<a name="knowledge--vector_db--couchbase_db--couchbase_dbpy"></a>

### `knowledge/vector_db/couchbase_db/couchbase_db.py`

```python
"""
Couchbase Vector DB Example
==========================

Setup Couchbase Cluster (Local via Docker):
-------------------------------------------
1. Run Couchbase locally:

   docker run -d --name couchbase-server \
     -p 8091-8096:8091-8096 \
     -p 11210:11210 \
     -e COUCHBASE_ADMINISTRATOR_USERNAME=Administrator \
     -e COUCHBASE_ADMINISTRATOR_PASSWORD=password \
     couchbase:latest

2. Access the Couchbase UI at: http://localhost:8091
   (Login with the username and password above)

3. Create a new cluster. You can select "Finish with defaults".

4. Create a bucket named 'recipe_bucket', a scope 'recipe_scope', and a collection 'recipes'.

Managed Couchbase (Capella):
----------------------------
- For a managed cluster, use Couchbase Capella: https://cloud.couchbase.com/
- Follow Capella's UI to create a database, bucket, scope, and collection as above.

Environment Variables (export before running):
----------------------------------------------
Create a shell script (e.g., set_couchbase_env.sh):

    export COUCHBASE_USER="Administrator"
    export COUCHBASE_PASSWORD="password"
    export COUCHBASE_CONNECTION_STRING="couchbase://localhost"
    export OPENAI_API_KEY="<your-openai-api-key>"

# For Capella, set COUCHBASE_CONNECTION_STRING to the Capella connection string.

Install couchbase-sdk:
----------------------
    pip install couchbase
"""

import os

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.couchbase import CouchbaseSearch
from couchbase.auth import PasswordAuthenticator
from couchbase.management.search import SearchIndex
from couchbase.options import ClusterOptions, KnownConfigProfiles

# Couchbase connection settings
username = os.getenv("COUCHBASE_USER")
password = os.getenv("COUCHBASE_PASSWORD")
connection_string = os.getenv("COUCHBASE_CONNECTION_STRING")

# Create cluster options with authentication
auth = PasswordAuthenticator(username, password)
cluster_options = ClusterOptions(auth)
cluster_options.apply_profile(KnownConfigProfiles.WanDevelopment)

# Define the vector search index
search_index = SearchIndex(
    name="vector_search",
    source_type="gocbcore",
    idx_type="fulltext-index",
    source_name="recipe_bucket",
    plan_params={"index_partitions": 1, "num_replicas": 0},
    params={
        "doc_config": {
            "docid_prefix_delim": "",
            "docid_regexp": "",
            "mode": "scope.collection.type_field",
            "type_field": "type",
        },
        "mapping": {
            "default_analyzer": "standard",
            "default_datetime_parser": "dateTimeOptional",
            "index_dynamic": True,
            "store_dynamic": True,
            "default_mapping": {"dynamic": True, "enabled": False},
            "types": {
                "recipe_scope.recipes": {
                    "dynamic": False,
                    "enabled": True,
                    "properties": {
                        "content": {
                            "enabled": True,
                            "fields": [
                                {
                                    "docvalues": True,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "content",
                                    "store": True,
                                    "type": "text",
                                }
                            ],
                        },
                        "embedding": {
                            "enabled": True,
                            "dynamic": False,
                            "fields": [
                                {
                                    "vector_index_optimized_for": "recall",
                                    "docvalues": True,
                                    "dims": 1536,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "embedding",
                                    "similarity": "dot_product",
                                    "store": True,
                                    "type": "vector",
                                }
                            ],
                        },
                        "meta": {
                            "dynamic": True,
                            "enabled": True,
                            "properties": {
                                "name": {
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "docvalues": True,
                                            "include_in_all": False,
                                            "include_term_vectors": False,
                                            "index": True,
                                            "name": "name",
                                            "store": True,
                                            "analyzer": "keyword",
                                            "type": "text",
                                        }
                                    ],
                                }
                            },
                        },
                    },
                }
            },
        },
    },
)
vector_db = CouchbaseSearch(
    bucket_name="recipe_bucket",
    scope_name="recipe_scope",
    collection_name="recipes",
    couchbase_connection_string=connection_string,
    cluster_options=cluster_options,
    search_index=search_index,
    embedder=OpenAIEmbedder(
        dimensions=1536,
    ),
    wait_until_index_ready=60,
    overwrite=True,
)

knowledge = Knowledge(
    name="Couchbase Knowledge Base",
    description="This is a knowledge base that uses a Couchbase DB",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)
agent = Agent(
    knowledge=knowledge,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--lance_db--lance_dbpy"></a>

### `knowledge/vector_db/lance_db/lance_db.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.lancedb import LanceDb

vector_db = LanceDb(
    table_name="vectors",
    uri="tmp/lancedb",
)

# Create Knowledge Instance with LanceDB
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with LanceDB",
    vector_db=vector_db,
)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"doc_type": "recipe_book"},
    )
)

# Create and use the agent
agent = Agent(knowledge=knowledge)
agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--lance_db--lance_db_hybrid_searchpy"></a>

### `knowledge/vector_db/lance_db/lance_db_hybrid_search.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    name="My PG Vector Knowledge Base",
    description="This is a knowledge base that uses a PG Vector DB",
    vector_db=LanceDb(
        table_name="vectors",
        uri="tmp/lancedb",
        search_type=SearchType.hybrid,
    ),
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge,
    search_knowledge=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
```

---

<a name="knowledge--vector_db--lance_db--lance_db_with_mistral_embedderpy"></a>

### `knowledge/vector_db/lance_db/lance_db_with_mistral_embedder.py`

```python
import asyncio

from agno.knowledge.embedder.mistral import MistralEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.lancedb import LanceDb, SearchType

embedder_mi = MistralEmbedder()

vector_db = LanceDb(
    uri="tmp/lancedb",
    table_name="documents",
    embedder=embedder_mi,
    search_type=SearchType.hybrid,
)

reader = PDFReader(
    chunk_size=1024,
)

knowledge = Knowledge(
    name="My Document Knowledge Base",
    vector_db=vector_db,
)


asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        reader=reader,
    )
)
```

---

<a name="knowledge--vector_db--langchain--async_langchain_dbpy"></a>

### `knowledge/vector_db/langchain/async_langchain_db.py`

```python
"""
pip install langchain langchain-community langchain-openai langchain-chroma agno
"""

import asyncio
import pathlib

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.langchaindb import LangChainVectorDb
from langchain.text_splitter import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings


async def main():
    # Define the directory where the Chroma database is located
    chroma_db_dir = pathlib.Path("./chroma_db")

    # Define the path to the document to be loaded into the knowledge base
    state_of_the_union = pathlib.Path(
        "cookbook/knowledge/testing_resources/state_of_the_union.txt"
    )

    # Load the document (this is synchronous in LangChain)
    raw_documents = TextLoader(str(state_of_the_union), encoding="utf-8").load()

    # Split the document into chunks
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    documents = text_splitter.split_documents(raw_documents)

    # Embed each chunk and load it into the vector store
    # Note: Chroma.from_documents is synchronous, but we can run it in a thread pool
    await asyncio.get_event_loop().run_in_executor(
        None,
        lambda: Chroma.from_documents(
            documents, OpenAIEmbeddings(), persist_directory=str(chroma_db_dir)
        ),
    )

    # Get the vector database
    db = Chroma(
        embedding_function=OpenAIEmbeddings(), persist_directory=str(chroma_db_dir)
    )

    # Create a knowledge retriever from the vector store
    knowledge_retriever = db.as_retriever()

    # Create a knowledge instance
    knowledge = Knowledge(
        vector_db=LangChainVectorDb(knowledge_retriever=knowledge_retriever)
    )

    # Create an agent with the knowledge base
    agent = Agent(model=OpenAIChat("gpt-5-mini"), knowledge=knowledge)
    # Use the agent to ask a question and print a response asynchronously
    await agent.aprint_response(
        "What did the president say about broadcasting and the State of the Union?",
        markdown=True,
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="knowledge--vector_db--langchain--langchain_dbpy"></a>

### `knowledge/vector_db/langchain/langchain_db.py`

```python
"""
pip install langchain langchain-community langchain-openai langchain-chroma agno
"""

import pathlib

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.langchaindb import LangChainVectorDb
from langchain.text_splitter import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings

# Define the directory where the Chroma database is located
chroma_db_dir = pathlib.Path("./chroma_db")

# Define the path to the document to be loaded into the knowledge base
state_of_the_union = pathlib.Path(
    "cookbook/knowledge/testing_resources/state_of_the_union.txt"
)

# Load the document
raw_documents = TextLoader(str(state_of_the_union), encoding="utf-8").load()

# Split the document into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)

# Embed each chunk and load it into the vector store
Chroma.from_documents(
    documents, OpenAIEmbeddings(), persist_directory=str(chroma_db_dir)
)

# Get the vector database
db = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=str(chroma_db_dir))

# Create a knowledge retriever from the vector store
knowledge_retriever = db.as_retriever()

# Create a knowledge instance
knowledge = Knowledge(
    vector_db=LangChainVectorDb(knowledge_retriever=knowledge_retriever)
)

# Create an agent with the knowledge base
agent = Agent(model=OpenAIChat("gpt-5-mini"), knowledge=knowledge)

# Use the agent to ask a question and print a response.
agent.print_response(
    "What did the president say about broadcasting and the State of the Union?",
    markdown=True,
)
```

---

<a name="knowledge--vector_db--lightrag--lightragpy"></a>

### `knowledge/vector_db/lightrag/lightrag.py`

```python
import asyncio
from os import getenv

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.wikipedia_reader import WikipediaReader
from agno.vectordb.lightrag import LightRag

vector_db = LightRag(
    api_key=getenv("LIGHTRAG_API_KEY"),
)

knowledge = Knowledge(
    name="My Pinecone Knowledge Base",
    description="This is a knowledge base that uses a Pinecone Vector DB",
    vector_db=vector_db,
)


asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"doc_type": "recipe_book"},
    )
)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        topics=["Manchester United"],
        reader=WikipediaReader(),
    )
)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://en.wikipedia.org/wiki/Manchester_United_F.C.",
    )
)


agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    read_chat_history=False,
)


asyncio.run(
    agent.aprint_response("What skills does Jordan Mitchell have?", markdown=True)
)

asyncio.run(
    agent.aprint_response(
        "In what year did Manchester United change their name?", markdown=True
    )
)
```

---

<a name="knowledge--vector_db--llamaindex_db--async_llamaindex_dbpy"></a>

### `knowledge/vector_db/llamaindex_db/async_llamaindex_db.py`

```python
import asyncio
from pathlib import Path
from shutil import rmtree

import httpx
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.llamaindex.llamaindexdb import LlamaIndexVectorDb
from llama_index.core import (
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever

data_dir = Path(__file__).parent.parent.parent.joinpath("wip", "data", "paul_graham")
if data_dir.is_dir():
    rmtree(path=data_dir, ignore_errors=True)
data_dir.mkdir(parents=True, exist_ok=True)

url = "https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt"
file_path = data_dir.joinpath("paul_graham_essay.txt")
response = httpx.get(url)
if response.status_code == 200:
    with open(file_path, "wb") as file:
        file.write(response.content)
    print(f"File downloaded and saved as {file_path}")
else:
    print("Failed to download the file")


documents = SimpleDirectoryReader(str(data_dir)).load_data()

splitter = SentenceSplitter(chunk_size=1024)

nodes = splitter.get_nodes_from_documents(documents)

storage_context = StorageContext.from_defaults()

index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)

knowledge_retriever = VectorIndexRetriever(index)

knowledge = Knowledge(
    vector_db=LlamaIndexVectorDb(knowledge_retriever=knowledge_retriever)
)

# Create an agent with the knowledge instance
agent = Agent(
    model=OpenAIChat("gpt-5-mini"),
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response(
            "Explain what this text means: low end eats the high end", markdown=True
        )
    )
```

---

<a name="knowledge--vector_db--llamaindex_db--llamaindex_dbpy"></a>

### `knowledge/vector_db/llamaindex_db/llamaindex_db.py`

```python
"""
pip install llama-index-core llama-index-readers-file llama-index-embeddings-openai agno
"""

from pathlib import Path
from shutil import rmtree

import httpx
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.llamaindex.llamaindexdb import LlamaIndexVectorDb
from llama_index.core import (
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever

data_dir = Path(__file__).parent.parent.parent.joinpath("wip", "data", "paul_graham")
if data_dir.is_dir():
    rmtree(path=data_dir, ignore_errors=True)
data_dir.mkdir(parents=True, exist_ok=True)

url = "https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt"
file_path = data_dir.joinpath("paul_graham_essay.txt")
response = httpx.get(url)
if response.status_code == 200:
    with open(file_path, "wb") as file:
        file.write(response.content)
    print(f"File downloaded and saved as {file_path}")
else:
    print("Failed to download the file")


documents = SimpleDirectoryReader(str(data_dir)).load_data()

splitter = SentenceSplitter(chunk_size=1024)

nodes = splitter.get_nodes_from_documents(documents)

storage_context = StorageContext.from_defaults()

index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)

knowledge_retriever = VectorIndexRetriever(index)

# Create a knowledge instance from the vector store
knowledge = Knowledge(
    vector_db=LlamaIndexVectorDb(knowledge_retriever=knowledge_retriever)
)

# Create an agent with the knowledge instance
agent = Agent(
    model=OpenAIChat("gpt-5-mini"),
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

# Use the agent to ask a question and print a response.
agent.print_response(
    "Explain what this text means: low end eats the high end", markdown=True
)
```

---

<a name="knowledge--vector_db--milvus_db--async_milvus_dbpy"></a>

### `knowledge/vector_db/milvus_db/async_milvus_db.py`

```python
# install pymilvus - `pip install pymilvus`

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.milvus import Milvus

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.
vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)
# Create knowledge base
knowledge = Knowledge(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=vector_db,
)

# Create and use the agent
agent = Agent(knowledge=knowledge)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
    )

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))
```

---

<a name="knowledge--vector_db--milvus_db--async_milvus_db_hybrid_searchpy"></a>

### `knowledge/vector_db/milvus_db/async_milvus_db_hybrid_search.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.milvus import Milvus, SearchType

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.
vector_db = Milvus(
    collection="recipes", uri="tmp/milvus.db", search_type=SearchType.hybrid
)
# Create knowledge base
knowledge_base = Knowledge(
    vector_db=vector_db,
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base)


if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(knowledge_base.aload(recreate=True))  # Comment out after first run

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))
```

---

<a name="knowledge--vector_db--milvus_db--milvus_dbpy"></a>

### `knowledge/vector_db/milvus_db/milvus_db.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.milvus import Milvus

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.

vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)
# Create knowledge base
knowledge = Knowledge(
    name="My Milvus Knowledge Base",
    description="This is a knowledge base that uses a Milvus DB",
    vector_db=vector_db,
)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"doc_type": "recipe_book"},
    )
)

# Create and use the agent
agent = Agent(knowledge=knowledge)
agent.print_response("How to make Tom Kha Gai", markdown=True)

vector_db.delete_by_name("Recipes")
# # or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--milvus_db--milvus_db_hybrid_searchpy"></a>

### `knowledge/vector_db/milvus_db/milvus_db_hybrid_search.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.milvus import Milvus, SearchType

# Initialize Milvus

# Set the uri and token for your Milvus server.
# - If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
# - If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use "<your_username>:<your_password>" as the token, otherwise don't set the token.
# - If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.
vector_db = Milvus(
    collection="recipes", uri="tmp/milvus.db", search_type=SearchType.hybrid
)

knowledge = Knowledge(
    vector_db=vector_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# Create and use the agent
agent = Agent(knowledge=knowledge)
agent.print_response("How to make Tom Kha Gai", markdown=True)
```

---

<a name="knowledge--vector_db--mongo_db--async_mongo_dbpy"></a>

### `knowledge/vector_db/mongo_db/async_mongo_db.py`

```python
"""
1. Create a MongoDB Atlas Account:
   - Go to https://www.mongodb.com/cloud/atlas/register
   - Sign up for a free account

2. Create a New Cluster:
   - Click "Build a Database"
   - Choose the FREE tier (M0)
   - Select your preferred cloud provider and region
   - Click "Create Cluster"

3. Set Up Database Access:
   - Follow the instructions in the MongoDB Atlas UI
   - Create a username and password
   - Click "Add New Database User"

5. Get Connection String:
   - Select "Drivers" as connection method
   - Select "Python" as driver
   - Copy the connection string

7. Test Connection:
   - Use the connection string in your code
   - Ensure pymongo is installed: pip install "pymongo[srv]"
   - Test with a simple query to verify connectivity

Alternatively to test locally, you can run a docker container

docker run -p 27017:27017 -d --name mongodb-container --rm -v ./tmp/mongo-data:/data/db mongodb/mongodb-atlas-local:8.0.3
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.mongodb import MongoDb

# MongoDB Atlas connection string
"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost:27017/agno?authSource=admin"
"""
mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

knowledge = Knowledge(
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
    ),
)

agent = Agent(knowledge=knowledge)

if __name__ == "__main__":
    # Comment out after the first run
    asyncio.run(
        knowledge.add_content_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
    )

    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))
```

---

<a name="knowledge--vector_db--mongo_db--cosmos_mongodb_vcorepy"></a>

### `knowledge/vector_db/mongo_db/cosmos_mongodb_vcore.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.mongodb import MongoDb

"""
Example connection strings:
"mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"
"""

mdb_connection_string = "mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"

knowledge_base = Knowledge(
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        search_index_name="recipes",
        cosmos_compatibility=True,
    ),
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base)

agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--vector_db--mongo_db--mongo_dbpy"></a>

### `knowledge/vector_db/mongo_db/mongo_db.py`

```python
"""
1. Create a MongoDB Atlas Account:
   - Go to https://www.mongodb.com/cloud/atlas/register
   - Sign up for a free account

2. Create a New Cluster:
   - Click "Build a Database"
   - Choose the FREE tier (M0)
   - Select your preferred cloud provider and region
   - Click "Create Cluster"

3. Set Up Database Access:
   - Follow the instructions in the MongoDB Atlas UI
   - Create a username and password
   - Click "Add New Database User"

5. Get Connection String:
   - Select "Drivers" as connection method
   - Select "Python" as driver
   - Copy the connection string

7. Test Connection:
   - Use the connection string in your code
   - Ensure pymongo is installed: pip install "pymongo[srv]"
   - Test with a simple query to verify connectivity

Alternatively to test locally, you can run a docker container

docker run -p 27017:27017 -d --name mongodb-container --rm -v ./tmp/mongo-data:/data/db mongodb/mongodb-atlas-local:8.0.3
"""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.mongodb import MongoDb

# MongoDB Atlas connection string
"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost:27017/agno?authSource=admin"
"""
mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

knowledge = Knowledge(
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        search_index_name="recipes",
    ),
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

# Create and use the agent
agent = Agent(knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="knowledge--vector_db--mongo_db--mongo_db_hybrid_searchpy"></a>

### `knowledge/vector_db/mongo_db/mongo_db_hybrid_search.py`

```python
import typer
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.mongodb import MongoDb
from agno.vectordb.search import SearchType
from rich.prompt import Prompt

# MongoDB Atlas connection string
"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost:27017/agno?authSource=admin"
"""
mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

vector_db = MongoDb(
    collection_name="recipes",
    db_url=mdb_connection_string,
    search_index_name="recipes",
    search_type=SearchType.hybrid,
)

knowledge_base = Knowledge(
    vector_db=vector_db,
)

knowledge_base.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)


def mongodb_agent(user: str = "user"):
    agent = Agent(
        user_id=user,
        knowledge=knowledge_base,
        search_knowledge=True,
    )

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message)


if __name__ == "__main__":
    typer.run(mongodb_agent)
```

---

<a name="knowledge--vector_db--pgvector--async_pg_vectorpy"></a>

### `knowledge/vector_db/pgvector/async_pg_vector.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="recipes", db_url=db_url)

knowledge_base = Knowledge(
    vector_db=vector_db,
)

agent = Agent(knowledge=knowledge_base)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge_base.add_content_async(
            url="https://docs.agno.com/introduction/agents.md"
        )
    )

    # Create and use the agent
    asyncio.run(
        agent.aprint_response("What is the purpose of an Agno Agent?", markdown=True)
    )
```

---

<a name="knowledge--vector_db--pgvector--pgvector_dbpy"></a>

### `knowledge/vector_db/pgvector/pgvector_db.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="vectors", db_url=db_url)

knowledge = Knowledge(
    name="My PG Vector Knowledge Base",
    description="This is a knowledge base that uses a PG Vector DB",
    vector_db=vector_db,
)
knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

agent = Agent(
    knowledge=knowledge,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

agent.print_response("How do I make pad thai?", markdown=True)

vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--pgvector--pgvector_hybrid_searchpy"></a>

### `knowledge/vector_db/pgvector/pgvector_hybrid_search.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    name="My PG Vector Knowledge Base",
    description="This is a knowledge base that uses a PG Vector DB",
    vector_db=PgVector(
        table_name="vectors", db_url=db_url, search_type=SearchType.hybrid
    ),
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge,
    search_knowledge=True,
    read_chat_history=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
agent.print_response("What was my last question?", stream=True)
```

---

<a name="knowledge--vector_db--pinecone_db--pinecone_dbpy"></a>

### `knowledge/vector_db/pinecone_db/pinecone_db.py`

```python
from os import getenv

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pineconedb import PineconeDb

api_key = getenv("PINECONE_API_KEY")
index_name = "thai-recipe-index"

vector_db = PineconeDb(
    name=index_name,
    dimension=1536,
    metric="cosine",
    spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
    api_key=api_key,
)

knowledge = Knowledge(
    name="My Pinecone Knowledge Base",
    description="This is a knowledge base that uses a Pinecone Vector DB",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    read_chat_history=True,
)

agent.print_response("How do I make pad thai?", markdown=True)

vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--qdrant_db--async_qdrant_dbpy"></a>

### `knowledge/vector_db/qdrant_db/async_qdrant_db.py`

```python
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "thai-recipes"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

# Create knowledge base
knowledge = Knowledge(
    vector_db=vector_db,
)

agent = Agent(knowledge=knowledge)


if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
    )

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))
```

---

<a name="knowledge--vector_db--qdrant_db--qdrant_dbpy"></a>

### `knowledge/vector_db/qdrant_db/qdrant_db.py`

```python
from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant

COLLECTION_NAME = "thai-recipes"

vector_db = Qdrant(collection=COLLECTION_NAME, url="http://localhost:6333")

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)

knowledge = Knowledge(
    name="My Qdrant Vector Knowledge Base",
    description="This is a knowledge base that uses a Qdrant Vector DB",
    vector_db=vector_db,
    contents_db=contents_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

# Create and use the agent
agent = Agent(knowledge=knowledge)
agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)


vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--qdrant_db--qdrant_db_hybrid_searchpy"></a>

### `knowledge/vector_db/qdrant_db/qdrant_db_hybrid_search.py`

```python
import typer
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant
from agno.vectordb.search import SearchType
from rich.prompt import Prompt

COLLECTION_NAME = "thai-recipes"

vector_db = Qdrant(
    collection=COLLECTION_NAME,
    url="http://localhost:6333",
    search_type=SearchType.hybrid,
)

knowledge = Knowledge(
    name="My Qdrant Vector Knowledge Base",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)


def qdrantdb_agent(user: str = "user"):
    agent = Agent(
        user_id=user,
        knowledge=knowledge,
        search_knowledge=True,
    )

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message)


if __name__ == "__main__":
    typer.run(qdrantdb_agent)
```

---

<a name="knowledge--vector_db--singlestore_db--lance_db--remote_lance_dbpy"></a>

### `knowledge/vector_db/singlestore_db/lance_db/remote_lance_db.py`

```python
"""
This example shows how to use a remote LanceDB database.

- Set URI obtained from https://cloud.lancedb.com/
- Export `LANCEDB_API_KEY` OR set `api_key` in the `LanceDb` constructor
"""

# install lancedb - `pip install lancedb`

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.lancedb import LanceDb

# Initialize Remote LanceDB
vector_db = LanceDb(
    table_name="recipes",
    uri="<URI>",
    # api_key="<API_KEY>",
)

# Create knowledge base
knowledge_base = Knowledge(
    vector_db=vector_db,
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

# Create and use the agent
agent = Agent(knowledge=knowledge_base)
agent.print_response("How to make Tom Kha Gai", markdown=True)
```

---

<a name="knowledge--vector_db--singlestore_db--singlestore_dbpy"></a>

### `knowledge/vector_db/singlestore_db/singlestore_db.py`

```python
"""
# Run the setup script
```shell
./cookbook/scripts/run_singlestore.sh
```

# Create the database

- Visit http://localhost:8080 and login with `root` and `admin`
- Create the database with your choice of name. Default setup script requires AGNO as database name. `CREATE DATABASE your_database_name;`
"""

import asyncio
from os import getenv

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.singlestore import SingleStore
from sqlalchemy.engine import create_engine

USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")
SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)

db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)
if SSL_CERT:
    db_url += f"&ssl_ca={SSL_CERT}&ssl_verify_cert=true"

db_engine = create_engine(db_url)

vector_db = SingleStore(
    collection="recipes",
    db_engine=db_engine,
    schema=DATABASE,
)

knowledge = Knowledge(name="My SingleStore Knowledge Base", vector_db=vector_db)

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"doc_type": "recipe_book"},
    )
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    read_chat_history=True,
)

agent.print_response("How do I make pad thai?", markdown=True)

vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--singlestore_db--surrealdb--async_surreal_dbpy"></a>

### `knowledge/vector_db/singlestore_db/surrealdb/async_surreal_db.py`

```python
# Run SurrealDB in a container before running this script
# docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import AsyncSurreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Create a client
client = AsyncSurreal(url=SURREALDB_URL)

surrealdb = SurrealDb(
    async_client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
)


async def async_demo():
    """Demonstrate asynchronous usage of SurrealDb"""

    await client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
    await client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

    knowledge_base = Knowledge(
        vector_db=surrealdb,
    )

    await knowledge_base.add_content_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )

    agent = Agent(knowledge=knowledge_base)
    await agent.aprint_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )


if __name__ == "__main__":
    # Run asynchronous demo
    print("\nRunning asynchronous demo...")
    asyncio.run(async_demo())
```

---

<a name="knowledge--vector_db--singlestore_db--surrealdb--surreal_dbpy"></a>

### `knowledge/vector_db/singlestore_db/surrealdb/surreal_db.py`

```python
# Run SurrealDB in a container before running this script
# docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import Surreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Create a client
client = Surreal(url=SURREALDB_URL)
client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

surrealdb = SurrealDb(
    client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
)


def sync_demo():
    """Demonstrate synchronous usage of SurrealDb"""
    knowledge_base = Knowledge(
        vector_db=surrealdb,
    )

    # Load data synchronously
    knowledge_base.add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )

    # Create agent and query synchronously
    agent = Agent(knowledge=knowledge_base)
    agent.print_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )


if __name__ == "__main__":
    # Run synchronous demo
    print("Running synchronous demo...")
    sync_demo()
```

---

<a name="knowledge--vector_db--surrealdb--async_surreal_dbpy"></a>

### `knowledge/vector_db/surrealdb/async_surreal_db.py`

```python
# Run SurrealDB in a container before running this script
# docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root

import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import AsyncSurreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Create a client
client = AsyncSurreal(url=SURREALDB_URL)

surrealdb = SurrealDb(
    async_client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
    embedder=OpenAIEmbedder(),
)


async def async_demo():
    """Demonstrate asynchronous usage of SurrealDb"""

    await client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
    await client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

    knowledge = Knowledge(
        vector_db=surrealdb,
    )

    await knowledge.add_content_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )

    agent = Agent(knowledge=knowledge)
    await agent.aprint_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )


if __name__ == "__main__":
    # Run asynchronous demo
    print("\nRunning asynchronous demo...")
    asyncio.run(async_demo())
```

---

<a name="knowledge--vector_db--surrealdb--surreal_dbpy"></a>

### `knowledge/vector_db/surrealdb/surreal_db.py`

```python
# Run SurrealDB in a container before running this script
# docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import Surreal

# SurrealDB connection parameters
SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

# Create a client
client = Surreal(url=SURREALDB_URL)
client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

surrealdb = SurrealDb(
    client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
    embedder=OpenAIEmbedder(),
)


def sync_demo():
    """Demonstrate synchronous usage of SurrealDb"""
    knowledge = Knowledge(
        vector_db=surrealdb,
    )

    # Load data synchronously
    knowledge.add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )

    # Create agent and query synchronously
    agent = Agent(knowledge=knowledge)
    agent.print_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )


if __name__ == "__main__":
    # Run synchronous demo
    print("Running synchronous demo...")
    sync_demo()
```

---

<a name="knowledge--vector_db--upstash_db--upstash_dbpy"></a>

### `knowledge/vector_db/upstash_db/upstash_db.py`

```python
# install upstash-vector - `uv pip install upstash-vector`
# Add OPENAI_API_KEY to your environment variables for the agent response

import os

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.upstashdb import UpstashVectorDb

# How to connect to an Upstash Vector index
# - Create a new index in Upstash Console with the correct dimension
# - Fetch the URL and token from Upstash Console
# - Replace the values below or use environment variables

vector_db = UpstashVectorDb(
    url=os.getenv("UPSTASH_VECTOR_REST_URL"),
    token=os.getenv("UPSTASH_VECTOR_REST_TOKEN"),
)

# Initialize Upstash DB
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with Upstash Vector DB",
    vector_db=vector_db,
)

# Add content with metadata
knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

# Create and use the agent
agent = Agent(knowledge=knowledge)
agent.print_response("How to make Pad Thai?", markdown=True)


vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--weaviate_db--async_weaviate_dbpy"></a>

### `knowledge/vector_db/weaviate_db/async_weaviate_db.py`

```python
"""
This example demonstrates using Weaviate as a vector database for semantic search.

Installation:
    pip install weaviate-client

You can use either Weaviate Cloud or a local instance.

Weaviate Cloud Setup:
1. Create account at https://console.weaviate.cloud/
2. Create a cluster and copy the "REST endpoint" and "Admin" API Key. Then set environment variables:
    export WCD_URL="your-cluster-url" 
    export WCD_API_KEY="your-api-key"

Local Development Setup:
1. Install Docker from https://docs.docker.com/get-docker/
2. Run Weaviate locally:
    docker run -d \
        -p 8080:8080 \
        -p 50051:50051 \
        --name weaviate \
        cr.weaviate.io/semitechnologies/weaviate:1.28.4
   or use the script `cookbook/scripts/run_weviate.sh` to start a local instance.
3. Remember to set `local=True` on the Weaviate instantiation.
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

vector_db = Weaviate(
    collection="recipes_async",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=True,  # Set to False if using Weaviate Cloud and True if using local instance
)
# Create knowledge base
knowledge = Knowledge(
    vector_db=vector_db,
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            name="Recipes",
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        )
    )

    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))
```

---

<a name="knowledge--vector_db--weaviate_db--weaviate_dbpy"></a>

### `knowledge/vector_db/weaviate_db/weaviate_db.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Weaviate
from agno.vectordb.weaviate.index import Distance, VectorIndex

vector_db = Weaviate(
    collection="vectors",
    search_type=SearchType.vector,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=False,  # Set to True if using Weaviate locally
)

# Create Knowledge Instance with Weaviate
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with Weaviate",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
    skip_if_exists=True,
)

# Create and use the agent
agent = Agent(knowledge=knowledge)
agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

# Delete operations
vector_db.delete_by_name("Recipes")
# or
vector_db.delete_by_metadata({"doc_type": "recipe_book"})
```

---

<a name="knowledge--vector_db--weaviate_db--weaviate_db_hybrid_searchpy"></a>

### `knowledge/vector_db/weaviate_db/weaviate_db_hybrid_search.py`

```python
import typer
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate
from rich.prompt import Prompt

vector_db = Weaviate(
    collection="recipes",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=False,  # Set to True if using Weaviate Cloud and False if using local instance
    # Adjust alpha for hybrid search (0.0-1.0, default is 0.5), where 0 is pure keyword search, 1 is pure vector search
    hybrid_search_alpha=0.6,
)

knowledge_base = Knowledge(
    name="Weaviate Hybrid Search",
    description="A knowledge base for Weaviate hybrid search",
    vector_db=vector_db,
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)


def weaviate_agent(user: str = "user"):
    agent = Agent(
        user_id=user,
        knowledge=knowledge_base,
        search_knowledge=True,
    )

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message)


if __name__ == "__main__":
    typer.run(weaviate_agent)
```

---

<a name="knowledge--vector_db--weaviate_db--weaviate_db_upsertpy"></a>

### `knowledge/vector_db/weaviate_db/weaviate_db_upsert.py`

```python
"""
This example demonstrates using Weaviate as a vector database.

Installation:
    pip install weaviate-client

You can use either Weaviate Cloud or a local instance.

Weaviate Cloud Setup:
1. Create account at https://console.weaviate.cloud/
2. Create a cluster and copy the "REST endpoint" and "Admin" API Key. Then set environment variables:
    export WCD_URL="your-cluster-url" 
    export WCD_API_KEY="your-api-key"

Local Development Setup:
1. Install Docker from https://docs.docker.com/get-docker/
2. Run Weaviate locally:
    docker run -d \
        -p 8080:8080 \
        -p 50051:50051 \
        --name weaviate \
        cr.weaviate.io/semitechnologies/weaviate:1.28.4
   or use the script `cookbook/scripts/run_weviate.sh` to start a local instance.
3. Remember to set `local=True` on the Weaviate instantiation.
"""

from agno.knowledge.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.utils.log import set_log_level_to_debug
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

embedder = SentenceTransformerEmbedder()

vector_db = Weaviate(
    collection="recipes",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    embedder=embedder,
    local=True,  # Set to False if using Weaviate Cloud and True if using local instance
)
# Create knowledge base
knowledge_base = Knowledge(
    vector_db=vector_db,
)

vector_db.drop()
set_log_level_to_debug()

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

print(
    "Knowledge base loaded with PDF content. Loading the same data again will not recreate it."
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    skip_if_exists=True,
)

vector_db.drop()
```

---

<a name="memory--01_agent_with_memorypy"></a>

### `memory/01_agent_with_memory.py`

```python
"""
This example shows you how to use persistent memory with an Agent.

After each run, user memories are created/updated.

To enable this, set `enable_user_memories=True` in the Agent config.
"""

from uuid import uuid4

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

db.clear_memories()

session_id = str(uuid4())
john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    enable_user_memories=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
    session_id=session_id,
)

agent.print_response(
    "What are my hobbies?", stream=True, user_id=john_doe_id, session_id=session_id
)

memories = agent.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)

agent.print_response(
    "Ok i dont like hiking anymore, i like to play soccer instead.",
    stream=True,
    user_id=john_doe_id,
    session_id=session_id,
)

# You can also get the user memories from the agent
memories = agent.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)
```

---

<a name="memory--02_agentic_memorypy"></a>

### `memory/02_agentic_memory.py`

```python
"""
This example shows you how to use persistent memory with an Agent.

During each run the Agent can create/update/delete user memories.

To enable this, set `enable_agentic_memory=True` in the Agent config.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)


john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    enable_agentic_memory=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
)

agent.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)

memories = agent.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)


agent.print_response(
    "Remove all existing memories of me.",
    stream=True,
    user_id=john_doe_id,
)

memories = agent.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)

agent.print_response(
    "My name is John Doe and I like to paint.", stream=True, user_id=john_doe_id
)

memories = agent.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)


agent.print_response(
    "I don't paint anymore, i draw instead.", stream=True, user_id=john_doe_id
)

memories = agent.get_user_memories(user_id=john_doe_id)

print("Memories about John Doe:")
pprint(memories)
```

---

<a name="memory--03_agents_share_memorypy"></a>

### `memory/03_agents_share_memory.py`

```python
"""
In this example, we have two agents that share the same memory.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

john_doe_id = "john_doe@example.com"

chat_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are a helpful assistant that can chat with users",
    db=db,
    enable_user_memories=True,
)

chat_agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
)

chat_agent.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)


research_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are a research assistant that can help users with their research questions",
    tools=[DuckDuckGoTools(cache_results=True)],
    db=db,
    enable_user_memories=True,
)

research_agent.print_response(
    "I love asking questions about quantum computing. What is the latest news on quantum computing?",
    stream=True,
    user_id=john_doe_id,
)

memories = research_agent.get_user_memories(user_id=john_doe_id)
print("Memories about John Doe:")
pprint(memories)
```

---

<a name="memory--04_custom_memory_managerpy"></a>

### `memory/04_custom_memory_manager.py`

```python
"""
This example shows how you can configure the Memory Manager and Summarizer models individually.

In this example, we use OpenRouter and LLama 3.3-70b-instruct for the memory manager and Claude 3.5 Sonnet for the summarizer. And we use Gemini for the Agent.

We also set custom system prompts for the memory manager and summarizer. You can either override the entire system prompt or add additional instructions which is added to the end of the system prompt.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

# You can also override the entire `system_message` for the memory manager
memory_manager = MemoryManager(
    model=OpenAIChat(id="gpt-4o"),
    additional_instructions="""
    IMPORTANT: Don't store any memories about the user's name. Just say "The User" instead of referencing the user's name.
    """,
    db=db,
)

john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    memory_manager=memory_manager,
    enable_user_memories=True,
    user_id=john_doe_id,
)

agent.print_response(
    "My name is John Doe and I like to swim and play soccer.", stream=True
)

agent.print_response("I dont like to swim", stream=True)


memories = agent.get_user_memories(user_id=john_doe_id)

print("John Doe's memories:")
pprint(memories)
```

---

<a name="memory--05_multi_user_multi_session_chatpy"></a>

### `memory/05_multi_user_multi_session_chat.py`

```python
"""
This example demonstrates how to run a multi-user, multi-session chat.

In this example, we have 3 users and 4 sessions.

User 1 has 2 sessions.
User 2 has 1 session.
User 3 has 1 session.
"""

import asyncio

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

user_1_id = "user_1@example.com"
user_2_id = "user_2@example.com"
user_3_id = "user_3@example.com"

user_1_session_1_id = "user_1_session_1"
user_1_session_2_id = "user_1_session_2"
user_2_session_1_id = "user_2_session_1"
user_3_session_1_id = "user_3_session_1"

chat_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
)


async def run_chat_agent():
    await chat_agent.aprint_response(
        "My name is Mark Gonzales and I like anime and video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )
    await chat_agent.aprint_response(
        "I also enjoy reading manga and playing video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    # Chat with user 1 - Session 2
    await chat_agent.aprint_response(
        "I'm going to the movies tonight.",
        user_id=user_1_id,
        session_id=user_1_session_2_id,
    )

    # Chat with user 2
    await chat_agent.aprint_response(
        "Hi my name is John Doe.", user_id=user_2_id, session_id=user_2_session_1_id
    )
    await chat_agent.aprint_response(
        "I'm planning to hike this weekend.",
        user_id=user_2_id,
        session_id=user_2_session_1_id,
    )

    # Chat with user 3
    await chat_agent.aprint_response(
        "Hi my name is Jane Smith.", user_id=user_3_id, session_id=user_3_session_1_id
    )
    await chat_agent.aprint_response(
        "I'm going to the gym tomorrow.",
        user_id=user_3_id,
        session_id=user_3_session_1_id,
    )

    # Continue the conversation with user 1
    # The agent should take into account all memories of user 1.
    await chat_agent.aprint_response(
        "What do you suggest I do this weekend?",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )


if __name__ == "__main__":
    # Chat with user 1 - Session 1
    asyncio.run(run_chat_agent())

    user_1_memories = chat_agent.get_user_memories(user_id=user_1_id)
    print("User 1's memories:")
    assert user_1_memories is not None
    for i, m in enumerate(user_1_memories):
        print(f"{i}: {m.memory}")

    user_2_memories = chat_agent.get_user_memories(user_id=user_2_id)
    print("User 2's memories:")
    assert user_2_memories is not None
    for i, m in enumerate(user_2_memories):
        print(f"{i}: {m.memory}")

    user_3_memories = chat_agent.get_user_memories(user_id=user_3_id)
    print("User 3's memories:")
    assert user_3_memories is not None
    for i, m in enumerate(user_3_memories):
        print(f"{i}: {m.memory}")
```

---

<a name="memory--06_multi_user_multi_session_chat_concurrentpy"></a>

### `memory/06_multi_user_multi_session_chat_concurrent.py`

```python
"""
This example shows how to run a multi-user, multi-session chat concurrently.

In this example, we have 3 users and 4 sessions.

User 1 has 2 sessions.
User 2 has 1 session.
User 3 has 1 session.
"""

import asyncio

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

user_1_id = "user_1@example.com"
user_2_id = "user_2@example.com"
user_3_id = "user_3@example.com"

user_1_session_1_id = "user_1_session_1"
user_1_session_2_id = "user_1_session_2"
user_2_session_1_id = "user_2_session_1"
user_3_session_1_id = "user_3_session_1"

chat_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    enable_user_memories=True,
)


async def user_1_conversation():
    """Handle conversation with user 1 across multiple sessions"""
    # User 1 - Session 1
    await chat_agent.arun(
        "My name is Mark Gonzales and I like anime and video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )
    await chat_agent.arun(
        "I also enjoy reading manga and playing video games.",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    # User 1 - Session 2
    await chat_agent.arun(
        "I'm going to the movies tonight.",
        user_id=user_1_id,
        session_id=user_1_session_2_id,
    )

    # Continue the conversation in session 1
    await chat_agent.arun(
        "What do you suggest I do this weekend?",
        user_id=user_1_id,
        session_id=user_1_session_1_id,
    )

    print("User 1 Done")


async def user_2_conversation():
    """Handle conversation with user 2"""
    await chat_agent.arun(
        "Hi my name is John Doe.", user_id=user_2_id, session_id=user_2_session_1_id
    )
    await chat_agent.arun(
        "I'm planning to hike this weekend.",
        user_id=user_2_id,
        session_id=user_2_session_1_id,
    )
    print("User 2 Done")


async def user_3_conversation():
    """Handle conversation with user 3"""
    await chat_agent.arun(
        "Hi my name is Jane Smith.", user_id=user_3_id, session_id=user_3_session_1_id
    )
    await chat_agent.arun(
        "I'm going to the gym tomorrow.",
        user_id=user_3_id,
        session_id=user_3_session_1_id,
    )
    print("User 3 Done")


async def run_concurrent_chat_agent():
    """Run all user conversations concurrently"""
    await asyncio.gather(
        user_1_conversation(), user_2_conversation(), user_3_conversation()
    )


if __name__ == "__main__":
    # Run all conversations concurrently
    asyncio.run(run_concurrent_chat_agent())

    user_1_memories = chat_agent.get_user_memories(user_id=user_1_id)
    print("User 1's memories:")
    assert user_1_memories is not None
    for i, m in enumerate(user_1_memories):
        print(f"{i}: {m.memory}")

    user_2_memories = chat_agent.get_user_memories(user_id=user_2_id)
    print("User 2's memories:")
    assert user_2_memories is not None
    for i, m in enumerate(user_2_memories):
        print(f"{i}: {m.memory}")

    user_3_memories = chat_agent.get_user_memories(user_id=user_3_id)
    print("User 3's memories:")
    assert user_3_memories is not None
    for i, m in enumerate(user_3_memories):
        print(f"{i}: {m.memory}")
```

---

<a name="memory--07_share_memory_and_history_between_agentspy"></a>

### `memory/07_share_memory_and_history_between_agents.py`

```python
from uuid import uuid4

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat

db = SqliteDb(db_file="tmp/agent_sessions.db")

session_id = str(uuid4())
user_id = "john_doe@example.com"

agent_1 = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are really friendly and helpful.",
    db=db,
    add_history_to_context=True,
    enable_user_memories=True,
)

agent_2 = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are really grumpy and mean.",
    db=db,
    add_history_to_context=True,
    enable_user_memories=True,
)

agent_1.print_response(
    "Hi! My name is John Doe.", session_id=session_id, user_id=user_id
)

agent_2.print_response("What is my name?", session_id=session_id, user_id=user_id)

agent_2.print_response(
    "I like to hike in the mountains on weekends.",
    session_id=session_id,
    user_id=user_id,
)

agent_1.print_response("What are my hobbies?", session_id=session_id, user_id=user_id)

agent_1.print_response(
    "What have we been discussing? Give me bullet points.",
    session_id=session_id,
    user_id=user_id,
)
```

---

<a name="memory--08_memory_toolspy"></a>

### `memory/08_memory_tools.py`

```python
"""
Here is a tool with reasoning capabilities to allow agents to manage user memories.

1. Run: `pip install openai agno lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY
3. Run: `python cookbook/reasoning/tools/knowledge_tools.py` to run the agent
"""

import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.memory import MemoryTools

db = SqliteDb(db_file="tmp/memory.db")

john_doe_id = "john_doe@example.com"

memory_tools = MemoryTools(
    db=db,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[memory_tools, DuckDuckGoTools()],
    instructions=[
        "You are a trip planner bot and you are helping the user plan their trip.",
        "You should use the DuckDuckGoTools to get information about the destination and activities.",
        "You should use the MemoryTools to store information about the user for future reference.",
        "Don't ask the user for more information, make up what you don't know.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response(
            "My name is John Doe and I like to hike in the mountains on weekends. "
            "I like to travel to new places and experience different cultures. "
            "I am planning to travel to Africa in December. ",
            stream=True,
            user_id=john_doe_id,
        )
    )

    asyncio.run(
        agent.aprint_response(
            "Make me a travel itinerary for my trip, and propose where I should go, how much I should budget, etc.",
            stream=True,
            user_id=john_doe_id,
        )
    )
```

---

<a name="memory--memory_manager--01_standalone_memorypy"></a>

### `memory/memory_manager/01_standalone_memory.py`

```python
"""
How to add, get, delete, and replace user memories manually
"""

from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager, UserMemory
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory = MemoryManager(db=PostgresDb(db_url=db_url))

# Add a memory for the default user
memory.add_user_memory(
    memory=UserMemory(memory="The user's name is John Doe", topics=["name"]),
)
print("Memories:")
pprint(memory.get_user_memories())

# Add memories for Jane Doe
jane_doe_id = "jane_doe@example.com"
print(f"\nUser: {jane_doe_id}")
memory_id_1 = memory.add_user_memory(
    memory=UserMemory(memory="The user's name is Jane Doe", topics=["name"]),
    user_id=jane_doe_id,
)
memory_id_2 = memory.add_user_memory(
    memory=UserMemory(memory="She likes to play tennis", topics=["hobbies"]),
    user_id=jane_doe_id,
)
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)

# Delete a memory
print("\nDeleting memory")
assert memory_id_2 is not None
memory.delete_user_memory(user_id=jane_doe_id, memory_id=memory_id_2)
print("Memory deleted\n")
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)

# Replace a memory
print("\nReplacing memory")
assert memory_id_1 is not None
memory.replace_user_memory(
    memory_id=memory_id_1,
    memory=UserMemory(memory="The user's name is Jane Mary Doe", topics=["name"]),
    user_id=jane_doe_id,
)
print("Memory replaced")
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)
```

---

<a name="memory--memory_manager--02_memory_creationpy"></a>

### `memory/memory_manager/02_memory_creation.py`

```python
"""
Create user memories with an Agent by providing a either text or a list of messages.
"""

from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager, UserMemory
from agno.models.message import Message
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = PostgresDb(db_url=db_url)

memory = MemoryManager(model=OpenAIChat(id="gpt-4o"), db=memory_db)

john_doe_id = "john_doe@example.com"
memory.add_user_memory(
    memory=UserMemory(
        memory="""
I enjoy hiking in the mountains on weekends,
reading science fiction novels before bed,
cooking new recipes from different cultures,
playing chess with friends,
and attending live music concerts whenever possible.
Photography has become a recent passion of mine, especially capturing landscapes and street scenes.
I also like to meditate in the mornings and practice yoga to stay centered.
"""
    ),
    user_id=john_doe_id,
)


memories = memory.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)

jane_doe_id = "jane_doe@example.com"
# Send a history of messages and add memories
memory.create_user_memories(
    messages=[
        Message(role="user", content="My name is Jane Doe"),
        Message(role="assistant", content="That is great!"),
        Message(role="user", content="I like to play chess"),
        Message(role="assistant", content="That is great!"),
    ],
    user_id=jane_doe_id,
)

memories = memory.get_user_memories(user_id=jane_doe_id)
print("Jane Doe's memories:")
pprint(memories)
```

---

<a name="memory--memory_manager--03_custom_memory_instructionspy"></a>

### `memory/memory_manager/03_custom_memory_instructions.py`

```python
"""
Create user memories with an Agent by providing a either text or a list of messages.
"""

from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager
from agno.models.anthropic.claude import Claude
from agno.models.message import Message
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = PostgresDb(db_url=db_url)

memory = MemoryManager(
    model=OpenAIChat(id="gpt-4o"),
    memory_capture_instructions="""\
                    Memories should only include details about the user's academic interests.
                    Only include which subjects they are interested in.
                    Ignore names, hobbies, and personal interests.
                    """,
    db=memory_db,
)

john_doe_id = "john_doe@example.com"

memory.create_user_memories(
    message="""\
My name is John Doe.

I enjoy hiking in the mountains on weekends,
reading science fiction novels before bed,
cooking new recipes from different cultures,
playing chess with friends.

I am interested to learn about the history of the universe and other astronomical topics.
""",
    user_id=john_doe_id,
)


memories = memory.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)


# Use default memory manager
memory = MemoryManager(model=Claude(id="claude-3-5-sonnet-latest"), db=memory_db)
jane_doe_id = "jane_doe@example.com"

# Send a history of messages and add memories
memory.create_user_memories(
    messages=[
        Message(role="user", content="Hi, how are you?"),
        Message(role="assistant", content="I'm good, thank you!"),
        Message(role="user", content="What are you capable of?"),
        Message(
            role="assistant",
            content="I can help you with your homework and answer questions about the universe.",
        ),
        Message(role="user", content="My name is Jane Doe"),
        Message(role="user", content="I like to play chess"),
        Message(
            role="user",
            content="Actually, forget that I like to play chess. I more enjoy playing table top games like dungeons and dragons",
        ),
        Message(
            role="user",
            content="I'm also interested in learning about the history of the universe and other astronomical topics.",
        ),
        Message(role="assistant", content="That is great!"),
        Message(
            role="user",
            content="I am really interested in physics. Tell me about quantum mechanics?",
        ),
    ],
    user_id=jane_doe_id,
)

memories = memory.get_user_memories(user_id=jane_doe_id)
print("Jane Doe's memories:")
pprint(memories)
```

---

<a name="memory--memory_manager--04_memory_searchpy"></a>

### `memory/memory_manager/04_memory_search.py`

```python
"""
How to search for user memories using different retrieval methods

- last_n: Retrieves the last n memories
- first_n: Retrieves the first n memories
- semantic: Retrieves memories using semantic search
"""

from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager, UserMemory
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = PostgresDb(db_url=db_url)

memory = MemoryManager(model=OpenAIChat(id="gpt-4o"), db=memory_db)

john_doe_id = "john_doe@example.com"
memory.add_user_memory(
    memory=UserMemory(memory="The user enjoys hiking in the mountains on weekends"),
    user_id=john_doe_id,
)
memory.add_user_memory(
    memory=UserMemory(
        memory="The user enjoys reading science fiction novels before bed"
    ),
    user_id=john_doe_id,
)
print("John Doe's memories:")
pprint(memory.get_user_memories(user_id=john_doe_id))

memories = memory.search_user_memories(
    user_id=john_doe_id, limit=1, retrieval_method="last_n"
)
print("\nJohn Doe's last_n memories:")
pprint(memories)

memories = memory.search_user_memories(
    user_id=john_doe_id, limit=1, retrieval_method="first_n"
)
print("\nJohn Doe's first_n memories:")
pprint(memories)

memories = memory.search_user_memories(
    user_id=john_doe_id,
    query="What does the user like to do on weekends?",
    retrieval_method="agentic",
)
print("\nJohn Doe's memories similar to the query (agentic):")
pprint(memories)
```

---

<a name="memory--memory_manager--05_db_tools_controlpy"></a>

### `memory/memory_manager/05_db_tools_control.py`

```python
"""
Control Memory Database Tools - Add, Update, Delete, and Clear Operations

This cookbook demonstrates how to control which memory database operations
are available to the AI model using the four DB tools parameters:
- add_memories: Controls whether the AI can add new memories
- update_memories: Controls whether the AI can update existing memories
- delete_memories: Controls whether the AI can delete individual memories
- clear_memories: Controls whether the AI can clear all memories

These parameters provide fine-grained control over memory operations for security
and functionality purposes.
"""

from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.memory.manager import MemoryManager
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

# Setup database and user
memory_db = SqliteDb(db_file="tmp/memory_control_demo.db")
john_doe_id = "john_doe@example.com"

memory_manager_full = MemoryManager(
    model=OpenAIChat(id="gpt-4o"),
    db=memory_db,
    add_memories=True,
    update_memories=True,
)

agent_full = Agent(
    model=OpenAIChat(id="gpt-4o"),
    memory_manager=memory_manager_full,
    enable_agentic_memory=True,
    db=memory_db,
)

# Add initial memory
agent_full.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends. I also enjoy photography.",
    stream=True,
    user_id=john_doe_id,
)

# Test memory recall
agent_full.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)

# Test memory update
agent_full.print_response(
    "I no longer enjoy photography. Instead, I've taken up rock climbing.",
    stream=True,
    user_id=john_doe_id,
)

print("\nMemories after update:")
memories = memory_manager_full.get_user_memories(user_id=john_doe_id)
pprint([m.memory for m in memories] if memories else [])
```

---

<a name="models--aimlapi--async_basicpy"></a>

### `models/aimlapi/async_basic.py`

```python
"""
Basic async example using AIMlAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.aimlapi import AIMLAPI

agent = Agent(
    model=AIMLAPI(id="gpt-4o-mini"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--aimlapi--async_basic_streampy"></a>

### `models/aimlapi/async_basic_stream.py`

```python
"""
Basic streaming async example using AIMlAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.aimlapi import AIMLAPI

agent = Agent(
    model=AIMLAPI(id="gpt-4o-mini"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--aimlapi--async_tool_usepy"></a>

### `models/aimlapi/async_tool_use.py`

```python
"""
Async example using AIMlAPI with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.aimlapi import AIMLAPI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AIMLAPI(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--aimlapi--basicpy"></a>

### `models/aimlapi/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.aimlapi import AIMLAPI

agent = Agent(model=AIMLAPI(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--aimlapi--basic_streampy"></a>

### `models/aimlapi/basic_stream.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.aimlapi import AIMLAPI

agent = Agent(model=AIMLAPI(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--aimlapi--image_agentpy"></a>

### `models/aimlapi/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.aimlapi import AIMLAPI

agent = Agent(
    model=AIMLAPI(id="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--aimlapi--image_agent_bytespy"></a>

### `models/aimlapi/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.aimlapi import AIMLAPI

agent = Agent(
    model=AIMLAPI(id="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--aimlapi--image_agent_with_memorypy"></a>

### `models/aimlapi/image_agent_with_memory.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.aimlapi import AIMLAPI

agent = Agent(
    model=AIMLAPI(id="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"),
    markdown=True,
    add_history_to_context=True,
    num_history_runs=3,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

agent.print_response("Tell me where I can get more images?")
```

---

<a name="models--aimlapi--structured_outputpy"></a>

### `models/aimlapi/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.aimlapi import AIMLAPI
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=AIMLAPI(id="gpt-4o-mini"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
json_mode_response: RunOutput = json_mode_agent.run("New York")
pprint(json_mode_response.content)

# json_mode_agent.print_response("New York")
```

---

<a name="models--aimlapi--tool_usepy"></a>

### `models/aimlapi/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aimlapi import AIMLAPI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AIMLAPI(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?")
```

---

<a name="models--anthropic--async_basicpy"></a>

### `models/anthropic/async_basic.py`

```python
"""
Basic async example using Claude.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--anthropic--async_basic_streampy"></a>

### `models/anthropic/async_basic_stream.py`

```python
"""
Basic streaming async example using Claude.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--anthropic--async_tool_usepy"></a>

### `models/anthropic/async_tool_use.py`

```python
"""
Async example using Claude with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--anthropic--basicpy"></a>

### `models/anthropic/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.anthropic import Claude

agent = Agent(model=Claude(id="claude-sonnet-4-20250514"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--anthropic--basic_streampy"></a>

### `models/anthropic/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.anthropic import Claude

agent = Agent(model=Claude(id="claude-sonnet-4-20250514"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--anthropic--code_executionpy"></a>

### `models/anthropic/code_execution.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        default_headers={"anthropic-beta": "code-execution-2025-05-22"},
    ),
    tools=[
        {
            "type": "code_execution_20250522",
            "name": "code_execution",
        }
    ],
    markdown=True,
)

agent.print_response(
    "Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]",
    stream=True,
)
```

---

<a name="models--anthropic--dbpy"></a>

### `models/anthropic/db.py`

```python
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--anthropic--financial_analyst_thinkingpy"></a>

### `models/anthropic/financial_analyst_thinking.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.calculator import CalculatorTools
from agno.tools.yfinance import YFinanceTools

# Complex multi-step reasoning problem that demonstrates interleaved thinking
task = (
    "I'm considering an investment portfolio. I want to invest $50,000 split equally "
    "between Apple (AAPL) and Tesla (TSLA). Calculate how many shares of each I can buy "
    "at current prices, then analyze what my total portfolio value would be if both stocks "
    "increased by 15%. Also calculate what percentage return that represents on my initial investment. "
    "Think through each step and show your reasoning process."
)

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        thinking={"type": "enabled", "budget_tokens": 2048},
        default_headers={"anthropic-beta": "interleaved-thinking-2025-05-14"},
    ),
    tools=[
        CalculatorTools(enable_all=True),
        YFinanceTools(),
    ],
    instructions=[
        "You are a financial analysis assistant with access to calculator and stock price tools.",
        "For complex problems, think through each step carefully before and after using tools.",
        "Show your reasoning process and explain your calculations clearly.",
        "Use the calculator tool for all mathematical operations to ensure accuracy.",
    ],
    markdown=True,
)

agent.print_response(task, stream=True)
```

---

<a name="models--anthropic--image_input_bytespy"></a>

### `models/anthropic/image_input_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.anthropic.claude import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--anthropic--image_input_file_uploadpy"></a>

### `models/anthropic/image_input_file_upload.py`

```python
"""
In this example, we upload a PDF file to Anthropic directly and then use it as an input to an agent.
"""

from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.anthropic import Claude
from agno.utils.media import download_file
from anthropic import Anthropic

img_path = Path(__file__).parent.joinpath("agno-intro.png")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.us-east-1.amazonaws.com/images/agno-intro.png",
    str(img_path),
)

# Initialize Anthropic client
client = Anthropic()

# Upload the file to Anthropic
uploaded_file = client.beta.files.upload(
    file=Path(img_path),
)

if uploaded_file is not None:
    agent = Agent(
        model=Claude(
            id="claude-opus-4-20250514",
            default_headers={"anthropic-beta": "files-api-2025-04-14"},
        ),
        markdown=True,
    )

    agent.print_response(
        "What does the attached image say.",
        images=[Image(content=uploaded_file)],
    )
```

---

<a name="models--anthropic--image_input_urlpy"></a>

### `models/anthropic/image_input_url.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and search the web for more information.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
        ),
    ],
    stream=True,
)
```

---

<a name="models--anthropic--knowledgepy"></a>

### `models/anthropic/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf anthropic openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.anthropic import Claude
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=AzureOpenAIEmbedder(),
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    knowledge=knowledge,
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--anthropic--mcp_connectorpy"></a>

### `models/anthropic/mcp_connector.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.models.claude import MCPServerConfiguration

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        default_headers={"anthropic-beta": "mcp-client-2025-04-04"},
        mcp_servers=[
            MCPServerConfiguration(
                type="url",
                name="deepwiki",
                url="https://mcp.deepwiki.com/sse",
            )
        ],
    ),
    markdown=True,
)

agent.print_response(
    "Tell me about https://github.com/agno-agi/agno",
    stream=True,
)
```

---

<a name="models--anthropic--memorypy"></a>

### `models/anthropic/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install anthropic sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/anthropic/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    # Pass the database to the Agent
    db=db,
    # Store the memories and summary in the database
    enable_user_memories=True,
    enable_session_summaries=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--anthropic--pdf_input_bytespy"></a>

### `models/anthropic/pdf_input_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(
            content=pdf_path.read_bytes(),
        ),
    ],
)
run_response = agent.get_last_run_output()
print("Citations:")
print(run_response.citations)
```

---

<a name="models--anthropic--pdf_input_file_uploadpy"></a>

### `models/anthropic/pdf_input_file_upload.py`

```python
"""
In this example, we upload a PDF file to Anthropic directly and then use it as an input to an agent.
"""

from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file
from anthropic import Anthropic

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

# Initialize Anthropic client
client = Anthropic()

# Upload the file to Anthropic
uploaded_file = client.beta.files.upload(
    file=Path(pdf_path),
)

if uploaded_file is not None:
    agent = Agent(
        model=Claude(
            id="claude-opus-4-20250514",
            default_headers={"anthropic-beta": "files-api-2025-04-14"},
        ),
        markdown=True,
    )

    agent.print_response(
        "Summarize the contents of the attached file.",
        files=[File(external=uploaded_file)],
    )
```

---

<a name="models--anthropic--pdf_input_localpy"></a>

### `models/anthropic/pdf_input_local.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(
            filepath=pdf_path,
        ),
    ],
)
run_response = agent.get_last_run_output()
print("Citations:")
print(run_response.citations)
```

---

<a name="models--anthropic--pdf_input_urlpy"></a>

### `models/anthropic/pdf_input_url.py`

```python
from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"),
    ],
)
```

---

<a name="models--anthropic--prompt_cachingpy"></a>

### `models/anthropic/prompt_caching.py`

```python
"""
This cookbook shows how to use prompt caching with Agents using Anthropic models, to catch the system prompt passed to the model.

This can significantly reduce processing time and costs.
Use it when working with a static and large system prompt.

You can check more about prompt caching with Anthropic models here: https://docs.anthropic.com/en/docs/prompt-caching
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.media import download_file

# Load an example large system message from S3. A large prompt like this would benefit from caching.
txt_path = Path(__file__).parent.joinpath("system_prompt.txt")
download_file(
    "https://agno-public.s3.amazonaws.com/prompts/system_promt.txt",
    str(txt_path),
)
system_message = txt_path.read_text()

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        cache_system_prompt=True,  # Activate prompt caching for Anthropic to cache the system prompt
    ),
    system_message=system_message,
    markdown=True,
)

# First run - this will create the cache
response = agent.run(
    "Explain the difference between REST and GraphQL APIs with examples"
)
if response and response.metrics:
    print(f"First run cache write tokens = {response.metrics.cache_write_tokens}")

# Second run - this will use the cached system prompt
response = agent.run(
    "What are the key principles of clean code and how do I apply them in Python?"
)
if response and response.metrics:
    print(f"Second run cache read tokens = {response.metrics.cache_read_tokens}")
```

---

<a name="models--anthropic--prompt_caching_extendedpy"></a>

### `models/anthropic/prompt_caching_extended.py`

```python
"""
This cookbook shows how to extend caching time for agents using cache with Anthropic models.

You can check more about extended prompt caching with Anthropic models here: https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#1-hour-cache-duration-beta
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.media import download_file

# Load an example large system message from S3. A large prompt like this would benefit from caching.
txt_path = Path(__file__).parent.joinpath("system_promt.txt")
download_file(
    "https://agno-public.s3.amazonaws.com/prompts/system_promt.txt",
    str(txt_path),
)
system_message = txt_path.read_text()

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        default_headers={"anthropic-beta": "extended-cache-ttl-2025-04-11"},
        system_prompt=system_message,
        cache_system_prompt=True,  # Activate prompt caching for Anthropic to cache the system prompt
        extended_cache_time=True,  # Extend the cache time from the default to 1 hour
    ),
    system_message=system_message,
    markdown=True,
)

# First run - this will create the cache
response = agent.run(
    "Explain the difference between REST and GraphQL APIs with examples"
)

if response and response.metrics:
    print(f"First run cache write tokens = {response.metrics.cache_write_tokens}")

# Second run - this will use the cached system prompt
response = agent.run(
    "What are the key principles of clean code and how do I apply them in Python?"
)
if response and response.metrics:
    print(f"Second run cache read tokens = {response.metrics.cache_read_tokens}")
```

---

<a name="models--anthropic--structured_outputpy"></a>

### `models/anthropic/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.anthropic import Claude
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
run: RunOutput = movie_agent.run("New York")
pprint(run.content)
```

---

<a name="models--anthropic--structured_output_streampy"></a>

### `models/anthropic/structured_output_stream.py`

```python
from typing import Dict, List

from agno.agent import Agent
from agno.models.anthropic import Claude
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

structured_output_agent.print_response(
    "New York", stream=True, stream_intermediate_steps=True
)
```

---

<a name="models--anthropic--thinkingpy"></a>

### `models/anthropic/thinking.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-3-7-sonnet-20250219",
        max_tokens=2048,
        thinking={"type": "enabled", "budget_tokens": 1024},
    ),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Share a very scary 2 sentence horror story")
```

---

<a name="models--anthropic--thinking_streampy"></a>

### `models/anthropic/thinking_stream.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-3-7-sonnet-20250219",
        max_tokens=2048,
        thinking={"type": "enabled", "budget_tokens": 1024},
    ),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Share a very scary 2 sentence horror story", stream=True)
```

---

<a name="models--anthropic--tool_usepy"></a>

### `models/anthropic/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--anthropic--tool_use_streampy"></a>

### `models/anthropic/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--anthropic--web_fetchpy"></a>

### `models/anthropic/web_fetch.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-opus-4-1-20250805",
        default_headers={"anthropic-beta": "web-fetch-2025-09-10"},
    ),
    tools=[
        {
            "type": "web_fetch_20250910",
            "name": "web_fetch",
            "max_uses": 5,
        }
    ],
    markdown=True,
)

agent.print_response(
    "Tell me more about https://en.wikipedia.org/wiki/Glacier_National_Park_(U.S.)",
    stream=True,
)
```

---

<a name="models--anthropic--web_searchpy"></a>

### `models/anthropic/web_search.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
    ),
    tools=[
        {
            "type": "web_search_20250305",
            "name": "web_search",
            "max_uses": 5,
        }
    ],
    markdown=True,
)

agent.print_response("What's the latest with Anthropic?", stream=True)
```

---

<a name="models--aws--bedrock--async_basicpy"></a>

### `models/aws/bedrock/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--aws--bedrock--async_basic_streampy"></a>

### `models/aws/bedrock/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--aws--bedrock--async_tool_use_streampy"></a>

### `models/aws/bedrock/async_tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AwsBedrock(id="amazon.nova-lite-v1:0"),
    tools=[DuckDuckGoTools()],
    instructions="You are a helpful assistant that can use the following tools to answer questions.",
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--aws--bedrock--basicpy"></a>

### `models/aws/bedrock/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--aws--bedrock--basic_streampy"></a>

### `models/aws/bedrock/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.aws import AwsBedrock

agent = Agent(model=AwsBedrock(id="mistral.mistral-small-2402-v1:0"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--aws--bedrock--image_agent_bytespy"></a>

### `models/aws/bedrock/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=AwsBedrock(id="amazon.nova-pro-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes, format="jpeg"),
    ],
)
```

---

<a name="models--aws--bedrock--pdf_agent_bytespy"></a>

### `models/aws/bedrock/pdf_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.aws import AwsBedrock
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=AwsBedrock(id="amazon.nova-pro-v1:0"),
    markdown=True,
)

pdf_bytes = pdf_path.read_bytes()

agent.print_response(
    "Give the recipe of Gaeng Kiew Wan Goong",
    files=[File(content=pdf_bytes, format="pdf", name="Thai Recipes")],
)
```

---

<a name="models--aws--bedrock--structured_outputpy"></a>

### `models/aws/bedrock/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.aws import AwsBedrock
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=AwsBedrock(id="mistral.mistral-large-2402-v1:0"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# movie_agent: RunOutput = movie_agent.run("New York")
# pprint(movie_agent.content)

movie_agent.print_response("New York")
```

---

<a name="models--aws--bedrock--tool_usepy"></a>

### `models/aws/bedrock/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AwsBedrock(id="mistral.mistral-large-2402-v1:0"),
    tools=[DuckDuckGoTools()],
    instructions="You are a helpful assistant that can use the following tools to answer questions.",
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--aws--bedrock--tool_use_streampy"></a>

### `models/aws/bedrock/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AwsBedrock(id="amazon.nova-lite-v1:0"),
    tools=[DuckDuckGoTools()],
    instructions="You are a helpful assistant that can use the following tools to answer questions.",
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--aws--claude--async_basicpy"></a>

### `models/aws/claude/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--aws--claude--async_basic_streampy"></a>

### `models/aws/claude/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--aws--claude--async_tool_usepy"></a>

### `models/aws/claude/async_tool_use.py`

```python
"""
Async example using Claude with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--aws--claude--basicpy"></a>

### `models/aws/claude/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--aws--claude--basic_streampy"></a>

### `models/aws/claude/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.aws import Claude

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--aws--claude--dbpy"></a>

### `models/aws/claude/db.py`

```python
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--aws--claude--image_agentpy"></a>

### `models/aws/claude/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and search the web for more information.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
        ),
    ],
    stream=True,
)
```

---

<a name="models--aws--claude--knowledgepy"></a>

### `models/aws/claude/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai anthropic` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.aws import Claude
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"), knowledge=knowledge
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--aws--claude--structured_outputpy"></a>

### `models/aws/claude/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.aws import Claude
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# movie_agent: RunOutput = movie_agent.run("New York")
# pprint(movie_agent.content)

movie_agent.print_response("New York")
```

---

<a name="models--aws--claude--tool_usepy"></a>

### `models/aws/claude/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--aws--claude--tool_use_streampy"></a>

### `models/aws/claude/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.aws import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(id="anthropic.claude-3-5-sonnet-20240620-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--azure--ai_foundry--async_basicpy"></a>

### `models/azure/ai_foundry/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(id="Phi-4"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))
```

---

<a name="models--azure--ai_foundry--async_basic_streampy"></a>

### `models/azure/ai_foundry/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry

assistant = Agent(
    model=AzureAIFoundry(id="Phi-4"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(
    assistant.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)
```

---

<a name="models--azure--ai_foundry--async_tool_usepy"></a>

### `models/azure/ai_foundry/async_tool_use.py`

```python
"""
Async example using Claude with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AzureAIFoundry(id="Cohere-command-r-08-2024"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))
```

---

<a name="models--azure--ai_foundry--basicpy"></a>

### `models/azure/ai_foundry/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(model=AzureAIFoundry(id="Phi-4"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--azure--ai_foundry--basic_streampy"></a>

### `models/azure/ai_foundry/basic_stream.py`

```python
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(
        id="Phi-4",
        azure_endpoint="",
    ),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--azure--ai_foundry--dbpy"></a>

### `models/azure/ai_foundry/db.py`

```python
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.azure import AzureAIFoundry
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=AzureAIFoundry(id="Phi-4"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--azure--ai_foundry--demo_coherepy"></a>

### `models/azure/ai_foundry/demo_cohere.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(model=AzureAIFoundry(id="Cohere-command-r-08-2024"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--azure--ai_foundry--demo_mistralpy"></a>

### `models/azure/ai_foundry/demo_mistral.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.azure import AzureAIFoundry

agent = Agent(model=AzureAIFoundry(id="Mistral-Large-2411"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--azure--ai_foundry--image_agentpy"></a>

### `models/azure/ai_foundry/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(id="Llama-3.2-11B-Vision-Instruct"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(
            url="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/sdk/ai/azure-ai-inference/samples/sample1.png",
            detail="high",
        )
    ],
    stream=True,
)
```

---

<a name="models--azure--ai_foundry--image_agent_bytespy"></a>

### `models/azure/ai_foundry/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.azure import AzureAIFoundry
from agno.utils.media import download_image

agent = Agent(
    model=AzureAIFoundry(id="Llama-3.2-11B-Vision-Instruct"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--azure--ai_foundry--knowledgepy"></a>

### `models/azure/ai_foundry/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.azure import AzureAIFoundry
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=AzureOpenAIEmbedder(),
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=AzureAIFoundry(id="Cohere-command-r-08-2024"),
    knowledge=knowledge,
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--azure--ai_foundry--structured_outputpy"></a>

### `models/azure/ai_foundry/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.azure import AzureAIFoundry
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=AzureAIFoundry(id="gpt-4o"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# run: RunOutput = agent.run("New York")
# pprint(run.content)

agent.print_response("New York")
```

---

<a name="models--azure--ai_foundry--tool_usepy"></a>

### `models/azure/ai_foundry/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AzureAIFoundry(id="Cohere-command-r-08-2024"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("What is currently happening in France?", stream=True)
```

---

<a name="models--azure--openai--async_basicpy"></a>

### `models/azure/openai/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureOpenAI

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))
```

---

<a name="models--azure--openai--async_basic_streampy"></a>

### `models/azure/openai/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.azure import AzureOpenAI

assistant = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(
    assistant.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)
```

---

<a name="models--azure--openai--basicpy"></a>

### `models/azure/openai/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.azure import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--azure--openai--basic_streampy"></a>

### `models/azure/openai/basic_stream.py`

```python
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.azure import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="gpt-4o-mini"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--azure--openai--dbpy"></a>

### `models/azure/openai/db.py`

```python
"""Run `pip install ddgs sqlalchemy anthropic` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.azure import AzureOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--azure--openai--knowledgepy"></a>

### `models/azure/openai/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.azure import AzureOpenAI
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=AzureOpenAIEmbedder(),
    ),
)
# Add content to the knowledge
asyncio.run(
    knowledge.add_content_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
)

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    knowledge=knowledge,
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--azure--openai--structured_outputpy"></a>

### `models/azure/openai/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.azure import AzureOpenAI
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
run: RunOutput = agent.run("New York")
pprint(run.content)

# agent.print_response("New York")
```

---

<a name="models--azure--openai--tool_usepy"></a>

### `models/azure/openai/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.azure import AzureOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--cerebras--async_basicpy"></a>

### `models/cerebras/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

asyncio.run(agent.aprint_response("write a two sentence horror story"))
```

---

<a name="models--cerebras--async_basic_streampy"></a>

### `models/cerebras/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

asyncio.run(agent.aprint_response("write a two sentence horror story", stream=True))
```

---

<a name="models--cerebras--async_tool_usepy"></a>

### `models/cerebras/async_tool_use.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))
```

---

<a name="models--cerebras--async_tool_use_streampy"></a>

### `models/cerebras/async_tool_use_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--cerebras--basicpy"></a>

### `models/cerebras/basic.py`

```python
from agno.agent import Agent
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story")
```

---

<a name="models--cerebras--basic_streampy"></a>

### `models/cerebras/basic_stream.py`

```python
from agno.agent import Agent  # noqa
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story", stream=True)
```

---

<a name="models--cerebras--dbpy"></a>

### `models/cerebras/db.py`

```python
"""Run `pip install ddgs sqlalchemy cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--cerebras--knowledgepy"></a>

### `models/cerebras/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.cerebras import Cerebras
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=Cerebras(id="llama-4-scout-17b-16e-instruct"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--cerebras--oss_gptpy"></a>

### `models/cerebras/oss_gpt.py`

```python
from agno.agent.agent import Agent
from agno.models.cerebras.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(
        id="gpt-oss-120b",
    ),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")
```

---

<a name="models--cerebras--structured_outputpy"></a>

### `models/cerebras/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.cerebras import Cerebras
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a JSON schema output
json_schema_output_agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    output_schema=MovieScript,
)

json_schema_output_agent.print_response("New York")
```

---

<a name="models--cerebras--tool_usepy"></a>

### `models/cerebras/tool_use.py`

```python
from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")
```

---

<a name="models--cerebras--tool_use_streampy"></a>

### `models/cerebras/tool_use_stream.py`

```python
from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--cerebras_openai--async_basicpy"></a>

### `models/cerebras_openai/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story"))
```

---

<a name="models--cerebras_openai--async_basic_streampy"></a>

### `models/cerebras_openai/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story", stream=True))
```

---

<a name="models--cerebras_openai--async_tool_usepy"></a>

### `models/cerebras_openai/async_tool_use.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-3.3-70b"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))
```

---

<a name="models--cerebras_openai--async_tool_use_streampy"></a>

### `models/cerebras_openai/async_tool_use_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-3.3-70b"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--cerebras_openai--basicpy"></a>

### `models/cerebras_openai/basic.py`

```python
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story")
```

---

<a name="models--cerebras_openai--basic_streampy"></a>

### `models/cerebras_openai/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story", stream=True)
```

---

<a name="models--cerebras_openai--dbpy"></a>

### `models/cerebras_openai/db.py`

```python
"""Run `pip install ddgs sqlalchemy cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--cerebras_openai--knowledgepy"></a>

### `models/cerebras_openai/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.cerebras import CerebrasOpenAI
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"), knowledge=knowledge
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--cerebras_openai--oss_gptpy"></a>

### `models/cerebras_openai/oss_gpt.py`

```python
from agno.agent.agent import Agent
from agno.models.cerebras.cerebras_openai import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(
        id="gpt-oss-120b",
    ),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")
```

---

<a name="models--cerebras_openai--structured_outputpy"></a>

### `models/cerebras_openai/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.cerebras import CerebrasOpenAI
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a structured output
structured_output_agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    output_schema=MovieScript,
)

structured_output_agent.print_response("New York")
```

---

<a name="models--cerebras_openai--tool_usepy"></a>

### `models/cerebras_openai/tool_use.py`

```python
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")
```

---

<a name="models--cerebras_openai--tool_use_streampy"></a>

### `models/cerebras_openai/tool_use_stream.py`

```python
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CerebrasOpenAI(id="llama-4-scout-17b-16e-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--cohere--async_basicpy"></a>

### `models/cohere/async_basic.py`

```python
"""
Basic async example using Cohere.
"""

import asyncio

from agno.agent import Agent
from agno.models.cohere import Cohere

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--cohere--async_basic_streampy"></a>

### `models/cohere/async_basic_stream.py`

```python
"""
Basic streaming async example using Cohere.
"""

import asyncio

from agno.agent import Agent
from agno.models.cohere import Cohere

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--cohere--async_structured_outputpy"></a>

### `models/cohere/async_structured_output.py`

```python
import asyncio
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.cohere import Cohere
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=Cohere(
        id="command-a-03-2025",
    ),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# response: RunOutput = await agent.arun("New York")
# pprint(response.content)

asyncio.run(agent.aprint_response("Find a cool movie idea about London and write it."))
```

---

<a name="models--cohere--async_tool_usepy"></a>

### `models/cohere/async_tool_use.py`

```python
"""
Async example using Cohere with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.cohere import Cohere
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--cohere--basicpy"></a>

### `models/cohere/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.cohere import Cohere

agent = Agent(model=Cohere(id="command-a-03-2025"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--cohere--basic_streampy"></a>

### `models/cohere/basic_stream.py`

```python
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.cohere import Cohere

agent = Agent(model=Cohere(id="command-a-03-2025"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--cohere--dbpy"></a>

### `models/cohere/db.py`

```python
"""Run `pip install ddgs sqlalchemy cohere` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.cohere import Cohere
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--cohere--image_agentpy"></a>

### `models/cohere/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.cohere import Cohere

agent = Agent(
    model=Cohere(id="c4ai-aya-vision-8b"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--cohere--image_agent_bytespy"></a>

### `models/cohere/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.cohere.chat import Cohere
from agno.utils.media import download_image

agent = Agent(
    model=Cohere(id="c4ai-aya-vision-8b"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--cohere--image_agent_local_filepy"></a>

### `models/cohere/image_agent_local_file.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.cohere.chat import Cohere
from agno.utils.media import download_image

agent = Agent(
    model=Cohere(id="c4ai-aya-vision-8b"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(filepath=image_path),
    ],
    stream=True,
)
```

---

<a name="models--cohere--knowledgepy"></a>

### `models/cohere/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai cohere` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.cohere import Cohere
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=Cohere(id="command-a-03-2025"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--cohere--memorypy"></a>

### `models/cohere/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install cohere sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/cohere/memory.py` to run the agent
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.cohere import Cohere

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    # Store agent sessions in a database
    db=PostgresDb(db_url=db_url),
    enable_user_memories=True,
    enable_session_summaries=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--cohere--structured_outputpy"></a>

### `models/cohere/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.cohere import Cohere
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


structured_output_agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
response: RunOutput = structured_output_agent.run("New York")
pprint(response.content)
```

---

<a name="models--cohere--tool_usepy"></a>

### `models/cohere/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.cohere import Cohere
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?")
```

---

<a name="models--cohere--tool_use_streampy"></a>

### `models/cohere/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.cohere import Cohere
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Cohere(id="command-a-03-2025"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--cometapi--async_basicpy"></a>

### `models/cometapi/async_basic.py`

```python
"""
Basic async example using CometAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.cometapi import CometAPI

agent = Agent(model=CometAPI(id="gpt-5-mini"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--cometapi--async_basic_streampy"></a>

### `models/cometapi/async_basic_stream.py`

```python
"""
Async streaming example using CometAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.cometapi import CometAPI

agent = Agent(model=CometAPI(id="gpt-5-mini"), markdown=True)

# Async streaming response
asyncio.run(
    agent.aprint_response(
        "Write a short poem about artificial intelligence", stream=True
    )
)
```

---

<a name="models--cometapi--async_tool_usepy"></a>

### `models/cometapi/async_tool_use.py`

```python
"""
Async tool use example using CometAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.cometapi import CometAPI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CometAPI(id="gpt-5"),
    tools=[DuckDuckGoTools(enable_search=True, enable_news=True)],
    markdown=True,
)

# Async tool use
asyncio.run(agent.aprint_response("What's the latest news about AI?"))
```

---

<a name="models--cometapi--async_tool_use_streampy"></a>

### `models/cometapi/async_tool_use_stream.py`

```python
"""
Async streaming tool use example using CometAPI.
"""

import asyncio

from agno.agent import Agent
from agno.models.cometapi import CometAPI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CometAPI(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Async streaming tool use
asyncio.run(
    agent.aprint_response(
        "Search for the latest developments in quantum computing and summarize them",
        stream=True,
    )
)
```

---

<a name="models--cometapi--basicpy"></a>

### `models/cometapi/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.cometapi import CometAPI

agent = Agent(model=CometAPI(id="gpt-5-mini"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Explain quantum computing in simple terms")
# print(run.content)

# Print the response in the terminal
agent.print_response("Explain quantum computing in simple terms")
```

---

<a name="models--cometapi--basic_streampy"></a>

### `models/cometapi/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.cometapi import CometAPI

agent = Agent(model=CometAPI(id="gpt-5-mini"), markdown=True)

# Print the response in the terminal with streaming enabled
agent.print_response("Explain quantum computing in simple terms", stream=True)
```

---

<a name="models--cometapi--image_agentpy"></a>

### `models/cometapi/image_agent.py`

```python
"""
Image analysis example using CometAPI with vision models.
"""

from agno.agent import Agent
from agno.media import Image
from agno.models.cometapi import CometAPI

# Use a vision-capable model from CometAPI
agent = Agent(
    model=CometAPI(id="gpt-4o"),  # GPT-4o has vision capabilities
    markdown=True,
)

agent.print_response(
    "Describe this image in detail and tell me what you can see",
    images=[
        Image(
            url="https://httpbin.org/image/png"  # Reliable test image
        )
    ],
    stream=True,
)
```

---

<a name="models--cometapi--image_agent_with_memorypy"></a>

### `models/cometapi/image_agent_with_memory.py`

```python
"""
Image analysis with memory example using CometAPI.
"""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.media import Image
from agno.models.cometapi import CometAPI

agent = Agent(
    model=CometAPI(id="gpt-4o"),  # GPT-4o has vision capabilities
    db=SqliteDb(db_file="tmp/cometapi_image_agent.db"),
    session_id="cometapi_image_session",
    markdown=True,
)

# First interaction with an image
agent.print_response(
    "Look at this image and remember what you see. Describe the character in detail.",
    images=[
        Image(
            url="https://httpbin.org/image/png"  # Reliable test image
        )
    ],
)

print("\n" + "=" * 50 + "\n")

# Follow-up question using memory
agent.print_response(
    "What was the main color of the character in the image I showed you earlier?"
)
```

---

<a name="models--cometapi--multi_modelpy"></a>

### `models/cometapi/multi_model.py`

```python
"""Example showcasing different models available through CometAPI."""

from agno.agent import Agent
from agno.models.cometapi import CometAPI


def test_model(
    model_id: str,
    prompt: str = "Explain what makes you unique as an AI model in 2-3 sentences.",
):
    """Test a specific model with a given prompt."""
    print(f"\n Testing {model_id}:")
    print("=" * 50)

    try:
        agent = Agent(model=CometAPI(id=model_id), markdown=True)
        agent.print_response(prompt)
    except Exception as e:
        print(f" Error with {model_id}: {e}")


def main():
    """Showcase different models available through CometAPI."""
    print(" CometAPI Multi-Model Showcase")
    print("This example demonstrates different AI models accessible through CometAPI")

    # Test different model categories
    models_to_test = [
        # OpenAI models
        ("gpt-5-mini", "Latest GPT-5 Mini model"),
        # Anthropic models
        ("claude-sonnet-4-20250514", "Claude Sonnet 4"),
        # Google models
        ("gemini-2.5-pro", "Gemini 2.5 Pro"),
        ("gemini-2.0-flash", "Gemini 2.0 Flash"),
        # DeepSeek models
        ("deepseek-v3", "DeepSeek V3"),
        ("deepseek-chat", "DeepSeek Chat"),
    ]

    for model_id, description in models_to_test:
        print(f"\n {description}")
        test_model(model_id)

        # Pause between models for readability
        # input("\nPress Enter to continue to the next model...")

    print("\n Multi-model showcase complete!")
    print(" Learn more about CometAPI at: https://www.cometapi.com/")


if __name__ == "__main__":
    main()
```

---

<a name="models--cometapi--structured_outputpy"></a>

### `models/cometapi/structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.cometapi import CometAPI
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(..., description="The setting of the movie")
    protagonist: str = Field(..., description="Name of the protagonist")
    antagonist: str = Field(..., description="Name of the antagonist")
    plot: str = Field(..., description="The plot of the movie")
    genre: str = Field(..., description="The genre of the movie")
    scenes: List[str] = Field(..., description="List of scenes in the movie")


agent = Agent(
    model=CometAPI(id="gpt-5-mini"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
    markdown=True,
)

agent.print_response("Generate a movie script about a time-traveling detective")
```

---

<a name="models--cometapi--tool_usepy"></a>

### `models/cometapi/tool_use.py`

```python
from agno.agent import Agent
from agno.models.cometapi import CometAPI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CometAPI(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("What is the latest price about BTCUSDT on Binance?")
```

---

<a name="models--cometapi--tool_use_streampy"></a>

### `models/cometapi/tool_use_stream.py`

```python
"""
from agno.agent import Agent
from agno.models.cometapi import CometAPI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CometAPI(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],e with streaming example using CometAPI.
"""

from agno.agent import Agent
from agno.models.cometapi import CometAPI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=CometAPI(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Tool use with streaming
agent.print_response(
    "What's the current weather in Tokyo and what are some popular tourist attractions there?",
    stream=True,
)
```

---

<a name="models--dashscope--async_basicpy"></a>

### `models/dashscope/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.dashscope import DashScope

agent = Agent(model=DashScope(id="qwen-plus", temperature=0.5), markdown=True)

# Get the response in a variable
# run: RunOutput = await agent.arun("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--dashscope--async_basic_streampy"></a>

### `models/dashscope/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent  # noqa
from agno.models.dashscope import DashScope

agent = Agent(model=DashScope(id="qwen-plus", temperature=0.5), markdown=True)


async def main():
    # Get the response in a variable
    # async for chunk in agent.arun("Share a 2 sentence horror story", stream=True):
    #     print(chunk.content, end="", flush=True)

    # Print the response in the terminal
    await agent.aprint_response("Share a 2 sentence horror story", stream=True)


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="models--dashscope--async_image_agentpy"></a>

### `models/dashscope/async_image_agent.py`

```python
import asyncio

from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-vl-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)


async def main():
    await agent.aprint_response(
        "What do you see in this image? Provide a detailed description and search for related information.",
        images=[
            Image(
                url="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg"
            )
        ],
        stream=True,
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="models--dashscope--async_tool_usepy"></a>

### `models/dashscope/async_tool_use.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)


async def main():
    await agent.aprint_response(
        "What's the latest news about artificial intelligence?", stream=True
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="models--dashscope--basicpy"></a>

### `models/dashscope/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.dashscope import DashScope

agent = Agent(model=DashScope(id="qwen-plus", temperature=0.5), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--dashscope--basic_streampy"></a>

### `models/dashscope/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.dashscope import DashScope

agent = Agent(model=DashScope(id="qwen-plus", temperature=0.5), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--dashscope--image_agentpy"></a>

### `models/dashscope/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-vl-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Analyze this image in detail and tell me what you see. Also search for more information about the subject.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--dashscope--image_agent_bytespy"></a>

### `models/dashscope/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=DashScope(id="qwen-vl-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Analyze this image of an ant. Describe its features, species characteristics, and search for more information about this type of ant.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--dashscope--structured_outputpy"></a>

### `models/dashscope/structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.dashscope import DashScope
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=DashScope(id="qwen-plus"),
    description="You write movie scripts and return them as structured JSON data.",
    output_schema=MovieScript,
)

structured_output_agent.print_response(
    "Create a movie script about llamas ruling the world. "
    "Return a JSON object with: name (movie title), setting, ending, genre, "
    "characters (list of character names), and storyline (3 sentences)."
)
```

---

<a name="models--dashscope--thinking_agentpy"></a>

### `models/dashscope/thinking_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope

agent = Agent(
    model=DashScope(id="qvq-max", enable_thinking=True),
)

image_url = "https://img.alicdn.com/imgextra/i1/O1CN01gDEY8M1W114Hi3XcN_!!6000000002727-0-tps-1024-406.jpg"

agent.print_response(
    "How do I solve this problem? Please think through each step carefully.",
    images=[Image(url=image_url)],
    stream=True,
)
```

---

<a name="models--dashscope--tool_usepy"></a>

### `models/dashscope/tool_use.py`

```python
from agno.agent import Agent
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DashScope(id="qwen-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("What's happening in AI today?", stream=True)
```

---

<a name="models--deepinfra--async_basicpy"></a>

### `models/deepinfra/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.deepinfra import DeepInfra  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Get the response in a variable
# def run_async() -> RunOutput:
#     return agent.arun("Share a 2 sentence horror story")
# response = asyncio.run(run_async())
# print(response.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--deepinfra--async_basic_streampy"></a>

### `models/deepinfra/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--deepinfra--async_tool_usepy"></a>

### `models/deepinfra/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa
from agno.tools.duckduckgo import DuckDuckGoTools  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("What's the latest news about AI?", stream=True))
```

---

<a name="models--deepinfra--basicpy"></a>

### `models/deepinfra/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.deepinfra import DeepInfra  # noqa


agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--deepinfra--basic_streampy"></a>

### `models/deepinfra/basic_stream.py`

```python
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    markdown=True,
)

# Get the response in a variable
run_response: Iterator[RunOutputEvent] = agent.run(
    "Share a 2 sentence horror story", stream=True
)
for chunk in run_response:
    print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--deepinfra--json_outputpy"></a>

### `models/deepinfra/json_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.deepinfra import DeepInfra  # noqa
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
agent = Agent(
    model=DeepInfra(id="microsoft/phi-4"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# response: RunOutput = agent.run("New York")
# pprint(response.content)

agent.print_response("New York")
```

---

<a name="models--deepinfra--tool_usepy"></a>

### `models/deepinfra/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent  # noqa
from agno.models.deepinfra import DeepInfra  # noqa
from agno.tools.duckduckgo import DuckDuckGoTools  # noqa

agent = Agent(
    model=DeepInfra(id="meta-llama/Llama-2-70b-chat-hf"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--deepseek--async_basicpy"></a>

### `models/deepseek/async_basic.py`

```python
"""
Basic async example using DeepSeek.
"""

import asyncio

from agno.agent import Agent
from agno.models.deepseek import DeepSeek

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--deepseek--async_basic_streamingpy"></a>

### `models/deepseek/async_basic_streaming.py`

```python
"""
Basic streaming async example using DeepSeek.
"""

import asyncio

from agno.agent import Agent
from agno.models.deepseek import DeepSeek

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--deepseek--async_tool_usepy"></a>

### `models/deepseek/async_tool_use.py`

```python
"""
Async example using DeepSeek with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--deepseek--basicpy"></a>

### `models/deepseek/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.deepseek import DeepSeek

agent = Agent(model=DeepSeek(id="deepseek-chat"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--deepseek--basic_streampy"></a>

### `models/deepseek/basic_stream.py`

```python
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.deepseek import DeepSeek

agent = Agent(model=DeepSeek(id="deepseek-chat"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--deepseek--reasoning_agentpy"></a>

### `models/deepseek/reasoning_agent.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

agent = Agent(
    model=DeepSeek(
        id="deepseek-reasoner",
    ),
    markdown=True,
)
agent.print_response(task, stream=True)
```

---

<a name="models--deepseek--structured_outputpy"></a>

### `models/deepseek/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.deepseek import DeepSeek
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
json_mode_response: RunOutput = json_mode_agent.run("New York")
pprint(json_mode_response.content)

# json_mode_agent.print_response("New York")
```

---

<a name="models--deepseek--tool_usepy"></a>

### `models/deepseek/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.tools.duckduckgo import DuckDuckGoTools

"""
The current version of the deepseek-chat model's Function Calling capabilitity is unstable, which may result in looped calls or empty responses.
Their development team is actively working on a fix, and it is expected to be resolved in the next version.
"""

agent = Agent(
    model=DeepSeek(id="deepseek-chat"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?")
```

---

<a name="models--fireworks--async_basicpy"></a>

### `models/fireworks/async_basic.py`

```python
"""
Basic async example using Fireworks.
"""

import asyncio

from agno.agent import Agent
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--fireworks--async_basic_streampy"></a>

### `models/fireworks/async_basic_stream.py`

```python
"""
Basic streaming async example using Fireworks.
"""

import asyncio

from agno.agent import Agent
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--fireworks--async_tool_usepy"></a>

### `models/fireworks/async_tool_use.py`

```python
"""
Async example using Fireworks with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.fireworks import Fireworks
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--fireworks--basicpy"></a>

### `models/fireworks/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--fireworks--basic_streampy"></a>

### `models/fireworks/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.fireworks import Fireworks

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--fireworks--structured_outputpy"></a>

### `models/fireworks/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.fireworks import Fireworks
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
response: RunOutput = agent.run("New York")
pprint(response.content)

# agent.print_response("New York")
```

---

<a name="models--fireworks--tool_usepy"></a>

### `models/fireworks/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.fireworks import Fireworks
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Fireworks(id="accounts/fireworks/models/llama-v3p1-405b-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--google--gemini--agent_with_thinking_budgetpy"></a>

### `models/google/gemini/agent_with_thinking_budget.py`

```python
"""
An example of how to use the thinking budget parameter with the Gemini model.
This requires `google-genai > 1.10.0`

- Turn off thinking use thinking_budget=0
- Turn on dynamic thinking use thinking_budget=-1
- To use a specific thinking token budget (e.g. 1280) use thinking_budget=1280
- Use include_thoughts=True to get the thought summaries in the response.
"""

from agno.agent import Agent
from agno.models.google import Gemini

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

agent = Agent(
    model=Gemini(id="gemini-2.5-pro", thinking_budget=1280, include_thoughts=True),
    markdown=True,
)
agent.print_response(task, stream=True)
```

---

<a name="models--google--gemini--async_basicpy"></a>

### `models/google/gemini/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-001",
        instructions=["You are a basic agent that writes short stories."],
    ),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--google--gemini--async_basic_streampy"></a>

### `models/google/gemini/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--google--gemini--async_image_editingpy"></a>

### `models/google/gemini/async_image_editing.py`

```python
import asyncio
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.media import Image
from agno.models.google import Gemini
from PIL import Image as PILImage

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)


async def modify_image():
    # Print the response in the terminal - using arun instead of run
    _ = await agent.arun(
        "Can you add a Llama in the background of this image?",
        images=[Image(filepath="generated_image.png")],
    )

    # Retrieve and display generated images using get_last_run_output
    run_response = agent.get_last_run_output()
    if run_response and isinstance(run_response, RunOutput) and run_response.images:
        for image_response in run_response.images:
            image_bytes = image_response.content
            if image_bytes:
                image = PILImage.open(BytesIO(image_bytes))
                image.show()
                # Save the image to a file
                # image.save("generated_image.png")
    else:
        print("No images found in run response")


if __name__ == "__main__":
    asyncio.run(modify_image())
```

---

<a name="models--google--gemini--async_image_generationpy"></a>

### `models/google/gemini/async_image_generation.py`

```python
import asyncio
import base64
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.db.in_memory import InMemoryDb
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    ),
    db=InMemoryDb(),
)


async def generate_image():
    # Print the response in the terminal - using arun instead of run
    _ = await agent.arun("Make me an image of a cat in a tree.")

    # Retrieve and display generated images using get_last_run_output
    run_response = agent.get_last_run_output()
    if run_response and isinstance(run_response, RunOutput) and run_response.images:
        for image_response in run_response.images:
            image_bytes = image_response.content
            if image_bytes:
                if isinstance(image_bytes, bytes):
                    image_bytes = base64.b64decode(image_bytes)

                image = Image.open(BytesIO(image_bytes))
                image.show()
                # Save the image to a file
                # image.save("generated_image.png")
    else:
        print("No images found in run response")


if __name__ == "__main__":
    asyncio.run(generate_image())
```

---

<a name="models--google--gemini--async_image_generation_streampy"></a>

### `models/google/gemini/async_image_generation_stream.py`

```python
import asyncio
import base64
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.db.in_memory import InMemoryDb
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    ),
    db=InMemoryDb(),
)


async def generate_image():
    # Stream the response
    response_stream = agent.arun("Make me an image of a cat in a tree.", stream=True)

    # Process streaming chunks (for text output)
    async for chunk in response_stream:
        # Just consume the stream, images will be in final result
        pass

    # Retrieve and display generated images using get_last_run_output
    run_response = agent.get_last_run_output()
    if run_response and isinstance(run_response, RunOutput) and run_response.images:
        for image_response in run_response.images:
            image_bytes = image_response.content
            if image_bytes:
                if isinstance(image_bytes, bytes):
                    image_bytes = base64.b64decode(image_bytes)

                image = Image.open(BytesIO(image_bytes))
                image.show()
                # Save the image to a file
                # image.save("generated_image.png")
    else:
        print("No images found in run response")


if __name__ == "__main__":
    asyncio.run(generate_image())
```

---

<a name="models--google--gemini--async_tool_usepy"></a>

### `models/google/gemini/async_tool_use.py`

```python
"""
Async example using Gemini with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--google--gemini--audio_input_bytes_contentpy"></a>

### `models/google/gemini/audio_input_bytes_content.py`

```python
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"

# Download the audio file from the URL as bytes
response = requests.get(url)
audio_content = response.content

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(content=audio_content)],
)
```

---

<a name="models--google--gemini--audio_input_file_uploadpy"></a>

### `models/google/gemini/audio_input_file_upload.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini
from google.genai.types import UploadFileConfig

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

# Please download a sample audio file to test this Agent and upload using:
audio_path = Path(__file__).parent.joinpath("sample.mp3")
audio_file = None

remote_file_name = f"files/{audio_path.stem.lower()}"
try:
    audio_file = model.get_client().files.get(name=remote_file_name)
except Exception as e:
    print(f"Error getting file {audio_path.stem}: {e}")
    pass

if not audio_file:
    try:
        audio_file = model.get_client().files.upload(
            file=audio_path,
            config=UploadFileConfig(name=audio_path.stem, display_name=audio_path.stem),
        )
        print(f"Uploaded audio: {audio_file}")
    except Exception as e:
        print(f"Error uploading audio: {e}")

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(content=audio_file)],
    stream=True,
)
```

---

<a name="models--google--gemini--audio_input_local_file_uploadpy"></a>

### `models/google/gemini/audio_input_local_file_upload.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

# Please download a sample audio file to test this Agent and upload using:
audio_path = Path(__file__).parent.joinpath("sample.mp3")

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(filepath=audio_path)],
    stream=True,
)
```

---

<a name="models--google--gemini--basicpy"></a>

### `models/google/gemini/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--google--gemini--basic_streampy"></a>

### `models/google/gemini/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--google--gemini--dbpy"></a>

### `models/google/gemini/db.py`

```python
"""Run `pip install ddgs sqlalchemy google.genai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--google--gemini--file_upload_with_cachepy"></a>

### `models/google/gemini/file_upload_with_cache.py`

```python
"""
In this example, we upload a text file to Google and then create a cache.

This greatly saves on tokens during normal prompting.
"""

from pathlib import Path
from time import sleep

import requests
from agno.agent import Agent
from agno.models.google import Gemini
from google import genai
from google.genai.types import UploadFileConfig

client = genai.Client()

# Download txt file
url = "https://storage.googleapis.com/generativeai-downloads/data/a11.txt"
path_to_txt_file = Path(__file__).parent.joinpath("a11.txt")
if not path_to_txt_file.exists():
    print("Downloading txt file...")
    with path_to_txt_file.open("wb") as wf:
        response = requests.get(url, stream=True)
        for chunk in response.iter_content(chunk_size=32768):
            wf.write(chunk)

# Upload the txt file using the Files API
remote_file_path = Path("a11.txt")
remote_file_name = f"files/{remote_file_path.stem.lower().replace('_', '-')}"

txt_file = None
try:
    txt_file = client.files.get(name=remote_file_name)
    print(f"Txt file exists: {txt_file.uri}")
except Exception:
    pass

if not txt_file:
    print("Uploading txt file...")
    txt_file = client.files.upload(
        file=path_to_txt_file, config=UploadFileConfig(name=remote_file_name)
    )

    # Wait for the file to finish processing
    while txt_file and txt_file.state and txt_file.state.name == "PROCESSING":
        print("Waiting for txt file to be processed.")
        sleep(2)
        txt_file = client.files.get(name=remote_file_name)

    print(f"Txt file processing complete: {txt_file.uri}")

# Create a cache with 5min TTL
cache = client.caches.create(
    model="gemini-2.0-flash-001",
    config={
        "system_instruction": "You are an expert at analyzing transcripts.",
        "contents": [txt_file],
        "ttl": "300s",
    },
)


if __name__ == "__main__":
    agent = Agent(
        model=Gemini(id="gemini-2.0-flash-001", cached_content=cache.name),
    )
    run_output = agent.run(
        "Find a lighthearted moment from this transcript",  # No need to pass the txt file
    )
    print("Metrics: ", run_output.metrics)
```

---

<a name="models--google--gemini--groundingpy"></a>

### `models/google/gemini/grounding.py`

```python
"""Grounding with Gemini.

Grounding enables Gemini to search the web and provide responses backed by
real-time information with citations. This is a legacy tool - for Gemini 2.0+
models, consider using the 'search' parameter instead.

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash",
        grounding=True,
        grounding_dynamic_threshold=0.7,  # Optional: set threshold for grounding
    ),
    add_datetime_to_context=True,
)

# Ask questions that benefit from real-time information
agent.print_response(
    "What are the current market trends in renewable energy?",
    stream=True,
    markdown=True,
)
```

---

<a name="models--google--gemini--image_editingpy"></a>

### `models/google/gemini/image_editing.py`

```python
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.media import Image
from agno.models.google import Gemini
from PIL import Image as PILImage

# No system message should be provided (Gemini requires only the image)
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

# Print the response in the terminal
response = agent.run(
    "Can you add a Llama in the background of this image?",
    images=[Image(filepath="tmp/test_photo.png")],
)

# Retrieve and display generated images using get_last_run_output
run_response = agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput) and run_response.images:
    for image_response in run_response.images:
        image_bytes = image_response.content
        if image_bytes:
            image = PILImage.open(BytesIO(image_bytes))
            image.show()
            # Save the image to a file
            # image.save("generated_image.png")
else:
    print("No images found in run response")
```

---

<a name="models--google--gemini--image_generationpy"></a>

### `models/google/gemini/image_generation.py`

```python
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

# Print the response in the terminal
run_response = agent.run("Make me an image of a cat in a tree.")

if run_response and isinstance(run_response, RunOutput) and run_response.images:
    for image_response in run_response.images:
        image_bytes = image_response.content
        if image_bytes:
            image = Image.open(BytesIO(image_bytes))
            image.show()
            # Save the image to a file
            # image.save("generated_image.png")
else:
    print("No images found in run response")
```

---

<a name="models--google--gemini--image_generation_streampy"></a>

### `models/google/gemini/image_generation_stream.py`

```python
import base64
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.db.in_memory import InMemoryDb
from agno.models.google import Gemini
from PIL import Image

# No system message should be provided
agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    ),
    db=InMemoryDb(),
)

# Print the response in the terminal - streaming
response = agent.run("Make me an image of a cat in a tree.", stream=True)

# Process streaming chunks (for text output)
for chunk in response:
    # Just consume the stream, images will be in final result
    pass

# Retrieve and display generated images using get_last_run_output
run_response = agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput) and run_response.images:
    for image_response in run_response.images:
        image_bytes = image_response.content
        if image_bytes:
            if isinstance(image_bytes, bytes):
                image_bytes = base64.b64decode(image_bytes)

            image = Image.open(BytesIO(image_bytes))
            image.show()
            # Save the image to a file
            # image.save("generated_image.png")
else:
    print("No images found in run response")
```

---

<a name="models--google--gemini--image_inputpy"></a>

### `models/google/gemini/image_input.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg"
        ),
    ],
)
```

---

<a name="models--google--gemini--image_input_file_uploadpy"></a>

### `models/google/gemini/image_input_file_upload.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from google.generativeai import upload_file
from google.generativeai.types import file_types

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
# Please download the image using
# wget https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg
image_path = Path(__file__).parent.joinpath("Krakow_-_Kosciol_Mariacki.jpg")
image_file: file_types.File = upload_file(image_path)
print(f"Uploaded image: {image_file}")

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[Image(content=image_file)],
    stream=True,
)
```

---

<a name="models--google--gemini--imagen_toolpy"></a>

### `models/google/gemini/imagen_tool.py`

```python
""" Example: Using the GeminiTools Toolkit for Image Generation

Make sure you have set the GOOGLE_API_KEY environment variable.
Example prompts to try:
- "Create a surreal painting of a floating city in the clouds at sunset"
- "Generate a photorealistic image of a cozy coffee shop interior"
- "Design a cute cartoon mascot for a tech startup, vector style"
- "Create an artistic portrait of a cyberpunk samurai in a rainy city"

Run `pip install google-genai agno` to install the necessary dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[GeminiTools()],
)

agent.print_response(
    "Create an artistic portrait of a cyberpunk samurai in a rainy city",
)
response = agent.run_response
if response and response.images:
    save_base64_data(str(response.images[0].content), "tmp/cyberpunk_samurai.png")
```

---

<a name="models--google--gemini--imagen_tool_advancedpy"></a>

### `models/google/gemini/imagen_tool_advanced.py`

```python
""" Example: Using the GeminiTools Toolkit for Image Generation

An Agent using the Gemini image generation tool.

Make sure to set the Vertex AI credentials. Here's the authentication guide: https://cloud.google.com/sdk/docs/initializing

Run `pip install google-genai agno` to install the required packages.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        GeminiTools(
            image_generation_model="imagen-4.0-generate-preview-05-20", vertexai=True
        )
    ],
)

agent.print_response(
    "Cinematic a visual shot using a stabilized drone flying dynamically alongside a pod of immense baleen whales as they breach spectacularly in deep offshore waters. The camera maintains a close, dramatic perspective as these colossal creatures launch themselves skyward from the dark blue ocean, creating enormous splashes and showering cascades of water droplets that catch the sunlight. In the background, misty, fjord-like coastlines with dense coniferous forests provide context. The focus expertly tracks the whales, capturing their surprising agility, immense power, and inherent grace. The color palette features the deep blues and greens of the ocean, the brilliant white spray, the dark grey skin of the whales, and the muted tones of the distant wild coastline, conveying the thrilling magnificence of marine megafauna."
)


response = agent.run_response
if response and response.images:
    save_base64_data(str(response.images[0].content), "tmp/baleen_whale.png")

"""
Example prompts to try:
- A horizontally oriented rectangular stamp features the Mission District's vibrant culture, portrayed in shades of warm terracotta orange using an etching style. The scene might depict a sun-drenched street like Valencia or Mission Street, lined with a mix of Victorian buildings and newer structures.
- Painterly landscape featuring a simple, isolated wooden cabin nestled amongst tall pine trees on the shore of a calm, reflective lake.
- Filmed cinematically from the driver's seat, offering a clear profile view of the young passenger on the front seat with striking red hair.
- A pile of books seen from above. The topmost book contains a watercolor illustration of a bird. VERTEX AI is written in bold letters on the book.
"""
```

---

<a name="models--google--gemini--knowledgepy"></a>

### `models/google/gemini/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai google.genai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.embedder.google import GeminiEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.google import Gemini
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=GeminiEmbedder(),
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--google--gemini--pdf_input_file_uploadpy"></a>

### `models/google/gemini/pdf_input_file_upload.py`

```python
"""
In this example, we upload a PDF file to Google GenAI directly and then use it as an input to an agent.

Note: If the size of the file is greater than 20MB, and a file path is provided, the file automatically gets uploaded to Google GenAI.
"""

from pathlib import Path
from time import sleep

from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini
from google import genai

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

client = genai.Client()

# Upload the file to Google GenAI
upload_result = client.files.upload(file=pdf_path)

# Get the file from Google GenAI
if upload_result and upload_result.name:
    retrieved_file = client.files.get(name=upload_result.name)
else:
    retrieved_file = None

# Retry up to 3 times if file is not ready
retries = 0
wait_time = 5
while retrieved_file is None and retries < 3:
    retries += 1
    sleep(wait_time)
    if upload_result and upload_result.name:
        retrieved_file = client.files.get(name=upload_result.name)
    else:
        retrieved_file = None

if retrieved_file is not None:
    agent = Agent(
        model=Gemini(id="gemini-2.0-flash-exp"),
        markdown=True,
        add_history_to_context=True,
    )

    agent.print_response(
        "Summarize the contents of the attached file.",
        files=[File(external=retrieved_file)],
    )

    agent.print_response(
        "Suggest me a recipe from the attached file.",
    )
else:
    print("Error: File was not ready after multiple attempts.")
```

---

<a name="models--google--gemini--pdf_input_localpy"></a>

### `models/google/gemini/pdf_input_local.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[File(filepath=pdf_path)],
)
agent.print_response("Suggest me a recipe from the attached file.")
```

---

<a name="models--google--gemini--pdf_input_urlpy"></a>

### `models/google/gemini/pdf_input_url.py`

```python
from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)

agent.print_response("Suggest me a recipe from the attached file.")
```

---

<a name="models--google--gemini--searchpy"></a>

### `models/google/gemini/search.py`

```python
"""Google Search with Gemini.

The search tool enables Gemini to access current information from Google Search.
This is useful for getting up-to-date facts, news, and web content.

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp", search=True),
    markdown=True,
)

# Ask questions that require current information
agent.print_response("What are the latest developments in AI technology this week?")
```

---

<a name="models--google--gemini--storage_and_memorypy"></a>

### `models/google/gemini/storage_and_memory.py`

```python
"""Run `pip install ddgs pgvector google.genai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge import PDFUrlKnowledgeBase
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
knowledge_base.load(recreate=True)  # Comment out after first run

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    knowledge=knowledge_base,
    # Store the memories and summary in a database
    db=PostgresDb(db_url=db_url, memory_table="agent_memory"),
    enable_user_memories=True,
    enable_session_summaries=True,
    # This setting adds a tool to search the knowledge base for information
    search_knowledge=True,
    # This setting adds a tool to get chat history
    read_chat_history=True,
    # Add the previous chat history to the messages sent to the Model.
    add_history_to_context=True,
    # This setting adds 6 previous messages from chat history to the messages sent to the LLM
    num_history_runs=6,
    markdown=True,
)
agent.print_response("Whats is the latest AI news?")
```

---

<a name="models--google--gemini--structured_outputpy"></a>

### `models/google/gemini/structured_output.py`

```python
from typing import Optional, Union

from agno.agent import Agent
from agno.models.google import Gemini
from pydantic import BaseModel, Field


class ContactInfo(BaseModel):
    """Contact information with structured properties"""

    contact_name: str = Field(description="Name of the contact person")
    contact_method: str = Field(
        description="Preferred communication method",
        enum=["email", "phone", "teams", "slack"],
    )
    contact_details: str = Field(description="Email address or phone number")


class EventSchema(BaseModel):
    event_id: str = Field(description="Unique event identifier")
    event_name: str = Field(description="Name of the event")

    event_date: str = Field(
        description="Event date in YYYY-MM-DD format",
        format="date",
    )

    start_time: str = Field(
        description="Event start time in HH:MM format",
        format="time",
    )

    duration: str = Field(
        description="Event duration in ISO 8601 format (e.g., PT2H30M)",
        format="duration",
    )

    status: str = Field(
        description="Current event status",
        enum=[
            "planning",
            "confirmed",
            "in_progress",
            "completed",
            "cancelled",
        ],
    )

    attendee_count: int = Field(
        description="Expected number of attendees",
        ge=1,
        le=10000,
    )

    budget_range: Union[float, str] = Field(
        description="Budget as number (USD) or 'TBD' if not determined"
    )

    optional_notes: Optional[str] = Field(
        description="Additional notes about the event (can be null)",
        default=None,
    )

    contact_info: ContactInfo = Field(
        description="Contact information with structured properties"
    )


structured_output_agent = Agent(
    name="Advanced Event Planner",
    model=Gemini(id="gemini-2.5-pro"),
    output_schema=EventSchema,
    instructions="""
    Create a detailed event plan that demonstrates all schema constraints:
    - Use proper date/time/duration formats
    - Set a realistic status from the enum options
    - Handle budget as either a number or 'TBD'
    - Include optional notes if relevant
    - Create contact info as a nested object
    """,
)

structured_output_agent.print_response(
    "Plan a corporate product launch event for 150 people next month"
)
```

---

<a name="models--google--gemini--structured_output_streampy"></a>

### `models/google/gemini/structured_output_stream.py`

```python
from typing import Dict, List

from agno.agent import Agent
from agno.models.google import Gemini
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

structured_output_agent.print_response(
    "New York", stream=True, stream_intermediate_steps=True
)
```

---

<a name="models--google--gemini--text_to_speechpy"></a>

### `models/google/gemini/text_to_speech.py`

```python
from agno.agent import Agent
from agno.models.google import Gemini
from agno.utils.audio import write_wav_audio_to_file

agent = Agent(
    model=Gemini(
        id="gemini-2.5-flash-preview-tts",
        response_modalities=["AUDIO"],
        speech_config={
            "voice_config": {"prebuilt_voice_config": {"voice_name": "Kore"}}
        },
    )
)

run_output = agent.run("Say cheerfully: Have a wonderful day!")

if run_output.response_audio is not None:
    audio_data = run_output.response_audio.content
    output_file = "tmp/cheerful_greeting.wav"
    write_wav_audio_to_file(output_file, audio_data)
```

---

<a name="models--google--gemini--thinking_agentpy"></a>

### `models/google/gemini/thinking_agent.py`

```python
from agno.agent import Agent
from agno.models.google import Gemini

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

agent = Agent(
    model=Gemini(
        id="gemini-2.5-pro",
        thinking_budget=1280,  # Enable thinking with token budget
        include_thoughts=True,  # Include thought summaries in response
    ),
    markdown=True,
)
agent.print_response(task)
```

---

<a name="models--google--gemini--thinking_agent_streampy"></a>

### `models/google/gemini/thinking_agent_stream.py`

```python
from agno.agent import Agent
from agno.models.google import Gemini

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

agent = Agent(
    model=Gemini(
        id="gemini-2.5-pro",
        thinking_budget=1280,  # Enable thinking with token budget
        include_thoughts=True,  # Include thought summaries in response
    ),
    markdown=True,
)
agent.print_response(task, stream=True)
```

---

<a name="models--google--gemini--tool_usepy"></a>

### `models/google/gemini/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--google--gemini--tool_use_streampy"></a>

### `models/google/gemini/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--google--gemini--url_contextpy"></a>

### `models/google/gemini/url_context.py`

```python
"""Run `pip install google-generativeai` to install dependencies."""

from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.5-flash", url_context=True),
    markdown=True,
)

url1 = "https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592"
url2 = "https://www.allrecipes.com/recipe/83557/juicy-roasted-chicken/"

agent.print_response(
    f"Compare the ingredients and cooking times from the recipes at {url1} and {url2}"
)
```

---

<a name="models--google--gemini--url_context_with_searchpy"></a>

### `models/google/gemini/url_context_with_search.py`

```python
"""Combine URL context with Google Search for comprehensive web analysis.

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

# Create agent with both Google Search and URL context enabled
agent = Agent(
    model=Gemini(id="gemini-2.5-flash", search=True, url_context=True),
    markdown=True,
)

# The agent will first search for relevant URLs, then analyze their content in detail
agent.print_response(
    "Analyze the content of the following URL: https://docs.agno.com/introduction and also give me latest updates on AI agents"
)
```

---

<a name="models--google--gemini--vertex_ai_searchpy"></a>

### `models/google/gemini/vertex_ai_search.py`

```python
"""Vertex AI Search with Gemini.

Vertex AI Search allows Gemini to search through your data stores,
providing grounded responses based on your private knowledge base.

Prerequisites:
1. Set up Vertex AI Search datastore in Google Cloud Console
2. Export environment variables:
   export GOOGLE_GENAI_USE_VERTEXAI="true"
   export GOOGLE_CLOUD_PROJECT="your-project-id"
   export GOOGLE_CLOUD_LOCATION="your-location"

Run `pip install google-generativeai` to install dependencies.
"""

from agno.agent import Agent
from agno.models.google import Gemini

# Replace with your actual Vertex AI Search datastore ID
# Format: "projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{datastore_id}"
datastore_id = "projects/your-project-id/locations/global/collections/default_collection/dataStores/your-datastore-id"

agent = Agent(
    model=Gemini(
        id="gemini-2.5-flash",
        vertexai_search=True,
        vertexai_search_datastore=datastore_id,
        vertexai=True,  # Use Vertex AI endpoint
    ),
    markdown=True,
)

# Ask questions that can be answered from your knowledge base
agent.print_response("What are our company's policies regarding remote work?")
```

---

<a name="models--google--gemini--vertexaipy"></a>

### `models/google/gemini/vertexai.py`

```python
"""
To use Vertex AI, with the Gemini Model class, you need to set the following environment variables:

export GOOGLE_GENAI_USE_VERTEXAI="true"
export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="your-location"

Or you can set the following parameters in the `Gemini` class:

gemini = Gemini(
    vertexai=True,
    project_id="your-google-cloud-project-id",
    location="your-google-cloud-location",
)
"""

from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--google--gemini--video_input_bytes_contentpy"></a>

### `models/google/gemini/video_input_bytes_content.py`

```python
import requests
from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://videos.pexels.com/video-files/5752729/5752729-uhd_2560_1440_30fps.mp4"

# Download the video file from the URL as bytes
response = requests.get(url)
video_content = response.content

agent.print_response(
    "Tell me about this video",
    videos=[Video(content=video_content)],
)
```

---

<a name="models--google--gemini--video_input_file_uploadpy"></a>

### `models/google/gemini/video_input_file_upload.py`

```python
import time
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger
from google.genai.types import UploadFileConfig

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

# Please download a sample video file to test this Agent
# Run: `wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4` to download a sample video
video_path = Path(__file__).parent.joinpath("samplevideo.mp4")
video_file = None
remote_file_name = f"files/{video_path.stem.lower().replace('_', '')}"
try:
    video_file = model.get_client().files.get(name=remote_file_name)
except Exception as e:
    logger.info(f"Error getting file {video_path.stem}: {e}")
    pass

# Upload the video file if it doesn't exist
if not video_file:
    try:
        logger.info(f"Uploading video: {video_path}")
        video_file = model.get_client().files.upload(
            file=video_path,
            config=UploadFileConfig(name=video_path.stem, display_name=video_path.stem),
        )

        # Check whether the file is ready to be used.
        while video_file and video_file.state and video_file.state.name == "PROCESSING":
            time.sleep(2)
            if video_file and video_file.name:
                video_file = model.get_client().files.get(name=video_file.name)
            else:
                video_file = None

        logger.info(f"Uploaded video: {video_file}")
    except Exception as e:
        logger.error(f"Error uploading video: {e}")

if __name__ == "__main__":
    agent.print_response(
        "Tell me about this video",
        videos=[Video(content=video_file)],
        stream=True,
    )
```

---

<a name="models--google--gemini--video_input_local_file_uploadpy"></a>

### `models/google/gemini/video_input_local_file_upload.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

# Get sample videos from https://www.pexels.com/search/videos/sample/
video_path = Path(__file__).parent.joinpath("sample_video.mp4")

agent.print_response("Tell me about this video?", videos=[Video(filepath=video_path)])
```

---

<a name="models--google--gemini--video_input_youtubepy"></a>

### `models/google/gemini/video_input_youtube.py`

```python
from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

agent.print_response(
    "Tell me about this video?",
    videos=[Video(url="https://www.youtube.com/watch?v=XinoY2LDdA0")],
)

# Video upload via URL is also supported with Vertex AI

# agent = Agent(
#     model=Gemini(id="gemini-2.0-flash-exp", vertexai=True),
#     markdown=True,
# )

# agent.print_response("Tell me about this video?", videos=[Video(url="https://www.youtube.com/watch?v=XinoY2LDdA0")])
```

---

<a name="models--groq--agent_teampy"></a>

### `models/groq/agent_team.py`

```python
from agno.agent import Agent
from agno.models.groq import Groq
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data",
    markdown=True,
)

agent_team = Team(
    members=[web_agent, finance_agent],
    model=Groq(
        id="llama-3.3-70b-versatile"
    ),  # You can use a different model for the team leader agent
    instructions=["Always include sources", "Use tables to display data"],
    markdown=True,
    show_members_responses=False,  # Comment to hide responses from team members
)

# Give the team a task
agent_team.print_response(
    input="Summarize the latest news about Nvidia and share its stock price?",
    stream=True,
)
```

---

<a name="models--groq--async_basicpy"></a>

### `models/groq/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))
```

---

<a name="models--groq--async_basic_streampy"></a>

### `models/groq/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the terminal
asyncio.run(
    agent.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)
```

---

<a name="models--groq--async_tool_usepy"></a>

### `models/groq/async_tool_use.py`

```python
"""Please install dependencies using:
pip install openai ddgs newspaper4k lxml_html_clean agno
"""

import asyncio

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    description="You are a senior NYT researcher writing an article on a topic.",
    instructions=[
        "For a given topic, search for the top 5 links.",
        "Then read each URL and extract the article text, if a URL isn't available, ignore it.",
        "Analyse and prepare an NYT worthy article based on the information.",
    ],
    markdown=True,
    add_datetime_to_context=True,
)

# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Simulation theory", stream=True))
```

---

<a name="models--groq--basicpy"></a>

### `models/groq/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.groq import Groq

agent = Agent(model=Groq(id="llama-3.3-70b-versatile"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--groq--basic_streampy"></a>

### `models/groq/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.groq import Groq

agent = Agent(model=Groq(id="llama-3.3-70b-versatile"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--groq--browser_searchpy"></a>

### `models/groq/browser_search.py`

```python
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(id="openai/gpt-oss-20b"),
    tools=[{"type": "browser_search"}],
)
agent.print_response("Is the Going-to-the-sun road open for public?")
```

---

<a name="models--groq--dbpy"></a>

### `models/groq/db.py`

```python
"""Run `pip install ddgs sqlalchemy groq` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.groq import Groq
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--groq--deep_knowledgepy"></a>

### `models/groq/deep_knowledge.py`

```python
""" DeepKnowledge - An AI Agent that iteratively searches a knowledge base to answer questions

This agent performs iterative searches through its knowledge base, breaking down complex
queries into sub-questions, and synthesizing comprehensive answers. It's designed to explore
topics deeply and thoroughly by following chains of reasoning.

In this example, the agent uses the Agno documentation as a knowledge base

Key Features:
- Iteratively searches a knowledge base
- Source attribution and citations

Run `pip install openai lancedb tantivy inquirer agno groq` to install dependencies.
"""

from textwrap import dedent
from typing import List, Optional

import inquirer
import typer
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.groq import Groq
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print


def initialize_knowledge_base():
    """Initialize the knowledge base with your preferred documentation or knowledge source
    Here we use Agno docs as an example, but you can replace with any relevant URLs
    """
    agent_knowledge = Knowledge(
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="deep_knowledge_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    )
    agent_knowledge.add_content(url="https://docs.agno.com/llms-full.txt")
    return agent_knowledge


def get_db():
    return SqliteDb(db_file="tmp/agents.db")


def create_agent(session_id: Optional[str] = None) -> Agent:
    """Create and return a configured DeepKnowledge agent."""
    agent_knowledge = initialize_knowledge_base()
    db = get_db()
    return Agent(
        name="DeepKnowledge",
        session_id=session_id,
        model=Groq(id="llama-3.3-70b-versatile"),
        description=dedent("""\
        You are DeepKnowledge, an advanced reasoning agent designed to provide thorough,
        well-researched answers to any query by searching your knowledge base.

        Your strengths include:
        - Breaking down complex topics into manageable components
        - Connecting information across multiple domains
        - Providing nuanced, well-researched answers
        - Maintaining intellectual honesty and citing sources
        - Explaining complex concepts in clear, accessible terms"""),
        instructions=dedent("""\
        Your mission is to leave no stone unturned in your pursuit of the correct answer.

        To achieve this, follow these steps:
        1. **Analyze the input and break it down into key components**.
        2. **Search terms**: You must identify at least 3-5 key search terms to search for.
        3. **Initial Search:** Searching your knowledge base for relevant information. You must make atleast 3 searches to get all relevant information.
        4. **Evaluation:** If the answer from the knowledge base is incomplete, ambiguous, or insufficient - Ask the user for clarification. Do not make informed guesses.
        5. **Iterative Process:**
            - Continue searching your knowledge base till you have a comprehensive answer.
            - Reevaluate the completeness of your answer after each search iteration.
            - Repeat the search process until you are confident that every aspect of the question is addressed.
        4. **Reasoning Documentation:** Clearly document your reasoning process:
            - Note when additional searches were triggered.
            - Indicate which pieces of information came from the knowledge base and where it was sourced from.
            - Explain how you reconciled any conflicting or ambiguous information.
        5. **Final Synthesis:** Only finalize and present your answer once you have verified it through multiple search passes.
            Include all pertinent details and provide proper references.
        6. **Continuous Improvement:** If new, relevant information emerges even after presenting your answer,
            be prepared to update or expand upon your response.

        **Communication Style:**
        - Use clear and concise language.
        - Organize your response with numbered steps, bullet points, or short paragraphs as needed.
        - Be transparent about your search process and cite your sources.
        - Ensure that your final answer is comprehensive and leaves no part of the query unaddressed.

        Remember: **Do not finalize your answer until every angle of the question has been explored.**"""),
        additional_context=dedent("""\
        You should only respond with the final answer and the reasoning process.
        No need to include irrelevant information.

        - User ID: {user_id}
        - Memory: You have access to your previous search results and reasoning process.
        """),
        knowledge=agent_knowledge,
        db=db,
        add_history_to_context=True,
        num_history_runs=3,
        read_chat_history=True,
        markdown=True,
    )


def get_example_topics() -> List[str]:
    """Return a list of example topics for the agent."""
    return [
        "What are AI agents and how do they work in Agno?",
        "What chunking strategies does Agno support for text processing?",
        "How can I implement custom tools in Agno?",
        "How does knowledge retrieval work in Agno?",
        "What types of embeddings does Agno support?",
    ]


def handle_session_selection() -> Optional[str]:
    """Handle session selection and return the selected session ID."""
    db = get_db()

    new = typer.confirm("Do you want to start a new session?", default=True)
    if new:
        return None

    existing_sessions = db.get_sessions()
    if not existing_sessions:
        print("No existing sessions found. Starting a new session.")
        return None

    print("\nExisting sessions:")
    for i, session in enumerate(existing_sessions, 1):
        print(f"{i}. {session.session_id}")  # type: ignore

    session_idx = typer.prompt(
        "Choose a session number to continue (or press Enter for most recent)",
        default=1,
    )

    try:
        return existing_sessions[int(session_idx) - 1].session_id  # type: ignore
    except (ValueError, IndexError):
        return existing_sessions[0].session_id  # type: ignore


def run_interactive_loop(agent: Agent):
    """Run the interactive question-answering loop."""
    example_topics = get_example_topics()

    while True:
        choices = [f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)]
        choices.extend(["Enter custom question...", "Exit"])

        questions = [
            inquirer.List(
                "topic",
                message="Select a topic or ask a different question:",
                choices=choices,
            )
        ]
        answer = inquirer.prompt(questions)

        if answer and answer["topic"] == "Exit":
            break

        if answer and answer["topic"] == "Enter custom question...":
            questions = [inquirer.Text("custom", message="Enter your question:")]
            custom_answer = inquirer.prompt(questions)
            topic = custom_answer["custom"]  # type: ignore
        else:
            topic = example_topics[int(answer["topic"].split(".")[0]) - 1]  # type: ignore

        agent.print_response(topic, stream=True)


def deep_knowledge_agent():
    """Main function to run the DeepKnowledge agent."""

    session_id = handle_session_selection()
    agent = create_agent(session_id)

    print("\n Welcome to DeepKnowledge - Your Advanced Research Assistant! ")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"[bold green]Started New Session: {session_id}[/bold green]\n")
        else:
            print("[bold green]Started New Session[/bold green]\n")
    else:
        print(f"[bold blue]Continuing Previous Session: {session_id}[/bold blue]\n")

    run_interactive_loop(agent)


if __name__ == "__main__":
    typer.run(deep_knowledge_agent)

# Example prompts to try:
"""
Explore Agno's capabilities with these queries:
1. "What are the different types of agents in Agno?"
2. "How does Agno handle knowledge base management?"
3. "What embedding models does Agno support?"
4. "How can I implement custom tools in Agno?"
5. "What storage options are available for workflow caching?"
6. "How does Agno handle streaming responses?"
7. "What types of LLM providers does Agno support?"
8. "How can I implement custom knowledge sources?"
"""
```

---

<a name="models--groq--image_agentpy"></a>

### `models/groq/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.groq import Groq

agent = Agent(model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"))

agent.print_response(
    "Tell me about this image",
    images=[
        Image(url="https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg"),
    ],
    stream=True,
)
```

---

<a name="models--groq--knowledgepy"></a>

### `models/groq/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai groq` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.groq import Groq
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    knowledge=knowledge,
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--groq--metricspy"></a>

### `models/groq/metrics.py`

```python
from agno.agent import Agent, RunOutput
from agno.models.groq import Groq
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[YFinanceTools()],
    markdown=True,
)

run_output: RunOutput = agent.run("What is the stock price of NVDA")
pprint_run_response(run_output)

# Print metrics per message
if run_output.messages:
    for message in run_output.messages:  # type: ignore
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(run_output.metrics)  # type: ignore
```

---

<a name="models--groq--reasoning--basicpy"></a>

### `models/groq/reasoning/basic.py`

```python
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(model=Groq(id="deepseek-r1-distill-llama-70b-specdec"), markdown=True)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--groq--reasoning--basic_streampy"></a>

### `models/groq/reasoning/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(model=Groq(id="deepseek-r1-distill-llama-70b-specdec"), markdown=True)

# Print the response on the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--groq--reasoning--demo_deepseek_qwenpy"></a>

### `models/groq/reasoning/demo_deepseek_qwen.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai groq` to install dependencies."""

from agno.agent import Agent
from agno.models.groq import Groq

agent_with_reasoning = Agent(
    model=Groq(id="Qwen-2.5-32b"),
    reasoning=True,
    reasoning_model=Groq(
        id="Deepseek-r1-distill-qwen-32b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)
agent_with_reasoning.print_response("9.11 and 9.9 -- which is bigger?", markdown=True)
```

---

<a name="models--groq--reasoning--demo_qwen_2_5_32bpy"></a>

### `models/groq/reasoning/demo_qwen_2_5_32B.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai groq` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.groq import Groq
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=Groq(id="Qwen-2.5-32b"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--groq--reasoning--finance_agentpy"></a>

### `models/groq/reasoning/finance_agent.py`

```python
from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.yfinance import YFinanceTools

# Create an Agent with Groq and YFinanceTools
finance_agent = Agent(
    model=Groq(id="deepseek-r1-distill-llama-70b-specdec"),
    tools=[YFinanceTools()],
    description="You are an investment analyst with deep expertise in market analysis",
    instructions=[
        "Use tables to display data where possible.",
        "Always call the tool before you answer.",
    ],
    add_datetime_to_context=True,
    markdown=True,
)

# Example usage
finance_agent.print_response(
    "Write a report on NVDA with stock price, analyst recommendations, and stock fundamentals.",
    stream=True,
)
```

---

<a name="models--groq--reasoning_agentpy"></a>

### `models/groq/reasoning_agent.py`

```python
from agno.agent import Agent
from agno.models.groq import Groq

# Create a reasoning agent that uses:
# - `deepseek-r1-distill-llama-70b` as the reasoning model
# - `llama-3.3-70b-versatile` to generate the final response
reasoning_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)

# Prompt the agent to solve the problem
reasoning_agent.print_response("Is 9.11 bigger or 9.9?", stream=True)
```

---

<a name="models--groq--research_agent_exapy"></a>

### `models/groq/research_agent_exa.py`

```python
"""Run `pip install groq exa-py` to install dependencies."""

from datetime import datetime
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.exa import ExaTools

cwd = Path(__file__).parent.resolve()
tmp = cwd.joinpath("tmp")
if not tmp.exists():
    tmp.mkdir(exist_ok=True, parents=True)

today = datetime.now().strftime("%Y-%m-%d")

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[ExaTools(start_published_date=today, type="keyword")],
    description="You are an advanced AI researcher writing a report on a topic.",
    instructions=[
        "For the provided topic, run 3 different searches.",
        "Read the results carefully and prepare a NYT worthy report.",
        "Focus on facts and make sure to provide references.",
    ],
    expected_output=dedent("""\
    An engaging, informative, and well-structured report in markdown format:

    ## Engaging Report Title

    ### Overview
    {give a brief introduction of the report and why the user should read this report}
    {make this section engaging and create a hook for the reader}

    ### Section 1
    {break the report into sections}
    {provide details/facts/processes in this section}

    ... more sections as necessary...

    ### Takeaways
    {provide key takeaways from the article}

    ### References
    - [Reference 1](link)
    - [Reference 2](link)
    - [Reference 3](link)

    ### About the Author
    {write a made up for yourself, give yourself a cyberpunk name and a title}

    - published on {date} in dd/mm/yyyy
    """),
    markdown=True,
    add_datetime_to_context=True,
    save_response_to_file=str(tmp.joinpath("{message}.md")),
)
agent.print_response("Llama 3.3 running on Groq", stream=True)
```

---

<a name="models--groq--structured_outputpy"></a>

### `models/groq/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.groq import Groq
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
run: RunOutput = json_mode_agent.run("New York")
pprint(run.content)

# json_mode_agent.print_response("New York")
```

---

<a name="models--groq--tool_usepy"></a>

### `models/groq/tool_use.py`

```python
"""Please install dependencies using:
pip install openai ddgs newspaper4k lxml_html_clean agno
"""

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    description="You are a senior NYT researcher writing an article on a topic.",
    instructions=[
        "For a given topic, search for the top 5 links.",
        "Then read each URL and extract the article text, if a URL isn't available, ignore it.",
        "Analyse and prepare an NYT worthy article based on the information.",
    ],
    markdown=True,
    add_datetime_to_context=True,
)
agent.print_response("Simulation theory", stream=True)
```

---

<a name="models--groq--transcription_agentpy"></a>

### `models/groq/transcription_agent.py`

```python
"""Run `pip install groq` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.groq import GroqTools

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

agent = Agent(
    name="Groq Transcription Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GroqTools(exclude_tools=["generate_speech"])],
)

agent.print_response(f"Please transcribe the audio file located at '{url}' to English")
```

---

<a name="models--groq--translation_agentpy"></a>

### `models/groq/translation_agent.py`

```python
import base64
from pathlib import Path

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.groq import GroqTools
from agno.utils.media import save_base64_data

path = "tmp/sample-fr.mp3"

agent = Agent(
    name="Groq Translation Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GroqTools()],
    cache_session=True,
)

response = agent.run(
    f"Let's transcribe the audio file located at '{path}' and translate it to English. After that generate a new music audio file using the translated text."
)

if response and response.audio:
    base64_audio = base64.b64encode(response.audio[0].content).decode("utf-8")
    save_base64_data(base64_audio, Path("tmp/sample-en.mp3"))  # type: ignore
```

---

<a name="models--huggingface--async_basicpy"></a>

### `models/huggingface/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(id="openai/gpt-oss-120b", max_tokens=4096, temperature=0),
)
asyncio.run(
    agent.aprint_response(
        "What is meaning of life and then recommend 5 best books to read about it"
    )
)
```

---

<a name="models--huggingface--async_basic_streampy"></a>

### `models/huggingface/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(id="openai/gpt-oss-120b", max_tokens=4096, temperature=0),
)
asyncio.run(
    agent.aprint_response(
        "What is meaning of life and then recommend 5 best books to read about it",
        stream=True,
    )
)
```

---

<a name="models--huggingface--basicpy"></a>

### `models/huggingface/basic.py`

```python
from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(
        id="mistralai/Mistral-7B-Instruct-v0.2", max_tokens=4096, temperature=0
    ),
)
agent.print_response(
    "What is meaning of life and then recommend 5 best books to read about it"
)
```

---

<a name="models--huggingface--basic_streampy"></a>

### `models/huggingface/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(id="openai/gpt-oss-120b", max_tokens=4096, temperature=0),
)
agent.print_response(
    "What is meaning of life and then recommend 5 best books to read about it",
    stream=True,
)
```

---

<a name="models--huggingface--llama_essay_writerpy"></a>

### `models/huggingface/llama_essay_writer.py`

```python
from agno.agent import Agent
from agno.models.huggingface import HuggingFace

agent = Agent(
    model=HuggingFace(
        id="openai/gpt-oss-120b",
        max_tokens=4096,
    ),
    description="You are an essay writer. Write a 300 words essay on topic that will be provided by user",
)
agent.print_response("topic: AI")
```

---

<a name="models--huggingface--tool_usepy"></a>

### `models/huggingface/tool_use.py`

```python
from agno.agent import Agent
from agno.models.huggingface import HuggingFace
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=HuggingFace(id="openai/gpt-oss-120b"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("What is the latest news on AI?")
```

---

<a name="models--huggingface--tool_use_streampy"></a>

### `models/huggingface/tool_use_stream.py`

```python
from agno.agent import Agent
from agno.models.huggingface import HuggingFace
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=HuggingFace(id="openai/gpt-oss-120b"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("What is the latest news on AI?", stream=True)
```

---

<a name="models--ibm--watsonx--async_basicpy"></a>

### `models/ibm/watsonx/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.ibm import WatsonX

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"), markdown=True
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--ibm--watsonx--async_basic_streampy"></a>

### `models/ibm/watsonx/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.ibm import WatsonX

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"),
    debug_mode=True,
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--ibm--watsonx--async_tool_usepy"></a>

### `models/ibm/watsonx/async_tool_use.py`

```python
"""
Async example using Claude with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.ibm import WatsonX
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--ibm--watsonx--basicpy"></a>

### `models/ibm/watsonx/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ibm import WatsonX

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"), markdown=True
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--ibm--watsonx--basic_streampy"></a>

### `models/ibm/watsonx/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.ibm import WatsonX

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--ibm--watsonx--dbpy"></a>

### `models/ibm/watsonx/db.py`

```python
"""Run `pip install ddgs sqlalchemy ibm-watsonx-ai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.ibm import WatsonX
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--ibm--watsonx--image_agent_bytespy"></a>

### `models/ibm/watsonx/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.ibm import WatsonX

agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-2-11b-vision-instruct"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--ibm--watsonx--knowledgepy"></a>

### `models/ibm/watsonx/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai ibm-watsonx-ai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.ibm import WatsonX
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"),
    knowledge=knowledge,
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--ibm--watsonx--structured_outputpy"></a>

### `models/ibm/watsonx/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.ibm import WatsonX
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


movie_agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"),
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# movie_agent: RunOutput = movie_agent.run("New York")
# pprint(movie_agent.content)

movie_agent.print_response("New York")
```

---

<a name="models--ibm--watsonx--tool_usepy"></a>

### `models/ibm/watsonx/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ibm import WatsonX
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=WatsonX(id="mistralai/mistral-small-3-1-24b-instruct-2503"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--langdb--agentpy"></a>

### `models/langdb/agent.py`

```python
"""Run `pip install yfinance` to install dependencies."""

from agno.agent import Agent, RunOutput  # noqa
from agno.models.langdb import LangDB
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LangDB(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions=["Use tables where possible."],
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("What is the stock price of NVDA and TSLA")
# print(run.content)

# Print the response in the terminal
agent.print_response("What is the stock price of NVDA and TSLA")
```

---

<a name="models--langdb--agent_streampy"></a>

### `models/langdb/agent_stream.py`

```python
"""Run `pip install yfinance` to install dependencies."""

from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.langdb import LangDB
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LangDB(id="gemini-1.5-pro-latest"),
    tools=[YFinanceTools()],
    instructions=["Use tables where possible."],
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("What is the stock price of NVDA and TSLA", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("What is the stock price of NVDA and TSLA", stream=True)
```

---

<a name="models--langdb--basicpy"></a>

### `models/langdb/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.langdb import LangDB

agent = Agent(model=LangDB(id="llama3-1-70b-instruct-v1.0"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--langdb--basic_streampy"></a>

### `models/langdb/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.langdb import LangDB

agent = Agent(
    model=LangDB(id="llama3-1-70b-instruct-v1.0"),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--langdb--data_analystpy"></a>

### `models/langdb/data_analyst.py`

```python
"""Run `pip install duckdb` to install dependencies."""

from textwrap import dedent

from agno.agent import Agent
from agno.models.langdb import LangDB
from agno.tools.duckdb import DuckDbTools

duckdb_tools = DuckDbTools(
    create_tables=False, export_tables=False, summarize_tables=False
)
duckdb_tools.create_table_from_path(
    path="https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
    table="movies",
)

agent = Agent(
    model=LangDB(id="llama3-1-70b-instruct-v1.0"),
    tools=[duckdb_tools],
    markdown=True,
    additional_context=dedent("""\
    You have access to the following tables:
    - movies: contains information about movies from IMDB.
    """),
)
agent.print_response("What is the average rating of movies?", stream=False)
```

---

<a name="models--langdb--finance_agentpy"></a>

### `models/langdb/finance_agent.py`

```python
"""Run `pip install yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.langdb import LangDB
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LangDB(id="llama3-1-70b-instruct-v1.0"),
    tools=[YFinanceTools()],
    description="You are an investment analyst that researches stocks and helps users make informed decisions.",
    instructions=["Use tables to display data where possible."],
    markdown=True,
)

# agent.print_response("Share the NVDA stock price and analyst recommendations", stream=True)
agent.print_response("Summarize fundamentals for TSLA", stream=True)
```

---

<a name="models--langdb--structured_outputpy"></a>

### `models/langdb/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.langdb import LangDB
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=LangDB(id="llama3-1-70b-instruct-v1.0"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=LangDB(id="llama3-1-70b-instruct-v1.0"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")
```

---

<a name="models--langdb--web_searchpy"></a>

### `models/langdb/web_search.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.langdb import LangDB
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LangDB(id="llama3-1-70b-instruct-v1.0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--litellm--async_basicpy"></a>

### `models/litellm/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

asyncio.run(openai_agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--litellm--async_basic_streampy"></a>

### `models/litellm/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(
    openai_agent.aprint_response("Share a 2 sentence horror story", stream=True)
)
```

---

<a name="models--litellm--async_tool_usepy"></a>

### `models/litellm/async_tool_use.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
    tools=[DuckDuckGoTools()],
)

# Ask a question that would likely trigger tool use
asyncio.run(agent.aprint_response("What is happening in France?"))
```

---

<a name="models--litellm--audio_input_agentpy"></a>

### `models/litellm/audio_input_agent.py`

```python
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.litellm import LiteLLM

# Fetch the QA audio file and convert it to a base64 encoded string
url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"
response = requests.get(url)
response.raise_for_status()
mp3_data = response.content

# Audio input requires specific audio-enabled models like gpt-4o-audio-preview
agent = Agent(
    model=LiteLLM(id="gpt-4o-audio-preview"),
    markdown=True,
)
agent.print_response(
    "What's the audio about?",
    audio=[Audio(content=mp3_data, format="mp3")],
    stream=True,
)
```

---

<a name="models--litellm--basicpy"></a>

### `models/litellm/basic.py`

```python
from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="huggingface/mistralai/Mistral-7B-Instruct-v0.2",
        top_p=0.95,
    ),
    markdown=True,
)

openai_agent.print_response("Whats happening in France?")
```

---

<a name="models--litellm--basic_gptpy"></a>

### `models/litellm/basic_gpt.py`

```python
from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

openai_agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--litellm--basic_streampy"></a>

### `models/litellm/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.litellm import LiteLLM

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
)

openai_agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--litellm--dbpy"></a>

### `models/litellm/db.py`

```python
"""Run `pip install ddgs openai` to install dependencies."""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db = SqliteDb(
    db_file="tmp/data.db",
)

# Add storage to the Agent
agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--litellm--image_agentpy"></a>

### `models/litellm/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--litellm--image_agent_bytespy"></a>

### `models/litellm/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--litellm--knowledgepy"></a>

### `models/litellm/knowledge.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.litellm import LiteLLM
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=LiteLLM(id="gpt-4o"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--litellm--memorypy"></a>

### `models/litellm/memory.py`

```python
from agno.agent import Agent
from agno.models.litellm import LiteLLM
from rich.pretty import pprint

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    # Set add_history_to_context=true to add the previous chat history to the context sent to the Model.
    add_history_to_context=True,
    # Number of historical responses to add to the messages.
    num_history_runs=3,
    description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
)

# -*- Create a run
agent.print_response("Share a 2 sentence horror story", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)

# -*- Ask a follow up question that continues the conversation
agent.print_response("What was my first message?", stream=True)
# -*- Print the messages in the memory
pprint(
    [
        m.model_dump(include={"role", "content"})
        for m in agent.get_messages_for_session()
    ]
)
```

---

<a name="models--litellm--metricspy"></a>

### `models/litellm/metrics.py`

```python
from agno.agent import Agent, RunOutput
from agno.models.litellm import LiteLLM
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
    ),
    tools=[YFinanceTools()],
    markdown=True,
)

run_output: RunOutput = agent.run("What is the stock price of NVDA")
pprint_run_response(run_output, markdown=True)

# Print metrics per message
if run_output.messages:
    for message in run_output.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(run_output.metrics)  # type: ignore
```

---

<a name="models--litellm--pdf_input_bytespy"></a>

### `models/litellm/pdf_input_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.litellm import LiteLLM
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=LiteLLM(id="openai/gpt-4o"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(
            content=pdf_path.read_bytes(),
        ),
    ],
)
```

---

<a name="models--litellm--pdf_input_localpy"></a>

### `models/litellm/pdf_input_local.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.litellm import LiteLLM
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "What is the recipe for Gaeng Som Phak Ruam? Also what are the health benefits. Refer to the attached file.",
    files=[File(filepath=pdf_path)],
)
```

---

<a name="models--litellm--pdf_input_urlpy"></a>

### `models/litellm/pdf_input_url.py`

```python
from agno.agent import Agent
from agno.media import File
from agno.models.litellm import LiteLLM

agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "Suggest me a recipe from the attached file.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)
```

---

<a name="models--litellm--structured_outputpy"></a>

### `models/litellm/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.litellm import LiteLLM
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=LiteLLM(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

json_mode_agent.print_response("New York")
```

---

<a name="models--litellm--tool_usepy"></a>

### `models/litellm/tool_use.py`

```python
from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.tools.yfinance import YFinanceTools

openai_agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    markdown=True,
    tools=[YFinanceTools()],
)

# Ask a question that would likely trigger tool use
openai_agent.print_response("How is TSLA stock doing right now?")
```

---

<a name="models--litellm--tool_use_streampy"></a>

### `models/litellm/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.litellm import LiteLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LiteLLM(
        id="gpt-4o",
        name="LiteLLM",
    ),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Ask a question that would likely trigger tool use
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--litellm_openai--audio_input_agentpy"></a>

### `models/litellm_openai/audio_input_agent.py`

```python
"""
Please first install litellm[proxy] by running: pip install 'litellm[proxy]'

Before running this script, you need to start the LiteLLM server:

litellm --model gpt-4o-audio-preview --host 127.0.0.1 --port 4000
"""

import requests
from agno.agent import Agent, RunResponse  # noqa
from agno.media import Audio
from agno.models.litellm import LiteLLMOpenAI

# Fetch the QA audio file and convert it to a base64 encoded string
url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"
response = requests.get(url)
response.raise_for_status()
mp3_data = response.content

# Provide the agent with the audio file and get result as text
# Note: Audio input requires specific audio-enabled models like gpt-4o-audio-preview
agent = Agent(
    model=LiteLLMOpenAI(id="gpt-4o-audio-preview"),
    markdown=True,
)
agent.print_response(
    "What is in this audio?", audio=[Audio(content=mp3_data, format="mp3")], stream=True
)
```

---

<a name="models--litellm_openai--basicpy"></a>

### `models/litellm_openai/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.litellm import LiteLLMOpenAI

agent = Agent(model=LiteLLMOpenAI(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--litellm_openai--basic_streampy"></a>

### `models/litellm_openai/basic_stream.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.litellm import LiteLLMOpenAI

agent = Agent(model=LiteLLMOpenAI(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--litellm_openai--tool_usepy"></a>

### `models/litellm_openai/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.litellm import LiteLLMOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LiteLLMOpenAI(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--llama_cpp--basicpy"></a>

### `models/llama_cpp/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.llama_cpp import LlamaCpp

agent = Agent(model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--llama_cpp--basic_streampy"></a>

### `models/llama_cpp/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.llama_cpp import LlamaCpp

agent = Agent(model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--llama_cpp--structured_outputpy"></a>

### `models/llama_cpp/structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.llama_cpp import LlamaCpp
from agno.run.agent import RunOutput
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Run the agent synchronously
structured_output_response: RunOutput = structured_output_agent.run("New York")
pprint(structured_output_response.content)
```

---

<a name="models--llama_cpp--tool_usepy"></a>

### `models/llama_cpp/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.llama_cpp import LlamaCpp
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--llama_cpp--tool_use_streampy"></a>

### `models/llama_cpp/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.llama_cpp import LlamaCpp
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--lmstudio--basicpy"></a>

### `models/lmstudio/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.lmstudio import LMStudio

agent = Agent(model=LMStudio(id="qwen2.5-7b-instruct-1m"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--lmstudio--basic_streampy"></a>

### `models/lmstudio/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.lmstudio import LMStudio

agent = Agent(model=LMStudio(id="qwen2.5-7b-instruct-1m"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--lmstudio--dbpy"></a>

### `models/lmstudio/db.py`

```python
"""Run `pip install ddgs sqlalchemy` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.lmstudio import LMStudio
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--lmstudio--image_agentpy"></a>

### `models/lmstudio/image_agent.py`

```python
import httpx
from agno.agent import Agent
from agno.media import Image
from agno.models.lmstudio import LMStudio

agent = Agent(
    model=LMStudio(id="llama3.2-vision"),
    markdown=True,
)

response = httpx.get(
    "https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
)

agent.print_response(
    "Tell me about this image",
    images=[Image(content=response.content)],
    stream=True,
)
```

---

<a name="models--lmstudio--knowledgepy"></a>

### `models/lmstudio/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai ollama` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.lmstudio import LMStudio
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=LMStudio(id="qwen2.5-7b-instruct-1m"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--lmstudio--memorypy"></a>

### `models/lmstudio/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install ollama sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/lmstudio/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.lmstudio import LMStudio

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    # Pass the database to the Agent
    db=db,
    # Enable user memories
    enable_user_memories=True,
    # Enable session summaries
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--lmstudio--structured_outputpy"></a>

### `models/lmstudio/structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.lmstudio import LMStudio
from agno.run.agent import RunOutput
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Run the agent synchronously
structured_output_response: RunOutput = structured_output_agent.run("New York")
pprint(structured_output_response.content)
```

---

<a name="models--lmstudio--tool_usepy"></a>

### `models/lmstudio/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.lmstudio import LMStudio
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--lmstudio--tool_use_streampy"></a>

### `models/lmstudio/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.lmstudio import LMStudio
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--meta--llama--async_basicpy"></a>

### `models/meta/llama/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import Llama

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = asyncio.run(agent.arun("Share a 2 sentence horror story"))
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--meta--llama--async_basic_streampy"></a>

### `models/meta/llama/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import Llama

agent = Agent(model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = asyncio.run(agent.arun("Share a 2 sentence horror story", stream=True))
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--meta--llama--async_knowledgepy"></a>

### `models/meta/llama/async_knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf llama-api-client` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.meta import Llama
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), knowledge=knowledge
)

if __name__ == "__main__":
    # Create and use the agent
    asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))
```

---

<a name="models--meta--llama--async_tool_usepy"></a>

### `models/meta/llama/async_tool_use.py`

```python
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?"))
```

---

<a name="models--meta--llama--async_tool_use_streampy"></a>

### `models/meta/llama/async_tool_use_stream.py`

```python
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?", stream=True))
```

---

<a name="models--meta--llama--basicpy"></a>

### `models/meta/llama/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import Llama

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--meta--llama--basic_streampy"></a>

### `models/meta/llama/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import Llama

agent = Agent(model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--meta--llama--dbpy"></a>

### `models/meta/llama/db.py`

```python
"""Run `pip install ddgs sqlalchemy llama-api-client` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.meta import Llama
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--meta--llama--image_input_bytespy"></a>

### `models/meta/llama/image_input_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import LlamaOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--meta--llama--image_input_filepy"></a>

### `models/meta/llama/image_input_file.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import Llama
from agno.utils.media import download_image

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

agent.print_response(
    "Tell me about this image?",
    images=[Image(filepath=image_path)],
    stream=True,
)
```

---

<a name="models--meta--llama--knowledgepy"></a>

### `models/meta/llama/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf llama-api-client` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.meta import Llama
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), knowledge=knowledge
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--meta--llama--memorypy"></a>

### `models/meta/llama/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.meta import Llama
from rich.pretty import pprint

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    user_id="test_user",
    session_id="test_session",
    # Pass the database to the Agent
    db=db,
    # Enable user memories
    enable_user_memories=True,
    # Enable session summaries
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
)

# -*- Share personal information
agent.print_response("My name is John Billings", stream=True)

# -*- Print memories and session summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I live in NYC", stream=True)
# -*- Print memories and session summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )


# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--meta--llama--metricspy"></a>

### `models/meta/llama/metrics.py`

```python
from agno.agent import Agent, RunOutput
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    markdown=True,
)

run_output: RunOutput = agent.run("What is the stock price of NVDA")
pprint_run_response(run_output, markdown=True)

# Print metrics per message
if run_output.messages:
    for message in run_output.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(run_output.metrics)  # type: ignore
```

---

<a name="models--meta--llama--structured_outputpy"></a>

### `models/meta/llama/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import Llama
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a JSON schema output
json_schema_output_agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8", temperature=0.1),
    output_schema=MovieScript,
)

json_schema_output_agent.print_response("New York")
```

---

<a name="models--meta--llama--tool_usepy"></a>

### `models/meta/llama/tool_use.py`

```python
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
agent.print_response("What is the price of AAPL stock?")
```

---

<a name="models--meta--llama--tool_use_streampy"></a>

### `models/meta/llama/tool_use_stream.py`

```python
"""Run `pip install agno llama-api-client yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
agent.print_response("Tell me the price of AAPL stock", stream=True)
```

---

<a name="models--meta--llama_openai--async_basicpy"></a>

### `models/meta/llama_openai/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = asyncio.run(agent.arun("Share a 2 sentence horror story"))
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--meta--llama_openai--async_basic_streampy"></a>

### `models/meta/llama_openai/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = asyncio.run(agent.arun("Share a 2 sentence horror story", stream=True))
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--meta--llama_openai--async_tool_usepy"></a>

### `models/meta/llama_openai/async_tool_use.py`

```python
"""Run `pip install openai yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?"))
```

---

<a name="models--meta--llama_openai--async_tool_use_streampy"></a>

### `models/meta/llama_openai/async_tool_use_stream.py`

```python
"""Run `pip install openai yfinance` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
asyncio.run(agent.aprint_response("Whats the price of AAPL stock?", stream=True))
```

---

<a name="models--meta--llama_openai--basicpy"></a>

### `models/meta/llama_openai/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--meta--llama_openai--basic_streampy"></a>

### `models/meta/llama_openai/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import LlamaOpenAI

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--meta--llama_openai--image_input_bytespy"></a>

### `models/meta/llama_openai/image_input_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import Llama
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--meta--llama_openai--image_input_filepy"></a>

### `models/meta/llama_openai/image_input_file.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import LlamaOpenAI
from agno.utils.media import download_image

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

agent.print_response(
    "Tell me about this image?",
    images=[Image(filepath=image_path)],
    stream=True,
)
```

---

<a name="models--meta--llama_openai--knowledgepy"></a>

### `models/meta/llama_openai/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.meta import LlamaOpenAI
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    knowledge=knowledge,
)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--meta--llama_openai--memorypy"></a>

### `models/meta/llama_openai/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.meta import LlamaOpenAI
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    # Store sessions, memories and summaries in the
    db=PostgresDb(db_url=db_url, memory_table="agent_memory"),
    enable_user_memories=True,
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
    debug_mode=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
pprint(agent.memory.memories)
# -*- Print summary
pprint(agent.memory.summaries)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--meta--llama_openai--metricspy"></a>

### `models/meta/llama_openai/metrics.py`

```python
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
    markdown=True,
)

run_stream: Iterator[RunOutputEvent] = agent.run(
    "What is the stock price of NVDA", stream=True
)
pprint_run_response(run_stream, markdown=True)

run_response = agent.get_last_run_output()

# Print metrics per message
if run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(run_response.metrics)
# Print the session metrics
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.get_session_metrics())
```

---

<a name="models--meta--llama_openai--storagepy"></a>

### `models/meta/llama_openai/storage.py`

```python
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.meta import LlamaOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    db=PostgresDb(db_url=db_url, session_table="llama_openai_sessions"),
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--meta--llama_openai--structured_outputpy"></a>

### `models/meta/llama_openai/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import LlamaOpenAI
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a JSON schema output
json_schema_output_agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8", temperature=0.1),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    output_schema=MovieScript,
)

json_schema_output_agent.print_response("New York")
```

---

<a name="models--meta--llama_openai--tool_usepy"></a>

### `models/meta/llama_openai/tool_use.py`

```python
"""Run `pip install openai yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
agent.print_response("Whats the price of AAPL stock?")
```

---

<a name="models--meta--llama_openai--tool_use_streampy"></a>

### `models/meta/llama_openai/tool_use_stream.py`

```python
"""Run `pip install openai yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.meta import LlamaOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[YFinanceTools()],
)
agent.print_response("Whats the price of AAPL stock?", stream=True)
```

---

<a name="models--mistral--async_basicpy"></a>

### `models/mistral/async_basic.py`

```python
"""
Basic async example using Mistral.
"""

import asyncio

from agno.agent import Agent
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--mistral--async_basic_streampy"></a>

### `models/mistral/async_basic_stream.py`

```python
"""
Basic streaming async example using Mistral.
"""

import asyncio

from agno.agent import Agent
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--mistral--async_structured_outputpy"></a>

### `models/mistral/async_structured_output.py`

```python
import asyncio
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=MistralChat(
        id="mistral-small-latest",
    ),
    tools=[DuckDuckGoTools()],
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

asyncio.run(agent.aprint_response("Find a cool movie idea about London and write it."))
```

---

<a name="models--mistral--async_tool_usepy"></a>

### `models/mistral/async_tool_use.py`

```python
"""
Async example using Mistral with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.mistral.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--mistral--basicpy"></a>

### `models/mistral/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.mistral import MistralChat


agent = Agent(
    model=MistralChat(id="mistral-small-latest"),
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--mistral--basic_streampy"></a>

### `models/mistral/basic_stream.py`

```python
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.mistral import MistralChat

agent = Agent(
    model=MistralChat(
        id="mistral-large-latest",
    ),
    markdown=True,
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--mistral--image_bytes_input_agentpy"></a>

### `models/mistral/image_bytes_input_agent.py`

```python
import requests
from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    markdown=True,
)

image_url = (
    "https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg"
)


def fetch_image_bytes(url: str) -> bytes:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.content


image_bytes_from_url = fetch_image_bytes(image_url)

agent.print_response(
    "Tell me about this image.",
    images=[
        Image(content=image_bytes_from_url),
    ],
)
```

---

<a name="models--mistral--image_compare_agentpy"></a>

### `models/mistral/image_compare_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    markdown=True,
)

agent.print_response(
    "what are the differences between two images?",
    images=[
        Image(
            url="https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg"
        ),
        Image(
            url="https://assets.visitorscoverage.com/production/wp-content/uploads/2024/04/AdobeStock_626542468-min-1024x683.jpeg"
        ),
    ],
    stream=True,
)
```

---

<a name="models--mistral--image_file_input_agentpy"></a>

### `models/mistral/image_file_input_agent.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    tools=[
        DuckDuckGoTools()
    ],  # pixtral-12b-2409 is not so great at tool calls, but it might work.
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpeg")

agent.print_response(
    "Tell me about this image and give me the latest news about it from duckduckgo.",
    images=[
        Image(filepath=image_path),
    ],
    stream=True,
)
```

---

<a name="models--mistral--image_ocr_with_structured_outputpy"></a>

### `models/mistral/image_ocr_with_structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat
from pydantic import BaseModel


class GroceryItem(BaseModel):
    item_name: str
    price: float


class GroceryListElements(BaseModel):
    bill_number: str
    items: List[GroceryItem]
    total_price: float


agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    instructions=[
        "Extract the text elements described by the user from the picture",
    ],
    output_schema=GroceryListElements,
    markdown=True,
)

agent.print_response(
    "From this restaurant bill, extract the bill number, item names and associated prices, and total price and return it as a string in a Json object",
    images=[Image(url="https://i.imghippo.com/files/kgXi81726851246.jpg")],
)
```

---

<a name="models--mistral--image_transcribe_document_agentpy"></a>

### `models/mistral/image_transcribe_document_agent.py`

```python
"""
This agent transcribes an old written document from an image.
"""

from agno.agent import Agent
from agno.media import Image
from agno.models.mistral.mistral import MistralChat

agent = Agent(
    model=MistralChat(id="pixtral-12b-2409"),
    markdown=True,
)

agent.print_response(
    "Transcribe this document.",
    images=[
        Image(url="https://ciir.cs.umass.edu/irdemo/hw-demo/page_example.jpg"),
    ],
)
```

---

<a name="models--mistral--memorypy"></a>

### `models/mistral/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install mistralai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/mistral/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.mistral.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
# Setup the database
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    tools=[DuckDuckGoTools()],
    # Pass the database to the Agent
    db=db,
    # Enable user memories
    enable_user_memories=True,
    # Enable session summaries
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# -*- Make tool call
agent.print_response("What is the weather in nyc?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--mistral--mistral_smallpy"></a>

### `models/mistral/mistral_small.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(id="mistral-small-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Tell me about mistrall small, any news", stream=True)
```

---

<a name="models--mistral--structured_outputpy"></a>

### `models/mistral/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


structured_output_agent = Agent(
    model=MistralChat(
        id="mistral-large-latest",
    ),
    tools=[DuckDuckGoTools()],
    description="You help people write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
structured_output_response: RunOutput = structured_output_agent.run("New York")
pprint(structured_output_response.content)
```

---

<a name="models--mistral--structured_output_with_tool_usepy"></a>

### `models/mistral/structured_output_with_tool_use.py`

```python
from agno.agent import Agent
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel


class Person(BaseModel):
    name: str
    description: str


model = MistralChat(
    id="mistral-medium-latest",
    temperature=0.0,
)

researcher = Agent(
    name="Researcher",
    model=model,
    role="You find people with a specific role at a provided company.",
    instructions=[
        "- Search the web for the person described"
        "- Find out if they have public contact details"
        "- Return the information in a structured format"
    ],
    tools=[DuckDuckGoTools()],
    output_schema=Person,
    add_datetime_to_context=True,
)

researcher.print_response("Find information about Elon Musk")
```

---

<a name="models--mistral--tool_usepy"></a>

### `models/mistral/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.mistral import MistralChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=MistralChat(
        id="mistral-large-latest",
    ),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--nebius--async_basicpy"></a>

### `models/nebius/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story"))
```

---

<a name="models--nebius--async_basic_streampy"></a>

### `models/nebius/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("write a two sentence horror story", stream=True))
```

---

<a name="models--nebius--async_tool_usepy"></a>

### `models/nebius/async_tool_use.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))
```

---

<a name="models--nebius--async_tool_use_streampy"></a>

### `models/nebius/async_tool_use_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--nebius--basicpy"></a>

### `models/nebius/basic.py`

```python
from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story")
```

---

<a name="models--nebius--basic_streampy"></a>

### `models/nebius/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.nebius import Nebius

agent = Agent(
    model=Nebius(),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("write a two sentence horror story", stream=True)
```

---

<a name="models--nebius--dbpy"></a>

### `models/nebius/db.py`

```python
"""Run `pip install ddgs sqlalchemy cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Nebius(),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--nebius--knowledgepy"></a>

### `models/nebius/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.nebius import Nebius
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=Nebius(id="Qwen/Qwen3-30B-A3B"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--nebius--structured_outputpy"></a>

### `models/nebius/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.nebius import Nebius
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses a structured output
structured_output_agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    output_schema=MovieScript,
)

structured_output_agent.print_response("New York")
```

---

<a name="models--nebius--tool_usepy"></a>

### `models/nebius/tool_use.py`

```python
from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?")
```

---

<a name="models--nebius--tool_use_streampy"></a>

### `models/nebius/tool_use_stream.py`

```python
from agno.agent import Agent
from agno.models.nebius import Nebius
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--nexus--async_basicpy"></a>

### `models/nexus/async_basic.py`

```python
"""
Basic async example using Nexus.
"""

import asyncio

from agno.agent import Agent
from agno.models.nexus import Nexus

agent = Agent(model=Nexus(id="anthropic/claude-sonnet-4-20250514"), markdown=True)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--nexus--async_basic_streampy"></a>

### `models/nexus/async_basic_stream.py`

```python
"""
Basic streaming async example using Nexus.
"""

import asyncio

from agno.agent import Agent
from agno.models.nexus import Nexus

agent = Agent(model=Nexus(id="anthropic/claude-sonnet-4-20250514"), markdown=True)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--nexus--async_tool_usepy"></a>

### `models/nexus/async_tool_use.py`

```python
"""
Async example using Nexus with tool calls
"""

import asyncio

from agno.agent import Agent
from agno.models.nexus import Nexus
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nexus(id="anthropic/claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?"))
```

---

<a name="models--nexus--async_tool_use_streampy"></a>

### `models/nexus/async_tool_use_stream.py`

```python
"""
Async example using Nexus with tool call streaming.
"""

import asyncio

from agno.agent import Agent
from agno.models.nexus import Nexus
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nexus(id="anthropic/claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--nexus--basicpy"></a>

### `models/nexus/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.nexus import Nexus

agent = Agent(model=Nexus(id="anthropic/claude-sonnet-4-20250514"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--nexus--basic_streampy"></a>

### `models/nexus/basic_stream.py`

```python
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--nexus--tool_usepy"></a>

### `models/nexus/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.nexus import Nexus
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nexus(id="anthropic/claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?")
```

---

<a name="models--nexus--tool_use_streampy"></a>

### `models/nexus/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.nexus import Nexus
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nexus(id="anthropic/claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--nvidia--async_basicpy"></a>

### `models/nvidia/async_basic.py`

```python
"""
Basic async example using Nvidia.
"""

import asyncio

from agno.agent import Agent
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--nvidia--async_basic_streampy"></a>

### `models/nvidia/async_basic_stream.py`

```python
"""
Basic streaming async example using Nvidia.
"""

import asyncio

from agno.agent import Agent
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--nvidia--async_tool_usepy"></a>

### `models/nvidia/async_tool_use.py`

```python
"""
Async example using Nvidia with tool calls.
"""

import asyncio

from agno.agent import Agent
from agno.models.nvidia import Nvidia
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nvidia(id="meta/llama-3.3-70b-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--nvidia--basicpy"></a>

### `models/nvidia/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--nvidia--basic_streampy"></a>

### `models/nvidia/basic_stream.py`

```python
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.nvidia import Nvidia

agent = Agent(model=Nvidia(id="meta/llama-3.3-70b-instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--nvidia--tool_usepy"></a>

### `models/nvidia/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.nvidia import Nvidia
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nvidia(id="meta/llama-3.3-70b-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?")
```

---

<a name="models--nvidia--tool_use_streampy"></a>

### `models/nvidia/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.nvidia import Nvidia
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Nvidia(id="meta/llama-3.3-70b-instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--ollama--async_basicpy"></a>

### `models/ollama/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.1:8b"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))
```

---

<a name="models--ollama--async_basic_streampy"></a>

### `models/ollama/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--ollama--basicpy"></a>

### `models/ollama/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--ollama--basic_streampy"></a>

### `models/ollama/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--ollama--dbpy"></a>

### `models/ollama/db.py`

```python
"""Run `pip install ddgs sqlalchemy ollama` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.ollama import Ollama
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Ollama(id="llama3.1:8b"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--ollama--demo_deepseek_r1py"></a>

### `models/ollama/demo_deepseek_r1.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="deepseek-r1:14b"), markdown=True)

# Print the response in the terminal
agent.print_response(
    "Write me python code to solve quadratic equations. Explain your reasoning."
)
```

---

<a name="models--ollama--demo_gemmapy"></a>

### `models/ollama/demo_gemma.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="gemma3:12b"), markdown=True)

image_path = Path(__file__).parent.joinpath("super-agents.png")
agent.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
    stream=True,
)
```

---

<a name="models--ollama--demo_phi4py"></a>

### `models/ollama/demo_phi4.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="phi4"), markdown=True)

# Print the response in the terminal
agent.print_response("Tell me a scary story in exactly 10 words.")
```

---

<a name="models--ollama--demo_qwenpy"></a>

### `models/ollama/demo_qwen.py`

```python
from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Ollama(id="qwen3:8b"),
    tools=[
        YFinanceTools(),
    ],
    instructions="Use tables to display data.",
)

agent.print_response("Write a report on NVDA", stream=True, markdown=True)
```

---

<a name="models--ollama--image_agentpy"></a>

### `models/ollama/image_agent.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.2-vision"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("super-agents.png")
agent.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)
```

---

<a name="models--ollama--knowledgepy"></a>

### `models/ollama/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai ollama` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.ollama import Ollama
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=OllamaEmbedder(id="llama3.2", dimensions=3072),
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=Ollama(id="llama3.2"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--ollama--memorypy"></a>

### `models/ollama/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install ollama sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/models/ollama/memory.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.ollama.chat import Ollama

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=Ollama(id="qwen2.5:latest"),
    # Pass the database to the Agent
    db=db,
    # Enable user memories
    enable_user_memories=True,
    # Enable session summaries
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--ollama--ollama_cloudpy"></a>

### `models/ollama/ollama_cloud.py`

```python
"""To use Ollama Cloud, you need to set the OLLAMA_API_KEY environment variable. Host is set to https://ollama.com by default."""

from agno.agent import Agent
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="deepseek-v3.1:671b", host="https://ollama.com"),
)

agent.print_response("How many r's in the word 'strawberry'?", stream=True)
```

---

<a name="models--ollama--set_clientpy"></a>

### `models/ollama/set_client.py`

```python
"""Run `pip install yfinance` to install dependencies."""

from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama
from ollama import Client as OllamaClient

agent = Agent(
    model=Ollama(id="llama3.1:8b", client=OllamaClient()),
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--ollama--set_temperaturepy"></a>

### `models/ollama/set_temperature.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.2", options={"temperature": 0.5}), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--ollama--structured_outputpy"></a>

### `models/ollama/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=Ollama(id="llama3.2"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

# Run the agent
structured_output_agent.print_response("New York")
```

---

<a name="models--ollama--tool_usepy"></a>

### `models/ollama/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--ollama--tool_use_streampy"></a>

### `models/ollama/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--ollama_tools--async_basicpy"></a>

### `models/ollama_tools/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.ollama import OllamaTools

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the cli
asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))
```

---

<a name="models--ollama_tools--async_basic_streampy"></a>

### `models/ollama_tools/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.ollama import OllamaTools

agent = Agent(model=OllamaTools(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--ollama_tools--async_tool_use_streampy"></a>

### `models/ollama_tools/async_tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.ollama import OllamaTools
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--ollama_tools--basicpy"></a>

### `models/ollama_tools/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import OllamaTools

agent = Agent(model=OllamaTools(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--ollama_tools--basic_streampy"></a>

### `models/ollama_tools/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import OllamaTools

agent = Agent(model=OllamaTools(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--ollama_tools--dbpy"></a>

### `models/ollama_tools/db.py`

```python
"""Run `pip install ddgs sqlalchemy ollama` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.ollama import OllamaTools
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--ollama_tools--knowledgepy"></a>

### `models/ollama_tools/knowledge.py`

```python
"""
Run `pip install ddgs sqlalchemy pgvector pypdf openai ollama` to install dependencies.

Run Ollama Server: `ollama serve`
Pull required models:
`ollama pull nomic-embed-text`
`ollama pull llama3.1:8b`

If you haven't deployed database yet, run:
`docker run --rm -it -e POSTGRES_PASSWORD=ai -e POSTGRES_USER=ai -e POSTGRES_DB=ai -p 5532:5432 --name postgres pgvector/pgvector:pg17`
to deploy a PostgreSQL database.

"""

from agno.agent import Agent
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.ollama import OllamaTools
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="ollama_recipes",
        db_url=db_url,
        embedder=OllamaEmbedder(id="nomic-embed-text", dimensions=768),
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=OllamaTools(id="llama3.1:8b"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--ollama_tools--structured_outputpy"></a>

### `models/ollama_tools/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import OllamaTools
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
movie_agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Get the response in a variable
# run: RunOutput = movie_agent.run("New York")
# pprint(run.content)

movie_agent.print_response("New York")
```

---

<a name="models--ollama_tools--tool_usepy"></a>

### `models/ollama_tools/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import OllamaTools
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OllamaTools(id="llama3.2:latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--ollama_tools--tool_use_streampy"></a>

### `models/ollama_tools/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.ollama import OllamaTools
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OllamaTools(id="llama3.1:8b"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--openai--chat--agent_flex_tierpy"></a>

### `models/openai/chat/agent_flex_tier.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="o4-mini", service_tier="flex"),
    markdown=True,
)

agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--openai--chat--async_basicpy"></a>

### `models/openai/chat/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--openai--chat--async_basic_streampy"></a>

### `models/openai/chat/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run(
#     "Share a 2 sentence horror story", stream=True
# )
# for chunk in run_response:
#     print(chunk.content, end="")

# # Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--openai--chat--async_structured_response_streampy"></a>

### `models/openai/chat/async_structured_response_stream.py`

```python
import asyncio
from typing import Dict, List

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)


async def main():
    await structured_output_agent.aprint_response(
        "New York", stream=True, stream_intermediate_steps=True
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="models--openai--chat--async_tool_usepy"></a>

### `models/openai/chat/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--openai--chat--audio_input_agentpy"></a>

### `models/openai/chat/audio_input_agent.py`

```python
import requests
from agno.agent import Agent, RunOutput  # noqa
from agno.media import Audio
from agno.models.openai import OpenAIChat

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

# Provide the agent with the audio file and get result as text
agent = Agent(
    model=OpenAIChat(id="gpt-4o-audio-preview", modalities=["text"]),
    markdown=True,
)
agent.print_response(
    "What is in this audio?", audio=[Audio(content=wav_data, format="wav")], stream=True
)
```

---

<a name="models--openai--chat--audio_input_and_output_multi_turnpy"></a>

### `models/openai/chat/audio_input_and_output_multi_turn.py`

```python
from pathlib import Path

import requests
from agno.agent import Agent, RunOutput  # noqa
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
    ),
    # Set add_history_to_context=true to add the previous chat history to the context sent to the Model.
    add_history_to_context=True,
    # Number of historical responses to add to the messages.
    num_history_runs=3,
)
run_output: RunOutput = agent.run(
    input="What is in this audio?", audio=[Audio(content=wav_data, format="wav")]
)

filename = Path(__file__).parent.joinpath("tmp/conversation_response_1.wav")
filename.unlink(missing_ok=True)
filename.parent.mkdir(parents=True, exist_ok=True)

# Save the response audio to a file
if run_output.response_audio is not None:
    write_audio_to_file(audio=run_output.response_audio.content, filename=str(filename))


run_output: RunOutput = agent.run("Tell me something more about the audio")

filename = Path(__file__).parent.joinpath("tmp/conversation_response_2.wav")
filename.unlink(missing_ok=True)

# Save the response audio to a file
if run_output.response_audio is not None:
    write_audio_to_file(audio=run_output.response_audio.content, filename=str(filename))


run_output: RunOutput = agent.run("Now tell me a 5 second story")

filename = Path(__file__).parent.joinpath("tmp/conversation_response_3.wav")
filename.unlink(missing_ok=True)

# Save the response audio to a file
if run_output.response_audio is not None:
    write_audio_to_file(audio=run_output.response_audio.content, filename=str(filename))
```

---

<a name="models--openai--chat--audio_input_local_file_uploadpy"></a>

### `models/openai/chat/audio_input_local_file_upload.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat

# Provide the agent with the audio file and get result as text
agent = Agent(
    model=OpenAIChat(id="gpt-4o-audio-preview", modalities=["text"]),
    markdown=True,
)

# Please download a sample audio file to test this Agent and upload using:
audio_path = Path(__file__).parent.joinpath("sample.mp3")

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(filepath=audio_path, format="mp3")],
    stream=True,
)
```

---

<a name="models--openai--chat--audio_output_agentpy"></a>

### `models/openai/chat/audio_output_agent.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file
from agno.db.in_memory import InMemoryDb


# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    db=InMemoryDb(),
    add_history_to_context=True,
    markdown=True,
)
run_output: RunOutput = agent.run("Tell me a 5 second scary story")

# Save the response audio to a file
if run_output.response_audio:
    write_audio_to_file(
        audio=run_output.response_audio.content, filename="tmp/scary_story.wav"
    )

run_output: RunOutput = agent.run("What would be in a sequal of this story?")

# Save the response audio to a file
if run_output.response_audio:
    write_audio_to_file(
        audio=run_output.response_audio.content,
        filename="tmp/scary_story_sequal.wav",
    )
```

---

<a name="models--openai--chat--audio_output_streampy"></a>

### `models/openai/chat/audio_output_stream.py`

```python
import base64
import wave
from typing import Iterator

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openai import OpenAIChat

# Audio Configuration
SAMPLE_RATE = 24000  # Hz (24kHz)
CHANNELS = 1  # Mono (Change to 2 if Stereo)
SAMPLE_WIDTH = 2  # Bytes (16 bits)

# Provide the agent with the audio file and audio configuration and get result as text + audio
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={
            "voice": "alloy",
            "format": "pcm16",
        },  # Only pcm16 is supported with streaming
    ),
)
output_stream: Iterator[RunOutputEvent] = agent.run(
    "Tell me a 10 second story", stream=True
)

filename = "tmp/response_stream.wav"

# Open the file once in append-binary mode
with wave.open(str(filename), "wb") as wav_file:
    wav_file.setnchannels(CHANNELS)
    wav_file.setsampwidth(SAMPLE_WIDTH)
    wav_file.setframerate(SAMPLE_RATE)

    # Iterate over generated audio
    for response in output_stream:
        response_audio = response.response_audio  # type: ignore
        if response_audio:
            if response_audio.transcript:
                print(response_audio.transcript, end="", flush=True)
            if response_audio.content:
                try:
                    pcm_bytes = response_audio.content
                    pcm_bytes = base64.b64decode(pcm_bytes)
                    wav_file.writeframes(pcm_bytes)
                except Exception as e:
                    print(f"Error decoding audio: {e}")
print()
print(f"Saved audio to {filename}")
```

---

<a name="models--openai--chat--basicpy"></a>

### `models/openai/chat/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o", temperature=0.5), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--openai--chat--basic_streampy"></a>

### `models/openai/chat/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--openai--chat--custom_role_mappy"></a>

### `models/openai/chat/custom_role_map.py`

```python
"""This example shows how to use a custom role map with the OpenAIChat class.

This is useful when using a custom model that doesn't support the default role map.

To run this example:
- Set the MISTRAL_API_KEY environment variable.
- Run `pip install openai agno` to install dependencies.
"""

from os import getenv

from agno.agent import Agent
from agno.models.openai import OpenAIChat

# Using these Mistral model and url as an example.
model_id = "mistral-medium-2505"
base_url = "https://api.mistral.ai/v1"
api_key = getenv("MISTRAL_API_KEY")
mistral_role_map = {
    "system": "system",
    "user": "user",
    "assistant": "assistant",
    "tool": "tool",
    "model": "assistant",
}

# When initializing the model, we pass our custom role map.
model = OpenAIChat(
    id=model_id,
    base_url=base_url,
    api_key=api_key,
    role_map=mistral_role_map,
)

agent = Agent(model=model, markdown=True)

# Running the agent with a custom role map.
res = agent.print_response("Hey, how are you doing?")
```

---

<a name="models--openai--chat--dbpy"></a>

### `models/openai/chat/db.py`

```python
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--openai--chat--generate_imagespy"></a>

### `models/openai/chat/generate_images.py`

```python
from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.tools.dalle import DalleTools

image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description="You are an AI agent that can generate images using DALL-E.",
    instructions="When the user asks you to create an image, use the `create_image` tool to create the image.",
    markdown=True,
)

image_agent.print_response("Generate an image of a white siamese cat")

# Retrieve and display generated images using get_last_run_output
run_response = image_agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput) and run_response.images:
    for image_response in run_response.images:
        image_url = image_response.url
        print(image_url)
else:
    print("No images found in run response")
```

---

<a name="models--openai--chat--image_agentpy"></a>

### `models/openai/chat/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--openai--chat--image_agent_bytespy"></a>

### `models/openai/chat/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--openai--chat--image_agent_with_memorypy"></a>

### `models/openai/chat/image_agent_with_memory.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    add_history_to_context=True,
    num_history_runs=3,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
)

agent.print_response("Tell me where I can get more images?")
```

---

<a name="models--openai--chat--knowledgepy"></a>

### `models/openai/chat/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=OpenAIChat(id="gpt-4o"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--openai--chat--memorypy"></a>

### `models/openai/chat/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    user_id="test_user",
    session_id="test_session",
    # Pass the database to the Agent
    db=db,
    # Enable user memories
    enable_user_memories=True,
    # Enable session summaries
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--openai--chat--metricspy"></a>

### `models/openai/chat/metrics.py`

```python
from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    markdown=True,
)

run_output: RunOutput = agent.run("What is the stock price of NVDA")
pprint_run_response(run_output, markdown=True)

# Print metrics per message
if run_output.messages:
    for message in run_output.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the metrics
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(run_output.metrics)  # type: ignore
```

---

<a name="models--openai--chat--pdf_input_file_uploadpy"></a>

### `models/openai/chat/pdf_input_file_upload.py`

```python
"""
In this example, we upload a PDF file to Google GenAI directly and then use it as an input to an agent.
"""

from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai import OpenAIChat

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Pass the local PDF file path directly; the client will inline small files or upload large files automatically
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "Suggest me a recipe from the attached file.",
    files=[File(filepath=str(pdf_path))],
)
```

---

<a name="models--openai--chat--pdf_input_localpy"></a>

### `models/openai/chat/pdf_input_local.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai import OpenAIChat
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "What is the recipe for Gaeng Som Phak Ruam? Also what are the health benefits. Refer to the attached file.",
    files=[File(filepath=pdf_path)],
)
```

---

<a name="models--openai--chat--pdf_input_urlpy"></a>

### `models/openai/chat/pdf_input_url.py`

```python
from agno.agent import Agent
from agno.media import File
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "Suggest me a recipe from the attached file.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)
```

---

<a name="models--openai--chat--reasoning_o3_minipy"></a>

### `models/openai/chat/reasoning_o3_mini.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="o3-mini", reasoning_effort="high"),
    tools=[YFinanceTools(enable_all=True)],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Write a report on the NVDA, is it a good buy?", stream=True)
```

---

<a name="models--openai--chat--structured_outputpy"></a>

### `models/openai/chat/structured_output.py`

```python
from typing import Dict, List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")
```

---

<a name="models--openai--chat--structured_output_streampy"></a>

### `models/openai/chat/structured_output_stream.py`

```python
from typing import Dict, List

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

structured_output_agent.print_response(
    "New York", stream=True, stream_intermediate_steps=True
)
```

---

<a name="models--openai--chat--text_to_speech_agentpy"></a>

### `models/openai/chat/text_to_speech_agent.py`

```python
""" Example: Using the OpenAITools Toolkit for Text-to-Speech

This script demonstrates how to use an agent to generate speech from a given text input and optionally save it to a specified audio file.

Run `pip install openai agno` to install the necessary dependencies.
"""

import base64
from pathlib import Path

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.openai import OpenAITools
from agno.utils.media import save_base64_data

output_file: str = str(Path("tmp/speech_output.mp3"))

agent: Agent = Agent(
    model=Gemini(id="gemini-2.5-pro"),
    tools=[OpenAITools(enable_speech_generation=True)],
    markdown=True,
)

# Ask the agent to generate speech, but not save it
response = agent.run(
    'Please generate speech for the following text: "Hello from Agno! This is a demonstration of the text-to-speech capability using OpenAI"'
)

print(f"Agent response: {response.get_content_as_string()}")

if response.audio:
    base64_audio = base64.b64encode(response.audio[0].content).decode("utf-8")
    save_base64_data(base64_audio, output_file)
    print(f"Successfully saved generated speech to{output_file}")
```

---

<a name="models--openai--chat--tool_usepy"></a>

### `models/openai/chat/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--openai--chat--tool_use_streampy"></a>

### `models/openai/chat/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--openai--chat--verbosity_controlpy"></a>

### `models/openai/chat/verbosity_control.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-5", verbosity="high"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data.",
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="models--openai--responses--agent_flex_tierpy"></a>

### `models/openai/responses/agent_flex_tier.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIResponses

agent = Agent(
    model=OpenAIResponses(id="o4-mini", service_tier="flex"),
    markdown=True,
)

agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--openai--responses--async_basicpy"></a>

### `models/openai/responses/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--openai--responses--async_basic_streampy"></a>

### `models/openai/responses/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--openai--responses--async_tool_usepy"></a>

### `models/openai/responses/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--openai--responses--basicpy"></a>

### `models/openai/responses/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--openai--responses--basic_streampy"></a>

### `models/openai/responses/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--openai--responses--dbpy"></a>

### `models/openai/responses/db.py`

```python
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--openai--responses--deep_research_agentpy"></a>

### `models/openai/responses/deep_research_agent.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIResponses

agent = Agent(
    model=OpenAIResponses(id="o4-mini-deep-research", max_tool_calls=1),
    instructions=dedent("""
        You are an expert research analyst with access to advanced research tools.

        When you are given a schema to use, pass it to the research tool as output_schema parameter to research tool.

        The research tool has two parameters:
        - instructions (str): The research topic/question
        - output_schema (dict, optional): A JSON schema for structured output
    """),
)

agent.print_response(
    """Research the economic impact of semaglutide on global healthcare systems.
    Do:
    - Include specific figures, trends, statistics, and measurable outcomes.
    - Prioritize reliable, up-to-date sources: peer-reviewed research, health
      organizations (e.g., WHO, CDC), regulatory agencies, or pharmaceutical
      earnings reports.
    - Include inline citations and return all source metadata.

    Be analytical, avoid generalities, and ensure that each section supports
    data-backed reasoning that could inform healthcare policy or financial modeling."""
)
```

---

<a name="models--openai--responses--image_agentpy"></a>

### `models/openai/responses/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIResponses
from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--openai--responses--image_agent_bytespy"></a>

### `models/openai/responses/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIResponses
from agno.tools.googlesearch import GoogleSearchTools
from agno.utils.media import download_image

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--openai--responses--image_agent_with_memorypy"></a>

### `models/openai/responses/image_agent_with_memory.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIResponses
from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    markdown=True,
    add_history_to_context=True,
    num_history_runs=3,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
)

agent.print_response("Tell me where I can get more images?")
```

---

<a name="models--openai--responses--image_generation_agentpy"></a>

### `models/openai/responses/image_generation_agent.py`

```python
""" Example: Using the OpenAITools Toolkit for Image Generation

This script demonstrates how to use the `OpenAITools` toolkit, which includes a tool for generating images using OpenAI's DALL-E within an Agno Agent.

Example prompts to try:
- "Create a surreal painting of a floating city in the clouds at sunset"
- "Generate a photorealistic image of a cozy coffee shop interior"
- "Design a cute cartoon mascot for a tech startup"
- "Create an artistic portrait of a cyberpunk samurai"

Run `pip install openai agno` to install the necessary dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.openai import OpenAITools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    markdown=True,
)

response = agent.run(
    "Generate a photorealistic image of a cozy coffee shop interior",
)

if response.images and response.images[0].content:
    save_base64_data(str(response.images[0].content), "tmp/coffee_shop.png")
```

---

<a name="models--openai--responses--knowledgepy"></a>

### `models/openai/responses/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIResponses
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=OpenAIResponses(id="gpt-4o"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--openai--responses--memorypy"></a>

### `models/openai/responses/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIResponses
from rich.pretty import pprint

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    user_id="test_user",
    session_id="test_session",
    # Pass the database to the Agent
    db=db,
    # Enable user memories
    enable_user_memories=True,
    # Enable session summaries
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--openai--responses--pdf_input_localpy"></a>

### `models/openai/responses/pdf_input_local.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai.responses import OpenAIResponses
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

# Download the file using the download_file function
download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=OpenAIResponses(id="gpt-4o-mini"),
    tools=[{"type": "file_search"}],
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[File(filepath=pdf_path)],
)
agent.print_response("Suggest me a recipe from the attached file.")
```

---

<a name="models--openai--responses--pdf_input_urlpy"></a>

### `models/openai/responses/pdf_input_url.py`

```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.media import File
from agno.models.openai.responses import OpenAIResponses

# Setup the database for the Agent Session to be stored
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIResponses(id="gpt-4o-mini"),
    db=db,
    tools=[{"type": "file_search"}, {"type": "web_search_preview"}],
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file and search the web for more information.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)

# Get the stored Agent session, to check the response citations
session = agent.get_session()
if session and session.runs and session.runs[-1].citations:
    print("Citations:")
    print(session.runs[-1].citations)
```

---

<a name="models--openai--responses--reasoning_o3_minipy"></a>

### `models/openai/responses/reasoning_o3_mini.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIResponses(id="o3-mini"),
    tools=[YFinanceTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("Write a report on the NVDA, is it a good buy?", stream=True)
```

---

<a name="models--openai--responses--structured_outputpy"></a>

### `models/openai/responses/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIResponses
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")
```

---

<a name="models--openai--responses--tool_usepy"></a>

### `models/openai/responses/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--openai--responses--tool_use_gpt_5py"></a>

### `models/openai/responses/tool_use_gpt_5.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIResponses(id="gpt-5"),
    tools=[YFinanceTools(cache_results=True)],
    markdown=True,
    telemetry=False,
)

agent.print_response("What is the current price of TSLA?", stream=True)
```

---

<a name="models--openai--responses--tool_use_o3py"></a>

### `models/openai/responses/tool_use_o3.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIResponses(id="o3"),
    tools=[YFinanceTools(cache_results=True)],
    markdown=True,
    telemetry=False,
)

agent.print_response("What is the current price of TSLA?", stream=True)
```

---

<a name="models--openai--responses--tool_use_streampy"></a>

### `models/openai/responses/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--openai--responses--verbosity_controlpy"></a>

### `models/openai/responses/verbosity_control.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-5", verbosity="high"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data.",
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="models--openai--responses--websearch_builtin_toolpy"></a>

### `models/openai/responses/websearch_builtin_tool.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.file import FileTools

agent = Agent(
    model=OpenAIResponses(id="gpt-4o"),
    tools=[{"type": "web_search_preview"}, FileTools()],
    instructions="Save the results to a file with a relevant name.",
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--openai--responses--zdr_reasoning_agentpy"></a>

### `models/openai/responses/zdr_reasoning_agent.py`

```python
"""
An example of using OpenAI Responses with reasoning features and ZDR mode enabled.

Read more about ZDR mode here: https://openai.com/enterprise-privacy/.
"""

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIResponses

agent = Agent(
    name="ZDR Compliant Agent",
    session_id="zdr_demo_session",
    model=OpenAIResponses(
        id="o4-mini",
        store=False,
        reasoning_summary="auto",  # Requesting a reasoning summary
    ),
    instructions="You are a helpful AI assistant operating in Zero Data Retention mode for maximum privacy and compliance.",
    db=InMemoryDb(),
    add_history_to_context=True,
    stream=True,
)

agent.print_response("What's the largest country in Europe by area?")
agent.print_response("What's the population of that country?")
agent.print_response("What's the population density per square kilometer?")
```

---

<a name="models--openrouter--async_basicpy"></a>

### `models/openrouter/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--openrouter--async_basic_streampy"></a>

### `models/openrouter/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--openrouter--async_tool_usepy"></a>

### `models/openrouter/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.openrouter import OpenRouter
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenRouter(id="openai/gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--openrouter--basicpy"></a>

### `models/openrouter/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--openrouter--basic_streampy"></a>

### `models/openrouter/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openrouter import OpenRouter

agent = Agent(model=OpenRouter(id="gpt-4o"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--openrouter--structured_outputpy"></a>

### `models/openrouter/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openrouter import OpenRouter
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenRouter(id="gpt-4o"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenRouter(id="gpt-4o-2024-08-06"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)


# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
structured_output_agent.print_response("New York")
```

---

<a name="models--openrouter--tool_usepy"></a>

### `models/openrouter/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openrouter import OpenRouter
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenRouter(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--perplexity--async_basicpy"></a>

### `models/perplexity/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--perplexity--async_basic_streampy"></a>

### `models/perplexity/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--perplexity--basicpy"></a>

### `models/perplexity/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar-pro"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--perplexity--basic_streampy"></a>

### `models/perplexity/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--perplexity--knowledgepy"></a>

### `models/perplexity/knowledge.py`

```python
"""Run `pip install ddgs sqlalchemy pgvector pypdf openai google.generativeai` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.perplexity import Perplexity
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes",
        db_url=db_url,
        embedder=OpenAIEmbedder(),
    ),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=Perplexity(id="sonar-pro"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--perplexity--memorypy"></a>

### `models/perplexity/memory.py`

```python
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.perplexity import Perplexity
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Perplexity(id="sonar-pro"),
    # Store the memories and summary in a database
    db=PostgresDb(db_url=db_url),
    enable_user_memories=True,
    enable_session_summaries=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--perplexity--structured_outputpy"></a>

### `models/perplexity/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.perplexity import Perplexity
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=Perplexity(id="sonar-pro"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    markdown=True,
)

# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
```

---

<a name="models--perplexity--web_searchpy"></a>

### `models/perplexity/web_search.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar-pro"), markdown=True)

# Print the response in the terminal
agent.print_response("Show me top 2 news stories from USA?")

# Get the response in a variable
# run: RunOutput = agent.run("What is happening in the world today?")
# print(run.content)
```

---

<a name="models--portkey--async_basicpy"></a>

### `models/portkey/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey

agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    markdown=True,
)

# Print the response in the terminal
asyncio.run(
    agent.aprint_response("What is Portkey and why would I use it as an AI gateway?")
)
```

---

<a name="models--portkey--async_basic_streampy"></a>

### `models/portkey/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey

agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    description="You help people with their health and fitness goals.",
    instructions=["Recipes should be under 5 ingredients"],
)
# -*- Print a response to the terminal
asyncio.run(
    agent.aprint_response("Share a breakfast recipe.", markdown=True, stream=True)
)
```

---

<a name="models--portkey--async_tool_usepy"></a>

### `models/portkey/async_tool_use.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
asyncio.run(agent.aprint_response("What are the latest developments in AI gateways?"))
```

---

<a name="models--portkey--async_tool_use_streampy"></a>

### `models/portkey/async_tool_use_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
asyncio.run(
    agent.aprint_response(
        "What are the latest developments in AI gateways?", stream=True
    )
)
```

---

<a name="models--portkey--basicpy"></a>

### `models/portkey/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.portkey import Portkey

# Create model using Portkey
model = Portkey(
    id="@first-integrati-707071/gpt-5-nano",
)

agent = Agent(model=model, markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("What is Portkey and why would I use it as an AI gateway?")
# print(run.content)

# Print the response in the terminal
agent.print_response("What is Portkey and why would I use it as an AI gateway?")
```

---

<a name="models--portkey--basic_streampy"></a>

### `models/portkey/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.portkey import Portkey

agent = Agent(
    model=Portkey(
        id="@first-integrati-707071/gpt-5-nano",
    ),
    markdown=True,
)

# Print the response in the terminal
agent.print_response(
    "What is Portkey and why would I use it as an AI gateway?", stream=True
)
```

---

<a name="models--portkey--structured_outputpy"></a>

### `models/portkey/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.portkey import Portkey
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    output_schema=MovieScript,
    markdown=True,
)

# Get the response in a variable
# run: RunOutput = agent.run("New York")
# print(run.content)

agent.print_response("New York")
```

---

<a name="models--portkey--tool_usepy"></a>

### `models/portkey/tool_use.py`

```python
from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("What are the latest developments in AI gateways?")
```

---

<a name="models--portkey--tool_use_streampy"></a>

### `models/portkey/tool_use_stream.py`

```python
from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Print the response in the terminal
agent.print_response("What are the latest developments in AI gateways?", stream=True)
```

---

<a name="models--sambanova--async_basicpy"></a>

### `models/sambanova/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--sambanova--async_basic_streampy"></a>

### `models/sambanova/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--sambanova--basicpy"></a>

### `models/sambanova/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--sambanova--basic_streampy"></a>

### `models/sambanova/basic_stream.py`

```python
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.sambanova import Sambanova

agent = Agent(model=Sambanova(id="Meta-Llama-3.1-8B-Instruct"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--siliconflow--async_basic_streamingpy"></a>

### `models/siliconflow/async_basic_streaming.py`

```python
"""
Basic streaming async example using siliconflow.
"""

import asyncio

from agno.agent import Agent
from agno.models.siliconflow import Siliconflow

agent = Agent(
    model=Siliconflow(id="openai/gpt-oss-120b"),
    markdown=True,
)

asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--siliconflow--basicpy"></a>

### `models/siliconflow/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.siliconflow import Siliconflow

agent = Agent(model=Siliconflow(id="openai/gpt-oss-120b"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Explain quantum computing in simple terms")
# print(run.content)

# Print the response in the terminal
agent.print_response("Explain quantum computing in simple terms")
```

---

<a name="models--siliconflow--basic_streampy"></a>

### `models/siliconflow/basic_stream.py`

```python
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.siliconflow import Siliconflow

agent = Agent(model=Siliconflow(id="openai/gpt-oss-120b"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Explain quantum computing in simple terms", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Explain quantum computing in simple terms", stream=True)
```

---

<a name="models--siliconflow--structured_outputpy"></a>

### `models/siliconflow/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.siliconflow import Siliconflow
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


json_mode_agent = Agent(
    model=Siliconflow(id="openai/gpt-oss-120b"),
    description="You help people write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
json_mode_response: RunOutput = json_mode_agent.run("New York")
pprint(json_mode_response.content)

# json_mode_agent.print_response("New York")
```

---

<a name="models--siliconflow--tool_usepy"></a>

### `models/siliconflow/tool_use.py`

```python
"""Run `pip install duckduckgo-search` to install dependencies."""

from agno.agent import Agent
from agno.models.siliconflow import Siliconflow
from agno.tools.duckduckgo import DuckDuckGoTools

"""
The current version of the siliconflow-chat model's Function Calling capability is stable and supports tool integration effectively.
"""

agent = Agent(
    model=Siliconflow(id="openai/gpt-oss-120b"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True,
    debug_mode=True,
)

agent.print_response("What happing in America?")
```

---

<a name="models--together--async_basicpy"></a>

### `models/together/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--together--async_basic_streampy"></a>

### `models/together/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--together--async_tool_usepy"></a>

### `models/together/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.together import Together
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--together--basicpy"></a>

### `models/together/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--together--basic_streampy"></a>

### `models/together/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"), markdown=True
)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--together--image_agentpy"></a>

### `models/together/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Llama-Vision-Free"),
    markdown=True,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--together--image_agent_bytespy"></a>

### `models/together/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Llama-Vision-Free"),
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--together--image_agent_with_memorypy"></a>

### `models/together/image_agent_with_memory.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.together import Together

agent = Agent(
    model=Together(id="meta-llama/Llama-Vision-Free"),
    markdown=True,
    add_history_to_context=True,
    num_history_runs=3,
)

agent.print_response(
    "Tell me about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

agent.print_response("Tell me where I can get more images?")
```

---

<a name="models--together--structured_outputpy"></a>

### `models/together/structured_output.py`

```python
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.together import Together
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that uses JSON mode
json_mode_agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    use_json_mode=True,
)

# Get the response in a variable
# json_mode_response: RunOutput = json_mode_agent.run("New York")
# pprint(json_mode_response.content)
# structured_output_response: RunOutput = structured_output_agent.run("New York")
# pprint(structured_output_response.content)

json_mode_agent.print_response("New York")
```

---

<a name="models--together--tool_usepy"></a>

### `models/together/tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.together import Together
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--together--tool_use_streampy"></a>

### `models/together/tool_use_stream.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.together import Together
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--vercel--async_basicpy"></a>

### `models/vercel/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.vercel import V0

agent = Agent(model=V0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--vercel--async_basic_streampy"></a>

### `models/vercel/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutput  # noqa
from agno.models.vercel import V0

agent = Agent(model=V0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--vercel--async_tool_usepy"></a>

### `models/vercel/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.vercel import V0
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=V0(id="v0-1.0-md"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--vercel--basicpy"></a>

### `models/vercel/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.vercel import V0

agent = Agent(model=V0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")

# agent.print_response("Create a simple web app that displays a random number between 1 and 100.")
```

---

<a name="models--vercel--basic_streampy"></a>

### `models/vercel/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.vercel import V0

agent = Agent(model=V0(id="v0-1.0-md"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--vercel--image_agentpy"></a>

### `models/vercel/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.vercel import V0
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=V0(id="v0-1.0-md"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--vercel--knowledgepy"></a>

### `models/vercel/knowledge.py`

```python
"""Run `pip install ddgs` to install dependencies."""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.vercel import V0
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)
# Add content to the knowledge
knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=V0(id="v0-1.0-md"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
```

---

<a name="models--vercel--tool_usepy"></a>

### `models/vercel/tool_use.py`

```python
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.vercel import V0
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=V0(id="v0-1.0-md"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--vllm--async_basicpy"></a>

### `models/vllm/async_basic.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.vllm import VLLM

agent = Agent(model=VLLM(id="Qwen/Qwen2.5-7B-Instruct"), markdown=True)
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--vllm--async_basic_streampy"></a>

### `models/vllm/async_basic_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.vllm import VLLM

agent = Agent(model=VLLM(id="Qwen/Qwen2.5-7B-Instruct"), markdown=True)
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--vllm--async_tool_usepy"></a>

### `models/vllm/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.vllm import VLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=VLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--vllm--basicpy"></a>

### `models/vllm/basic.py`

```python
from agno.agent import Agent
from agno.models.vllm import VLLM

agent = Agent(
    model=VLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    markdown=True,
)

agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--vllm--basic_streampy"></a>

### `models/vllm/basic_stream.py`

```python
from agno.agent import Agent
from agno.models.vllm import VLLM

agent = Agent(
    model=VLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    markdown=True,
)
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--vllm--code_generationpy"></a>

### `models/vllm/code_generation.py`

```python
"""Code generation example with DeepSeek-Coder.
Run vLLM model: vllm serve deepseek-ai/deepseek-coder-6.7b-instruct \
        --dtype float32 \
        --tool-call-parser pythonic
"""

from agno.agent import Agent
from agno.models.vllm import VLLM

agent = Agent(
    model=VLLM(id="deepseek-ai/deepseek-coder-6.7b-instruct"),
    description="You are an expert Python developer.",
    markdown=True,
)

agent.print_response(
    "Write a Python function that returns the nth Fibonacci number using dynamic programming."
)
```

---

<a name="models--vllm--dbpy"></a>

### `models/vllm/db.py`

```python
"""Run `pip install sqlalchemy` and ensure Postgres is running (`./cookbook/scripts/run_pgvector.sh`)."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.vllm import VLLM
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=VLLM(id="Qwen/Qwen2.5-7B-Instruct"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
```

---

<a name="models--vllm--memorypy"></a>

### `models/vllm/memory.py`

```python
"""
Personalized memory and session summaries with vLLM.
Prerequisites:
1. Start a Postgres + pgvector container (helper script is provided):
       ./cookbook/scripts/run_pgvector.sh
2. Install dependencies:
       pip install sqlalchemy 'psycopg[binary]' pgvector
3. Run a vLLM server (any open model).  Example with Phi-3:
       vllm serve microsoft/Phi-3-mini-128k-instruct \
         --dtype float32 \
         --enable-auto-tool-choice \
         --tool-call-parser pythonic
Then execute this script  it will remember facts you tell it and generate a
summary.
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.vllm import VLLM
from agno.utils.pprint import pprint

# Change this if your Postgres container is running elsewhere
DB_URL = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=VLLM(id="microsoft/Phi-3-mini-128k-instruct"),
    db=PostgresDb(db_url=DB_URL),
    enable_user_memories=True,
    enable_session_summaries=True,
)

# -*- Share personal information
agent.print_response("My name is john billings?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I live in nyc?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# -*- Share personal information
agent.print_response("I'm going to a concert tomorrow?", stream=True)
# -*- Print memories and summary
if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

# Ask about the conversation
agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)
```

---

<a name="models--vllm--structured_outputpy"></a>

### `models/vllm/structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.vllm import VLLM
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


agent = Agent(
    model=VLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

agent.print_response("Llamas ruling the world")
```

---

<a name="models--vllm--tool_usepy"></a>

### `models/vllm/tool_use.py`

```python
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.vllm import VLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=VLLM(
        id="NousResearch/Nous-Hermes-2-Mistral-7B-DPO", top_k=20, enable_thinking=False
    ),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--vllm--tool_use_streampy"></a>

### `models/vllm/tool_use_stream.py`

```python
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.vllm import VLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=VLLM(
        id="NousResearch/Nous-Hermes-2-Mistral-7B-DPO", top_k=20, enable_thinking=False
    ),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="models--xai--async_basicpy"></a>

### `models/xai/async_basic.py`

```python
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story"))
```

---

<a name="models--xai--async_basic_streampy"></a>

### `models/xai/async_basic_stream.py`

```python
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
asyncio.run(agent.aprint_response("Share a 2 sentence horror story", stream=True))
```

---

<a name="models--xai--async_tool_usepy"></a>

### `models/xai/async_tool_use.py`

```python
"""Run `pip install ddgs` to install dependencies."""

import asyncio

from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
asyncio.run(agent.aprint_response("Whats happening in France?", stream=True))
```

---

<a name="models--xai--basicpy"></a>

### `models/xai/basic.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

---

<a name="models--xai--basic_streampy"></a>

### `models/xai/basic_stream.py`

```python
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

# Get the response in a variable
# run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)
# for chunk in run_response:
#     print(chunk.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

<a name="models--xai--finance_agentpy"></a>

### `models/xai/finance_agent.py`

```python
""" Finance Agent - Your Personal Market Analyst!

This example shows how to create a sophisticated financial analyst that provides
comprehensive market insights using real-time data. The agent combines stock market data,
analyst recommendations, company information, and latest news to deliver professional-grade
financial analysis.

Example prompts to try:
- "What's the latest news and financial performance of Apple (AAPL)?"
- "Give me a detailed analysis of Tesla's (TSLA) current market position"
- "How are Microsoft's (MSFT) financials looking? Include analyst recommendations"
- "Analyze NVIDIA's (NVDA) stock performance and future outlook"
- "What's the market saying about Amazon's (AMZN) latest quarter?"

Run: `pip install openai yfinance agno` to install the dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.yfinance import YFinanceTools

finance_agent = Agent(
    model=xAI(id="grok-3-mini-beta"),
    tools=[YFinanceTools()],
    instructions=dedent("""\
        You are a seasoned Wall Street analyst with deep expertise in market analysis! 

        Follow these steps for comprehensive financial analysis:
        1. Market Overview
           - Latest stock price
           - 52-week high and low
        2. Financial Deep Dive
           - Key metrics (P/E, Market Cap, EPS)
        3. Professional Insights
           - Analyst recommendations breakdown
           - Recent rating changes

        4. Market Context
           - Industry trends and positioning
           - Competitive analysis
           - Market sentiment indicators

        Your reporting style:
        - Begin with an executive summary
        - Use tables for data presentation
        - Include clear section headers
        - Add emoji indicators for trends ( )
        - Highlight key insights with bullet points
        - Compare metrics to industry averages
        - Include technical term explanations
        - End with a forward-looking analysis

        Risk Disclosure:
        - Always highlight potential risk factors
        - Note market uncertainties
        - Mention relevant regulatory concerns
    """),
    add_datetime_to_context=True,
    markdown=True,
)

# Example usage with detailed market analysis request
finance_agent.print_response(
    "Write a comprehensive report on TSLA",
    stream=True,
    stream_intermediate_steps=True,
)

# # Semiconductor market analysis example
# finance_agent.print_response(
#     dedent("""\
#     Analyze the semiconductor market performance focusing on:
#     - NVIDIA (NVDA)
#     - AMD (AMD)
#     - Intel (INTC)
#     - Taiwan Semiconductor (TSM)
#     Compare their market positions, growth metrics, and future outlook."""),
#     stream=True,
# )

# # Automotive market analysis example
# finance_agent.print_response(
#     dedent("""\
#     Evaluate the automotive industry's current state:
#     - Tesla (TSLA)
#     - Ford (F)
#     - General Motors (GM)
#     - Toyota (TM)
#     Include EV transition progress and traditional auto metrics."""),
#     stream=True,
# )

# More example prompts to explore:
"""
Advanced analysis queries:
1. "Compare Tesla's valuation metrics with traditional automakers"
2. "Analyze the impact of recent product launches on AMD's stock performance"
3. "How do Meta's financial metrics compare to its social media peers?"
4. "Evaluate Netflix's subscriber growth impact on financial metrics"
5. "Break down Amazon's revenue streams and segment performance"

Industry-specific analyses:
Semiconductor Market:
1. "How is the chip shortage affecting TSMC's market position?"
2. "Compare NVIDIA's AI chip revenue growth with competitors"
3. "Analyze Intel's foundry strategy impact on stock performance"
4. "Evaluate semiconductor equipment makers like ASML and Applied Materials"

Automotive Industry:
1. "Compare EV manufacturers' production metrics and margins"
2. "Analyze traditional automakers' EV transition progress"
3. "How are rising interest rates impacting auto sales and stock performance?"
4. "Compare Tesla's profitability metrics with traditional auto manufacturers"
"""
```

---

<a name="models--xai--image_agentpy"></a>

### `models/xai/image_agent.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2-vision-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

---

<a name="models--xai--image_agent_bytespy"></a>

### `models/xai/image_agent_bytes.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=xAI(id="grok-2-vision-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

# Read the image file content as bytes
image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)
```

---

<a name="models--xai--image_agent_with_memorypy"></a>

### `models/xai/image_agent_with_memory.py`

```python
from agno.agent import Agent
from agno.media import Image
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2-vision-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    add_history_to_context=True,
    num_history_runs=3,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
)

agent.print_response("Tell me where I can get more images?")
```

---

<a name="models--xai--live_search_agentpy"></a>

### `models/xai/live_search_agent.py`

```python
from agno.agent import Agent
from agno.models.xai.xai import xAI

agent = Agent(
    model=xAI(
        id="grok-3",
        search_parameters={
            "mode": "on",
            "max_search_results": 20,
            "return_citations": True,
        },
    ),
    markdown=True,
)
agent.print_response("Provide me a digest of world news in the last 24 hours.")
```

---

<a name="models--xai--live_search_agent_streampy"></a>

### `models/xai/live_search_agent_stream.py`

```python
from agno.agent import Agent
from agno.models.xai.xai import xAI

agent = Agent(
    model=xAI(
        id="grok-3",
        search_parameters={
            "mode": "on",
            "max_search_results": 20,
            "return_citations": True,
        },
    ),
    markdown=True,
)
agent.print_response(
    "Provide me a digest of world news in the last 24 hours.", stream=True
)
```

---

<a name="models--xai--reasoning_agentpy"></a>

### `models/xai/reasoning_agent.py`

```python
from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=xAI(id="grok-3-beta"),
    tools=[
        ReasoningTools(add_instructions=True, add_few_shot=True),
        YFinanceTools(),
    ],
    instructions=[
        "Use tables to display data",
        "Only output the report, no other text",
    ],
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report on TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="models--xai--structured_outputpy"></a>

### `models/xai/structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.xai.xai import xAI
from agno.run.agent import RunOutput
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


# Agent that returns a structured output
structured_output_agent = Agent(
    model=xAI(id="grok-2-latest"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

# Run the agent synchronously
structured_output_response: RunOutput = structured_output_agent.run(
    "Llamas ruling the world"
)
pprint(structured_output_response.content)
```

---

<a name="models--xai--tool_usepy"></a>

### `models/xai/tool_use.py`

```python
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?")
```

---

<a name="models--xai--tool_use_streampy"></a>

### `models/xai/tool_use_stream.py`

```python
"""Build a Web Search Agent using xAI."""

from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=xAI(id="grok-2"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)
agent.print_response("Whats happening in France?", stream=True)
```

---

<a name="reasoning--agents--analyse_treaty_of_versaillespy"></a>

### `reasoning/agents/analyse_treaty_of_versailles.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Analyze the key factors that led to the signing of the Treaty of Versailles in 1919. "
    "Discuss the political, economic, and social impacts of the treaty on Germany and how it "
    "contributed to the onset of World War II. Provide a nuanced assessment that includes "
    "multiple historical perspectives."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--capture_reasoning_content_default_cotpy"></a>

### `reasoning/agents/capture_reasoning_content_default_COT.py`

```python
"""
Cookbook: Working with reasoning_content in Agents

This example demonstrates how to access and print the reasoning_content
when using either reasoning=True or setting a specific reasoning_model.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat

print("\n=== Example 1: Using reasoning=True (default COT) ===\n")

# Create agent with reasoning=True (default model COT)
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

# Run the agent (non-streaming)
print("Running with reasoning=True (non-streaming)...")
response = agent.run("What is the sum of the first 10 natural numbers?")

# Print the reasoning_content
print("\n--- reasoning_content from response ---")
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print(" reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in non-streaming response")


print("\n\n=== Example 2: Using a custom reasoning_model ===\n")

# Create agent with a specific reasoning_model
agent_with_reasoning_model = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(id="gpt-4o"),  # Should default to manual COT
    markdown=True,
)

# Run the agent (non-streaming)
print("Running with reasoning_model specified (non-streaming)...")
response = agent_with_reasoning_model.run(
    "What is the sum of the first 10 natural numbers?"
)

# Print the reasoning_content
print("\n--- reasoning_content from response ---")
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print(" reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in non-streaming response")


print("\n\n=== Example 3: Processing stream with reasoning=True ===\n")

# Create a fresh agent for streaming
streaming_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

# Process streaming responses and look for the final RunOutput
print("Running with reasoning=True (streaming)...")
final_response = None
for event in streaming_agent.run(
    "What is the value of 5! (factorial)?",
    stream=True,
    stream_intermediate_steps=True,
):
    # Print content as it streams (optional)
    if hasattr(event, "content") and event.content:
        print(event.content, end="", flush=True)

    # The final event in the stream should be a RunOutput object
    if hasattr(event, "reasoning_content"):
        final_response = event

print("\n\n--- reasoning_content from final stream event ---")
if (
    final_response
    and hasattr(final_response, "reasoning_content")
    and final_response.reasoning_content
):
    print(" reasoning_content FOUND in final stream event")
    print(f"   Length: {len(final_response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (streaming) ===")
    preview = final_response.reasoning_content[:1000]
    if len(final_response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in final stream event")


print("\n\n=== Example 4: Processing stream with reasoning_model ===\n")

# Create a fresh agent with reasoning_model for streaming
streaming_agent_with_model = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(id="gpt-4o"),
    markdown=True,
)

# Process streaming responses and look for the final RunOutput
print("Running with reasoning_model specified (streaming)...")
final_response_with_model = None
for event in streaming_agent_with_model.run(
    "What is the value of 7! (factorial)?",
    stream=True,
    stream_intermediate_steps=True,
):
    # Print content as it streams (optional)
    if hasattr(event, "content") and event.content:
        print(event.content, end="", flush=True)

    # The final event in the stream should be a RunOutput object
    if hasattr(event, "reasoning_content"):
        final_response_with_model = event

print("\n\n--- reasoning_content from final stream event (reasoning_model) ---")
if (
    final_response_with_model
    and hasattr(final_response_with_model, "reasoning_content")
    and final_response_with_model.reasoning_content
):
    print(" reasoning_content FOUND in final stream event")
    print(f"   Length: {len(final_response_with_model.reasoning_content)} characters")
    print("\n=== reasoning_content preview (streaming with reasoning_model) ===")
    preview = final_response_with_model.reasoning_content[:1000]
    if len(final_response_with_model.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in final stream event")
```

---

<a name="reasoning--agents--cerebras_llama_default_cotpy"></a>

### `reasoning/agents/cerebras_llama_default_COT.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.cerebras import Cerebras

"""
This example demonstrates how it works when you pass a non-reasoning model as a reasoning model.
It defaults to using the default OpenAI reasoning model.
We recommend using the appropriate reasoning model or passing reasoning=True for the default COT.
"""

reasoning_agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    reasoning=True,
    debug_mode=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)
```

---

<a name="reasoning--agents--default_chain_of_thoughtpy"></a>

### `reasoning/agents/default_chain_of_thought.py`

```python
"""
This example demonstrates how it works when you pass a non-reasoning model as a reasoning model.
It defaults to using the default OpenAI reasoning model.
We recommend using the appropriate reasoning model or passing reasoning=True for the default COT.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=OpenAIChat(
        id="gpt-4o", max_tokens=1200
    ),  # Should default to manual COT because it is not a native reasoning model
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)


# It uses the default model of the Agent
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o", max_tokens=1200),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)
```

---

<a name="reasoning--agents--fibonaccipy"></a>

### `reasoning/agents/fibonacci.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Give me steps to write a python script for fibonacci series"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--finance_agentpy"></a>

### `reasoning/agents/finance_agent.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data",
    use_json_mode=True,
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA", stream=True, show_full_reasoning=True
)
```

---

<a name="reasoning--agents--ibm_watsonx_default_cotpy"></a>

### `reasoning/agents/ibm_watsonx_default_COT.py`

```python
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ibm import WatsonX

"""
This example demonstrates how it works when you pass a non-reasoning model as a reasoning model.
It defaults to using the default OpenAI reasoning model.
We recommend using the appropriate reasoning model or passing reasoning=True for the default COT.
"""

reasoning_agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-3-70b-instruct"),
    reasoning=True,
    debug_mode=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)
```

---

<a name="reasoning--agents--is_9_11_bigger_than_9_9py"></a>

### `reasoning/agents/is_9_11_bigger_than_9_9.py`

```python
from agno.agent import Agent
from agno.cli.console import console
from agno.models.openai import OpenAIChat

task = "9.11 and 9.9 -- which is bigger?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

console.rule("[bold green]Regular Agent[/bold green]")
regular_agent.print_response(task, stream=True)
console.rule("[bold yellow]Reasoning Agent[/bold yellow]")
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--life_in_500000_yearspy"></a>

### `reasoning/agents/life_in_500000_years.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Write a short story about life in 500000 years"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--logical_puzzlepy"></a>

### `reasoning/agents/logical_puzzle.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--mathematical_proofpy"></a>

### `reasoning/agents/mathematical_proof.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Prove that for any positive integer n, the sum of the first n odd numbers is equal to n squared. Provide a detailed proof."

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--mistral_reasoning_cotpy"></a>

### `reasoning/agents/mistral_reasoning_cot.py`

```python
from agno.agent import Agent
from agno.models.mistral import MistralChat

# It uses the default model of the Agent
reasoning_agent = Agent(
    model=MistralChat(id="mistral-large-latest"),
    reasoning=True,
    markdown=True,
    use_json_mode=True,
)
reasoning_agent.print_response(
    "Give me steps to write a python script for fibonacci series",
    stream=True,
    show_full_reasoning=True,
)
```

---

<a name="reasoning--agents--plan_itenerarypy"></a>

### `reasoning/agents/plan_itenerary.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Plan an itinerary from Los Angeles to Las Vegas"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--python_101_curriculumpy"></a>

### `reasoning/agents/python_101_curriculum.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = "Craft a curriculum for Python 101"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--scientific_researchpy"></a>

### `reasoning/agents/scientific_research.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Read the following abstract of a scientific paper and provide a critical evaluation of its methodology,"
    "results, conclusions, and any potential biases or flaws:\n\n"
    "Abstract: This study examines the effect of a new teaching method on student performance in mathematics. "
    "A sample of 30 students was selected from a single school and taught using the new method over one semester. "
    "The results showed a 15% increase in test scores compared to the previous semester. "
    "The study concludes that the new teaching method is effective in improving mathematical performance among high school students."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--ship_of_theseuspy"></a>

### `reasoning/agents/ship_of_theseus.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

task = (
    "Discuss the concept of 'The Ship of Theseus' and its implications on the notions of identity and change. "
    "Present arguments for and against the idea that an object that has had all of its components replaced remains "
    "fundamentally the same object. Conclude with your own reasoned position on the matter."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
```

---

<a name="reasoning--agents--strawberrypy"></a>

### `reasoning/agents/strawberry.py`

```python
import asyncio

from agno.agent import Agent
from agno.cli.console import console
from agno.models.openai import OpenAIChat

task = "How many 'r' are in the word 'strawberry'?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)


async def main():
    console.rule("[bold blue]Counting 'r's in 'strawberry'[/bold blue]")

    console.rule("[bold green]Regular Agent[/bold green]")
    await regular_agent.aprint_response(task, stream=True)
    console.rule("[bold yellow]Reasoning Agent[/bold yellow]")
    await reasoning_agent.aprint_response(task, stream=True, show_full_reasoning=True)


asyncio.run(main())
```

---

<a name="reasoning--agents--trolley_problempy"></a>

### `reasoning/agents/trolley_problem.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
    show_full_reasoning=True,
)
```

---

<a name="reasoning--models--azure_ai_foundry--reasoning_model_deepseekpy"></a>

### `reasoning/models/azure_ai_foundry/reasoning_model_deepseek.py`

```python
import os

from agno.agent import Agent
from agno.models.azure import AzureAIFoundry

agent = Agent(
    model=AzureAIFoundry(id="gpt-4o"),
    reasoning=True,
    reasoning_model=AzureAIFoundry(
        id="DeepSeek-R1",
        azure_endpoint=os.getenv("AZURE_ENDPOINT"),
        api_key=os.getenv("AZURE_API_KEY"),
    ),
)

agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--azure_openai--o1py"></a>

### `reasoning/models/azure_openai/o1.py`

```python
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="o1"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--azure_openai--o3_mini_with_toolspy"></a>

### `reasoning/models/azure_openai/o3_mini_with_tools.py`

```python
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=AzureOpenAI(id="o3-mini"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data.",
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="reasoning--models--azure_openai--o4_minipy"></a>

### `reasoning/models/azure_openai/o4_mini.py`

```python
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="o4-mini"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--azure_openai--reasoning_model_gpt_4_1py"></a>

### `reasoning/models/azure_openai/reasoning_model_gpt_4_1.py`

```python
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI

agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"), reasoning_model=AzureOpenAI(id="gpt-4.1")
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--deepseek--9_11_or_9_9py"></a>

### `reasoning/models/deepseek/9_11_or_9_9.py`

```python
from agno.agent import Agent
from agno.cli.console import console
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "9.11 and 9.9 -- which is bigger?"

regular_agent_claude = Agent(model=Claude("claude-3-5-sonnet-20241022"))
reasoning_agent_claude = Agent(
    model=Claude("claude-3-5-sonnet-20241022"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
)

regular_agent_openai = Agent(model=OpenAIChat(id="gpt-4o"))
reasoning_agent_openai = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
)

console.rule("[bold blue]Regular Claude Agent[/bold blue]")
regular_agent_claude.print_response(task, stream=True)

console.rule("[bold green]Claude Reasoning Agent[/bold green]")
reasoning_agent_claude.print_response(task, stream=True)

console.rule("[bold red]Regular OpenAI Agent[/bold red]")
regular_agent_openai.print_response(task, stream=True)

console.rule("[bold yellow]OpenAI Reasoning Agent[/bold yellow]")
reasoning_agent_openai.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--analyse_treaty_of_versaillespy"></a>

### `reasoning/models/deepseek/analyse_treaty_of_versailles.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Analyze the key factors that led to the signing of the Treaty of Versailles in 1919. "
    "Discuss the political, economic, and social impacts of the treaty on Germany and how it "
    "contributed to the onset of World War II. Provide a nuanced assessment that includes "
    "multiple historical perspectives."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--ethical_dilemmapy"></a>

### `reasoning/models/deepseek/ethical_dilemma.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "You are a train conductor faced with an emergency: the brakes have failed, and the train is heading towards "
    "five people tied on the track. You can divert the train onto another track, but there is one person tied there. "
    "Do you divert the train, sacrificing one to save five? Provide a well-reasoned answer considering utilitarian "
    "and deontological ethical frameworks. "
    "Provide your answer also as an ascii art diagram."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--fibonaccipy"></a>

### `reasoning/models/deepseek/fibonacci.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Give me steps to write a python script for fibonacci series"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--finance_agentpy"></a>

### `reasoning/models/deepseek/finance_agent.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions=["Use tables where possible"],
    markdown=True,
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
)
reasoning_agent.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="reasoning--models--deepseek--life_in_500000_yearspy"></a>

### `reasoning/models/deepseek/life_in_500000_years.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Write a short story about life in 500000 years"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--logical_puzzlepy"></a>

### `reasoning/models/deepseek/logical_puzzle.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Three missionaries and three cannibals need to cross a river. "
    "They have a boat that can carry up to two people at a time. "
    "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
    "How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram"
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--mathematical_proofpy"></a>

### `reasoning/models/deepseek/mathematical_proof.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Prove that for any positive integer n, the sum of the first n odd numbers is equal to n squared. Provide a detailed proof."

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--plan_itenerarypy"></a>

### `reasoning/models/deepseek/plan_itenerary.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Plan an itinerary from Los Angeles to Las Vegas"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--python_101_curriculumpy"></a>

### `reasoning/models/deepseek/python_101_curriculum.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "Craft a curriculum for Python 101"

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--scientific_researchpy"></a>

### `reasoning/models/deepseek/scientific_research.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Read the following abstract of a scientific paper and provide a critical evaluation of its methodology,"
    "results, conclusions, and any potential biases or flaws:\n\n"
    "Abstract: This study examines the effect of a new teaching method on student performance in mathematics. "
    "A sample of 30 students was selected from a single school and taught using the new method over one semester. "
    "The results showed a 15% increase in test scores compared to the previous semester. "
    "The study concludes that the new teaching method is effective in improving mathematical performance among high school students."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--ship_of_theseuspy"></a>

### `reasoning/models/deepseek/ship_of_theseus.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "Discuss the concept of 'The Ship of Theseus' and its implications on the notions of identity and change. "
    "Present arguments for and against the idea that an object that has had all of its components replaced remains "
    "fundamentally the same object. Conclude with your own reasoned position on the matter."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--deepseek--strawberrypy"></a>

### `reasoning/models/deepseek/strawberry.py`

```python
import asyncio

from agno.agent import Agent
from agno.cli.console import console
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = "How many 'r' are in the word 'strawberry'?"

regular_agent = Agent(model=OpenAIChat(id="gpt-4o"), markdown=True)
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)


async def main():
    console.rule("[bold blue]Counting 'r's in 'strawberry'[/bold blue]")

    console.rule("[bold green]Regular Agent[/bold green]")
    await regular_agent.aprint_response(task, stream=True)
    console.rule("[bold yellow]Reasoning Agent[/bold yellow]")
    await reasoning_agent.aprint_response(task, stream=True)


asyncio.run(main())
```

---

<a name="reasoning--models--deepseek--trolley_problempy"></a>

### `reasoning/models/deepseek/trolley_problem.py`

```python
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat

task = (
    "You are a philosopher tasked with analyzing the classic 'Trolley Problem'. In this scenario, a runaway trolley "
    "is barreling down the tracks towards five people who are tied up and unable to move. You are standing next to "
    "a large stranger on a footbridge above the tracks. The only way to save the five people is to push this stranger "
    "off the bridge onto the tracks below. This will kill the stranger, but save the five people on the tracks. "
    "Should you push the stranger to save the five people? Provide a well-reasoned answer considering utilitarian, "
    "deontological, and virtue ethics frameworks. "
    "Include a simple ASCII art diagram to illustrate the scenario."
)

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning_model=DeepSeek(id="deepseek-reasoner"),
    markdown=True,
)
reasoning_agent.print_response(task, stream=True)
```

---

<a name="reasoning--models--groq--9_11_or_9_9py"></a>

### `reasoning/models/groq/9_11_or_9_9.py`

```python
from agno.agent import Agent
from agno.models.groq import Groq

agent = Agent(
    model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
    markdown=True,
)
agent.print_response("9.11 and 9.9 -- which is bigger?", stream=True)
```

---

<a name="reasoning--models--groq--deepseek_plus_claudepy"></a>

### `reasoning/models/groq/deepseek_plus_claude.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.groq import Groq

deepseek_plus_claude = Agent(
    model=Claude(id="claude-3-7-sonnet-20250219"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)
deepseek_plus_claude.print_response("9.11 and 9.9 -- which is bigger?", stream=True)
```

---

<a name="reasoning--models--ollama--reasoning_model_deepseekpy"></a>

### `reasoning/models/ollama/reasoning_model_deepseek.py`

```python
from agno.agent import Agent
from agno.models.ollama.chat import Ollama

agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    reasoning_model=Ollama(id="deepseek-r1:14b", max_tokens=4096),
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--openai--o1_propy"></a>

### `reasoning/models/openai/o1_pro.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="o1-pro"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--openai--o3_minipy"></a>

### `reasoning/models/openai/o3_mini.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--models--openai--o3_mini_with_toolspy"></a>

### `reasoning/models/openai/o3_mini_with_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools(search=True)],
    instructions="Use tables to display data.",
    markdown=True,
)

agent.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="reasoning--models--openai--o4_minipy"></a>

### `reasoning/models/openai/o4_mini.py`

```python
from agno.agent import Agent
from agno.models.openai.responses import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="o4-mini"))
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--openai--reasoning_effortpy"></a>

### `reasoning/models/openai/reasoning_effort.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="o3-mini", reasoning_effort="high"),
    tools=[DuckDuckGoTools(search=True)],
    instructions="Use tables to display data.",
    markdown=True,
)

agent.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="reasoning--models--openai--reasoning_model_gpt_4_1py"></a>

### `reasoning/models/openai/reasoning_model_gpt_4_1.py`

```python
from agno.agent import Agent
from agno.models.openai.responses import OpenAIResponses

agent = Agent(
    model=OpenAIResponses(id="gpt-4o-mini"),
    reasoning_model=OpenAIResponses(id="gpt-4.1"),
)
agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. "
    "Include an ASCII diagram of your solution.",
    stream=True,
)
```

---

<a name="reasoning--models--openai--reasoning_summarypy"></a>

### `reasoning/models/openai/reasoning_summary.py`

```python
"""This example shows how to get reasoning summaries with our OpenAIResponses model.
Useful for contexts where a long reasoning process is relevant and directly relevant to the user."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

# Setup the reasoning Agent
agent = Agent(
    model=OpenAIResponses(
        id="o4-mini",
        reasoning_summary="auto",  # Requesting a reasoning summary
    ),
    tools=[DuckDuckGoTools(enable_search=True)],
    instructions="Use tables to display the analysis",
    markdown=True,
)

agent.print_response(
    "Write a brief report comparing NVDA to TSLA",
    stream=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--models--xai--reasoning_effortpy"></a>

### `reasoning/models/xai/reasoning_effort.py`

```python
from agno.agent import Agent
from agno.models.xai import xAI
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=xAI(id="grok-3-mini-fast", reasoning_effort="high"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data.",
    markdown=True,
)
agent.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="reasoning--teams--finance_team_chain_of_thoughtpy"></a>

### `reasoning/teams/finance_team_chain_of_thought.py`

```python
import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_context=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools(search=True)],
    instructions=[
        "You are a financial data specialist. Provide concise and accurate data.",
        "Use tables to display stock prices, fundamentals (P/E, Market Cap), and recommendations.",
        "Clearly state the company name and ticker symbol.",
        "Briefly summarize recent company-specific news if available.",
        "Focus on delivering the requested financial data points clearly.",
    ],
    add_datetime_to_context=True,
)

team_leader = Team(
    name="Reasoning Finance Team Leader",
    members=[
        web_agent,
        finance_agent,
    ],
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    reasoning=True,
    show_members_responses=True,
)


async def run_team(task: str):
    await team_leader.aprint_response(
        task,
        stream=True,
        stream_intermediate_steps=True,
        show_full_reasoning=True,
    )


if __name__ == "__main__":
    asyncio.run(
        run_team(
            dedent("""\
    Analyze the impact of recent US tariffs on market performance across these key sectors:
    - Steel & Aluminum: (X, NUE, AA)
    - Technology Hardware: (AAPL, DELL, HPQ)
    - Agricultural Products: (ADM, BG, INGR)
    - Automotive: (F, GM, TSLA)

    For each sector:
    1. Compare stock performance before and after tariff implementation
    2. Identify supply chain disruptions and cost impact percentages
    3. Analyze companies' strategic responses (reshoring, price adjustments, supplier diversification)
    4. Assess analyst outlook changes directly attributed to tariff policies
    """)
        )
    )
```

---

<a name="reasoning--teams--knowledge_tool_teampy"></a>

### `reasoning/teams/knowledge_tool_team.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.knowledge import KnowledgeTools
from agno.vectordb.lancedb import LanceDb, SearchType

agno_docs = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
    ),
)
# Add content to the knowledge
agno_docs.add_content(url="https://www.paulgraham.com/read.html")

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    enable_think=True,
    enable_search=True,
    enable_analyze=True,
    add_few_shot=True,
)

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_context=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools(search=True)],
    add_datetime_to_context=True,
)

team_leader = Team(
    name="Reasoning Finance Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[knowledge_tools],
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    show_members_responses=True,
    add_datetime_to_context=True,
)


def run_team(task: str):
    team_leader.print_response(
        task,
        stream=True,
        stream_intermediate_steps=True,
        show_full_reasoning=True,
    )


if __name__ == "__main__":
    run_team("What does Paul Graham talk about the need to read in this essay?")
```

---

<a name="reasoning--teams--reasoning_finance_teampy"></a>

### `reasoning/teams/reasoning_finance_team.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    add_datetime_to_context=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Handle financial data requests",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools(search=True)],
    instructions=[
        "You are a financial data specialist. Provide concise and accurate data.",
        "Use tables to display stock prices, fundamentals (P/E, Market Cap), and recommendations.",
        "Clearly state the company name and ticker symbol.",
        "Briefly summarize recent company-specific news if available.",
        "Focus on delivering the requested financial data points clearly.",
    ],
    add_datetime_to_context=True,
)

team_leader = Team(
    name="Reasoning Finance Team Leader",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        web_agent,
        finance_agent,
    ],
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Only output the final answer, no other text.",
        "Use tables to display data",
    ],
    markdown=True,
    show_members_responses=True,
    add_datetime_to_context=True,
)


def run_team(task: str):
    team_leader.print_response(
        task,
        stream=True,
        stream_intermediate_steps=True,
        show_full_reasoning=True,
    )


if __name__ == "__main__":
    run_team(
        dedent("""\
    Analyze the impact of recent US tariffs on market performance across these key sectors:
    - Steel & Aluminum: (X, NUE, AA)
    - Technology Hardware: (AAPL, DELL, HPQ)
    - Agricultural Products: (ADM, BG, INGR)
    - Automotive: (F, GM, TSLA)

    For each sector:
    1. Compare stock performance before and after tariff implementation
    2. Identify supply chain disruptions and cost impact percentages
    3. Analyze companies' strategic responses (reshoring, price adjustments, supplier diversification)
    4. Assess analyst outlook changes directly attributed to tariff policies
    """)
    )

    # run_team(dedent("""\
    # Assess the impact of recent semiconductor export controls on:
    # - US chip designers (Nvidia, AMD, Intel)
    # - Asian manufacturers (TSMC, Samsung)
    # - Equipment makers (ASML, Applied Materials)
    # Include effects on R&D investments, supply chain restructuring, and market share shifts."""))

    # run_team(dedent("""\
    # Compare the retail sector's response to consumer goods tariffs:
    # - Major retailers (Walmart, Target, Amazon)
    # - Consumer brands (Nike, Apple, Hasbro)
    # - Discount retailers (Dollar General, Five Below)
    # Include pricing strategy changes, inventory management, and consumer behavior impacts."""))

    # run_team(dedent("""\
    # Analyze the semiconductor market performance focusing on:
    # - NVIDIA (NVDA)
    # - AMD (AMD)
    # - Intel (INTC)
    # - Taiwan Semiconductor (TSM)
    # Compare their market positions, growth metrics, and future outlook."""))

    # run_team(dedent("""\
    # Evaluate the automotive industry's current state:
    # - Tesla (TSLA)
    # - Ford (F)
    # - General Motors (GM)
    # - Toyota (TM)
    # Include EV transition progress and traditional auto metrics."""))

    # run_team(dedent("""\
    # Compare the financial metrics of Apple (AAPL) and Google (GOOGL):
    # - Market Cap
    # - P/E Ratio
    # - Revenue Growth
    # - Profit Margin"""))

    # run_team(dedent("""\
    # Analyze the impact of recent Chinese solar panel tariffs on:
    # - US solar manufacturers (First Solar, SunPower)
    # - Chinese exporters (JinkoSolar, Trina Solar)
    # - US installation companies (Sunrun, SunPower)
    # Include effects on pricing, supply chains, and installation rates."""))
```

---

<a name="reasoning--tools--azure_openai_reasoning_toolspy"></a>

### `reasoning/tools/azure_openai_reasoning_tools.py`

```python
from agno.agent import Agent
from agno.models.azure.openai_chat import AzureOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=AzureOpenAI(id="gpt-4o-mini"),
    tools=[
        DuckDuckGoTools(),
        ReasoningTools(
            enable_think=True,
            enable_analyze=True,
            add_instructions=True,
            add_few_shot=True,
        ),
    ],
    instructions="Use tables where possible. Think about the problem step by step.",
    markdown=True,
)

reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA.",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--tools--capture_reasoning_content_knowledge_toolspy"></a>

### `reasoning/tools/capture_reasoning_content_knowledge_tools.py`

```python
"""
Cookbook: Capturing reasoning_content with KnowledgeTools

This example demonstrates how to access and print the reasoning_content
when using KnowledgeTools with URL knowledge, in both streaming and non-streaming modes.
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.tools.knowledge import KnowledgeTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge containing information from a URL
print("Setting up URL knowledge...")
agno_docs = Knowledge(
    # Use LanceDB as the vector database
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="cookbook_knowledge_tools",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Add content to the knowledge
asyncio.run(agno_docs.add_content_async(url="https://www.paulgraham.com/read.html"))
print("Knowledge ready.")


print("\n=== Example 1: Using KnowledgeTools in non-streaming mode ===\n")

# Create agent with KnowledgeTools
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        KnowledgeTools(
            knowledge=agno_docs,
            enable_think=True,
            enable_search=True,
            enable_analyze=True,
            add_instructions=True,
        )
    ],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 
        Use the knowledge tools to organize your thoughts, search for information, 
        and analyze results step-by-step.
        \
    """),
    markdown=True,
)

# Run the agent (non-streaming) using agent.run() to get the response
print("Running with KnowledgeTools (non-streaming)...")
response = agent.run(
    "What does Paul Graham explain here with respect to need to read?", stream=False
)

# Check reasoning_content from the response
print("\n--- reasoning_content from response ---")
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print(" reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in non-streaming response")


print("\n\n=== Example 2: Using KnowledgeTools in streaming mode ===\n")

# Create a fresh agent for streaming
streaming_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        KnowledgeTools(
            knowledge=agno_docs,
            enable_think=True,
            enable_search=True,
            enable_analyze=True,
            add_instructions=True,
        )
    ],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 
        Use the knowledge tools to organize your thoughts, search for information, 
        and analyze results step-by-step.
        \
    """),
    markdown=True,
)

# Process streaming responses and look for the final RunOutput
print("Running with KnowledgeTools (streaming)...")
final_response = None
for event in streaming_agent.run(
    "What does Paul Graham explain here with respect to need to read?",
    stream=True,
    stream_intermediate_steps=True,
):
    # Print content as it streams (optional)
    if hasattr(event, "content") and event.content:
        print(event.content, end="", flush=True)

    # The final event in the stream should be a RunOutput object
    if hasattr(event, "reasoning_content"):
        final_response = event

print("\n\n--- reasoning_content from final stream event ---")
if (
    final_response
    and hasattr(final_response, "reasoning_content")
    and final_response.reasoning_content
):
    print(" reasoning_content FOUND in final stream event")
    print(f"   Length: {len(final_response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (streaming) ===")
    preview = final_response.reasoning_content[:1000]
    if len(final_response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in final stream event")
```

---

<a name="reasoning--tools--capture_reasoning_content_reasoning_toolspy"></a>

### `reasoning/tools/capture_reasoning_content_reasoning_tools.py`

```python
"""Test for reasoning_content generation

This script tests whether reasoning_content is properly populated in the RunOutput
when using ReasoningTools. It tests both streaming and non-streaming modes.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.reasoning import ReasoningTools

"""Test function to verify reasoning_content is populated in RunOutput."""
print("\n=== Testing reasoning_content generation ===\n")

# Create an agent with ReasoningTools
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 
        Use step-by-step reasoning to solve the problem.
        \
    """),
)

# Test 1: Non-streaming mode
print("Running with stream=False...")
response = agent.run("What is the sum of the first 10 natural numbers?", stream=False)

# Check reasoning_content
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print(" reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in non-streaming response")

# Process streaming responses to find the final one
print("\n\n=== Test 2: Processing stream to find final response ===\n")

# Create another fresh agent
streaming_agent_alt = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 
        Use step-by-step reasoning to solve the problem.
        \
    """),
)

# Process streaming responses and look for the final RunOutput
final_response = None
for event in streaming_agent_alt.run(
    "What is the value of 3! (factorial)?",
    stream=True,
    stream_intermediate_steps=True,
):
    # The final event in the stream should be a RunOutput object
    if hasattr(event, "reasoning_content"):
        final_response = event

print("--- Checking reasoning_content from final stream event ---")
if (
    final_response
    and hasattr(final_response, "reasoning_content")
    and final_response.reasoning_content
):
    print(" reasoning_content FOUND in final stream event")
    print(f"   Length: {len(final_response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (final stream event) ===")
    preview = final_response.reasoning_content[:1000]
    if len(final_response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print(" reasoning_content NOT FOUND in final stream event")
```

---

<a name="reasoning--tools--cerebras_llama_reasoning_toolspy"></a>

### `reasoning/tools/cerebras_llama_reasoning_tools.py`

```python
""" Problem-Solving Reasoning Agent

This example shows how to create an agent that uses the ReasoningTools to solve
complex problems through step-by-step reasoning. The agent breaks down questions,
analyzes intermediate results, and builds structured reasoning paths to arrive at
well-justified conclusions.

Example prompts to try:
- "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river."
- "Is it better to rent or buy a home given current interest rates?"
- "Evaluate the pros and cons of remote work versus office work."
- "How would increasing interest rates affect the housing market?"
- "What's the best strategy for saving for retirement in your 30s?"
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 
        
        Your approach to problems:
        1. First, break down complex questions into component parts
        2. Clearly state your assumptions
        3. Develop a structured reasoning path
        4. Consider multiple perspectives
        5. Evaluate evidence and counter-arguments
        6. Draw well-justified conclusions
        
        When solving problems:
        - Use explicit step-by-step reasoning
        - Identify key variables and constraints
        - Explore alternative scenarios
        - Highlight areas of uncertainty
        - Explain your thought process clearly
        - Consider both short and long-term implications
        - Evaluate trade-offs explicitly
        
        For quantitative problems:
        - Show your calculations
        - Explain the significance of numbers
        - Consider confidence intervals when appropriate
        - Identify source data reliability
        
        For qualitative reasoning:
        - Assess how different factors interact
        - Consider psychological and social dynamics
        - Evaluate practical constraints
        - Address value considerations
        \
    """),
    add_datetime_to_context=True,
    stream_intermediate_steps=True,
    markdown=True,
)

# Example usage with a complex reasoning problem
reasoning_agent.print_response(
    "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river. "
    "The boat is only big enough for the man and one item. If left unattended together, the fox will "
    "eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?",
    stream=True,
)

# # Economic analysis example
# reasoning_agent.print_response(
#     "Is it better to rent or buy a home given current interest rates, inflation, and market trends? "
#     "Consider both financial and lifestyle factors in your analysis.",
#     stream=True
# )

# # Strategic decision-making example
# reasoning_agent.print_response(
#     "A startup has $500,000 in funding and needs to decide between spending it on marketing or "
#     "product development. They want to maximize growth and user acquisition within 12 months. "
#     "What factors should they consider and how should they analyze this decision?",
#     stream=True
# )
```

---

<a name="reasoning--tools--claude_reasoning_toolspy"></a>

### `reasoning/tools/claude_reasoning_tools.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[
        ReasoningTools(add_instructions=True),
        DuckDuckGoTools(enable_search=True),
    ],
    instructions="Use tables to display data.",
    markdown=True,
)

# Semiconductor market analysis example
reasoning_agent.print_response(
    """\
    Analyze the semiconductor market performance focusing on:
    - NVIDIA (NVDA)
    - AMD (AMD)
    - Intel (INTC)
    - Taiwan Semiconductor (TSM)
    Compare their market positions, growth metrics, and future outlook.""",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--tools--gemini_finance_agentpy"></a>

### `reasoning/tools/gemini_finance_agent.py`

```python
# -*- coding: utf-8 -*-
"""gemini_finance_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XYnlyi_LFuvtR0qSHDLOWMRGMmgi60RQ
"""

# ! pip install -U agno

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

thinking_agent = Agent(
    model=Gemini(id="gemini-2.0-flash"),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(),
    ],
    instructions="Use tables where possible",
    markdown=True,
    stream_intermediate_steps=True,
)
thinking_agent.print_response(
    "Write a report comparing NVDA to TSLA in detail", stream=True, show_reasoning=True
)
```

---

<a name="reasoning--tools--gemini_reasoning_toolspy"></a>

### `reasoning/tools/gemini_reasoning_tools.py`

```python
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=Gemini(id="gemini-2.5-pro"),
    tools=[
        ReasoningTools(
            enable_think=True,
            enable_analyze=True,
        ),
        YFinanceTools(),
    ],
    instructions="Use tables where possible",
    stream_intermediate_steps=True,
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA.", show_full_reasoning=True
)
```

---

<a name="reasoning--tools--groq_llama_finance_agentpy"></a>

### `reasoning/tools/groq_llama_finance_agent.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

thinking_llama = Agent(
    model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
    tools=[
        ReasoningTools(),
        DuckDuckGoTools(),
    ],
    instructions=dedent("""\
    ## General Instructions
    - Always start by using the think tool to map out the steps needed to complete the task.
    - After receiving tool results, use the think tool as a scratchpad to validate the results for correctness
    - Before responding to the user, use the think tool to jot down final thoughts and ideas.
    - Present final outputs in well-organized tables whenever possible.

    ## Using the think tool
    At every step, use the think tool as a scratchpad to:
    - Restate the object in your own words to ensure full comprehension.
    - List the  specific rules that apply to the current request
    - Check if all required information is collected and is valid
    - Verify that the planned action completes the task\
    """),
    markdown=True,
)
thinking_llama.print_response("Write a report comparing NVDA to TSLA", stream=True)
```

---

<a name="reasoning--tools--ibm_watsonx_reasoning_toolspy"></a>

### `reasoning/tools/ibm_watsonx_reasoning_tools.py`

```python
from textwrap import dedent

from agno.agent import Agent, RunOutput  # noqa
from agno.models.ibm import WatsonX
from agno.tools.reasoning import ReasoningTools

""" Problem-Solving Reasoning Agent

This example shows how to create an agent that uses the ReasoningTools to solve
complex problems through step-by-step reasoning. The agent breaks down questions,
analyzes intermediate results, and builds structured reasoning paths to arrive at
well-justified conclusions.

Example prompts to try:
- "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river."
- "Is it better to rent or buy a home given current interest rates?"
- "Evaluate the pros and cons of remote work versus office work."
- "How would increasing interest rates affect the housing market?"
- "What's the best strategy for saving for retirement in your 30s?"
"""


reasoning_agent = Agent(
    model=WatsonX(id="meta-llama/llama-3-3-70b-instruct"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 
        
        Your approach to problems:
        1. First, break down complex questions into component parts
        2. Clearly state your assumptions
        3. Develop a structured reasoning path
        4. Consider multiple perspectives
        5. Evaluate evidence and counter-arguments
        6. Draw well-justified conclusions
        
        When solving problems:
        - Use explicit step-by-step reasoning
        - Identify key variables and constraints
        - Explore alternative scenarios
        - Highlight areas of uncertainty
        - Explain your thought process clearly
        - Consider both short and long-term implications
        - Evaluate trade-offs explicitly
        
        For quantitative problems:
        - Show your calculations
        - Explain the significance of numbers
        - Consider confidence intervals when appropriate
        - Identify source data reliability
        
        For qualitative reasoning:
        - Assess how different factors interact
        - Consider psychological and social dynamics
        - Evaluate practical constraints
        - Address value considerations
        \
    """),
    add_datetime_to_context=True,
    stream_intermediate_steps=True,
    markdown=True,
)

# Example usage with a complex reasoning problem
reasoning_agent.print_response(
    "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river. "
    "The boat is only big enough for the man and one item. If left unattended together, the fox will "
    "eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?",
    stream=True,
)

# # Economic analysis example
# reasoning_agent.print_response(
#     "Is it better to rent or buy a home given current interest rates, inflation, and market trends? "
#     "Consider both financial and lifestyle factors in your analysis.",
#     stream=True
# )

# # Strategic decision-making example
# reasoning_agent.print_response(
#     "A startup has $500,000 in funding and needs to decide between spending it on marketing or "
#     "product development. They want to maximize growth and user acquisition within 12 months. "
#     "What factors should they consider and how should they analyze this decision?",
#     stream=True
# )
```

---

<a name="reasoning--tools--knowledge_toolspy"></a>

### `reasoning/tools/knowledge_tools.py`

```python
"""
1. Run: `pip install openai agno lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY
3. Run: `python cookbook/reasoning/tools/knowledge_tools.py` to run the agent
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.tools.knowledge import KnowledgeTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge containing information from a URL
agno_docs = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Add content to the knowledge
agno_docs.add_content(url="https://docs.agno.com/llms-full.txt")

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    enable_think=True,
    enable_search=True,
    enable_analyze=True,
    add_few_shot=True,
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[knowledge_tools],
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response(
        "How do I build a team of agents in agno?",
        markdown=True,
        stream=True,
    )
```

---

<a name="reasoning--tools--llama_reasoning_toolspy"></a>

### `reasoning/tools/llama_reasoning_tools.py`

```python
from agno.agent import Agent
from agno.models.meta import Llama
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

reasoning_agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[
        ReasoningTools(
            enable_think=True,
            enable_analyze=True,
            add_instructions=True,
        ),
        YFinanceTools(),
    ],
    instructions="Use tables where possible",
    markdown=True,
)
reasoning_agent.print_response(
    "What is the NVDA stock price? Write me a report",
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--tools--memory_toolspy"></a>

### `reasoning/tools/memory_tools.py`

```python
"""
1. Run: `pip install openai agno lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY
3. Run: `python cookbook/reasoning/tools/knowledge_tools.py` to run the agent
"""

import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.memory import MemoryTools

db = SqliteDb(db_file="tmp/memory.db")

john_doe_id = "john_doe@example.com"

memory_tools = MemoryTools(
    db=db,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[memory_tools, DuckDuckGoTools()],
    instructions=[
        "You are a trip planner bot and you are helping the user plan their trip.",
        "You should use the DuckDuckGoTools to get information about the destination and activities.",
        "You should use the MemoryTools to store information about the user for future reference.",
    ],
    markdown=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response(
            "My name is John Doe and I like to hike in the mountains on weekends. "
            "I like to travel to new places and experience different cultures. "
            "I am planning to travel to Africa in December. ",
            stream=True,
            user_id=john_doe_id,
        )
    )

    asyncio.run(
        agent.aprint_response(
            "Make me a travel itinerary for my trip, and propose where I should go, how much I should budget, etc.",
            stream=True,
            user_id=john_doe_id,
        )
    )
```

---

<a name="reasoning--tools--ollama_reasoning_toolspy"></a>

### `reasoning/tools/ollama_reasoning_tools.py`

```python
from agno.agent import Agent
from agno.models.ollama.chat import Ollama
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    tools=[
        ReasoningTools(
            enable_think=True,
            enable_analyze=True,
            add_instructions=True,
            add_few_shot=True,
        ),
        DuckDuckGoTools(),
    ],
    instructions="Use tables where possible",
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--tools--openai_reasoning_toolspy"></a>

### `reasoning/tools/openai_reasoning_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ReasoningTools(
            enable_think=True,
            enable_analyze=True,
            add_instructions=True,
            add_few_shot=True,
        ),
        DuckDuckGoTools(),
    ],
    instructions="Use tables where possible",
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report comparing NVDA to TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--tools--reasoning_toolspy"></a>

### `reasoning/tools/reasoning_tools.py`

```python
""" Problem-Solving Reasoning Agent

This example shows how to create an agent that uses the ReasoningTools to solve
complex problems through step-by-step reasoning. The agent breaks down questions,
analyzes intermediate results, and builds structured reasoning paths to arrive at
well-justified conclusions.

Example prompts to try:
- "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river."
- "Is it better to rent or buy a home given current interest rates?"
- "Evaluate the pros and cons of remote work versus office work."
- "How would increasing interest rates affect the housing market?"
- "What's the best strategy for saving for retirement in your 30s?"
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 
        
        Your approach to problems:
        1. First, break down complex questions into component parts
        2. Clearly state your assumptions
        3. Develop a structured reasoning path
        4. Consider multiple perspectives
        5. Evaluate evidence and counter-arguments
        6. Draw well-justified conclusions
        
        When solving problems:
        - Use explicit step-by-step reasoning
        - Identify key variables and constraints
        - Explore alternative scenarios
        - Highlight areas of uncertainty
        - Explain your thought process clearly
        - Consider both short and long-term implications
        - Evaluate trade-offs explicitly
        
        For quantitative problems:
        - Show your calculations
        - Explain the significance of numbers
        - Consider confidence intervals when appropriate
        - Identify source data reliability
        
        For qualitative reasoning:
        - Assess how different factors interact
        - Consider psychological and social dynamics
        - Evaluate practical constraints
        - Address value considerations
        \
    """),
    add_datetime_to_context=True,
    stream_intermediate_steps=True,
    markdown=True,
)

# Example usage with a complex reasoning problem
reasoning_agent.print_response(
    "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river. "
    "The boat is only big enough for the man and one item. If left unattended together, the fox will "
    "eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?",
    stream=True,
)

# # Economic analysis example
# reasoning_agent.print_response(
#     "Is it better to rent or buy a home given current interest rates, inflation, and market trends? "
#     "Consider both financial and lifestyle factors in your analysis.",
#     stream=True
# )

# # Strategic decision-making example
# reasoning_agent.print_response(
#     "A startup has $500,000 in funding and needs to decide between spending it on marketing or "
#     "product development. They want to maximize growth and user acquisition within 12 months. "
#     "What factors should they consider and how should they analyze this decision?",
#     stream=True
# )
```

---

<a name="reasoning--tools--vercel_reasoning_toolspy"></a>

### `reasoning/tools/vercel_reasoning_tools.py`

```python
from agno.agent import Agent
from agno.models.vercel import v0
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=v0(id="v0-1.0-md"),
    tools=[
        ReasoningTools(add_instructions=True, add_few_shot=True),
        DuckDuckGoTools(),
    ],
    instructions=[
        "Use tables to display data",
        "Only output the report, no other text",
    ],
    markdown=True,
)
reasoning_agent.print_response(
    "Write a report on TSLA",
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

---

<a name="reasoning--tools--workflow_toolspy"></a>

### `reasoning/tools/workflow_tools.py`

```python
"""
1. Run: `pip install openai agno lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY
3. Run: `python cookbook/reasoning/tools/knowledge_tools.py` to run the agent
"""

from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.workflow import WorkflowTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

FEW_SHOT_EXAMPLES = dedent("""\
    You can refer to the examples below as guidance for how to use each tool.
    ### Examples
    #### Example: Blog Post Workflow
    User: Please create a blog post on the topic: AI trends in 2024
    Think: The user wants to process customer feedback data. I need to understand what format the data is in and what kind of summary they want. Let me start with a basic workflow run.
    Run: input_data="AI trends in 2024", additional_data={"topic": "AI, AI agents, AI workflows", "style": "The blog post should be written in a style that is easy to understand and follow."}
    Analyze: The workflow ran successfully and generated a basic blog post. However, the format might not be exactly what the user wants. Let me check if the results meet their expectations.
    Final Answer: I've created a blog post on the topic: AI trends in 2024 through the workflow. The blog post shows...
""")


# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


def prepare_input_for_web_search(step_input: StepInput) -> StepOutput:
    title = step_input.input
    topic = step_input.additional_data.get("topic")
    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post with the title: {title}
	<topic>
	{topic}
	</topic>
	Search the web for atleast 10 articles\
	""")
    )


def prepare_input_for_writer(step_input: StepInput) -> StepOutput:
    title = step_input.additional_data.get("title")
    topic = step_input.additional_data.get("topic")
    style = step_input.additional_data.get("style")

    research_team_output = step_input.previous_step_content

    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post with the title: {title}
	<required_style>
	{style}
	</required_style>
	<topic>
	{topic}
	</topic>
	Here is information from the web:
	<research_results>
	{research_team_output}
	<research_results>\
	""")
    )


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )

    workflow_tools = WorkflowTools(
        workflow=content_creation_workflow,
        enable_think=True,
        enable_analyze=True,
        add_few_shot=True,
        few_shot_examples=FEW_SHOT_EXAMPLES,
    )

    agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        tools=[workflow_tools],
        markdown=True,
    )

    agent.print_response(
        "Create a blog post with the following title: AI trends in 2024",
        instructions="When you run the workflow using the `run_workflow` tool, remember to pass `additional_data` as a dictionary of key-value pairs.",
        markdown=True,
        stream=True,
    )
```

---

<a name="scripts--cookbook_runnerpy"></a>

### `scripts/cookbook_runner.py`

```python
import os
import subprocess
import sys

import click
import inquirer

"""
CLI Tool: Cookbook runner

This tool allows users to interactively navigate through directories, select a target directory,
and execute all `.py` files in the selected directory. It also tracks cookbooks that fail to execute
and prompts the user to rerun all failed cookbooks until all succeed or the user decides to exit.

Usage:
    1. Run the tool from the command line:
        python cookbook/scripts/cookbook_runner.py [base_directory]

    2. Navigate through the directory structure using the interactive prompts:
        - Select a directory to drill down or choose the current directory.
        - The default starting directory is the current working directory (".").

    3. The tool runs all `.py` files in the selected directory and logs any that fail.

    4. If any cookbook fails, the tool prompts the user to rerun all failed cookbooks:
        - Select "yes" to rerun all failed cookbooks.
        - Select "no" to exit, and the tool will log remaining failures.

Dependencies:
    - click
    - inquirer

Example:
    $ python cookbook/scripts/cookbook_runner.py cookbook
    Current directory: /cookbook
    > [Select this directory]
    > folder1
    > folder2
    > [Go back]

    Running script1.py...
    Running script2.py...

    --- Error Log ---
    Script: failing_cookbook.py failed to execute.

    Some cookbooks failed. Do you want to rerun all failed cookbooks? [y/N]: y
"""


def select_directory(base_directory):
    while True:
        # Get all subdirectories and files in the current directory
        items = [
            item
            for item in os.listdir(base_directory)
            if os.path.isdir(os.path.join(base_directory, item))
        ]
        items.sort()
        # Add options to select the current directory or go back
        items.insert(0, "[Select this directory]")
        if base_directory != "/":
            items.insert(1, "[Go back]")

        # Prompt the user to select an option
        questions = [
            inquirer.List(
                "selected_item",
                message=f"Current directory: {base_directory}",
                choices=items,
            )
        ]
        answers = inquirer.prompt(questions)

        if not answers or "selected_item" not in answers:
            print("No selection made. Exiting.")
            return None

        selected_item = answers["selected_item"]

        # Handle the user's choice
        if selected_item == "[Select this directory]":
            return base_directory
        elif selected_item == "[Go back]":
            base_directory = os.path.dirname(base_directory)
        else:
            # Drill down into the selected directory
            base_directory = os.path.join(base_directory, selected_item)


def run_python_script(script_path):
    """
    Run a Python script and display its output in real time.
    Pauses execution on failure to allow user intervention.
    """
    print(f"Running {script_path}...\n")
    try:
        with subprocess.Popen(
            ["python", script_path],
            stdout=sys.stdout,
            stderr=sys.stderr,
            text=True,
        ) as process:
            process.wait()

        if process.returncode != 0:
            raise subprocess.CalledProcessError(process.returncode, script_path)

        return True  # Script ran successfully

    except subprocess.CalledProcessError as e:
        print(f"\nError: {script_path} failed with return code {e.returncode}.")
        return False  # Script failed

    except Exception as e:
        print(f"\nUnexpected error while running {script_path}: {e}")
        return False  # Script failed


@click.command()
@click.argument(
    "base_directory",
    type=click.Path(exists=True, file_okay=False, dir_okay=True),
    default=".",
)
def drill_and_run_scripts(base_directory):
    """
    A CLI tool that lets the user drill down into directories and runs all .py files in the selected directory.
    Tracks cookbooks that encounter errors and keeps prompting to rerun until user decides to exit.
    """
    selected_directory = select_directory(base_directory)

    if not selected_directory:
        print("No directory selected. Exiting.")
        return

    print(f"\nRunning .py files in directory: {selected_directory}\n")

    python_files = [
        filename
        for filename in os.listdir(selected_directory)
        if filename.endswith(".py")
        and os.path.isfile(os.path.join(selected_directory, filename))
        and filename not in ["__pycache__", "__init__.py"]
    ]

    if not python_files:
        print("No .py files found in the selected directory.")
        return

    error_log = []

    for py_file in python_files:
        file_path = os.path.join(selected_directory, py_file)
        if not run_python_script(file_path):
            error_log.append(py_file)

    while error_log:
        print("\n--- Error Log ---")
        for py_file in error_log:
            print(f"Cookbook: {py_file} failed to execute.\n")

        # Prompt the user for action
        questions = [
            inquirer.List(
                "action",
                message="Some cookbooks failed. What would you like to do?",
                choices=[
                    "Retry failed scripts",
                    "Pause for manual intervention and retry",
                    "Exit with error log",
                ],
            )
        ]
        answers = inquirer.prompt(questions)

        if answers and answers.get("action") == "Retry failed scripts":
            print("\nRe-running failed cookbooks...\n")
            new_error_log = []
            for py_file in error_log:
                file_path = os.path.join(selected_directory, py_file)
                if not run_python_script(file_path):
                    new_error_log.append(py_file)

            error_log = new_error_log

        elif (
            answers
            and answers.get("action") == "Pause for manual intervention and retry"
        ):
            print(
                "\nPaused for manual intervention. A shell is now open for you to execute commands (e.g., installing packages)."
            )
            print("Type 'exit' or 'Ctrl+D' to return and retry failed cookbooks.\n")

            # Open an interactive shell for the user
            try:
                subprocess.run(["bash"], check=True)  # For Unix-like systems
            except FileNotFoundError:
                try:
                    subprocess.run(["cmd"], check=True, shell=True)  # For Windows
                except Exception as e:
                    print(f"Error opening shell: {e}")
                    print(
                        "Please manually install required packages in a separate terminal."
                    )

            print("\nRe-running failed cookbooks after manual intervention...\n")
            new_error_log = []
            for py_file in error_log:
                file_path = os.path.join(selected_directory, py_file)
                if not run_python_script(file_path):
                    new_error_log.append(py_file)

            error_log = new_error_log

        elif answers and answers.get("action") == "Exit with error log":
            print("\nExiting. Remaining cookbooks that failed:")
            for py_file in error_log:
                print(f" - {py_file}")
            return

    print("\nAll cookbooks executed successfully!")


if __name__ == "__main__":
    drill_and_run_scripts()
```

---

<a name="teams--async--01_async_coordination_teampy"></a>

### `teams/async/01_async_coordination_team.py`

```python
"""
1. Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
2. Run: `python cookbook/teams/async/modes/02_async_coordinate.py` to run the agent

This example demonstrates a coordinated team of AI agents working together to research topics across different platforms.

The team consists of three specialized agents:
1. HackerNews Researcher - Uses HackerNews API to find and analyze relevant HackerNews posts
2. Web Searcher - Uses DuckDuckGo to find and analyze relevant web pages
3. Article Reader - Reads articles from URLs

The team leader coordinates the agents by:
- Giving each agent a specific task
- Providing clear instructions for each agent
- Collecting and summarizing the results from each agent

"""

import asyncio
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.newspaper4k import Newspaper4kTools
from pydantic import BaseModel, Field


class Article(BaseModel):
    title: str = Field(..., description="The title of the article")
    summary: str = Field(..., description="A summary of the article")
    reference_links: List[str] = Field(
        ..., description="A list of reference links to the article"
    )


hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("o3-mini"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

article_reader = Agent(
    name="Article Reader",
    role="Reads articles from URLs.",
    tools=[Newspaper4kTools()],
)


hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("o3"),
    members=[hn_researcher, web_searcher, article_reader],
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the article reader to read the links for the stories to get more information.",
        "Important: you must provide the article reader with the links to read.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    add_member_tools_to_context=False,
    markdown=True,
    show_members_responses=True,
)


async def main():
    """Main async function demonstrating coordinated team mode."""
    await hn_team.aprint_response(
        input="Write an article about the top 2 stories on hackernews"
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="teams--async--02_async_delegate_to_all_memberspy"></a>

### `teams/async/02_async_delegate_to_all_members.py`

```python
"""
This example demonstrates a collaborative team of AI agents working together to research topics across different platforms.

The team consists of two specialized agents:
1. Reddit Researcher - Uses DuckDuckGo to find and analyze relevant Reddit posts
2. HackerNews Researcher - Uses HackerNews API to find and analyze relevant HackerNews posts

The agents work in "collaborate" mode, meaning they:
- Both are given the same task at the same time
- Work towards reaching consensus through discussion
- Are coordinated by a team leader that guides the discussion

The team leader moderates the discussion and determines when consensus is reached.

This setup is useful for:
- Getting diverse perspectives from different online communities
- Cross-referencing information across platforms
- Having agents collaborate to form more comprehensive analysis
- Reaching balanced conclusions through structured discussion

"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant posts on Reddit.
    """),
)

hackernews_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("o3-mini"),
    role="Research a topic on HackerNews.",
    tools=[HackerNewsTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a HackerNews researcher.
    You will be given a topic to research on HackerNews.
    You will need to find the most relevant posts on HackerNews.
    """),
)


agent_team = Team(
    name="Discussion Team",
    model=OpenAIChat("o3-mini"),
    members=[
        reddit_researcher,
        hackernews_researcher,
    ],
    instructions=[
        "You are a discussion master.",
        "You have to stop the discussion when you think the team has reached a consensus.",
    ],
    markdown=True,
    delegate_task_to_all_members=True,
    show_members_responses=True,
)


async def main():
    """Main async function demonstrating collaborative team mode."""
    await agent_team.aprint_response(
        input="Start the discussion on the topic: 'What is the best way to learn to code?'",
        stream=True,
        stream_intermediate_steps=True,
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="teams--async--03_async_respond_directlypy"></a>

### `teams/async/03_async_respond_directly.py`

```python
"""
This example demonstrates a route team of AI agents working together to answer questions in different languages.

The team consists of six specialized agents:
1. English Agent - Can only answer in English
2. Japanese Agent - Can only answer in Japanese
3. Chinese Agent - Can only answer in Chinese
4. Spanish Agent - Can only answer in Spanish
5. French Agent - Can only answer in French
6. German Agent - Can only answer in German

The team leader routes the user's question to the appropriate language agent. It can only forward the question and cannot answer itself.
"""

import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.deepseek import DeepSeek
from agno.models.openai import OpenAIChat
from agno.team.team import Team

english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="o3-mini"),
)
japanese_agent = Agent(
    name="Japanese Agent",
    role="You only answer in Japanese",
    model=DeepSeek(id="deepseek-chat"),
)
chinese_agent = Agent(
    name="Chinese Agent",
    role="You only answer in Chinese",
    model=DeepSeek(id="deepseek-chat"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You can only answer in Spanish",
    model=OpenAIChat(id="o3-mini"),
)
french_agent = Agent(
    name="French Agent",
    role="You can only answer in French",
    model=OpenAIChat(id="o3-mini"),
)
german_agent = Agent(
    name="German Agent",
    role="You can only answer in German",
    model=Claude("claude-3-5-sonnet-20241022"),
)

multi_language_team = Team(
    name="Multi Language Team",
    model=OpenAIChat("o3-mini"),
    respond_directly=True,
    members=[
        english_agent,
        spanish_agent,
        japanese_agent,
        french_agent,
        german_agent,
        chinese_agent,
    ],
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English, Spanish, Japanese, French and German. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    show_members_responses=True,
    debug_mode=True,
)


async def main():
    """Main async function demonstrating team routing mode."""
    # Ask "How are you?" in all supported languages
    # await multi_language_team.aprint_response(
    #     "How are you?", stream=True  # English
    # )

    # await multi_language_team.aprint_response(
    #     "", stream=True  # Chinese
    # )

    # await multi_language_team.aprint_response(
    #     "?", stream=True  # Japanese
    # )

    await multi_language_team.aprint_response("Comment allez-vous?", stream=True)

    # await multi_language_team.aprint_response(
    #     "Wie geht es Ihnen?", stream=True  # German
    # )

    # await multi_language_team.aprint_response(
    #     "Come stai?", stream=True  # Italian
    # )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="teams--basic--few_shot_learningpy"></a>

### `teams/basic/few_shot_learning.py`

```python
"""
This example shows a straightforward use case of additional_input
to teach a customer support team proper response patterns.
"""

from agno.agent import Agent
from agno.models.message import Message
from agno.models.openai.chat import OpenAIChat
from agno.team import Team

# Few-shot examples showing good support patterns
support_examples = [
    # Example 1: Simple issue resolution
    Message(role="user", content="I forgot my password and can't log in"),
    Message(
        role="assistant",
        content="""
    I'll help you reset your password right away.
    
    **Transferring to Support Specialist**: Please assist this customer with password reset:
    - Guide them through the password reset process
    - Verify their identity if needed  
    - Ensure they can successfully log in
    - Provide tips for creating a secure password
    
    Expected outcome: Customer successfully logs in with new password.
    """.strip(),
    ),
    # Example 2: Escalation pattern
    Message(
        role="user",
        content="I've been charged twice for the same order and your chat bot couldn't help!",
    ),
    Message(
        role="assistant",
        content="""
        I sincerely apologize for the billing issue and the frustration with our chat system.

        **Transferring to Escalation Manager**: Priority billing issue requiring immediate attention:
        - Customer charged twice for same order
        - Previous automated support was unsuccessful
        - Customer is understandably frustrated
        - Needs immediate resolution and possible compensation

        Expected outcome: Billing corrected, customer satisfaction restored, and follow-up to prevent recurrence.
        """.strip(),
    ),
    # Example 3: Proactive support
    Message(role="user", content="Your app keeps crashing when I try to upload photos"),
    Message(
        role="assistant",
        content="""
    I'm sorry you're experiencing crashes with photo uploads. Let me get this resolved for you. 

    **Transferring to Support Specialist**: Technical issue with photo upload feature:
    - App crashes during photo upload process
    - Need to troubleshoot device compatibility and app version
    - Provide workaround solutions if available
    - Escalate to technical team if it's a known bug    

    Expected outcome: Upload feature working properly or clear timeline for fix provided.
        """.strip(),
    ),
]

if __name__ == "__main__":
    # Support Agent
    support_agent = Agent(
        name="Support Specialist",
        role="Handle customer inquiries",
        model=OpenAIChat(id="o3-mini"),
        instructions=[
            "You are a helpful customer support specialist.",
            "Always be polite, professional, and solution-oriented.",
        ],
    )

    # Escalation Agent
    escalation_agent = Agent(
        name="Escalation Manager",
        role="Handle complex issues",
        model=OpenAIChat(id="o3-mini"),
        instructions=[
            "You handle escalated customer issues that require management attention.",
            "Focus on customer satisfaction and finding solutions.",
        ],
    )

    # Create team with few-shot learning
    team = Team(
        name="Customer Support Team",
        members=[support_agent, escalation_agent],
        model=OpenAIChat(id="o3-mini"),
        add_name_to_context=True,
        additional_input=support_examples,  #  Teaching examples
        instructions=[
            "You coordinate customer support with excellence and empathy.",
            "Follow established patterns for proper issue resolution.",
            "Always prioritize customer satisfaction and clear communication.",
        ],
        markdown=True,
    )

    scenarios = [
        "I can't find my order confirmation email",
        "The product I received is damaged",
        "I want to cancel my subscription but the website won't let me",
    ]

    for i, scenario in enumerate(scenarios, 1):
        print(f" Scenario {i}: {scenario}")
        print("-" * 50)
        team.print_response(scenario)
```

---

<a name="teams--basic--input_as_dictpy"></a>

### `teams/basic/input_as_dict.py`

```python
from agno.agent import Agent
from agno.team import Team

# Create a research team
team = Team(
    members=[
        Agent(
            name="Sarah",
            role="Data Researcher",
            instructions="Focus on gathering and analyzing data",
        ),
        Agent(
            name="Mike",
            role="Technical Writer",
            instructions="Create clear, concise summaries",
        ),
    ],
    stream=True,
    markdown=True,
)

team.print_response(
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                },
            },
        ],
    },
    stream=True,
    markdown=True,
)
```

---

<a name="teams--basic--input_as_listpy"></a>

### `teams/basic/input_as_list.py`

```python
from agno.agent import Agent
from agno.team import Team

team = Team(
    members=[
        Agent(
            name="Sarah",
            role="Data Researcher",
            instructions="Focus on gathering and analyzing data",
        ),
        Agent(
            name="Mike",
            role="Technical Writer",
            instructions="Create clear, concise summaries",
        ),
    ],
)

team.print_response(
    [
        {"type": "text", "text": "What's in this image?"},
        {
            "type": "image_url",
            "image_url": {
                "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            },
        },
    ],
    stream=True,
    markdown=True,
)
```

---

<a name="teams--basic--input_as_messages_listpy"></a>

### `teams/basic/input_as_messages_list.py`

```python
from agno.agent import Agent
from agno.models.message import Message
from agno.team import Team

# Create a research team
research_team = Team(
    name="Research Team",
    members=[
        Agent(
            name="Sarah",
            role="Data Researcher",
            instructions="Focus on gathering and analyzing data",
        ),
        Agent(
            name="Mike",
            role="Technical Writer",
            instructions="Create clear, concise summaries",
        ),
    ],
    stream=True,
    markdown=True,
)

research_team.print_response(
    [
        Message(
            role="user",
            content="I'm preparing a presentation for my company about renewable energy adoption.",
        ),
        Message(
            role="assistant",
            content="I'd be happy to help with your renewable energy presentation. What specific aspects would you like me to focus on?",
        ),
        Message(
            role="user",
            content="Could you research the latest solar panel efficiency improvements in 2024?",
        ),
        Message(
            role="user",
            content="Also, please summarize the key findings in bullet points for my slides.",
        ),
    ],
    markdown=True,
)
```

---

<a name="teams--basic--response_as_variablepy"></a>

### `teams/basic/response_as_variable.py`

```python
"""
This example demonstrates how to capture team responses as variables.

Shows how to get structured responses from teams and validate them using
Pydantic models for different types of queries.
"""

from typing import Iterator  # noqa
from pydantic import BaseModel
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response


class StockAnalysis(BaseModel):
    """Stock analysis data structure."""

    symbol: str
    company_name: str
    analysis: str


class CompanyAnalysis(BaseModel):
    """Company analysis data structure."""

    company_name: str
    analysis: str


# Stock price analysis agent
stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("o3-mini"),
    output_schema=StockAnalysis,
    role="Searches for stock price and analyst information",
    tools=[
        YFinanceTools(
            include_tools=["get_current_stock_price", "get_analyst_recommendations"],
        )
    ],
    instructions=[
        "Provide detailed stock analysis with price information",
        "Include analyst recommendations when available",
    ],
)

# Company news and information agent
company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches for company news and information",
    output_schema=CompanyAnalysis,
    tools=[
        YFinanceTools(
            include_tools=["get_company_info", "get_company_news"],
        )
    ],
    instructions=[
        "Focus on company news and business information",
        "Provide comprehensive analysis of company developments",
    ],
)

# Create routing team
team = Team(
    name="Stock Research Team",
    model=OpenAIChat("o3-mini"),
    respond_directly=True,
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
    instructions=[
        "Route stock price questions to the Stock Searcher",
        "Route company news and info questions to the Company Info Searcher",
    ],
)

# Example 1: Get stock price analysis as a variable
print("=" * 50)
print("STOCK PRICE ANALYSIS")
print("=" * 50)

stock_response = team.run("What is the current stock price of NVDA?")
assert isinstance(stock_response.content, StockAnalysis)
print(f"Response type: {type(stock_response.content)}")
print(f"Symbol: {stock_response.content.symbol}")
print(f"Company: {stock_response.content.company_name}")
print(f"Analysis: {stock_response.content.analysis}")
pprint_run_response(stock_response)

# Example 2: Get company news analysis as a variable
print("\n" + "=" * 50)
print("COMPANY NEWS ANALYSIS")
print("=" * 50)

news_response = team.run("What is in the news about NVDA?")
assert isinstance(news_response.content, CompanyAnalysis)
print(f"Response type: {type(news_response.content)}")
print(f"Company: {news_response.content.company_name}")
print(f"Analysis: {news_response.content.analysis}")
pprint_run_response(news_response)

# Example 3: Process multiple responses
print("\n" + "=" * 50)
print("BATCH PROCESSING")
print("=" * 50)

companies = ["AAPL", "GOOGL", "MSFT"]
responses = []

for company in companies:
    response = team.run(f"Analyze {company} stock")
    responses.append(response)
    print(f"Processed {company}: {type(response.content).__name__}")

print(f"Total responses processed: {len(responses)}")
```

---

<a name="teams--basic--run_as_clipy"></a>

### `teams/basic/run_as_cli.py`

```python
""" Interactive Writing Team - CLI App Example

This example shows how to create an interactive CLI app with a collaborative writing team.

Run `pip install openai agno duckduckgo-search` to install dependencies.
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

research_agent = Agent(
    name="Research Specialist",
    role="Information Research and Fact Verification",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions=dedent("""\
        You are an expert research specialist! 
        
        Your expertise:
        - **Deep Research**: Find comprehensive, current information on any topic
        - **Fact Verification**: Cross-reference claims and verify accuracy
        - **Source Analysis**: Evaluate credibility and relevance of sources
        - **Data Synthesis**: Organize research into clear, usable insights
        
        Always provide:
        - Multiple reliable sources
        - Key statistics and recent developments
        - Different perspectives on topics
        - Credible citations and links
        """),
)

brainstorm_agent = Agent(
    name="Creative Brainstormer",
    role="Idea Generation and Creative Concepts",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=dedent("""\
        You are a creative brainstorming expert! 
        
        Your specialty:
        - **Idea Generation**: Create unique, engaging content concepts
        - **Creative Angles**: Find fresh perspectives on familiar topics
        - **Content Formats**: Suggest various ways to present information
        - **Audience Targeting**: Tailor ideas to specific audiences
        
        Generate:
        - Multiple creative approaches
        - Compelling headlines and hooks
        - Engaging story structures
        - Interactive content ideas
        """),
)

writer_agent = Agent(
    name="Content Writer",
    role="Content Creation and Storytelling",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=dedent("""\
        You are a skilled content writer! 
        
        Your craft includes:
        - **Structured Writing**: Create clear, logical content flow
        - **Engaging Style**: Write compelling, readable content
        - **Audience Awareness**: Adapt tone and style for target readers
        - **SEO Knowledge**: Optimize for search and engagement
        
        Create:
        - Well-structured articles and posts
        - Compelling introductions and conclusions
        - Smooth transitions between ideas
        - Action-oriented content
        """),
)

editor_agent = Agent(
    name="Editor",
    role="Content Editing and Quality Assurance",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=dedent("""\
        You are a meticulous editor! 
        
        Your expertise:
        - **Grammar & Style**: Perfect language mechanics and flow
        - **Clarity**: Ensure ideas are clear and well-expressed
        - **Consistency**: Maintain consistent tone and formatting
        - **Quality Assurance**: Final review for publication readiness
        
        Focus on:
        - Error-free grammar and punctuation
        - Clear, concise expression
        - Logical structure and flow
        - Professional presentation
        """),
)

writing_team = Team(
    name="Writing Team",
    members=[research_agent, brainstorm_agent, writer_agent, editor_agent],
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=dedent("""\
        You are a collaborative writing team that excels at creating high-quality content!
        
        Team Process:
        1. **Research Phase**: Gather comprehensive, current information
        2. **Creative Phase**: Brainstorm unique angles and approaches  
        3. **Writing Phase**: Create structured, engaging content
        4. **Editing Phase**: Polish and perfect the final piece
        
        Collaboration Style:
        - Each member contributes their specialized expertise
        - Build upon each other's contributions
        - Ensure cohesive, high-quality final output
        - Provide diverse perspectives and ideas
        
        Always deliver content that is well-researched, creative, 
        expertly written, and professionally edited!
        """),
    show_members_responses=True,
    markdown=True,
)

if __name__ == "__main__":
    print(" Tell us about your writing project and watch the team collaborate!")
    print(" Type 'exit', 'quit', or 'bye' to end our session.\n")

    writing_team.cli_app(
        input="Hello! We're excited to work on your writing project. What would you like us to help you create today? Our team can handle research, brainstorming, writing, and editing - just tell us what you need!",
        user="Client",
        emoji="",
        stream=True,
    )

    ###########################################################################
    # ASYNC CLI APP
    ###########################################################################
    # import asyncio

    # asyncio.run(writing_team.acli_app(
    #     input="Hello! We're excited to work on your writing project. What would you like us to help you create today? Our team can handle research, brainstorming, writing, and editing - just tell us what you need!",
    #     user="Client",
    #     emoji="",
    #     stream=True,
    # ))
```

---

<a name="teams--basic--team_cancel_a_runpy"></a>

### `teams/basic/team_cancel_a_run.py`

```python
"""
Example demonstrating how to cancel a running team execution.

This example shows how to:
1. Start a team run in a separate thread
2. Cancel the run from another thread
3. Handle the cancelled response
"""

import threading
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.agent import RunEvent
from agno.run.base import RunStatus
from agno.run.team import TeamRunEvent
from agno.team import Team


def long_running_task(team: Team, run_id_container: dict):
    """
    Simulate a long-running team task that can be cancelled.

    Args:
        team: The team to run
        run_id_container: Dictionary to store the run_id for cancellation

    Returns:
        Dictionary with run results and status
    """
    try:
        # Start the team run - this simulates a long task
        final_response = None
        content_pieces = []

        for chunk in team.run(
            "Write a very long story about a dragon who learns to code. "
            "Make it at least 2000 words with detailed descriptions and dialogue. "
            "Take your time and be very thorough.",
            stream=True,
        ):
            if "run_id" not in run_id_container and chunk.run_id:
                print(f" Team run started: {chunk.run_id}")
                run_id_container["run_id"] = chunk.run_id

            if chunk.event in [TeamRunEvent.run_content, RunEvent.run_content]:
                print(chunk.content, end="", flush=True)
                content_pieces.append(chunk.content)
            elif chunk.event == RunEvent.run_cancelled:
                print(f"\n Member run was cancelled: {chunk.run_id}")
                run_id_container["result"] = {
                    "status": "cancelled",
                    "run_id": chunk.run_id,
                    "cancelled": True,
                    "content": "".join(content_pieces)[:200] + "..."
                    if content_pieces
                    else "No content before cancellation",
                }
                return
            elif chunk.event == TeamRunEvent.run_cancelled:
                print(f"\n Team run was cancelled: {chunk.run_id}")
                run_id_container["result"] = {
                    "status": "cancelled",
                    "run_id": chunk.run_id,
                    "cancelled": True,
                    "content": "".join(content_pieces)[:200] + "..."
                    if content_pieces
                    else "No content before cancellation",
                }
                return
            elif hasattr(chunk, "status") and chunk.status == RunStatus.completed:
                final_response = chunk

        # If we get here, the run completed successfully
        if final_response:
            run_id_container["result"] = {
                "status": final_response.status.value
                if final_response.status
                else "completed",
                "run_id": final_response.run_id,
                "cancelled": final_response.status == RunStatus.cancelled,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }
        else:
            run_id_container["result"] = {
                "status": "unknown",
                "run_id": run_id_container.get("run_id"),
                "cancelled": False,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }

    except Exception as e:
        print(f"\n Exception in run: {str(e)}")
        run_id_container["result"] = {
            "status": "error",
            "error": str(e),
            "run_id": run_id_container.get("run_id"),
            "cancelled": True,
            "content": "Error occurred",
        }


def cancel_after_delay(team: Team, run_id_container: dict, delay_seconds: int = 3):
    """
    Cancel the team run after a specified delay.

    Args:
        team: The team whose run should be cancelled
        run_id_container: Dictionary containing the run_id to cancel
        delay_seconds: How long to wait before cancelling
    """
    print(f" Will cancel team run in {delay_seconds} seconds...")
    time.sleep(delay_seconds)

    run_id = run_id_container.get("run_id")
    if run_id:
        print(f" Cancelling team run: {run_id}")
        success = team.cancel_run(run_id)
        if success:
            print(f" Team run {run_id} marked for cancellation")
        else:
            print(
                f" Failed to cancel team run {run_id} (may not exist or already completed)"
            )
    else:
        print("  No run_id found to cancel")


def main():
    """Main function demonstrating team run cancellation."""

    # Create team members
    storyteller_agent = Agent(
        name="StorytellerAgent",
        model=OpenAIChat(id="o3-mini"),
        description="An agent that writes creative stories",
    )

    editor_agent = Agent(
        name="EditorAgent",
        model=OpenAIChat(id="o3-mini"),
        description="An agent that reviews and improves stories",
    )

    # Initialize the team with agents
    team = Team(
        name="Storytelling Team",
        members=[storyteller_agent, editor_agent],
        model=OpenAIChat(id="o3-mini"),  # Team leader model
        description="A team that collaborates to write detailed stories",
    )

    print(" Starting team run cancellation example...")
    print("=" * 50)

    # Container to share run_id between threads
    run_id_container = {}

    # Start the team run in a separate thread
    team_thread = threading.Thread(
        target=lambda: long_running_task(team, run_id_container), name="TeamRunThread"
    )

    # Start the cancellation thread
    cancel_thread = threading.Thread(
        target=cancel_after_delay,
        args=(team, run_id_container, 8),  # Cancel after 8 seconds
        name="CancelThread",
    )

    # Start both threads
    print(" Starting team run thread...")
    team_thread.start()

    print(" Starting cancellation thread...")
    cancel_thread.start()

    # Wait for both threads to complete
    print(" Waiting for threads to complete...")
    team_thread.join()
    cancel_thread.join()

    # Print the results
    print("\n" + "=" * 50)
    print(" RESULTS:")
    print("=" * 50)

    result = run_id_container.get("result")
    if result:
        print(f"Status: {result['status']}")
        print(f"Run ID: {result['run_id']}")
        print(f"Was Cancelled: {result['cancelled']}")

        if result.get("error"):
            print(f"Error: {result['error']}")
        else:
            print(f"Content Preview: {result['content']}")

        if result["cancelled"]:
            print("\n SUCCESS: Team run was successfully cancelled!")
        else:
            print("\n  WARNING: Team run completed before cancellation")
    else:
        print(" No result obtained - check if cancellation happened during streaming")

    print("\n Team cancellation example completed!")


if __name__ == "__main__":
    # Run the main example
    main()
```

---

<a name="teams--basic--team_exponential_backoffpy"></a>

### `teams/basic/team_exponential_backoff.py`

```python
from agno.agent import Agent
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

# Create a research team
team = Team(
    members=[
        Agent(
            name="Sarah",
            role="Data Researcher",
            tools=[DuckDuckGoTools()],
            instructions="Focus on gathering and analyzing data",
        ),
        Agent(
            name="Mike",
            role="Technical Writer",
            instructions="Create clear, concise summaries",
        ),
    ],
    retries=3,
    exponential_backoff=True,
)

team.print_response(
    "Search for latest news about the latest AI models",
    stream=True,
)
```

---

<a name="teams--dependencies--access_dependencies_in_toolpy"></a>

### `teams/dependencies/access_dependencies_in_tool.py`

```python
"""
Example showing how team tools can access dependencies passed to the team.

This demonstrates:
1. Passing dependencies to team.run()
2. A team tool that receives resolved dependencies
3. Team members working together with shared data sources
"""

from datetime import datetime
from typing import Any, Dict, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team


def get_current_context() -> dict:
    """Get current contextual information like time, weather, etc."""
    return {
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": "PST",
        "day_of_week": datetime.now().strftime("%A"),
    }


def analyze_team_performance(
    team_id: str, dependencies: Optional[Dict[str, Any]] = None
) -> str:
    """
    Analyze team performance using available data sources.

    This tool analyzes team metrics and provides insights.
    Call this tool with the team_id you want to analyze.

    Args:
        team_id: The team ID to analyze (e.g., 'engineering_team', 'sales_team')
        dependencies: Available data sources (automatically provided)

    Returns:
        Detailed team performance analysis and insights
    """
    if not dependencies:
        return "No data sources available for analysis."

    print(f"--> Team tool received data sources: {list(dependencies.keys())}")

    results = [f"=== TEAM PERFORMANCE ANALYSIS FOR {team_id.upper()} ==="]

    # Use team metrics data if available
    if "team_metrics" in dependencies:
        metrics_data = dependencies["team_metrics"]
        results.append(f"Team Metrics: {metrics_data}")

        # Add analysis based on the metrics
        if metrics_data.get("productivity_score"):
            score = metrics_data["productivity_score"]
            if score >= 8:
                results.append(
                    f"Performance Analysis: Excellent performance with {score}/10 productivity score"
                )
            elif score >= 6:
                results.append(
                    f"Performance Analysis: Good performance with {score}/10 productivity score"
                )
            else:
                results.append(
                    f"Performance Analysis: Needs improvement with {score}/10 productivity score"
                )

    # Use current context data if available
    if "current_context" in dependencies:
        context_data = dependencies["current_context"]
        results.append(f"Current Context: {context_data}")
        results.append(
            f"Time-based Analysis: Team analysis performed on {context_data['day_of_week']} at {context_data['current_time']}"
        )

    print(f"--> Team tool returned results: {results}")

    return "\n\n".join(results)


# Create team members
data_analyst = Agent(
    model=OpenAIChat(id="gpt-4o"),
    name="Data Analyst",
    description="Specialist in analyzing team metrics and performance data",
    instructions=[
        "You are a data analysis expert focusing on team performance metrics.",
        "Interpret quantitative data and identify trends.",
        "Provide data-driven insights and recommendations.",
    ],
)

team_lead = Agent(
    model=OpenAIChat(id="gpt-4o"),
    name="Team Lead",
    description="Experienced team leader who provides strategic insights",
    instructions=[
        "You are an experienced team leader and management expert.",
        "Focus on leadership insights and team dynamics.",
        "Provide strategic recommendations for team improvement.",
        "Collaborate with the data analyst to get comprehensive insights.",
    ],
)

# Create a team with the analysis tool
performance_team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[data_analyst, team_lead],
    tools=[analyze_team_performance],
    name="Team Performance Analysis Team",
    description="A team specialized in analyzing team performance using integrated data sources.",
    instructions=[
        "You are a team performance analysis unit with access to team metrics and analysis tools.",
        "When asked to analyze any team, use the analyze_team_performance tool first.",
        "This tool has access to team metrics and current context through integrated data sources.",
        "Data Analyst: Focus on the quantitative metrics and trends.",
        "Team Lead: Provide strategic insights and management recommendations.",
        "Work together to provide comprehensive team performance insights.",
    ],
)

print("=== Team Tool Dependencies Access Example ===\n")

response = performance_team.run(
    input="Please analyze the 'engineering_team' performance and provide comprehensive insights about their productivity and recommendations for improvement.",
    dependencies={
        "team_metrics": {
            "team_name": "Engineering Team Alpha",
            "team_size": 8,
            "productivity_score": 7.5,
            "sprint_velocity": 85,
            "bug_resolution_rate": 92,
            "code_review_turnaround": "2.3 days",
            "areas": ["Backend Development", "Frontend Development", "DevOps"],
        },
        "current_context": get_current_context,
    },
    session_id="test_team_tool_dependencies",
)

print(f"\nTeam Response: {response.content}")
```

---

<a name="teams--dependencies--add_dependencies_on_runpy"></a>

### `teams/dependencies/add_dependencies_on_run.py`

```python
from agno.models.openai import OpenAIChat
from agno.team import Team


def get_user_profile(user_id: str = "john_doe") -> dict:
    """Get user profile information that can be referenced in responses."""
    profiles = {
        "john_doe": {
            "name": "John Doe",
            "preferences": {
                "communication_style": "professional",
                "topics_of_interest": ["AI/ML", "Software Engineering", "Finance"],
                "experience_level": "senior",
            },
            "location": "San Francisco, CA",
            "role": "Senior Software Engineer",
        }
    }

    return profiles.get(user_id, {"name": "Unknown User"})


def get_current_context() -> dict:
    """Get current contextual information like time, weather, etc."""
    from datetime import datetime

    return {
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": "PST",
        "day_of_week": datetime.now().strftime("%A"),
    }


team = Team(
    name="PersonalizationTeam",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[],
    instructions=[
        "Analyze the user profile and current context to provide a personalized summary of today's priorities."
    ],
    markdown=True,
)

response = team.run(
    "Please provide me with a personalized summary of today's priorities based on my profile and interests.",
    dependencies={
        "user_profile": get_user_profile,
        "current_context": get_current_context,
    },
    add_dependencies_to_context=True,
)

print(response.content)
```

---

<a name="teams--dependencies--add_dependencies_to_contextpy"></a>

### `teams/dependencies/add_dependencies_to_context.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team


def get_user_profile(user_id: str = "john_doe") -> dict:
    """Get user profile information that can be referenced in responses."""
    profiles = {
        "john_doe": {
            "name": "John Doe",
            "preferences": {
                "communication_style": "professional",
                "topics_of_interest": ["AI/ML", "Software Engineering", "Finance"],
                "experience_level": "senior",
            },
            "location": "San Francisco, CA",
            "role": "Senior Software Engineer",
        }
    }

    return profiles.get(user_id, {"name": "Unknown User"})


def get_current_context() -> dict:
    """Get current contextual information like time, weather, etc."""
    from datetime import datetime

    return {
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": "PST",
        "day_of_week": datetime.now().strftime("%A"),
    }


profile_agent = Agent(
    name="ProfileAnalyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You analyze user profiles and provide personalized recommendations.",
)

context_agent = Agent(
    name="ContextAnalyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You analyze current context and timing to provide relevant insights.",
)

team = Team(
    name="PersonalizationTeam",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[profile_agent, context_agent],
    dependencies={
        "user_profile": get_user_profile,
        "current_context": get_current_context,
    },
    add_dependencies_to_context=True,
    debug_mode=True,
    markdown=True,
)

response = team.run(
    "Please provide me with a personalized summary of today's priorities based on my profile and interests.",
)

print(response.content)

# ------------------------------------------------------------
# ASYNC EXAMPLE
# ------------------------------------------------------------
# async def test_async():
#     async_response = await team.arun(
#         "Based on my profile, what should I focus on this week? Include specific recommendations.",
#     )
#
#     print("\n=== Async Run Response ===")
#     print(async_response.content)

# # Run the async test
# import asyncio
# asyncio.run(test_async())
```

---

<a name="teams--dependencies--add_dependencies_to_member_contextpy"></a>

### `teams/dependencies/add_dependencies_to_member_context.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team


def get_user_profile(user_id: str = "john_doe") -> dict:
    """Get user profile information that can be referenced in responses."""
    profiles = {
        "john_doe": {
            "name": "John Doe",
            "preferences": {
                "communication_style": "professional",
                "topics_of_interest": ["AI/ML", "Software Engineering", "Finance"],
                "experience_level": "senior",
            },
            "location": "San Francisco, CA",
            "role": "Senior Software Engineer",
        }
    }

    return profiles.get(user_id, {"name": "Unknown User"})


def get_current_context() -> dict:
    """Get current contextual information like time, weather, etc."""
    from datetime import datetime

    return {
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": "PST",
        "day_of_week": datetime.now().strftime("%A"),
    }


profile_agent = Agent(
    name="ProfileAnalyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You analyze user profiles and provide personalized recommendations.",
)

context_agent = Agent(
    name="ContextAnalyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You analyze current context and timing to provide relevant insights.",
)

team = Team(
    name="PersonalizationTeam",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[profile_agent, context_agent],
    markdown=True,
    show_members_responses=True,
)

team.print_response(
    "Please provide me with a personalized summary of today's priorities based on my profile and interests.",
    dependencies={
        "user_profile": get_user_profile,
        "current_context": get_current_context,
    },
    add_dependencies_to_context=True,  # This setting will pass through to the member agents
    # So we can observe the dependencies sent to the member agents
    debug_mode=True,
)
```

---

<a name="teams--dependencies--reference_dependenciespy"></a>

### `teams/dependencies/reference_dependencies.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team


def get_user_profile(user_id: str = "john_doe") -> dict:
    """Get user profile information that can be referenced in responses."""
    profiles = {
        "john_doe": {
            "name": "John Doe",
            "preferences": {
                "communication_style": "professional",
                "topics_of_interest": ["AI/ML", "Software Engineering", "Finance"],
                "experience_level": "senior",
            },
            "location": "San Francisco, CA",
            "role": "Senior Software Engineer",
        }
    }

    return profiles.get(user_id, {"name": "Unknown User"})


def get_current_context() -> dict:
    """Get current contextual information like time, weather, etc."""
    from datetime import datetime

    return {
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": "PST",
        "day_of_week": datetime.now().strftime("%A"),
    }


profile_agent = Agent(
    name="ProfileAnalyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You analyze user profiles and provide personalized recommendations.",
)

context_agent = Agent(
    name="ContextAnalyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You analyze current context and timing to provide relevant insights.",
)

team = Team(
    name="PersonalizationTeam",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[profile_agent, context_agent],
    dependencies={
        "user_profile": get_user_profile,
        "current_context": get_current_context,
    },
    instructions=[
        "You are a personalization team that provides personalized recommendations based on the user's profile and context.",
        "Here is the user profile: {user_profile}",
        "Here is the current context: {current_context}",
    ],
    debug_mode=True,
    markdown=True,
)

response = team.run(
    "Please provide me with a personalized summary of today's priorities based on my profile and interests.",
)

print(response.content)
```

---

<a name="teams--distributed_rag--01_distributed_rag_pgvectorpy"></a>

### `teams/distributed_rag/01_distributed_rag_pgvector.py`

```python
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses using distributed PostgreSQL vector databases with
pgvector for scalable, production-ready retrieval.

Team Composition:
- Vector Retriever: Specialized in vector similarity search using pgvector
- Hybrid Searcher: Combines vector and text search for comprehensive results
- Data Validator: Validates retrieved data quality and relevance
- Response Composer: Composes final responses with proper source attribution

Setup:
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector agno`
3. Run this script to see distributed PgVector RAG in action
"""

import asyncio  # noqa: F401

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.vectordb.pgvector import PgVector, SearchType

# Database connection URL
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Vector-focused knowledge base for similarity search
vector_knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes_vector",
        db_url=db_url,
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Hybrid knowledge base for comprehensive search
hybrid_knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes_hybrid",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Vector Retriever Agent - Specialized in vector similarity search
vector_retriever = Agent(
    name="Vector Retriever",
    model=OpenAIChat(id="o3-mini"),
    role="Retrieve information using vector similarity search in PostgreSQL",
    knowledge=vector_knowledge,
    search_knowledge=True,
    instructions=[
        "Use vector similarity search to find semantically related content.",
        "Focus on finding information that matches the semantic meaning of queries.",
        "Leverage pgvector's efficient similarity search capabilities.",
        "Retrieve content that has high semantic relevance to the user's query.",
    ],
    markdown=True,
)

# Hybrid Searcher Agent - Specialized in hybrid search
hybrid_searcher = Agent(
    name="Hybrid Searcher",
    model=OpenAIChat(id="o3-mini"),
    role="Perform hybrid search combining vector and text search",
    knowledge=hybrid_knowledge,
    search_knowledge=True,
    instructions=[
        "Combine vector similarity and text search for comprehensive results.",
        "Find information that matches both semantic and lexical criteria.",
        "Use PostgreSQL's hybrid search capabilities for best coverage.",
        "Ensure retrieval of both conceptually and textually relevant content.",
    ],
    markdown=True,
)

# Data Validator Agent - Specialized in data quality validation
data_validator = Agent(
    name="Data Validator",
    model=OpenAIChat(id="o3-mini"),
    role="Validate retrieved data quality and relevance",
    instructions=[
        "Assess the quality and relevance of retrieved information.",
        "Check for consistency across different search results.",
        "Identify the most reliable and accurate information.",
        "Filter out any irrelevant or low-quality content.",
        "Ensure data integrity and relevance to the user's query.",
    ],
    markdown=True,
)

# Response Composer Agent - Specialized in response composition
response_composer = Agent(
    name="Response Composer",
    model=OpenAIChat(id="o3-mini"),
    role="Compose comprehensive responses with proper source attribution",
    instructions=[
        "Combine validated information from all team members.",
        "Create well-structured, comprehensive responses.",
        "Include proper source attribution and data provenance.",
        "Ensure clarity and coherence in the final response.",
        "Format responses for optimal user experience.",
    ],
    markdown=True,
)

# Create distributed PgVector RAG team
distributed_pgvector_team = Team(
    name="Distributed PgVector RAG Team",
    model=OpenAIChat(id="o3-mini"),
    members=[vector_retriever, hybrid_searcher, data_validator, response_composer],
    instructions=[
        "Work together to provide comprehensive RAG responses using PostgreSQL pgvector.",
        "Vector Retriever: First perform vector similarity search.",
        "Hybrid Searcher: Then perform hybrid search for comprehensive coverage.",
        "Data Validator: Validate and filter the retrieved information quality.",
        "Response Composer: Compose the final response with proper attribution.",
        "Leverage PostgreSQL's scalability and pgvector's performance.",
        "Ensure enterprise-grade reliability and accuracy.",
    ],
    show_members_responses=True,
    markdown=True,
)


async def async_pgvector_rag_demo():
    """Demonstrate async distributed PgVector RAG processing."""
    print(" Async Distributed PgVector RAG Demo")
    print("=" * 40)

    query = "How do I make chicken and galangal in coconut milk soup? What are the key ingredients and techniques?"

    try:
        # Add content to knowledge bases
        await vector_knowledge.add_contents_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        await hybrid_knowledge.add_contents_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        # Run async distributed PgVector RAG
        await distributed_pgvector_team.aprint_response(input=query)
    except Exception as e:
        print(f" Error: {e}")
        print(" Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")


def sync_pgvector_rag_demo():
    """Demonstrate sync distributed PgVector RAG processing."""
    print(" Distributed PgVector RAG Demo")
    print("=" * 35)

    query = "How do I make chicken and galangal in coconut milk soup? What are the key ingredients and techniques?"

    try:
        # Add content to knowledge bases
        vector_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        hybrid_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        # Run distributed PgVector RAG
        distributed_pgvector_team.print_response(input=query)
    except Exception as e:
        print(f" Error: {e}")
        print(" Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")


def complex_query_demo():
    """Demonstrate distributed RAG for complex culinary queries."""
    print(" Complex Culinary Query with Distributed PgVector RAG")
    print("=" * 60)

    query = """I'm planning a Thai dinner party for 8 people. Can you help me plan a complete menu?
    I need appetizers, main courses, and desserts. Please include:
    - Preparation timeline
    - Shopping list
    - Cooking techniques for each dish
    - Any dietary considerations or alternatives"""

    try:
        # Add content to knowledge bases
        vector_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        hybrid_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )

        distributed_pgvector_team.print_response(input=query)
    except Exception as e:
        print(f" Error: {e}")
        print(" Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")


if __name__ == "__main__":
    # Choose which demo to run

    # asyncio.run(async_pgvector_rag_demo())

    # complex_query_demo()

    sync_pgvector_rag_demo()
```

---

<a name="teams--distributed_rag--02_distributed_rag_lancedbpy"></a>

### `teams/distributed_rag/02_distributed_rag_lancedb.py`

```python
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses using distributed knowledge bases and specialized
retrieval strategies with LanceDB.

Team Composition:
- Primary Retriever: Handles primary document retrieval from main knowledge base
- Context Expander: Expands context by finding related information
- Answer Synthesizer: Synthesizes retrieved information into comprehensive answers
- Quality Validator: Validates answer quality and suggests improvements

Setup:
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno`
2. Run this script to see distributed RAG in action
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.vectordb.lancedb import LanceDb, SearchType

# Primary knowledge base for main retrieval
primary_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_primary",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Secondary knowledge base for context expansion
context_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_context",
        uri="tmp/lancedb",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Primary Retriever Agent - Specialized in main document retrieval
primary_retriever = Agent(
    name="Primary Retriever",
    model=OpenAIChat(id="o3-mini"),
    role="Retrieve primary documents and core information from knowledge base",
    knowledge=primary_knowledge,
    search_knowledge=True,
    instructions=[
        "Search the knowledge base for directly relevant information to the user's query.",
        "Focus on retrieving the most relevant and specific documents first.",
        "Provide detailed information with proper context.",
        "Ensure accuracy and completeness of retrieved information.",
    ],
    markdown=True,
)

# Context Expander Agent - Specialized in expanding context
context_expander = Agent(
    name="Context Expander",
    model=OpenAIChat(id="o3-mini"),
    role="Expand context by finding related and supplementary information",
    knowledge=context_knowledge,
    search_knowledge=True,
    instructions=[
        "Find related information that complements the primary retrieval.",
        "Look for background context, related topics, and supplementary details.",
        "Search for information that helps understand the broader context.",
        "Identify connections between different pieces of information.",
    ],
    markdown=True,
)

# Answer Synthesizer Agent - Specialized in synthesis
answer_synthesizer = Agent(
    name="Answer Synthesizer",
    model=OpenAIChat(id="o3-mini"),
    role="Synthesize retrieved information into comprehensive answers",
    instructions=[
        "Combine information from the Primary Retriever and Context Expander.",
        "Create a comprehensive, well-structured response.",
        "Ensure logical flow and coherence in the final answer.",
        "Include relevant details while maintaining clarity.",
        "Organize information in a user-friendly format.",
    ],
    markdown=True,
)

# Quality Validator Agent - Specialized in validation
quality_validator = Agent(
    name="Quality Validator",
    model=OpenAIChat(id="o3-mini"),
    role="Validate answer quality and suggest improvements",
    instructions=[
        "Review the synthesized answer for accuracy and completeness.",
        "Check if the answer fully addresses the user's query.",
        "Identify any gaps or areas that need clarification.",
        "Suggest improvements or additional information if needed.",
        "Ensure the response meets high quality standards.",
    ],
    markdown=True,
)

# Create distributed RAG team
distributed_rag_team = Team(
    name="Distributed RAG Team",
    model=OpenAIChat(id="o3-mini"),
    members=[
        primary_retriever,
        context_expander,
        answer_synthesizer,
        quality_validator,
    ],
    instructions=[
        "Work together to provide comprehensive, high-quality RAG responses.",
        "Primary Retriever: First retrieve core relevant information.",
        "Context Expander: Then expand with related context and background.",
        "Answer Synthesizer: Synthesize all information into a comprehensive answer.",
        "Quality Validator: Finally validate and suggest any improvements.",
        "Ensure all responses are accurate, complete, and well-structured.",
    ],
    show_members_responses=True,
    markdown=True,
)


async def async_distributed_rag_demo():
    """Demonstrate async distributed RAG processing."""
    print(" Async Distributed RAG with LanceDB Demo")
    print("=" * 50)

    query = "How do I make chicken and galangal in coconut milk soup? Include cooking tips and variations."

    # Add content to knowledge bases
    await primary_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    await context_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

    # # Run async distributed RAG
    # await distributed_rag_team.aprint_response(
    #     query, stream=True, stream_intermediate_steps=True
    # )
    await distributed_rag_team.aprint_response(input=query)


def sync_distributed_rag_demo():
    """Demonstrate sync distributed RAG processing."""
    print(" Distributed RAG with LanceDB Demo")
    print("=" * 40)

    query = "How do I make chicken and galangal in coconut milk soup? Include cooking tips and variations."

    # Add content to knowledge bases
    primary_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    context_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

    # Run distributed RAG
    distributed_rag_team.print_response(input=query)


def multi_course_meal_demo():
    """Demonstrate distributed RAG for complex multi-part queries."""
    print(" Multi-Course Meal Planning with Distributed RAG")
    print("=" * 55)

    query = """Hi, I want to make a 3 course Thai meal. Can you recommend some recipes? 
    I'd like to start with a soup, then a thai curry for the main course and finish with a dessert.
    Please include cooking techniques and any special tips."""

    # Add content to knowledge bases
    primary_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    context_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

    distributed_rag_team.print_response(input=query)


if __name__ == "__main__":
    # Choose which demo to run
    asyncio.run(async_distributed_rag_demo())

    # multi_course_meal_demo()

    # sync_distributed_rag_demo()
```

---

<a name="teams--distributed_rag--03_distributed_rag_with_rerankingpy"></a>

### `teams/distributed_rag/03_distributed_rag_with_reranking.py`

```python
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses using advanced reranking strategies for optimal
information retrieval and synthesis.

Team Composition:
- Initial Retriever: Performs broad initial retrieval from knowledge base
- Reranking Specialist: Applies advanced reranking for result optimization
- Context Analyzer: Analyzes context and relevance of reranked results
- Final Synthesizer: Synthesizes reranked results into optimal responses

Setup:
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno`
2. Run this script to see advanced reranking RAG in action
"""

import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker import CohereReranker
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.print_response.team import aprint_response, print_response
from agno.vectordb.lancedb import LanceDb, SearchType

# Knowledge base with advanced reranking
reranked_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_reranked",
        uri="tmp/lancedb",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

# Secondary knowledge base for cross-validation
validation_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_validation",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Initial Retriever Agent - Specialized in broad initial retrieval
initial_retriever = Agent(
    name="Initial Retriever",
    model=OpenAIChat(id="o3-mini"),
    role="Perform broad initial retrieval to gather candidate information",
    knowledge=reranked_knowledge,
    search_knowledge=True,
    instructions=[
        "Perform comprehensive initial retrieval from the knowledge base.",
        "Cast a wide net to gather all potentially relevant information.",
        "Focus on recall rather than precision in this initial phase.",
        "Retrieve diverse content that might be relevant to the query.",
    ],
    markdown=True,
)

# Reranking Specialist Agent - Specialized in result optimization
reranking_specialist = Agent(
    name="Reranking Specialist",
    model=OpenAIChat(id="o3-mini"),
    role="Apply advanced reranking to optimize retrieval results",
    knowledge=reranked_knowledge,
    search_knowledge=True,
    instructions=[
        "Apply advanced reranking techniques to optimize result relevance.",
        "Focus on precision and ranking quality over quantity.",
        "Use the Cohere reranker to identify the most relevant content.",
        "Prioritize results that best match the user's specific needs.",
    ],
    markdown=True,
)

# Context Analyzer Agent - Specialized in context analysis
context_analyzer = Agent(
    name="Context Analyzer",
    model=OpenAIChat(id="o3-mini"),
    role="Analyze context and relevance of reranked results",
    knowledge=validation_knowledge,
    search_knowledge=True,
    instructions=[
        "Analyze the context and relevance of reranked results.",
        "Cross-validate information against the validation knowledge base.",
        "Assess the quality and accuracy of retrieved content.",
        "Identify the most contextually appropriate information.",
    ],
    markdown=True,
)

# Final Synthesizer Agent - Specialized in optimal synthesis
final_synthesizer = Agent(
    name="Final Synthesizer",
    model=OpenAIChat(id="o3-mini"),
    role="Synthesize reranked results into optimal comprehensive responses",
    instructions=[
        "Synthesize information from all team members into optimal responses.",
        "Leverage the reranked and analyzed results for maximum quality.",
        "Create responses that demonstrate the benefits of advanced reranking.",
        "Ensure optimal information organization and presentation.",
        "Include confidence levels and source quality indicators.",
    ],
    markdown=True,
)

# Create distributed reranking RAG team
distributed_reranking_team = Team(
    name="Distributed Reranking RAG Team",
    model=OpenAIChat(id="o3-mini"),
    members=[
        initial_retriever,
        reranking_specialist,
        context_analyzer,
        final_synthesizer,
    ],
    instructions=[
        "Work together to provide optimal RAG responses using advanced reranking.",
        "Initial Retriever: First perform broad comprehensive retrieval.",
        "Reranking Specialist: Apply advanced reranking for result optimization.",
        "Context Analyzer: Analyze and validate the reranked results.",
        "Final Synthesizer: Create optimal responses from reranked information.",
        "Leverage advanced reranking for superior result quality.",
        "Demonstrate the benefits of specialized reranking in team coordination.",
    ],
    show_members_responses=True,
    markdown=True,
)


async def async_reranking_rag_demo():
    """Demonstrate async distributed reranking RAG processing."""
    print(" Async Distributed Reranking RAG Demo")
    print("=" * 45)

    query = "What's the best way to prepare authentic Tom Kha Gai? I want traditional methods and modern variations."

    # Add content to knowledge bases
    await reranked_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    await validation_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

    # Run async distributed reranking RAG
    await aprint_response(input=query, team=distributed_reranking_team)


def sync_reranking_rag_demo():
    """Demonstrate sync distributed reranking RAG processing."""
    print(" Distributed Reranking RAG Demo")
    print("=" * 35)

    query = "What's the best way to prepare authentic Tom Kha Gai? I want traditional methods and modern variations."

    # Add content to knowledge bases
    reranked_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    validation_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

    # Run distributed reranking RAG
    print_response(distributed_reranking_team, query)


def advanced_culinary_demo():
    """Demonstrate advanced reranking for complex culinary queries."""
    print(" Advanced Culinary Analysis with Reranking RAG")
    print("=" * 55)

    query = """I want to understand the science behind Thai curry pastes. Can you explain:
    - Traditional preparation methods vs modern techniques
    - How different ingredients affect flavor profiles
    - Regional variations and their historical origins
    - Best practices for storage and usage
    - How to adapt recipes for different dietary needs"""

    # Add content to knowledge bases
    reranked_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    validation_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

    print_response(distributed_reranking_team, query)


if __name__ == "__main__":
    # Choose which demo to run
    asyncio.run(async_reranking_rag_demo())

    # advanced_culinary_demo()

    # sync_reranking_rag_demo()
```

---

<a name="teams--knowledge--01_team_with_knowledgepy"></a>

### `teams/knowledge/01_team_with_knowledge.py`

```python
"""
This example demonstrates how to create a team with knowledge base integration.

The team has access to a knowledge base with Agno documentation and can combine
this knowledge with web search capabilities.
"""

from pathlib import Path

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Setup paths for knowledge storage
cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

# Initialize knowledge base with vector database
agno_docs_knowledge = Knowledge(
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Add content to knowledge base
agno_docs_knowledge.add_content(url="https://docs.agno.com/llms-full.txt")

# Create web search agent for supplementary information
web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

# Create team with knowledge base integration
team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[web_agent],
    model=OpenAIChat(id="o3-mini"),
    knowledge=agno_docs_knowledge,
    show_members_responses=True,
    markdown=True,
)

if __name__ == "__main__":
    team_with_knowledge.print_response("Tell me about the Agno framework", stream=True)
```

---

<a name="teams--knowledge--02_team_with_knowledge_filterspy"></a>

### `teams/knowledge/02_team_with_knowledge_filters.py`

```python
"""
This example demonstrates how to use knowledge filters with teams.

Knowledge filters allow you to restrict knowledge searches to specific documents
or metadata criteria, enabling personalized and contextual responses.
"""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB vector database
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Create knowledge base
knowledge_base = Knowledge(
    vector_db=vector_db,
)

# Add documents with metadata for filtering
knowledge_base.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    reader=PDFReader(chunk=True),
)

# Create knowledge search agent
web_agent = Agent(
    name="Knowledge Search Agent",
    role="Handle knowledge search",
    knowledge=knowledge_base,
    model=OpenAIChat(id="o3-mini"),
)

# Create team with knowledge filters
team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[
        web_agent
    ],  # If you omit the member, the leader will search the knowledge base itself.
    model=OpenAIChat(id="o3-mini"),
    knowledge=knowledge_base,
    show_members_responses=True,
    markdown=True,
    knowledge_filters={
        "user_id": "jordan_mitchell"
    },  # Filter to specific user's documents
)

# Test knowledge filtering
team_with_knowledge.print_response(
    "Tell me about Jordan Mitchell's work and experience"
)
```

---

<a name="teams--knowledge--03_team_with_agentic_knowledge_filterspy"></a>

### `teams/knowledge/03_team_with_agentic_knowledge_filters.py`

```python
"""
This example demonstrates how to use agentic knowledge filters with teams.

Agentic knowledge filters allow the AI to dynamically determine which documents
to search based on the query context, rather than using predefined filters.
"""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

# Download all sample CVs and get their paths
downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

# Initialize LanceDB vector database
# By default, it stores data in /tmp/lancedb
vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

# Create knowledge base
knowledge = Knowledge(
    vector_db=vector_db,
)

# Add documents with metadata for agentic filtering
knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

# Create knowledge search agent with filter awareness
web_agent = Agent(
    name="Knowledge Search Agent",
    role="Handle knowledge search",
    knowledge=knowledge,
    model=OpenAIChat(id="o3-mini"),
    instructions=["Always take into account filters"],
)

# Create team with agentic knowledge filters enabled
team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[
        web_agent
    ],  # If you omit the member, the leader will search the knowledge base itself.
    model=OpenAIChat(id="o3-mini"),
    knowledge=knowledge,
    show_members_responses=True,
    markdown=True,
    enable_agentic_knowledge_filters=True,  # Allow AI to determine filters
)

# Test agentic knowledge filtering
team_with_knowledge.print_response(
    "Tell me about Jordan Mitchell's work and experience with user_id as jordan_mitchell"
)
```

---

<a name="teams--memory--01_team_with_memory_managerpy"></a>

### `teams/memory/01_team_with_memory_manager.py`

```python
"""
This example shows you how to use persistent memory with an Agent.

After each run, user memories are created/updated.

To enable this, set `enable_user_memories=True` in the Agent config.
"""

from uuid import uuid4

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager  # noqa: F401
from agno.models.openai import OpenAIChat
from agno.team import Team
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

session_id = str(uuid4())
john_doe_id = "john_doe@example.com"

memory_manager = MemoryManager(model=OpenAIChat(id="o3-mini"))

memory_manager.clear()

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)
team = Team(
    model=OpenAIChat(id="o3-mini"),
    memory_manager=memory_manager,
    members=[agent],
    db=db,
    enable_user_memories=True,
)

team.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
    session_id=session_id,
)
team.print_response(
    "What are my hobbies?", stream=True, user_id=john_doe_id, session_id=session_id
)

# # You can also get the user memories from the agent
memories = team.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)
```

---

<a name="teams--memory--02_team_with_agentic_memorypy"></a>

### `teams/memory/02_team_with_agentic_memory.py`

```python
"""
This example shows you how to use persistent memory with an Agent.

During each run the Agent can create/update/delete user memories.

To enable this, set `enable_agentic_memory=True` in the Agent config.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager  # noqa: F401
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
    enable_agentic_memory=True,
)

team.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
)

team.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)

# More examples:
# agent.print_response(
#     "Remove all existing memories of me.",
#     stream=True,
#     user_id=john_doe_id,
# )

# agent.print_response(
#     "My name is John Doe and I like to paint.", stream=True, user_id=john_doe_id
# )

# agent.print_response(
#     "I don't pain anymore, i draw instead.", stream=True, user_id=john_doe_id
# )
```

---

<a name="teams--metrics--01_team_metricspy"></a>

### `teams/metrics/01_team_metrics.py`

```python
"""
This example demonstrates how to access and analyze team metrics.

Shows how to retrieve detailed metrics for team execution, including
message-level metrics, session metrics, and member-specific metrics.

Prerequisites:
1. Run: cookbook/run_pgvector.sh (to start PostgreSQL)
2. Ensure PostgreSQL is running on localhost:5532
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

# Database configuration for metrics storage
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url, session_table="team_metrics_sessions")

# Create stock research agent
stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    tools=[YFinanceTools()],
)

# Create team with metrics tracking enabled
team = Team(
    name="Stock Research Team",
    model=OpenAIChat("o3-mini"),
    members=[stock_searcher],
    db=db,  # Database required for session metrics
    session_id="team_metrics_demo",
    markdown=True,
    show_members_responses=True,
    store_member_responses=True,
)

# Run the team and capture metrics
run_output = team.run("What is the stock price of NVDA")
pprint_run_response(run_output, markdown=True)

# Analyze team leader message metrics
print("=" * 50)
print("TEAM LEADER MESSAGE METRICS")
print("=" * 50)

if run_output.messages:
    for message in run_output.messages:
        if message.role == "assistant":
            if message.content:
                print(f" Message: {message.content[:100]}...")
            elif message.tool_calls:
                print(f" Tool calls: {message.tool_calls}")

            print("-" * 30, "Metrics", "-" * 30)
            pprint(message.metrics)
            print("-" * 70)

# Analyze aggregated team metrics
print("=" * 50)
print("AGGREGATED TEAM METRICS")
print("=" * 50)
pprint(run_output.metrics)

# Analyze session-level metrics
print("=" * 50)
print("SESSION METRICS")
print("=" * 50)
pprint(team.get_session_metrics(session_id="team_metrics_demo"))

# Analyze individual member metrics
print("=" * 50)
print("TEAM MEMBER MESSAGE METRICS")
print("=" * 50)

if run_output.member_responses:
    for member_response in run_output.member_responses:
        if member_response.messages:
            for message in member_response.messages:
                if message.role == "assistant":
                    if message.content:
                        print(f" Member Message: {message.content[:100]}...")
                    elif message.tool_calls:
                        print(f" Member Tool calls: {message.tool_calls}")

                    print("-" * 20, "Member Metrics", "-" * 20)
                    pprint(message.metrics)
                    print("-" * 60)
```

---

<a name="teams--multimodal--audio_sentiment_analysispy"></a>

### `teams/multimodal/audio_sentiment_analysis.py`

```python
import requests
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.media import Audio
from agno.models.google import Gemini
from agno.team import Team

transcription_agent = Agent(
    name="Audio Transcriber",
    role="Transcribe audio conversations accurately",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Transcribe audio with speaker identification",
        "Maintain conversation structure and flow",
    ],
)

sentiment_analyst = Agent(
    name="Sentiment Analyst",
    role="Analyze emotional tone and sentiment in conversations",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Analyze sentiment for each speaker separately",
        "Identify emotional patterns and conversation dynamics",
        "Provide detailed sentiment insights",
    ],
)

# Create a team for collaborative audio sentiment analysis
sentiment_team = Team(
    name="Audio Sentiment Team",
    members=[transcription_agent, sentiment_analyst],
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Analyze audio sentiment with conversation memory.",
        "Audio Transcriber: First transcribe audio with speaker identification.",
        "Sentiment Analyst: Analyze emotional tone and conversation dynamics.",
    ],
    add_history_to_context=True,
    markdown=True,
    db=SqliteDb(
        session_table="audio_sentiment_team_sessions",
        db_file="tmp/audio_sentiment_team.db",
    ),
)

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

response = requests.get(url)
audio_content = response.content

sentiment_team.print_response(
    "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)

sentiment_team.print_response(
    "What else can you tell me about this audio conversation?",
    stream=True,
)
```

---

<a name="teams--multimodal--audio_to_textpy"></a>

### `teams/multimodal/audio_to_text.py`

```python
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini
from agno.team import Team

transcription_specialist = Agent(
    name="Transcription Specialist",
    role="Convert audio to accurate text transcriptions",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Transcribe audio with high accuracy",
        "Identify speakers clearly as Speaker A, Speaker B, etc.",
        "Maintain conversation flow and context",
    ],
)

content_analyzer = Agent(
    name="Content Analyzer",
    role="Analyze transcribed content for insights",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Analyze transcription for key themes and insights",
        "Provide summaries and extract important information",
    ],
)

# Create a team for collaborative audio-to-text processing
audio_team = Team(
    name="Audio Analysis Team",
    model=Gemini(id="gemini-2.0-flash-exp"),
    members=[transcription_specialist, content_analyzer],
    instructions=[
        "Work together to transcribe and analyze audio content.",
        "Transcription Specialist: First convert audio to accurate text with speaker identification.",
        "Content Analyzer: Analyze transcription for insights and key themes.",
    ],
    markdown=True,
)

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"

response = requests.get(url)
audio_content = response.content

audio_team.print_response(
    "Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)
```

---

<a name="teams--multimodal--generate_image_with_teampy"></a>

### `teams/multimodal/generate_image_with_team.py`

```python
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.dalle import DalleTools
from agno.utils.common import dataclass_to_dict
from rich.pretty import pprint

image_generator = Agent(
    name="Image Creator",
    role="Generate images using DALL-E",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    instructions=[
        "Use the DALL-E tool to create high-quality images",
        "Return image URLs in markdown format: `![description](URL)`",
    ],
)

prompt_engineer = Agent(
    name="Prompt Engineer",
    role="Optimize and enhance image generation prompts",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Enhance user prompts for better image generation results",
        "Consider artistic style, composition, and technical details",
    ],
)

# Create a team for collaborative image generation
image_team = Team(
    name="Image Generation Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[prompt_engineer, image_generator],
    instructions=[
        "Generate high-quality images from user prompts.",
        "Prompt Engineer: First enhance and optimize the user's prompt.",
        "Image Creator: Generate images using the enhanced prompt with DALL-E.",
    ],
    markdown=True,
)

run_stream: Iterator[RunOutputEvent] = image_team.run(
    "Create an image of a yellow siamese cat",
    stream=True,
    stream_intermediate_steps=True,
)
for chunk in run_stream:
    pprint(dataclass_to_dict(chunk, exclude={"messages"}))
    print("---" * 20)
```

---

<a name="teams--multimodal--image_to_image_transformationpy"></a>

### `teams/multimodal/image_to_image_transformation.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.fal import FalTools

style_advisor = Agent(
    name="Style Advisor",
    role="Analyze and recommend artistic styles and transformations",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Analyze the input image and transformation request",
        "Provide style recommendations and enhancement suggestions",
        "Consider artistic elements like composition, lighting, and mood",
    ],
)

image_transformer = Agent(
    name="Image Transformer",
    role="Transform images using AI tools",
    model=OpenAIChat(id="gpt-4o"),
    tools=[FalTools()],
    instructions=[
        "Use the `image_to_image` tool to generate transformed images",
        "Apply the recommended styles and transformations",
        "Return the image URL as provided without markdown conversion",
    ],
)

# Create a team for collaborative image transformation
transformation_team = Team(
    name="Image Transformation Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[style_advisor, image_transformer],
    instructions=[
        "Transform images with artistic style and precision.",
        "Style Advisor: First analyze transformation requirements and recommend styles.",
        "Image Transformer: Apply transformations using AI tools with style guidance.",
    ],
    markdown=True,
)

transformation_team.print_response(
    "a cat dressed as a wizard with a background of a mystic forest. Make it look like 'https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png'",
    stream=True,
)
```

---

<a name="teams--multimodal--image_to_structured_outputpy"></a>

### `teams/multimodal/image_to_structured_output.py`

```python
from typing import List

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.team import Team
from pydantic import BaseModel, Field
from rich.pretty import pprint


class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )


image_analyst = Agent(
    name="Image Analyst",
    role="Analyze visual content and extract key elements",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Analyze images for visual elements, setting, and characters",
        "Focus on details that can inspire creative content",
    ],
)

script_writer = Agent(
    name="Script Writer",
    role="Create structured movie scripts from visual inspiration",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Transform visual analysis into compelling movie concepts",
        "Follow the structured output format precisely",
    ],
)

# Create a team for collaborative structured output generation
movie_team = Team(
    name="Movie Script Team",
    members=[image_analyst, script_writer],
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Create structured movie scripts from visual content.",
        "Image Analyst: First analyze the image for visual elements and context.",
        "Script Writer: Transform analysis into structured movie concepts.",
        "Ensure all output follows the MovieScript schema precisely.",
    ],
    output_schema=MovieScript,
)

response = movie_team.run(
    "Write a movie about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

for event in response:
    pprint(event.content)
```

---

<a name="teams--multimodal--image_to_textpy"></a>

### `teams/multimodal/image_to_text.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.team import Team

image_analyzer = Agent(
    name="Image Analyst",
    role="Analyze and describe images in detail",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Analyze images carefully and provide detailed descriptions",
        "Focus on visual elements, composition, and key details",
    ],
)

creative_writer = Agent(
    name="Creative Writer",
    role="Create engaging stories and narratives",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Transform image descriptions into compelling fiction stories",
        "Use vivid language and creative storytelling techniques",
    ],
)

# Create a team for collaborative image-to-text processing
image_team = Team(
    name="Image Story Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[image_analyzer, creative_writer],
    instructions=[
        "Work together to create compelling fiction stories from images.",
        "Image Analyst: First analyze the image for visual details and context.",
        "Creative Writer: Transform the analysis into engaging fiction narratives.",
        "Ensure the story captures the essence and mood of the image.",
    ],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")
image_team.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)
```

---

<a name="teams--multimodal--media_input_for_toolpy"></a>

### `teams/multimodal/media_input_for_tool.py`

```python
"""
Example showing how team tools can access media (images, videos, audio, files) passed to the team.

This demonstrates:
1. Uploading a PDF file to a team
2. A team tool that can access and process the uploaded file (OCR simulation)
3. The team leader responding directly without delegating to member agents
"""

from typing import Optional, Sequence

from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini
from agno.models.openai import OpenAIChat  # noqa: F401
from agno.team import Team
from agno.tools import Toolkit


class DocumentProcessingTools(Toolkit):
    def __init__(self):
        tools = [
            self.extract_text_from_pdf,
        ]

        super().__init__(name="document_processing_tools", tools=tools)

    def extract_text_from_pdf(self, files: Optional[Sequence[File]] = None) -> str:
        """
        Extract text from uploaded PDF files using OCR.

        This tool can access any files that were passed to the team.
        In a real implementation, you would use a proper OCR service.

        Args:
            files: Files passed to the team (automatically injected)

        Returns:
            Extracted text from the PDF files
        """
        if not files:
            return "No files were uploaded to process."

        print(f"--> Files: {files}")

        extracted_texts = []
        for i, file in enumerate(files):
            if file.content:
                # Simulate OCR processing
                # In reality, you'd use a service like Tesseract, AWS Textract, etc.
                file_size = len(file.content)
                extracted_text = f"""
                    [SIMULATED OCR RESULT FOR FILE {i + 1}]
                    Document processed successfully!
                    File size: {file_size} bytes

                    Sample extracted content:
                    "This is a sample document with important information about quarterly sales figures.
                    Q1 Revenue: $125,000
                    Q2 Revenue: $150,000
                    Q3 Revenue: $175,000

                    The growth trend shows a 20% increase quarter over quarter."
                """
                extracted_texts.append(extracted_text)
            else:
                extracted_texts.append(
                    f"File {i + 1}: Content is empty or inaccessible."
                )

        return "\n\n".join(extracted_texts)


def create_sample_pdf_content() -> bytes:
    """Create a sample PDF-like content for demonstration."""
    # This is just sample binary content - in reality you'd have actual PDF bytes
    sample_content = """
    %PDF-1.4
    Sample PDF content for demonstration
    This would be actual PDF binary data in a real scenario
    """.encode("utf-8")
    return sample_content


def main():
    # Create a simple member agent (required for team, but won't be used)
    member_agent = Agent(
        model=Gemini(id="gemini-2.5-pro"),
        name="Assistant",
        description="A general assistant agent.",
    )

    # Create a team with document processing tools
    team = Team(
        members=[member_agent],
        model=Gemini(id="gemini-2.5-pro"),
        tools=[DocumentProcessingTools()],
        name="Document Processing Team",
        description="A team that can process uploaded documents and analyze their content directly using team tools. You have access to document processing tools that can extract text from PDF files. Use these tools to process any uploaded documents and provide analysis directly without delegating to team members.",
        instructions=[
            "You are a document processing expert who can handle PDF analysis directly.",
            "When files are uploaded, use the extract_text_from_pdf tool to process them.",
            "Analyze the extracted content and provide insights directly in your response.",
            "Do not delegate tasks to team members - handle everything yourself using the available tools.",
        ],
        debug_mode=True,
        send_media_to_model=False,
        store_media=True,
    )

    print("=== Team Media Access Example (No Delegation) ===\n")

    # Example: PDF Processing handled directly by team leader
    print("1. Testing PDF processing handled directly by team leader...")

    # Create sample file content
    pdf_content = create_sample_pdf_content()
    sample_file = File(content=pdf_content)

    response = team.run(
        input="I've uploaded a PDF document. Please extract the text from it and provide a brief analysis of the financial information. Handle this directly using your tools - no need to delegate to team members.",
        files=[sample_file],
        session_id="test_team_files",
    )

    print(f"Team Response: {response.content}")
    print("\n" + "=" * 50 + "\n")


if __name__ == "__main__":
    main()
```

---

<a name="teams--multimodal--video_caption_generationpy"></a>

### `teams/multimodal/video_caption_generation.py`

```python
"""Please install dependencies using:
pip install openai moviepy ffmpeg
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.moviepy_video import MoviePyVideoTools
from agno.tools.openai import OpenAITools

video_processor = Agent(
    name="Video Processor",
    role="Handle video processing and audio extraction",
    model=OpenAIChat(id="gpt-4o"),
    tools=[MoviePyVideoTools(process_video=True, generate_captions=True)],
    instructions=[
        "Extract audio from videos for processing",
        "Handle video file operations efficiently",
    ],
)

caption_generator = Agent(
    name="Caption Generator",
    role="Generate and embed captions in videos",
    model=OpenAIChat(id="gpt-4o"),
    tools=[MoviePyVideoTools(embed_captions=True), OpenAITools()],
    instructions=[
        "Transcribe audio to create accurate captions",
        "Generate SRT format captions with proper timing",
        "Embed captions seamlessly into videos",
    ],
)

# Create a team for collaborative video caption generation
caption_team = Team(
    name="Video Caption Team",
    members=[video_processor, caption_generator],
    model=OpenAIChat(id="gpt-4o"),
    description="Team that generates and embeds captions for videos",
    instructions=[
        "Process videos to generate captions in this sequence:",
        "1. Extract audio from the video using extract_audio",
        "2. Transcribe the audio using transcribe_audio",
        "3. Generate SRT captions using create_srt",
        "4. Embed captions into the video using embed_captions",
    ],
    markdown=True,
)

caption_team.print_response(
    "Generate captions for {video with location} and embed them in the video"
)
```

---

<a name="teams--reasoning--01_reasoning_multi_purpose_teampy"></a>

### `teams/reasoning/01_reasoning_multi_purpose_team.py`

```python
"""
This example demonstrates a team of agents that can answer a variety of questions.

The team uses reasoning tools to reason about the questions and delegate to the appropriate agent.

The team consists of:
- A web agent that can search the web for information
- A finance agent that can get financial data
- A writer agent that can write content
- A calculator agent that can calculate
- A FastAPI assistant that can explain how to write FastAPI code
- A code execution agent that can execute code in a secure E2B sandbox
"""

import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.anthropic import Claude
from agno.models.openai.chat import OpenAIChat
from agno.team.team import Team
from agno.tools.calculator import CalculatorTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.file import FileTools
from agno.tools.github import GithubTools
from agno.tools.knowledge import KnowledgeTools
from agno.tools.pubmed import PubmedTools
from agno.tools.python import PythonTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.lancedb.lance_db import LanceDb
from agno.vectordb.search import SearchType

cwd = Path(__file__).parent.resolve()

# Agent that can search the web for information
web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[DuckDuckGoTools(cache_results=True)],
    instructions=["Always include sources"],
)

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[DuckDuckGoTools(cache_results=True)],
    add_name_to_context=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant information on Reddit.
    """),
)

# Agent that can get financial data
finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[YFinanceTools()],
    instructions=["Use tables to display data"],
)

# Writer agent that can write content
writer_agent = Agent(
    name="Write Agent",
    role="Write content",
    model=Claude(id="claude-3-5-sonnet-latest"),
    description="You are an AI agent that can write content.",
    instructions=[
        "You are a versatile writer who can create content on any topic.",
        "When given a topic, write engaging and informative content in the requested format and style.",
        "If you receive mathematical expressions or calculations from the calculator agent, convert them into clear written text.",
        "Ensure your writing is clear, accurate and tailored to the specific request.",
        "Maintain a natural, engaging tone while being factually precise.",
        "Write something that would be good enough to be published in a newspaper like the New York Times.",
    ],
)

# Writer agent that can write content
medical_agent = Agent(
    name="Medical Agent",
    role="Write content",
    model=Claude(id="claude-3-5-sonnet-latest"),
    description="You are an AI agent that can write content.",
    tools=[PubmedTools()],
    instructions=[
        "You are a medical agent that can answer questions about medical topics.",
    ],
)

# Calculator agent that can calculate
calculator_agent = Agent(
    name="Calculator Agent",
    model=Claude(id="claude-3-5-sonnet-latest"),
    role="Calculate",
    tools=[CalculatorTools()],
)

agno_assist_knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

agno_assist = Agent(
    name="Agno Assist",
    role="You help answer questions about the Agno framework.",
    model=OpenAIChat(id="o3-mini"),
    instructions="Search your knowledge before answering the question. Help me to write working code for Agno Agents.",
    tools=[
        KnowledgeTools(
            knowledge=agno_assist_knowledge, add_instructions=True, add_few_shot=True
        ),
    ],
    add_history_to_context=True,
    add_datetime_to_context=True,
)

github_agent = Agent(
    name="Github Agent",
    role="Do analysis on Github repositories",
    model=OpenAIChat(id="o3-mini"),
    instructions=[
        "Use your tools to answer questions about the repo: agno-agi/agno",
        "Do not create any issues or pull requests unless explicitly asked to do so",
    ],
    tools=[
        GithubTools(
            list_issues=True,
            list_issue_comments=True,
            get_pull_request=True,
            get_issue=True,
            get_pull_request_comments=True,
        )
    ],
)

local_python_agent = Agent(
    name="Local Python Agent",
    role="Run Python code locally",
    model=OpenAIChat(id="o3-mini"),
    instructions=[
        "Use your tools to run Python code locally",
    ],
    tools=[
        FileTools(base_dir=cwd),
        PythonTools(
            base_dir=Path(cwd), list_files=True, run_files=True, uv_pip_install=True
        ),
    ],
)


agent_team = Team(
    name="Multi-Purpose Team",
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[
        ReasoningTools(add_instructions=True, add_few_shot=True),
    ],
    members=[
        web_agent,
        finance_agent,
        writer_agent,
        calculator_agent,
        agno_assist,
        github_agent,
        local_python_agent,
    ],
    instructions=[
        "You are a team of agents that can answer a variety of questions.",
        "You can use your member agents to answer the questions.",
        "You can also answer directly, you don't HAVE to forward the question to a member agent.",
        "Reason about more complex questions before delegating to a member agent.",
        "If the user is only being conversational, don't use any tools, just answer directly.",
    ],
    markdown=True,
    show_members_responses=True,
    share_member_interactions=True,
)

if __name__ == "__main__":
    # Load the knowledge base
    asyncio.run(
        agno_assist_knowledge.add_contents_async(
            url="https://docs.agno.com/llms-full.txt"
        )
    )

    # asyncio.run(agent_team.aprint_response("Hi! What are you capable of doing?"))

    # Python code execution
    # asyncio.run(agent_team.aprint_response(dedent("""What is the right way to implement an Agno Agent that searches Hacker News for good articles?
    #                                        Create a minimal example for me and test it locally to ensure it won't immediately crash.
    #                                        Make save the created code in a file called `./python/hacker_news_agent.py`.
    #                                        Don't mock anything. Use the real information from the Agno documentation."""), stream=True))

    # # Reddit research
    # asyncio.run(agent_team.aprint_response(dedent("""What should I be investing in right now?
    #                                        Find some popular subreddits and do some reseach of your own.
    #                                        Write a detailed report about your findings that could be given to a financial advisor."""), stream=True))

    # Github analysis
    # asyncio.run(agent_team.aprint_response(dedent("""List open pull requests in the agno-agi/agno repository.
    #                                        Find an issue that you think you can resolve and give me the issue number,
    #                                        your suggested solution and some code snippets."""), stream=True))

    # Medical research
    txt_path = Path(__file__).parent.resolve() / "medical_history.txt"
    loaded_txt = open(txt_path, "r", encoding="utf-8").read()
    agent_team.print_response(
        input=dedent(f"""I have a patient with the following medical information:\n {loaded_txt}
                         What is the most likely diagnosis?
                        """),
    )
```

---

<a name="teams--reasoning--02_async_multi_purpose_reasoning_teampy"></a>

### `teams/reasoning/02_async_multi_purpose_reasoning_team.py`

```python
"""
This example demonstrates an async multi-purpose reasoning team.

The team uses reasoning tools to analyze questions and delegate to appropriate
specialist agents asynchronously, showcasing coordination and intelligent task routing.
"""

import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.calculator import CalculatorTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.e2b import E2BTools
from agno.tools.knowledge import KnowledgeTools
from agno.tools.pubmed import PubmedTools
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools
from agno.vectordb.lancedb.lance_db import LanceDb
from agno.vectordb.search import SearchType

cwd = Path(__file__).parent.resolve()

# Web search agent
web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[DuckDuckGoTools(cache_results=True)],
    instructions=["Always include sources"],
)

# Financial data agent
finance_agent = Agent(
    name="Finance Agent",
    model=Claude(id="claude-3-5-sonnet-latest"),
    role="Get financial data",
    tools=[YFinanceTools()],
    instructions=[
        "You are a finance agent that can get financial data about stocks, companies, and the economy.",
        "Always use real-time data when possible.",
    ],
)

# Medical research agent
medical_agent = Agent(
    name="Medical Agent",
    model=Claude(id="claude-3-5-sonnet-latest"),
    role="Medical researcher",
    tools=[PubmedTools()],
    instructions=[
        "You are a medical agent that can answer questions about medical topics.",
        "Always search for recent medical literature and evidence.",
    ],
)

# Calculator agent
calculator_agent = Agent(
    name="Calculator Agent",
    model=Claude(id="claude-3-5-sonnet-latest"),
    role="Perform mathematical calculations",
    tools=[CalculatorTools()],
    instructions=[
        "Perform accurate mathematical calculations.",
        "Show your work step by step.",
    ],
)

# Agno documentation knowledge base
agno_assist_knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

# Agno framework assistant
agno_assist = Agent(
    name="Agno Assist",
    role="Help with Agno framework questions and code",
    model=OpenAIChat(id="o3-mini"),
    instructions="Search your knowledge before answering. Help write working Agno code.",
    tools=[
        KnowledgeTools(
            knowledge=agno_assist_knowledge, add_instructions=True, add_few_shot=True
        ),
    ],
    add_history_to_context=True,
    add_datetime_to_context=True,
)

# Code execution agent
code_agent = Agent(
    name="Code Agent",
    model=Claude(id="claude-3-5-sonnet-latest"),
    role="Execute and test code",
    tools=[E2BTools()],
    instructions=[
        "Execute code safely in the sandbox environment.",
        "Test code thoroughly before providing results.",
        "Provide clear explanations of code execution.",
    ],
)

# Create multi-purpose reasoning team
agent_team = Team(
    name="Multi-Purpose Agent Team",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[ReasoningTools()],  # Enable reasoning capabilities
    members=[
        web_agent,
        finance_agent,
        medical_agent,
        calculator_agent,
        agno_assist,
        code_agent,
    ],
    instructions=[
        "You are a team of agents that can answer a variety of questions.",
        "Use reasoning tools to analyze questions before delegating.",
        "You can answer directly or forward to appropriate specialist agents.",
        "For complex questions, reason about the best approach first.",
        "If the user is just being conversational, respond directly without tools.",
    ],
    markdown=True,
    show_members_responses=True,
    share_member_interactions=True,
)


async def main():
    """Main async function to demonstrate different team capabilities."""

    # Add Agno documentation content
    await agno_assist_knowledge.add_contents_async(
        url="https://docs.agno.com/llms-full.txt"
    )

    # Example interactions:

    # 1. General capability query
    await agent_team.aprint_response(input="Hi! What are you capable of doing?")

    # 2. Technical code question
    # await agent_team.aprint_response(dedent("""
    #     Create a minimal Agno Agent that searches Hacker News for articles.
    #     Test it locally and save it as './python/hacker_news_agent.py'.
    #     Use real Agno documentation, don't mock anything.
    # """), stream=True)

    # 3. Financial research
    # await agent_team.aprint_response(dedent("""
    #     What should I be investing in right now?
    #     Research current market trends and write a detailed report
    #     suitable for a financial advisor.
    # """), stream=True)

    # 4. Medical analysis (using external medical history file)
    # txt_path = Path(__file__).parent.resolve() / "medical_history.txt"
    # if txt_path.exists():
    #     loaded_txt = open(txt_path, "r").read()
    #     await agent_team.aprint_response(
    #         f"Analyze this medical information and suggest a likely diagnosis:\n{loaded_txt}",
    #         stream=True,
    #     )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="teams--search_coordination--01_coordinated_agentic_ragpy"></a>

### `teams/search_coordination/01_coordinated_agentic_rag.py`

```python
"""
This example demonstrates how multiple specialized agents can coordinate to provide
comprehensive RAG (Retrieval-Augmented Generation) responses by dividing search
and analysis tasks across team members.

Team Composition:
- Knowledge Searcher: Searches knowledge base for relevant information
- Content Analyzer: Analyzes and synthesizes retrieved content
- Response Synthesizer: Creates final comprehensive response with sources

Setup:
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy`
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run this script to see coordinated RAG in action
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.cohere import CohereReranker
from agno.models.anthropic import Claude
from agno.team.team import Team
from agno.vectordb.lancedb import LanceDb, SearchType

# Shared knowledge base for the team
knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_team",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

# Add documentation content
knowledge.add_contents(urls=["https://docs.agno.com/introduction/agents.md"])

# Knowledge Searcher Agent - Specialized in finding relevant information
knowledge_searcher = Agent(
    name="Knowledge Searcher",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Search and retrieve relevant information from the knowledge base",
    knowledge=knowledge,
    search_knowledge=True,
    instructions=[
        "You are responsible for searching the knowledge base thoroughly.",
        "Find all relevant information for the user's query.",
        "Provide detailed search results with context and sources.",
        "Focus on comprehensive information retrieval.",
    ],
    markdown=True,
)

# Content Analyzer Agent - Specialized in analyzing retrieved content
content_analyzer = Agent(
    name="Content Analyzer",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Analyze and extract key insights from retrieved content",
    instructions=[
        "Analyze the content provided by the Knowledge Searcher.",
        "Extract key concepts, relationships, and important details.",
        "Identify gaps or areas needing additional clarification.",
        "Organize information logically for synthesis.",
    ],
    markdown=True,
)

# Response Synthesizer Agent - Specialized in creating comprehensive responses
response_synthesizer = Agent(
    name="Response Synthesizer",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Create comprehensive final response with proper citations",
    instructions=[
        "Synthesize information from team members into a comprehensive response.",
        "Include proper source citations and references.",
        "Ensure accuracy and completeness of the final answer.",
        "Structure the response clearly with appropriate formatting.",
    ],
    markdown=True,
)

# Create coordinated RAG team
coordinated_rag_team = Team(
    name="Coordinated RAG Team",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[knowledge_searcher, content_analyzer, response_synthesizer],
    instructions=[
        "Work together to provide comprehensive responses using the knowledge base.",
        "Knowledge Searcher: First search for relevant information thoroughly.",
        "Content Analyzer: Then analyze and organize the retrieved content.",
        "Response Synthesizer: Finally create a well-structured response with sources.",
        "Ensure all responses include proper citations and are factually accurate.",
    ],
    show_members_responses=True,
    markdown=True,
)


def main():
    """Run the coordinated agentic RAG team example."""
    print(" Coordinated Agentic RAG Team Demo")
    print("=" * 50)

    # Example query that benefits from coordinated search and analysis
    query = "What are Agents and how do they work with tools and knowledge?"

    # Run the coordinated team
    coordinated_rag_team.print_response(
        query, stream=True, stream_intermediate_steps=True
    )


if __name__ == "__main__":
    main()
```

---

<a name="teams--search_coordination--02_coordinated_reasoning_ragpy"></a>

### `teams/search_coordination/02_coordinated_reasoning_rag.py`

```python
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses with distributed reasoning capabilities. Each agent
has specific reasoning responsibilities to ensure thorough analysis.

Team Composition:
- Information Gatherer: Searches knowledge base and gathers raw information
- Reasoning Analyst: Applies logical reasoning to analyze gathered information
- Evidence Evaluator: Evaluates evidence quality and identifies gaps
- Response Coordinator: Synthesizes everything into a final reasoned response

Setup:
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy`
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run this script to see coordinated reasoning RAG in action
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.cohere import CohereReranker
from agno.models.anthropic import Claude
from agno.team.team import Team
from agno.tools.reasoning import ReasoningTools
from agno.vectordb.lancedb import LanceDb, SearchType

# Shared knowledge base for the reasoning team
knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_reasoning_team",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

# Information Gatherer Agent - Specialized in comprehensive information retrieval
information_gatherer = Agent(
    name="Information Gatherer",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Gather comprehensive information from knowledge sources",
    knowledge=knowledge,
    search_knowledge=True,
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Search the knowledge base thoroughly for all relevant information.",
        "Use reasoning tools to plan your search strategy.",
        "Gather comprehensive context and supporting details.",
        "Document all sources and evidence found.",
    ],
    markdown=True,
)

# Reasoning Analyst Agent - Specialized in logical analysis
reasoning_analyst = Agent(
    name="Reasoning Analyst",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Apply logical reasoning to analyze gathered information",
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Analyze information using structured reasoning approaches.",
        "Identify logical connections and relationships.",
        "Apply deductive and inductive reasoning where appropriate.",
        "Break down complex topics into logical components.",
        "Use reasoning tools to structure your analysis.",
    ],
    markdown=True,
)

# Evidence Evaluator Agent - Specialized in evidence assessment
evidence_evaluator = Agent(
    name="Evidence Evaluator",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Evaluate evidence quality and identify information gaps",
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Evaluate the quality and reliability of gathered evidence.",
        "Identify gaps in information or reasoning.",
        "Assess the strength of logical connections.",
        "Highlight areas needing additional clarification.",
        "Use reasoning tools to structure your evaluation.",
    ],
    markdown=True,
)

# Response Coordinator Agent - Specialized in synthesis and coordination
response_coordinator = Agent(
    name="Response Coordinator",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Coordinate team findings into comprehensive reasoned response",
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Synthesize all team member contributions into a coherent response.",
        "Ensure logical flow and consistency across the response.",
        "Include proper citations and evidence references.",
        "Present reasoning chains clearly and transparently.",
        "Use reasoning tools to structure the final response.",
    ],
    markdown=True,
)

# Create coordinated reasoning RAG team
coordinated_reasoning_team = Team(
    name="Coordinated Reasoning RAG Team",
    model=Claude(id="claude-sonnet-4-20250514"),
    members=[
        information_gatherer,
        reasoning_analyst,
        evidence_evaluator,
        response_coordinator,
    ],
    instructions=[
        "Work together to provide comprehensive, well-reasoned responses.",
        "Information Gatherer: First search and gather all relevant information.",
        "Reasoning Analyst: Then apply structured reasoning to analyze the information.",
        "Evidence Evaluator: Evaluate the evidence quality and identify any gaps.",
        "Response Coordinator: Finally synthesize everything into a clear, reasoned response.",
        "All agents should use reasoning tools to structure their contributions.",
        "Show your reasoning process transparently in responses.",
    ],
    show_members_responses=True,
    markdown=True,
)


async def async_reasoning_demo():
    """Demonstrate async coordinated reasoning RAG with streaming."""
    print(" Async Coordinated Reasoning RAG Team Demo")
    print("=" * 60)

    query = "What are Agents and how do they work with tools? Explain the reasoning behind their design."

    # Add documentation content
    await knowledge.add_contents_async(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )

    # Run async with streaming and reasoning
    await coordinated_reasoning_team.aprint_response(
        query, stream=True, stream_intermediate_steps=True, show_full_reasoning=True
    )


def sync_reasoning_demo():
    """Demonstrate sync coordinated reasoning RAG."""
    print(" Coordinated Reasoning RAG Team Demo")
    print("=" * 50)

    query = "What are Agents and how do they work with tools? Explain the reasoning behind their design."

    # Add documentation content
    knowledge.add_contents(urls=["https://docs.agno.com/introduction/agents.md"])

    # Run with detailed reasoning output
    coordinated_reasoning_team.print_response(
        query, stream=True, stream_intermediate_steps=True, show_full_reasoning=True
    )


if __name__ == "__main__":
    # Choose which demo to run
    # asyncio.run(async_reasoning_demo())

    sync_reasoning_demo()
```

---

<a name="teams--search_coordination--03_distributed_infinity_searchpy"></a>

### `teams/search_coordination/03_distributed_infinity_search.py`

```python
"""
This example demonstrates how multiple agents coordinate to perform distributed
search using Infinity reranker for high-performance ranking across team members.

Team Composition:
- Primary Searcher: Performs initial broad search with infinity reranking
- Secondary Searcher: Performs targeted search on specific topics
- Cross-Reference Validator: Validates information across different sources
- Result Synthesizer: Combines and ranks all results for final response

Setup:
1. Install dependencies: `pip install agno anthropic infinity-client lancedb`
2. Set up Infinity Server:
   ```bash
   pip install "infinity-emb[all]"
   infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
   ```
3. Export ANTHROPIC_API_KEY
4. Run this script
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.infinity import InfinityReranker
from agno.models.anthropic import Claude
from agno.team.team import Team
from agno.vectordb.lancedb import LanceDb, SearchType

# Knowledge base with Infinity reranker for high performance
knowledge_primary = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_primary",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=InfinityReranker(
            base_url="http://localhost:7997/rerank", model="BAAI/bge-reranker-base"
        ),
    ),
)

# Secondary knowledge base for targeted search
knowledge_secondary = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_secondary",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=InfinityReranker(
            base_url="http://localhost:7997/rerank", model="BAAI/bge-reranker-base"
        ),
    ),
)

# Primary Searcher Agent - Broad search with infinity reranking
primary_searcher = Agent(
    name="Primary Searcher",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Perform comprehensive primary search with high-performance reranking",
    knowledge=knowledge_primary,
    search_knowledge=True,
    instructions=[
        "Conduct broad, comprehensive searches across the knowledge base.",
        "Use the infinity reranker to ensure high-quality result ranking.",
        "Focus on capturing the most relevant information first.",
        "Provide detailed context and multiple perspectives on topics.",
    ],
    markdown=True,
)

# Secondary Searcher Agent - Targeted and specialized search
secondary_searcher = Agent(
    name="Secondary Searcher",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Perform targeted searches on specific topics and edge cases",
    knowledge=knowledge_secondary,
    search_knowledge=True,
    instructions=[
        "Perform targeted searches on specific aspects of the query.",
        "Look for edge cases, technical details, and specialized information.",
        "Use infinity reranking to find the most precise matches.",
        "Focus on details that complement the primary search results.",
    ],
    markdown=True,
)

# Cross-Reference Validator Agent - Validates across sources
cross_reference_validator = Agent(
    name="Cross-Reference Validator",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Validate information consistency across different search results",
    instructions=[
        "Compare and validate information from both searchers.",
        "Identify consistencies and discrepancies in the results.",
        "Highlight areas where information aligns or conflicts.",
        "Assess the reliability of different information sources.",
    ],
    markdown=True,
)

# Result Synthesizer Agent - Combines and ranks all findings
result_synthesizer = Agent(
    name="Result Synthesizer",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Synthesize and rank all search results into comprehensive response",
    instructions=[
        "Combine results from all team members into a unified response.",
        "Rank information based on relevance and reliability.",
        "Ensure comprehensive coverage of the query topic.",
        "Present results with clear source attribution and confidence levels.",
    ],
    markdown=True,
)

# Create distributed search team
distributed_search_team = Team(
    name="Distributed Search Team with Infinity Reranker",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        primary_searcher,
        secondary_searcher,
        cross_reference_validator,
        result_synthesizer,
    ],
    instructions=[
        "Work together to provide comprehensive search results using distributed processing.",
        "Primary Searcher: Conduct broad comprehensive search first.",
        "Secondary Searcher: Perform targeted specialized search.",
        "Cross-Reference Validator: Validate consistency across all results.",
        "Result Synthesizer: Combine everything into a ranked, comprehensive response.",
        "Leverage the infinity reranker for high-performance result ranking.",
        "Ensure all results are properly attributed and ranked by relevance.",
    ],
    show_members_responses=True,
    markdown=True,
)


async def async_distributed_search():
    """Demonstrate async distributed search with infinity reranking."""
    print(" Async Distributed Search with Infinity Reranker Demo")
    print("=" * 65)

    query = "How do Agents work with tools and what are the performance considerations?"

    # Add content to both knowledge bases
    await knowledge_primary.add_contents_async(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )
    await knowledge_secondary.add_contents_async(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )

    # Run async distributed search
    await distributed_search_team.aprint_response(
        query, stream=True, stream_intermediate_steps=True
    )


def sync_distributed_search():
    """Demonstrate sync distributed search with infinity reranking."""
    print(" Distributed Search with Infinity Reranker Demo")
    print("=" * 55)

    query = "How do Agents work with tools and what are the performance considerations?"

    # Add content to both knowledge bases
    knowledge_primary.add_contents(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )
    knowledge_secondary.add_contents(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )

    # Run distributed search
    distributed_search_team.print_response(
        query, stream=True, stream_intermediate_steps=True
    )


if __name__ == "__main__":
    # Choose which demo to run

    try:
        # asyncio.run(async_distributed_search())

        sync_distributed_search()
    except Exception as e:
        print(f" Error: {e}")
        print("\n Make sure Infinity server is running:")
        print("   pip install 'infinity-emb[all]'")
        print("   infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997")
```

---

<a name="teams--session--01_persistent_sessionpy"></a>

### `teams/session/01_persistent_session.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(model=OpenAIChat(id="o3-mini"))

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
)

team.print_response("Tell me a new interesting fact about space")
```

---

<a name="teams--session--02_persistent_session_historypy"></a>

### `teams/session/02_persistent_session_history.py`

```python
"""
This example shows how to use the session history to store the conversation history.
add_history_to_context flag is used to add the history to the messages.
num_history_runs is used to set the number of history runs to add to the messages.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(model=OpenAIChat(id="o3-mini"))

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
    add_history_to_context=True,
    num_history_runs=2,
)

team.print_response("Tell me a new interesting fact about space")
```

---

<a name="teams--session--03_session_summarypy"></a>

### `teams/session/03_session_summary.py`

```python
"""
This example shows how to use the session summary to store the conversation summary.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.session.summary import SessionSummaryManager  # noqa: F401
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

# Method 1: Set enable_session_summaries to True

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
    enable_session_summaries=True,
)

team.print_response("Hi my name is John and I live in New York")
team.print_response("I like to play basketball and hike in the mountains")

# Method 2: Set session_summary_manager

# session_summary_manager = SessionSummaryManager(model=OpenAIChat(id="o3-mini"))

# agent = Agent(
#     model=OpenAIChat(id="o3-mini"),
# )

# team = Team(
#     model=OpenAIChat(id="o3-mini"),
#     members=[agent],
#     db=db,
#     session_summary_manager=session_summary_manager,
# )

# team.print_response("Hi my name is John and I live in New York")
# team.print_response("I like to play basketball and hike in the mountains")
```

---

<a name="teams--session--04_session_summary_referencespy"></a>

### `teams/session/04_session_summary_references.py`

```python
"""
This example shows how to use the `add_session_summary_to_context` parameter in the Team config to
add session summaries to the Team context.

Start the postgres db locally on Docker by running: cookbook/scripts/run_pgvector.sh
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
    enable_session_summaries=True,
)

# This will create a new session summary
team.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
)

# You can use existing session summaries from session storage without creating or updating any new ones.
team = Team(
    model=OpenAIChat(id="o3-mini"),
    db=db,
    session_id="session_summary",
    add_session_summary_to_context=True,
    members=[agent],
)

team.print_response("I also like to play basketball.")

# Alternatively, you can create a new session summary without adding the session summary to context.

# team = Team(
#     model=OpenAIChat(id="o3-mini"),
#     db=db,
#     session_id="session_summary",
#     enable_session_summaries=True,
#     add_session_summary_to_context=False,
# )

# team.print_response("I also like to play basketball.")
```

---

<a name="teams--session--05_chat_historypy"></a>

### `teams/session/05_chat_history.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
)

team.print_response("Tell me a new interesting fact about space")
print(team.get_chat_history())

team.print_response("Tell me a new interesting fact about oceans")
print(team.get_chat_history())
```

---

<a name="teams--session--06_rename_sessionpy"></a>

### `teams/session/06_rename_session.py`

```python
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
)

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
)

team.print_response("Tell me a new interesting fact about space")
team.set_session_name(session_name="Interesting Space Facts")
print(team.get_session_name())

# Autogenerate session name
team.set_session_name(autogenerate=True)
print(team.get_session_name())
```

---

<a name="teams--session--07_in_memory_dbpy"></a>

### `teams/session/07_in_memory_db.py`

```python
"""This example shows how to use an in-memory database with teams.

With this you will be able to store team sessions, user memories, etc. without setting up a database.
Keep in mind that in production setups it is recommended to use a database.
"""

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from rich.pretty import pprint

# Setup the in-memory database
db = InMemoryDb()

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    name="Research Assistant",
)

team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
    # Set add_history_to_context=true to add the previous chat history to the context sent to the Model.
    add_history_to_context=True,
    # Number of historical responses to add to the messages.
    num_history_runs=3,
    session_id="test_session",
)

# -*- Create a run
team.print_response("Share a 2 sentence horror story", stream=True)

# -*- Print the messages in the memory
print("\n" + "=" * 50)
print("CHAT HISTORY AFTER FIRST RUN")
print("=" * 50)
try:
    chat_history = team.get_chat_history(session_id="test_session")
    pprint([m.model_dump(include={"role", "content"}) for m in chat_history])
except Exception as e:
    print(f"Error getting chat history: {e}")
    print("This might be expected on first run with in-memory database")

# -*- Ask a follow up question that continues the conversation
team.print_response("What was my first message?", stream=True)

# -*- Print the messages in the memory
print("\n" + "=" * 50)
print("CHAT HISTORY AFTER SECOND RUN")
print("=" * 50)
try:
    chat_history = team.get_chat_history(session_id="test_session")
    pprint([m.model_dump(include={"role", "content"}) for m in chat_history])
except Exception as e:
    print(f"Error getting chat history: {e}")
    print("This indicates an issue with in-memory database session handling")
```

---

<a name="teams--session--08_cache_sessionpy"></a>

### `teams/session/08_cache_session.py`

```python
"""Example of how to cache the team session in memory for faster access."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team

# Setup the database
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    name="Research Assistant",
)

# Setup the team
team = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[agent],
    db=db,
    session_id="team_session_cache",
    add_history_to_context=True,
    # Activate session caching. The session will be cached in memory for faster access.
    cache_session=True,
)

team.print_response("Tell me a new interesting fact about space")
```

---

<a name="teams--state--agentic_session_statepy"></a>

### `teams/state/agentic_session_state.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team

db = SqliteDb(db_file="tmp/agents.db")
shopping_agent = Agent(
    name="Shopping List Agent",
    role="Manage the shopping list",
    model=OpenAIChat(id="o3-mini"),
    db=db,
    add_session_state_to_context=True,  # Required so the agent is aware of the session state
    enable_agentic_state=True,
)

team = Team(
    members=[shopping_agent],
    session_state={"shopping_list": []},
    db=db,
    add_session_state_to_context=True,  # Required so the team is aware of the session state
    enable_agentic_state=True,
    description="You are a team that manages a shopping list and chores",
    show_members_responses=True,
)


team.print_response("Add milk, eggs, and bread to the shopping list")

team.print_response("I picked up the eggs, now what's on my list?")

print(f"Session state: {team.get_session_state()}")
```

---

<a name="teams--state--change_state_on_runpy"></a>

### `teams/state/change_state_on_run.py`

```python
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team

team = Team(
    db=InMemoryDb(),
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[],
    instructions="Users name is {user_name} and age is {age}",
)

# Sets the session state for the session with the id "user_1_session_1"
team.print_response(
    "What is my name?",
    session_id="user_1_session_1",
    user_id="user_1",
    session_state={"user_name": "John", "age": 30},
)

# Will load the session state from the session with the id "user_1_session_1"
team.print_response("How old am I?", session_id="user_1_session_1", user_id="user_1")

# Sets the session state for the session with the id "user_2_session_1"
team.print_response(
    "What is my name?",
    session_id="user_2_session_1",
    user_id="user_2",
    session_state={"user_name": "Jane", "age": 25},
)

# Will load the session state from the session with the id "user_2_session_1"
team.print_response("How old am I?", session_id="user_2_session_1", user_id="user_2")
```

---

<a name="teams--state--pass_state_to_memberspy"></a>

### `teams/state/pass_state_to_members.py`

```python
from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team

agent = Agent(
    role="User Advisor",
    description="You answer questions related to the user.",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="User's name is {user_name} and age is {age}",
)

team = Team(
    db=InMemoryDb(),
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a team that answers questions related to the user. Delegate to the member agent to address user requests or answer any questions about the user.",
    members=[agent],
    respond_directly=True,
)

# Sets the session state for the session with the id "session_1"
team.print_response(
    "Write a short poem about my name and age",
    session_id="session_1",
    user_id="user_1",
    session_state={"user_name": "John", "age": 30},
    add_session_state_to_context=True,
)

# Will load the session state from the session with the id "session_1"
team.print_response(
    "How old am I?",
    session_id="session_1",
    user_id="user_1",
    add_session_state_to_context=True,
)
```

---

<a name="teams--state--session_state_in_instructionspy"></a>

### `teams/state/session_state_in_instructions.py`

```python
from agno.team.team import Team

team = Team(
    members=[],
    # Initialize the session state with a variable
    session_state={"user_name": "John"},
    instructions="Users name is {user_name}",
    markdown=True,
)

team.print_response("What is my name?", stream=True)
```

---

<a name="teams--state--share_member_interactionspy"></a>

### `teams/state/share_member_interactions.py`

```python
from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

db = SqliteDb(db_file="tmp/agents.db")

web_research_agent = Agent(
    name="Web Research Agent",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
    instructions="You are a web research agent that can answer questions from the web.",
)

report_agent = Agent(
    name="Report Agent",
    model=OpenAIChat(id="o3-mini"),
    instructions="You are a report agent that can write a report from the web research.",
)

team = Team(
    model=OpenAIChat(id="o3-mini"),
    db=db,
    members=[web_research_agent, report_agent],
    share_member_interactions=True,
    instructions=[
        "You are a team of agents that can research the web and write a report.",
        "First, research the web for information about the topic.",
        "Then, use your report agent to write a report from the web research.",
    ],
    show_members_responses=True,
    debug_mode=True,
)

team.print_response("How are LEDs made?")
```

---

<a name="teams--state--team_with_nested_shared_statepy"></a>

### `teams/state/team_with_nested_shared_state.py`

```python
"""
This example demonstrates the nested Team functionality in a hierarchical team structure.
Each team and agent has a clearly defined role that guides their behavior and specialization:

Team Hierarchy & Roles:
 Shopping List Team (Orchestrator)
   Role: "Orchestrate comprehensive shopping list management and meal planning"
    Shopping Management Team (Operations Specialist)
      Role: "Execute precise shopping list operations through delegation"
       Shopping List Agent
          Role: "Maintain and modify the shopping list with precision and accuracy"
    Meal Planning Team (Culinary Expert)
       Role: "Transform shopping list ingredients into creative meal suggestions"
        Recipe Suggester Agent
           Role: "Create innovative and practical recipe suggestions"

"""

from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.team import Team

db = SqliteDb(db_file="tmp/example.db")


# Define tools to manage our shopping list
def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list and return confirmation.

    Args:
        item (str): The item to add to the shopping list.
    """
    # Add the item if it's not already in the list
    if item.lower() not in [i.lower() for i in session_state["shopping_list"]]:
        session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"


def remove_item(session_state, item: str) -> str:
    """Remove an item from the shopping list by name.

    Args:
        item (str): The item to remove from the shopping list.
    """
    # Case-insensitive search
    for i, list_item in enumerate(session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

    return f"'{item}' was not found in the shopping list. Current shopping list: {session_state['shopping_list']}"


def remove_all_items(session_state) -> str:
    """Remove all items from the shopping list."""
    session_state["shopping_list"] = []
    return "All items removed from the shopping list"


shopping_list_agent = Agent(
    name="Shopping List Agent",
    role="Manage the shopping list",
    id="shopping_list_manager",
    model=OpenAIChat(id="o3-mini"),
    tools=[add_item, remove_item, remove_all_items],
    instructions=[
        "Manage the shopping list by adding and removing items",
        "Always confirm when items are added or removed",
        "If the task is done, update the session state to log the changes & chores you've performed",
    ],
)


# Shopping management team - new layer for handling all shopping list modifications
shopping_mgmt_team = Team(
    name="Shopping Management Team",
    role="Execute shopping list operations",
    id="shopping_management",
    model=OpenAIChat(id="o3-mini"),
    members=[shopping_list_agent],
    instructions=[
        "Manage adding and removing items from the shopping list using the Shopping List Agent",
        "Forward requests to add or remove items to the Shopping List Agent",
    ],
)


def get_ingredients(session_state) -> str:
    """Retrieve ingredients from the shopping list to use for recipe suggestions."""
    shopping_list = session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty. Add some ingredients first to get recipe suggestions."

    # Just return the ingredients - the agent will create recipes
    return f"Available ingredients from shopping list: {', '.join(shopping_list)}"


recipe_agent = Agent(
    name="Recipe Suggester",
    id="recipe_suggester",
    role="Suggest recipes based on available ingredients",
    model=OpenAIChat(id="o3-mini"),
    tools=[get_ingredients],
    instructions=[
        "First, use the get_ingredients tool to get the current ingredients from the shopping list",
        "After getting the ingredients, create detailed recipe suggestions based on those ingredients",
        "Create at least 3 different recipe ideas using the available ingredients",
        "For each recipe, include: name, ingredients needed (highlighting which ones are from the shopping list), and brief preparation steps",
        "Be creative but practical with recipe suggestions",
        "Consider common pantry items that people usually have available in addition to shopping list items",
        "Consider dietary preferences if mentioned by the user",
        "If no meal type is specified, suggest a variety of options (breakfast, lunch, dinner, snacks)",
    ],
)


def list_items(session_state) -> str:
    """List all items in the shopping list."""
    shopping_list = session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty."

    items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"


# Create meal planning subteam
meal_planning_team = Team(
    name="Meal Planning Team",
    role="Plan meals based on shopping list items",
    id="meal_planning",
    model=OpenAIChat(id="o3-mini"),
    members=[recipe_agent],
    instructions=[
        "You are a meal planning team that suggests recipes based on shopping list items.",
        "IMPORTANT: When users ask 'What can I make with these ingredients?' or any recipe-related questions, IMMEDIATELY forward the EXACT SAME request to the recipe_agent WITHOUT asking for further information.",
        "DO NOT ask the user for ingredients - the recipe_agent will work with what's already in the shopping list.",
        "Your primary job is to forward recipe requests directly to the recipe_agent without modification.",
    ],
)


def add_chore(session_state, chore: str, priority: str = "medium") -> str:
    """Add a chore to the list with priority level.

    Args:
        chore (str): The chore to add to the list
        priority (str): Priority level of the chore (low, medium, high)

    Returns:
        str: Confirmation message
    """
    # Initialize chores list if it doesn't exist
    if "chores" not in session_state:
        session_state["chores"] = []

    # Validate priority
    valid_priorities = ["low", "medium", "high"]
    if priority.lower() not in valid_priorities:
        priority = "medium"  # Default to medium if invalid

    # Add the chore with timestamp and priority
    from datetime import datetime

    chore_entry = {
        "description": chore,
        "priority": priority.lower(),
        "added_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
    }

    session_state["chores"].append(chore_entry)

    return f"Added chore: '{chore}' with {priority} priority"


# Orchestrates the entire shopping and meal planning ecosystem
shopping_team = Team(
    id="shopping_list_team",
    name="Shopping List Team",
    role="Orchestrate shopping list management and meal planning",
    model=OpenAIChat(id="o3-mini"),
    session_state={"shopping_list": [], "chores": []},
    tools=[list_items, add_chore],
    db=db,
    members=[
        shopping_mgmt_team,
        meal_planning_team,
    ],
    markdown=True,
    instructions=[
        "You are the orchestration layer for a comprehensive shopping and meal planning ecosystem",
        "If you need to add or remove items from the shopping list, forward the full request to the Shopping Management Team",
        "IMPORTANT: If the user asks about recipes or what they can make with ingredients, IMMEDIATELY forward the EXACT request to the meal_planning_team with NO additional questions",
        "Example: When user asks 'What can I make with these ingredients?', you should simply forward this exact request to meal_planning_team without asking for more information",
        "If you need to list the items in the shopping list, use the list_items tool",
        "If the user got something from the shopping list, it means it can be removed from the shopping list",
        "After each completed task, use the add_chore tool to log exactly what was done with high priority",
        "Provide a seamless experience by leveraging your specialized teams for their expertise",
    ],
    show_members_responses=True,
)

# =============================================================================
# DEMONSTRATION
# =============================================================================

# Example 1: Adding items (demonstrates role-based delegation)
print("Example 1: Adding Items to Shopping List")
print("-" * 50)
shopping_team.print_response(
    "Add milk, eggs, and bread to the shopping list", stream=True
)
print(f"Session state: {shopping_team.get_session_state()}")
print()

# Example 2: Item consumption and removal
print("Example 2: Item Consumption & Removal")
print("-" * 50)
shopping_team.print_response("I got bread from the store", stream=True)
print(f"Session state: {shopping_team.get_session_state()}")
print()

# Example 3: Adding more ingredients
print("Example 3: Adding Fresh Ingredients")
print("-" * 50)
shopping_team.print_response(
    "I need apples and oranges for my fruit salad", stream=True
)
print(f"Session state: {shopping_team.get_session_state()}")
print()

# Example 4: Listing current items
print("Example 4: Viewing Current Shopping List")
print("-" * 50)
shopping_team.print_response("What's on my shopping list right now?", stream=True)
print(f"Session state: {shopping_team.get_session_state()}")
print()

# Example 5: Recipe suggestions (demonstrates culinary expertise role)
print("Example 5: Recipe Suggestions from Culinary Team")
print("-" * 50)
shopping_team.print_response("What can I make with these ingredients?", stream=True)
print(f"Session state: {shopping_team.get_session_state()}")
print()

# Example 6: Complete list management
print("Example 6: Complete List Reset & Restart")
print("-" * 50)
shopping_team.print_response(
    "Clear everything from my list and start over with just bananas and yogurt",
    stream=True,
)
print(f"Shared Session state: {shopping_team.get_session_state()}")
print()

# Example 7: Quick recipe check with new ingredients
print("Example 7: Quick Recipe Check with New Ingredients")
print("-" * 50)
shopping_team.print_response("What healthy breakfast can I make now?", stream=True)
print()

print(f"Team Session State: {shopping_team.get_session_state()}")
```

---

<a name="teams--streaming--01_team_streamingpy"></a>

### `teams/streaming/01_team_streaming.py`

```python
"""
This example demonstrates streaming responses from a team.

The team uses specialized agents with financial tools to provide real-time
stock information with streaming output.
"""

from typing import Iterator  # noqa
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools

# Stock price and analyst data agent
stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            include_tools=["get_current_stock_price", "get_analyst_recommendations"],
        )
    ],
)

# Company information agent
company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            include_tools=["get_company_info", "get_company_news"],
        )
    ],
)

# Create team with streaming capabilities
team = Team(
    name="Stock Research Team",
    model=OpenAIChat("o3-mini"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
)

# Test streaming response
team.print_response(
    "What is the current stock price of NVDA?",
    stream=True,
    stream_intermediate_steps=True,
)
```

---

<a name="teams--streaming--02_eventspy"></a>

### `teams/streaming/02_events.py`

```python
import asyncio
from uuid import uuid4

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude

# from agno.models.mistral.mistral import MistralChat
from agno.models.openai.chat import OpenAIChat
from agno.team import Team, TeamRunEvent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

wikipedia_agent = Agent(
    id="hacker-news-agent",
    name="Hacker News Agent",
    role="Search Hacker News for information",
    # model=MistralChat(id="mistral-large-latest"),
    tools=[HackerNewsTools()],
    instructions=[
        "Find articles about the company in the Hacker News",
    ],
)

website_agent = Agent(
    id="website-agent",
    name="Website Agent",
    role="Search the website for information",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Search the website for information",
    ],
)

user_id = str(uuid4())
team_id = str(uuid4())

company_info_team = Team(
    name="Company Info Team",
    id=team_id,
    user_id=user_id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        wikipedia_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)


async def run_team_with_events(prompt: str):
    content_started = False
    async for run_output_event in company_info_team.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_output_event.event in [
            TeamRunEvent.run_started,
            TeamRunEvent.run_completed,
        ]:
            print(f"\nTEAM EVENT: {run_output_event.event}")

        if run_output_event.event in [TeamRunEvent.tool_call_started]:
            print(f"\nTEAM EVENT: {run_output_event.event}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_output_event.tool.tool_args}")

        if run_output_event.event in [TeamRunEvent.tool_call_completed]:
            print(f"\nTEAM EVENT: {run_output_event.event}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL RESULT: {run_output_event.tool.result}")

        # Member events
        if run_output_event.event in [RunEvent.tool_call_started]:
            print(f"\nMEMBER EVENT: {run_output_event.event}")
            print(f"AGENT ID: {run_output_event.agent_id}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_output_event.tool.tool_args}")

        if run_output_event.event in [RunEvent.tool_call_completed]:
            print(f"\nMEMBER EVENT: {run_output_event.event}")
            print(f"AGENT ID: {run_output_event.agent_id}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL RESULT: {run_output_event.tool.result}")

        if run_output_event.event in [TeamRunEvent.run_content]:
            if not content_started:
                print("CONTENT")
                content_started = True
            else:
                print(run_output_event.content, end="")


if __name__ == "__main__":
    asyncio.run(
        run_team_with_events(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        )
    )
```

---

<a name="teams--streaming--03_route_mode_eventspy"></a>

### `teams/streaming/03_route_mode_events.py`

```python
import asyncio

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.team import Team, TeamRunEvent
from agno.tools.yfinance import YFinanceTools

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            include_tools=["get_current_stock_price", "get_analyst_recommendations"],
        )
    ],
)

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            include_tools=["get_company_info", "get_company_news"],
        )
    ],
)

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("o3-mini"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    # If you want to disable the member events, set this to False (default is True)
    # stream_member_events=False
)


async def run_team_with_events(prompt: str):
    content_started = False
    member_content_started = False
    async for run_output_event in team.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        if run_output_event.event in [
            TeamRunEvent.run_started,
            TeamRunEvent.run_completed,
        ]:
            print(f"\nTEAM EVENT: {run_output_event.event}")
        if run_output_event.event in [
            RunEvent.run_started,
            RunEvent.run_completed,
        ]:
            print(f"\nMEMBER RUN EVENT: {run_output_event.event}")

        if run_output_event.event in [TeamRunEvent.tool_call_started]:
            print(f"\nTEAM EVENT: {run_output_event.event}")
            print(f"TEAM TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TEAM TOOL CALL ARGS: {run_output_event.tool.tool_args}")

        if run_output_event.event in [TeamRunEvent.tool_call_completed]:
            print(f"\nTEAM EVENT: {run_output_event.event}")
            print(f"TEAM TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TEAM TOOL CALL RESULT: {run_output_event.tool.result}")

        # Member events
        if run_output_event.event in [RunEvent.tool_call_started]:
            print(f"\nMEMBER EVENT: {run_output_event.event}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_output_event.tool.tool_args}")

        if run_output_event.event in [RunEvent.tool_call_completed]:
            print(f"\nMEMBER EVENT: {run_output_event.event}")
            print(f"MEMBER TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"MEMBER TOOL CALL RESULT: {run_output_event.tool.result}")

        if run_output_event.event in [TeamRunEvent.run_content]:
            if not content_started:
                print("TEAM CONTENT:")
                content_started = True
            print(run_output_event.content, end="")

        if run_output_event.event in [RunEvent.run_content]:
            if not member_content_started:
                print("MEMBER CONTENT:")
                member_content_started = True
            print(run_output_event.content, end="")


if __name__ == "__main__":
    asyncio.run(
        run_team_with_events(
            "What is the current stock price of NVDA?",
        )
    )
```

---

<a name="teams--streaming--04_async_team_streamingpy"></a>

### `teams/streaming/04_async_team_streaming.py`

```python
"""
This example demonstrates asynchronous streaming responses from a team.

The team uses specialized agents with financial tools to provide real-time
stock information with async streaming output.
"""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import apprint_run_response

# Stock price and analyst data agent
stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            include_tools=["get_current_stock_price", "get_analyst_recommendations"],
        )
    ],
)

# Company information agent
company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a company.",
    tools=[
        YFinanceTools(
            include_tools=["get_company_info", "get_company_news"],
        )
    ],
)

# Create team with async streaming capabilities
team = Team(
    name="Stock Research Team",
    model=OpenAIChat("o3-mini"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
)


async def streaming_with_arun():
    """Demonstrate async streaming using arun() method."""
    await apprint_run_response(
        team.arun(input="What is the current stock price of NVDA?", stream=True)
    )


async def streaming_with_aprint_response():
    """Demonstrate async streaming using aprint_response() method."""
    await team.aprint_response("What is the current stock price of NVDA?", stream=True)


if __name__ == "__main__":
    asyncio.run(streaming_with_arun())

    # asyncio.run(streaming_with_aprint_response())
```

---

<a name="teams--streaming--05_async_team_eventspy"></a>

### `teams/streaming/05_async_team_events.py`

```python
"""
This example demonstrates how to handle and monitor team events asynchronously.

Shows how to capture and respond to various events during async team execution,
including tool calls, run states, and content generation events.
"""

import asyncio
from uuid import uuid4

from agno.agent import RunEvent
from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team, TeamRunEvent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

# Hacker News search agent
hacker_news_agent = Agent(
    id="hacker-news-agent",
    name="Hacker News Agent",
    role="Search Hacker News for information",
    tools=[HackerNewsTools()],
    instructions=[
        "Find articles about the company in the Hacker News",
    ],
)

# Web search agent
website_agent = Agent(
    id="website-agent",
    name="Website Agent",
    role="Search the website for information",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Search the website for information",
    ],
)

# Generate unique IDs
user_id = str(uuid4())
team_id = str(uuid4())

# Create team with event monitoring
company_info_team = Team(
    name="Company Info Team",
    id=team_id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        hacker_news_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and Hacker News for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)


async def run_team_with_events(prompt: str):
    """
    Run the team and capture all events for monitoring and debugging.

    This function demonstrates how to handle different types of events:
    - Team-level events (run start/completion, tool calls)
    - Member-level events (agent tool calls)
    - Content generation events
    """
    content_started = False

    async for run_response_event in company_info_team.arun(
        prompt,
        stream=True,
        stream_intermediate_steps=True,
    ):
        # Handle team-level events
        if run_response_event.event in [
            TeamRunEvent.run_started,
            TeamRunEvent.run_completed,
        ]:
            print(f"\n TEAM EVENT: {run_response_event.event}")

        # Handle team tool call events
        if run_response_event.event in [TeamRunEvent.tool_call_started]:
            print(f"\n TEAM TOOL STARTED: {run_response_event.tool.tool_name}")
            print(f"   Args: {run_response_event.tool.tool_args}")

        if run_response_event.event in [TeamRunEvent.tool_call_completed]:
            print(f"\n TEAM TOOL COMPLETED: {run_response_event.tool.tool_name}")
            print(f"   Result: {run_response_event.tool.result}")

        # Handle member-level events
        if run_response_event.event in [RunEvent.tool_call_started]:
            print(f"\n MEMBER TOOL STARTED: {run_response_event.agent_id}")
            print(f"   Tool: {run_response_event.tool.tool_name}")
            print(f"   Args: {run_response_event.tool.tool_args}")

        if run_response_event.event in [RunEvent.tool_call_completed]:
            print(f"\n MEMBER TOOL COMPLETED: {run_response_event.agent_id}")
            print(f"   Tool: {run_response_event.tool.tool_name}")
            print(
                f"   Result: {run_response_event.tool.result[:100]}..."
            )  # Truncate for readability

        # Handle content generation
        if run_response_event.event in [TeamRunEvent.run_content]:
            if not content_started:
                print("\n CONTENT:")
                content_started = True
            else:
                print(run_response_event.content, end="")


if __name__ == "__main__":
    asyncio.run(
        run_team_with_events(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        )
    )
```

---

<a name="teams--structured_input_output--00_pydantic_model_outputpy"></a>

### `teams/structured_input_output/00_pydantic_model_output.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.pprint import pprint_run_response
from pydantic import BaseModel


class StockAnalysis(BaseModel):
    symbol: str
    company_name: str
    analysis: str


class CompanyAnalysis(BaseModel):
    company_name: str
    analysis: str


stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-4o"),
    output_schema=StockAnalysis,
    role="Searches for information on stocks and provides price analysis.",
    tools=[DuckDuckGoTools()],
)

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches for information about companies and recent news.",
    output_schema=CompanyAnalysis,
    tools=[DuckDuckGoTools()],
)


class StockReport(BaseModel):
    symbol: str
    company_name: str
    analysis: str


team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-4o"),
    respond_directly=True,
    members=[stock_searcher, company_info_agent],
    output_schema=StockReport,
    markdown=True,
)

# This should route to the stock_searcher
response = team.run("What is the current stock price of NVDA?")
assert isinstance(response.content, StockReport)
pprint_run_response(response)
```

---

<a name="teams--structured_input_output--01_pydantic_model_as_inputpy"></a>

### `teams/structured_input_output/01_pydantic_model_as_input.py`

```python
"""
This example demonstrates how to use Pydantic models as input to teams.

Shows how structured data can be passed as messages to teams for more
precise and validated input handling.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements."""

    topic: str = Field(description="The main research topic")
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Create specialized Hacker News research agent
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="o3-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
    instructions=[
        "Search Hacker News for relevant articles and discussions",
        "Extract key insights and summarize findings",
        "Focus on high-quality, well-discussed posts",
    ],
)

# Create collaborative research team
team = Team(
    name="Hackernews Research Team",
    model=OpenAIChat(id="o3-mini"),
    members=[hackernews_agent],
    determine_input_for_members=False,  # The member gets the input directly, without the team leader synthesizing it
    instructions=[
        "Conduct thorough research based on the structured input",
        "Address all focus areas mentioned in the research topic",
        "Tailor the research to the specified target audience",
        "Provide the requested number of sources",
    ],
    show_members_responses=True,
)

# Use Pydantic model as structured input
research_request = ResearchTopic(
    topic="AI Agent Frameworks",
    focus_areas=["AI Agents", "Framework Design", "Developer Tools", "Open Source"],
    target_audience="Software Developers and AI Engineers",
    sources_required=7,
)

# Execute research with structured input
team.print_response(input=research_request)

# Alternative example with different topic
alternative_research = ResearchTopic(
    topic="Distributed Systems",
    focus_areas=["Microservices", "Event-Driven Architecture", "Scalability"],
    target_audience="Backend Engineers",
    sources_required=5,
)

team.print_response(input=alternative_research)
```

---

<a name="teams--structured_input_output--02_team_with_parser_modelpy"></a>

### `teams/structured_input_output/02_team_with_parser_model.py`

```python
import random
from typing import Iterator, List  # noqa

from agno.agent import Agent, RunOutput, RunOutputEvent  # noqa
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team import Team
from pydantic import BaseModel, Field
from rich.pretty import pprint


class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )


itinerary_planner = Agent(
    name="Itinerary Planner",
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
)

weather_expert = Agent(
    name="Weather Expert",
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You are a weather expert and can provide detailed weather information for a given location.",
)

national_park_expert = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[itinerary_planner, weather_expert],
    output_schema=NationalParkAdventure,
    parser_model=OpenAIChat(id="o3-mini"),
)

# Get the response in a variable
national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]
# Get the response in a variable
run: RunOutput = national_park_expert.run(
    f"What is the best season to visit {national_parks[random.randint(0, len(national_parks) - 1)]}? Please provide a detailed one week itinerary for a visit to the park."
)
pprint(run.content)

# Stream the response
# run_events: Iterator[RunOutputEvent] = national_park_expert.run(f"What is the best season to visit {national_parks[random.randint(0, len(national_parks) - 1)]}? Please provide a detailed one week itinerary for a visit to the park.", stream=True)
# for event in run_events:
#     pprint(event)
```

---

<a name="teams--structured_input_output--03_team_with_output_modelpy"></a>

### `teams/structured_input_output/03_team_with_output_model.py`

```python
"""
This example shows how to use the output_model parameter to specify the model that should be used to generate the final response.
"""

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

itinerary_planner = Agent(
    name="Itinerary Planner",
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing vacations. Use the tools at your disposal to find latest information about the destination.",
    tools=[DuckDuckGoTools()],
)

travel_expert = Team(
    model=OpenAIChat(id="o3-mini"),
    members=[itinerary_planner],
    output_model=OpenAIChat(id="o3-mini"),
)

travel_expert.print_response("Plan a summer vacation in Paris", stream=True)
```

---

<a name="teams--structured_input_output--04_structured_output_streamingpy"></a>

### `teams/structured_input_output/04_structured_output_streaming.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from pydantic import BaseModel


class StockAnalysis(BaseModel):
    symbol: str
    company_name: str
    analysis: str


stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("o3-mini"),
    output_schema=StockAnalysis,
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            include_tools=["get_current_stock_price", "get_analyst_recommendations"],
        )
    ],
)


class CompanyAnalysis(BaseModel):
    company_name: str
    analysis: str


company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    output_schema=CompanyAnalysis,
    tools=[
        YFinanceTools(
            include_tools=["get_company_info", "get_company_news"],
        )
    ],
)


class StockReport(BaseModel):
    symbol: str
    company_name: str
    analysis: str


team = Team(
    name="Stock Research Team",
    model=OpenAIChat("o3-mini"),
    members=[stock_searcher, company_info_agent],
    output_schema=StockReport,
    markdown=True,
    show_members_responses=True,
)

# Run with streaming and consume the generator to get the final response
stream_generator = team.run(
    "Give me a stock report for NVDA",
    stream=True,
    stream_intermediate_steps=True,
)

# Consume the streaming events and get the final response
run_response = None
for event_or_response in stream_generator:
    # The last item in the stream is the final TeamRunOutput
    run_response = event_or_response

assert isinstance(run_response.content, StockReport)
print(
    f" Response content is correctly typed as StockReport: {type(run_response.content)}"
)
print(f" Stock Symbol: {run_response.content.symbol}")
print(f" Company Name: {run_response.content.company_name}")
```

---

<a name="teams--structured_input_output--05_async_structured_output_streamingpy"></a>

### `teams/structured_input_output/05_async_structured_output_streaming.py`

```python
"""
This example demonstrates async structured output streaming from a team.

The team uses Pydantic models to ensure structured responses while streaming,
providing both real-time output and validated data structures asynchronously.
"""

import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import apprint_run_response
from pydantic import BaseModel


class StockAnalysis(BaseModel):
    """Stock analysis data structure."""

    symbol: str
    company_name: str
    analysis: str


class CompanyAnalysis(BaseModel):
    """Company analysis data structure."""

    company_name: str
    analysis: str


class StockReport(BaseModel):
    """Final stock report data structure."""

    symbol: str
    company_name: str
    analysis: str


# Stock price and analyst data agent with structured output
stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("o3-mini"),
    output_schema=StockAnalysis,
    role="Searches the web for information on a stock.",
    tools=[
        YFinanceTools(
            include_tools=["get_current_stock_price", "get_analyst_recommendations"],
        )
    ],
)

# Company information agent with structured output
company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("o3-mini"),
    role="Searches the web for information on a stock.",
    output_schema=CompanyAnalysis,
    tools=[
        YFinanceTools(
            include_tools=["get_company_info", "get_company_news"],
        )
    ],
)

# Create team with structured output streaming
team = Team(
    name="Stock Research Team",
    model=OpenAIChat("o3-mini"),
    members=[stock_searcher, company_info_agent],
    output_schema=StockReport,
    markdown=True,
    show_members_responses=True,
)


async def test_structured_streaming():
    """Test async structured output streaming."""
    # Run with streaming and consume the async generator to get the final response
    async_stream = team.arun(
        "Give me a stock report for NVDA", stream=True, stream_intermediate_steps=True
    )

    # Consume the async streaming events and get the final response
    run_response = None
    async for event_or_response in async_stream:
        # The last item in the stream is the final TeamRunOutput
        run_response = event_or_response

    assert isinstance(run_response.content, StockReport)
    print(f" Stock Symbol: {run_response.content.symbol}")
    print(f" Company Name: {run_response.content.company_name}")


async def test_structured_streaming_with_arun():
    """Test async structured output streaming using arun() method."""
    await apprint_run_response(
        team.arun(
            input="Give me a stock report for AAPL",
            stream=True,
            stream_intermediate_steps=True,
        )
    )


if __name__ == "__main__":
    asyncio.run(test_structured_streaming())

    asyncio.run(test_structured_streaming_with_arun())
```

---

<a name="teams--structured_input_output--06_input_schema_on_teampy"></a>

### `teams/structured_input_output/06_input_schema_on_team.py`

```python
"""
This example demonstrates how to use input_schema with teams for automatic
input validation and structured data handling.

The input_schema feature allows teams to automatically validate and convert
dictionary inputs into Pydantic models, ensuring type safety and data validation.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchProject(BaseModel):
    """Structured research project with validation requirements."""

    project_name: str = Field(description="Name of the research project")
    research_topics: List[str] = Field(
        description="List of topics to research", min_items=1
    )
    target_audience: str = Field(description="Intended audience for the research")
    depth_level: str = Field(
        description="Research depth level", pattern="^(basic|intermediate|advanced)$"
    )
    max_sources: int = Field(
        description="Maximum number of sources to use", ge=3, le=20, default=10
    )
    include_recent_only: bool = Field(
        description="Whether to focus only on recent sources", default=True
    )


# Create research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat(id="o3-mini"),
    tools=[HackerNewsTools()],
    role="Research trending topics and discussions on HackerNews",
    instructions=[
        "Search for relevant discussions and articles",
        "Focus on high-quality posts with good engagement",
        "Extract key insights and technical details",
    ],
)

web_researcher = Agent(
    name="Web Researcher",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
    role="Conduct comprehensive web research",
    instructions=[
        "Search for authoritative sources and documentation",
        "Find recent articles and blog posts",
        "Gather diverse perspectives on the topics",
    ],
)

# Create team with input_schema for automatic validation
research_team = Team(
    name="Research Team with Input Validation",
    model=OpenAIChat(id="o3-mini"),
    members=[hackernews_agent, web_researcher],
    delegate_task_to_all_members=True,  # We want all members to get the task
    input_schema=ResearchProject,
    instructions=[
        "Conduct thorough research based on the validated input",
        "Coordinate between team members to avoid duplicate work",
        "Ensure research depth matches the specified level",
        "Respect the maximum sources limit",
        "Focus on recent sources if requested",
    ],
)

print("=== Example 1: Valid Dictionary Input (will be auto-validated) ===")
# Pass a dictionary - it will be automatically validated against ResearchProject schema
research_team.print_response(
    input={
        "project_name": "AI Framework Comparison 2024",
        "research_topics": ["LangChain", "CrewAI", "AutoGen", "Agno"],
        "target_audience": "AI Engineers and Developers",
        "depth_level": "intermediate",
        "max_sources": 15,
        "include_recent_only": True,
    }
)

print("\n=== Example 2: Pydantic Model Input (direct pass-through) ===")
# Pass a Pydantic model directly - no additional validation needed
research_request = ResearchProject(
    project_name="Blockchain Development Tools",
    research_topics=["Ethereum", "Solana", "Web3 Libraries"],
    target_audience="Blockchain Developers",
    depth_level="advanced",
    max_sources=12,
    include_recent_only=False,
)

research_team.print_response(input=research_request)
```

---

<a name="teams--team_with_local_agentic_ragpy"></a>

### `teams/team_with_local_agentic_rag.py`

```python
"""
pip install agno
pip install fastembed, qdrant-client
pip install ollama [ollama pull qwen2.5:7b]
pip install pypdf
"""

from agno.agent import Agent
from agno.embedder.fastembed import FastEmbedEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.ollama import Ollama
from agno.team import Team
from agno.vectordb.qdrant import Qdrant

collection_name = "science"
physics_tb = "https://ncert.nic.in/textbook/pdf/keph101.pdf"
chemistry_tb = "https://ncert.nic.in/textbook/pdf/lech101.pdf"

vector_db = Qdrant(
    path="/tmp/qdrant",
    collection=collection_name,
    embedder=FastEmbedEmbedder(),
)

knowledge_base = PDFUrlKnowledgeBase(
    urls=[physics_tb, chemistry_tb],
    vector_db=vector_db,
    num_documents=4,
)
knowledge_base.load()  # once the data is stored, comment this line

physics_agent = Agent(
    name="Physics Agent",
    role="Expert in Physics",
    instructions="Answer questions based on the knowledge base",
    model=Ollama(id="qwen2.5:7b"),
    read_chat_history=True,
    show_tool_calls=True,
    markdown=True,
)

chemistry_agent = Agent(
    name="Chemistry Agent",
    role="Expert in Chemistry",
    instructions="Answer questions based on the knowledge base",
    model=Ollama(id="qwen2.5:7b"),
    read_chat_history=True,
    show_tool_calls=True,
    markdown=True,
)

science_master = Team(
    name="Team with Knowledge",
    members=[physics_agent, chemistry_agent],
    model=Ollama(id="qwen2.5:7b"),
    knowledge=knowledge_base,
    search_knowledge=True,
    show_members_responses=True,
    markdown=True,
)

# science_master.print_response("give dimensional equation for volume, speed and force",stream=True)
science_master.print_response("state Henry's law", stream=True)
```

---

<a name="teams--tools--01_team_with_custom_toolspy"></a>

### `teams/tools/01_team_with_custom_tools.py`

```python
"""
This example demonstrates how to create a team with custom tools.

The team uses custom tools alongside agent tools to answer questions from a knowledge base
and fall back to web search when needed.
"""

from agno.agent import Agent
from agno.team.team import Team
from agno.tools import tool
from agno.tools.duckduckgo import DuckDuckGoTools


@tool()
def answer_from_known_questions(question: str) -> str:
    """Answer a question from a list of known questions

    Args:
        question: The question to answer

    Returns:
        The answer to the question
    """

    # FAQ knowledge base
    faq = {
        "What is the capital of France?": "Paris",
        "What is the capital of Germany?": "Berlin",
        "What is the capital of Italy?": "Rome",
        "What is the capital of Spain?": "Madrid",
        "What is the capital of Portugal?": "Lisbon",
        "What is the capital of Greece?": "Athens",
        "What is the capital of Turkey?": "Ankara",
    }

    # Check if question is in FAQ
    if question in faq:
        return f"From my knowledge base: {faq[question]}"
    else:
        return "I don't have that information in my knowledge base. Try asking the web search agent."


# Create web search agent for fallback
web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    tools=[DuckDuckGoTools()],
    markdown=True,
)

# Create team with custom tool and agent members
team = Team(name="Q & A team", members=[web_agent], tools=[answer_from_known_questions])

# Test the team
team.print_response("What is the capital of France?", stream=True)

# Check if team has session state and display information
print("\n Team Session Info:")
print(f"   Session ID: {team.session_id}")
print(f"   Session State: {team.session_state}")

# Show team capabilities
print("\n Team Tools Available:")
for t in team.tools:
    print(f"   - {t.name}: {t.description}")

print("\n Team Members:")
for member in team.members:
    print(f"   - {member.name}: {member.role}")
```

---

<a name="teams--tools--02_team_with_tool_hookspy"></a>

### `teams/tools/02_team_with_tool_hooks.py`

```python
"""
This example demonstrates how to use tool hooks with teams and agents.

Tool hooks allow you to intercept and monitor tool function calls, providing
logging, timing, and other observability features.
"""

import time
from typing import Any, Callable, Dict
from uuid import uuid4

from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reddit import RedditTools
from agno.utils.log import logger


def logger_hook(function_name: str, function_call: Callable, arguments: Dict[str, Any]):
    """
    Tool hook that logs function calls and measures execution time.

    Args:
        function_name: Name of the function being called
        function_call: The actual function to call
        arguments: Arguments passed to the function

    Returns:
        The result of the function call
    """
    if function_name == "delegate_task_to_member":
        member_id = arguments.get("member_id")
        logger.info(f"Delegating task to member {member_id}")

    # Start timer
    start_time = time.time()
    result = function_call(**arguments)
    # End timer
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"Function {function_name} took {duration:.2f} seconds to execute")
    return result


# Reddit search agent with tool hooks
reddit_agent = Agent(
    name="Reddit Agent",
    id="reddit-agent",
    role="Search reddit for information",
    tools=[RedditTools(cache_results=True)],
    instructions=[
        "Find information about the company on Reddit",
    ],
    tool_hooks=[logger_hook],
)

# Web search agent with tool hooks
website_agent = Agent(
    name="Website Agent",
    id="website-agent",
    role="Search the website for information",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools(cache_results=True)],
    instructions=[
        "Search the website for information",
    ],
    tool_hooks=[logger_hook],
)

# Generate unique user ID
user_id = str(uuid4())

# Create team with tool hooks
company_info_team = Team(
    name="Company Info Team",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        reddit_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
    tool_hooks=[logger_hook],
)

if __name__ == "__main__":
    company_info_team.print_response(
        "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        stream=True,
    )
```

---

<a name="teams--tools--03_async_team_with_toolspy"></a>

### `teams/tools/03_async_team_with_tools.py`

```python
"""
This example demonstrates how to create an async team with various tools for information gathering.

The team uses multiple agents with different tools (Wikipedia, DuckDuckGo, AgentQL) to
gather comprehensive information about a company asynchronously.
"""

import asyncio
from uuid import uuid4

from agno.agent.agent import Agent
from agno.models.anthropic.claude import Claude
from agno.models.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.agentql import AgentQLTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.wikipedia import WikipediaTools

# Wikipedia search agent
wikipedia_agent = Agent(
    name="Wikipedia Agent",
    role="Search wikipedia for information",
    model=MistralChat(id="mistral-large-latest"),
    tools=[WikipediaTools()],
    instructions=[
        "Find information about the company in the wikipedia",
    ],
)

# Web search agent
website_agent = Agent(
    name="Website Agent",
    role="Search the website for information",
    model=OpenAIChat(id="o3-mini"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Search the website for information",
    ],
)

# Define custom AgentQL query for specific data extraction (see https://docs.agentql.com/concepts/query-language)
custom_query = """
{
    title
    text_content[]
}
"""

# Generate unique IDs
user_id = str(uuid4())
team_id = str(uuid4())

# Create the company information gathering team
company_info_team = Team(
    name="Company Info Team",
    id=team_id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[AgentQLTools(agentql_query=custom_query)],
    members=[
        wikipedia_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        company_info_team.aprint_response(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="tools--agentql_toolspy"></a>

### `tools/agentql_tools.py`

```python
"""
AgentQL Tools for scraping websites.

Prerequisites:
- Set the environment variable `AGENTQL_API_KEY` with your AgentQL API key.
  You can obtain the API key from the AgentQL website:
  https://agentql.com/
- Run `playwright install` to install a browser extension for playwright.

AgentQL will open up a browser instance (don't close it) and do scraping on the site.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.agentql import AgentQLTools

# Example 1: Enable specific AgentQL functions
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        AgentQLTools(
            enable_scrape_website=True,
            enable_custom_scrape_website=False,
            agentql_query="your_query_here",
        )
    ],
)

# Example 2: Enable all AgentQL functions
agent_all = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[AgentQLTools(all=True, agentql_query="your_query_here")],
)

# Example 3: Custom query with specific function enabled
custom_query = """
{
    title
    text_content[]
}
"""

custom_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        AgentQLTools(
            enable_scrape_website=True,
            enable_custom_scrape_website=True,
            agentql_query=custom_query,
        )
    ],
)

# Test the agents
agent.print_response(
    "Scrape the main content from https://docs.agno.com/introduction", markdown=True
)
custom_agent.print_response(
    "Extract title and content from https://docs.agno.com/introduction", markdown=True
)
```

---

<a name="tools--airflow_toolspy"></a>

### `tools/airflow_tools.py`

```python
"""
Airflow Tools - DAG Management and Workflow Automation

This example demonstrates how to use AirflowTools for managing Apache Airflow DAGs.
Shows enable_ flag patterns for selective function access.
AirflowTools is a small tool (<6 functions) so it uses enable_ flags.

Run: `pip install apache-airflow` to install the dependencies
"""

from agno.agent import Agent
from agno.tools.airflow import AirflowTools

# Example 1: All functions enabled (default behavior)
agent_full = Agent(
    tools=[AirflowTools(dags_dir="tmp/dags")],  # All functions enabled by default
    description="You are an Airflow specialist with full DAG management capabilities.",
    instructions=[
        "Help users create, read, and manage Airflow DAGs",
        "Ensure DAG files follow Airflow best practices",
        "Provide clear explanations of DAG structure and components",
    ],
    markdown=True,
)

# Example 2: Enable specific functions using enable_ flags
agent_readonly = Agent(
    tools=[
        AirflowTools(
            dags_dir="tmp/dags",
            enable_save_dag=False,  # Disable DAG creation
            enable_read_dag=True,  # Enable DAG reading
        )
    ],
    description="You are an Airflow analyst focused on reading and analyzing existing DAGs.",
    instructions=[
        "Analyze existing DAG files and provide insights",
        "Explain DAG structure and dependencies",
        "Cannot create or modify DAGs, only read them",
    ],
    markdown=True,
)

# Example 3: Enable all functions explicitly
agent_explicit = Agent(
    tools=[
        AirflowTools(
            dags_dir="tmp/dags",
            enable_save_dag=True,
            enable_read_dag=True,
        )
    ],
    description="You are an Airflow developer with explicit permissions for all DAG operations.",
    instructions=[
        "Create and manage Airflow DAGs with best practices",
        "Read existing DAGs to understand current workflows",
        "Provide comprehensive DAG analysis and recommendations",
    ],
    markdown=True,
)

# Example 4: Using the 'all=True' pattern
agent_all = Agent(
    tools=[AirflowTools(dags_dir="tmp/dags", all=True)],  # Enable all functions
    description="You are a comprehensive Airflow manager with all capabilities enabled.",
    instructions=[
        "Manage complete Airflow workflows and DAG lifecycle",
        "Create, read, and analyze DAGs as needed",
        "Provide end-to-end Airflow development support",
    ],
    markdown=True,
)

# Use the full agent for the main example
agent = agent_full


dag_content = """
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Using 'schedule' instead of deprecated 'schedule_interval'
with DAG(
    'example_dag',
    default_args=default_args,
    description='A simple example DAG',
    schedule='@daily',  # Changed from schedule_interval
    catchup=False
) as dag:

    def print_hello():
        print("Hello from Airflow!")
        return "Hello task completed"

    task = PythonOperator(
        task_id='hello_task',
        python_callable=print_hello,
        dag=dag,
    )
"""

agent.run(f"Save this DAG file as 'example_dag.py': {dag_content}")


agent.print_response("Read the contents of 'example_dag.py'")
```

---

<a name="tools--apify_toolspy"></a>

### `tools/apify_tools.py`

```python
from agno.agent import Agent
from agno.tools.apify import ApifyTools

# Apify Tools Demonstration Script
"""
This script showcases the power of web scraping and data extraction using Apify's Actors (serverless tools). 
The Apify ecosystem has 4000+ pre-built Actors for almost any web data extraction need!

---
Configuration Instructions:
1. Install required dependencies:
   pip install agno langchain-apify apify-client

2. Set the APIFY_API_TOKEN environment variable:
   Add a .env file with APIFY_API_TOKEN=your_apify_api_token
---

Tip: Check out the Apify Store (https://apify.com/store) to find tools for almost any web scraping or data extraction task.
"""

# Create an Apify Tools agent with versatile capabilities
agent = Agent(
    name="Web Insights Explorer",
    instructions=[
        "You are a sophisticated web research assistant capable of extracting insights from various online sources. "
        "Use the available tools for your tasks to gather accurate, well-structured information."
    ],
    tools=[
        ApifyTools(
            actors=[
                "apify/rag-web-browser",
                "compass/crawler-google-places",
                "clockworks/free-tiktok-scraper",
            ]
        )
    ],
    markdown=True,
)


def demonstrate_tools():
    print("Apify Tools Exploration ")

    # RAG Web Search Demonstrations
    print("\n1.1  RAG Web Search Scenarios:")
    prompt = "Research the latest AI ethics guidelines from top tech companies. Compile a summary from at least 3 different sources comparing their approaches using RAG Web Browser."
    agent.print_response(prompt, show_full_reasoning=True)

    print("\n1.2  RAG Web Search Scenarios:")
    prompt = "Carefully extract the key introduction details from https://docs.agno.com/introduction"  #  Extract content from specific website
    agent.print_response(prompt)

    # Google Places Demonstration
    print("\n2. Google Places Crawler:")
    prompt = "Find the top 5 highest-rated coffee shops in San Francisco with detailed information about each location"
    agent.print_response(prompt)

    # Tiktok Scraper Demonstration
    print("\n3. Tiktok Profile Analysis:")
    prompt = "Analyze two profiles on Tiktok that lately added #AI (hashtag AI), extracting their statistics and recent content trends"
    agent.print_response(prompt)


if __name__ == "__main__":
    demonstrate_tools()

"""
Want to add a new tool? It's easy!
- Browse Apify Store
- Find an Actor that matches your needs
- Add a new method to ApifyTools following the existing pattern
- Register the method in the __init__

Examples of potential tools:
- YouTube video info scraper
- Twitter/X profile analyzer
- Product price trackers
- Job board crawlers
- News article extractors
- And SO MUCH MORE!
"""
```

---

<a name="tools--arxiv_toolspy"></a>

### `tools/arxiv_tools.py`

```python
"""
ArXiv Tools - Academic Paper Search and Research

This example demonstrates how to use ArxivTools for searching academic papers.
Shows enable_ flag patterns for selective function access.
ArxivTools is a small tool (<6 functions) so it uses enable_ flags.

Run: `pip install arxiv` to install the dependencies
"""

from agno.agent import Agent
from agno.tools.arxiv import ArxivTools

# Example 1: All functions enabled (default behavior)
agent_full = Agent(
    tools=[ArxivTools()],  # All functions enabled by default
    description="You are a research assistant with full ArXiv search capabilities.",
    instructions=[
        "Help users find and analyze academic papers from ArXiv",
        "Provide detailed paper summaries and insights",
        "Support comprehensive literature reviews",
    ],
    markdown=True,
)

# Example 2: Enable specific search functions
agent_search_only = Agent(
    tools=[
        ArxivTools(
            enable_search_arxiv=True,
            enable_read_arxiv_papers=False,  # Disable detailed paper analysis
        )
    ],
    description="You are a research search specialist focused on finding relevant papers.",
    instructions=[
        "Search for academic papers based on keywords and topics",
        "Provide basic paper information and abstracts",
        "Focus on broad literature discovery",
    ],
    markdown=True,
)

# Example 3: Enable all functions using the 'all=True' pattern
agent_comprehensive = Agent(
    tools=[ArxivTools(all=True)],  # Enable all functions explicitly
    description="You are a comprehensive research assistant for academic literature analysis.",
    instructions=[
        "Perform detailed academic research using all ArXiv capabilities",
        "Provide in-depth paper analysis and cross-references",
        "Support advanced research methodologies",
    ],
    markdown=True,
)

# Example 4: Custom configuration for specific research needs
agent_focused = Agent(
    tools=[
        ArxivTools(
            enable_search_arxiv=True,
            enable_read_arxiv_papers=True,
            # Add other enable_ flags as needed based on available functions
        )
    ],
    description="You are a focused research assistant for specific academic domains.",
    instructions=[
        "Conduct targeted searches in specific academic fields",
        "Provide detailed analysis of relevant papers",
        "Maintain focus on research objectives",
    ],
    markdown=True,
)

# Basic search example
print("=== ArXiv Paper Search Example ===")
agent_full.print_response("Search arxiv for 'language models'", markdown=True)

print("\n=== Focused Research Example ===")
agent_focused.print_response(
    "Find recent papers on 'transformer architectures' and provide detailed analysis",
    markdown=True,
)

print("\n=== Search-Only Example ===")
agent_search_only.print_response(
    "Search for papers related to 'machine learning interpretability'", markdown=True
)
```

---

<a name="tools--async--groq-demopy"></a>

### `tools/async/groq-demo.py`

```python
import asyncio
import time

from agno.agent import Agent
from agno.models.groq import Groq
from agno.utils.log import logger

#####################################
# Async execution
#####################################


async def atask1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


async def atask2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


async def atask3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


async_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[atask2, atask1, atask3],
    markdown=True,
)

# Non-streaming response
# asyncio.run(async_agent.aprint_response("Please run all tasks with a delay of 3s"))
# Streaming response
asyncio.run(
    async_agent.aprint_response("Please run all tasks with a delay of 3s", stream=True)
)


#####################################
# Sync execution
#####################################
def task1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


def task2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


def task3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


sync_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[task2, task1, task3],
    markdown=True,
)

# Non-streaming response
# sync_agent.print_response("Please run all tasks with a delay of 3s")
# Streaming response
sync_agent.print_response("Please run all tasks with a delay of 3s", stream=True)
```

---

<a name="tools--async--openai-demopy"></a>

### `tools/async/openai-demo.py`

```python
import asyncio
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.log import logger

#####################################
# Async execution
#####################################


async def atask1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


async def atask2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


async def atask3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        await asyncio.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


async_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[atask2, atask1, atask3],
    markdown=True,
)

# Non-streaming response
# asyncio.run(async_agent.aprint_response("Please run all tasks with a delay of 3s"))
# Streaming response
asyncio.run(
    async_agent.aprint_response("Please run all tasks with a delay of 3s", stream=True)
)


#####################################
# Sync execution
#####################################
def task1(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 1 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 1 has slept for 1s")
    logger.info("Task 1 has completed")
    return f"Task 1 completed in {delay:.2f}s"


def task2(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 2 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 2 has slept for 1s")
    logger.info("Task 2 has completed")
    return f"Task 2 completed in {delay:.2f}s"


def task3(delay: int):
    """Simulate a task that takes a random amount of time to complete
    Args:
        delay (int): The amount of time to delay the task
    """
    logger.info("Task 3 has started")
    for _ in range(delay):
        time.sleep(1)
        logger.info("Task 3 has slept for 1s")
    logger.info("Task 3 has completed")
    return f"Task 3 completed in {delay:.2f}s"


sync_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[task2, task1, task3],
    markdown=True,
)

# Non-streaming response
# sync_agent.print_response("Please run all tasks with a delay of 3s")
# Streaming response
sync_agent.print_response("Please run all tasks with a delay of 3s", stream=True)
```

---

<a name="tools--aws_lambda_toolspy"></a>

### `tools/aws_lambda_tools.py`

```python
"""
AWS Lambda Tools - Serverless Function Management

This example demonstrates how to use AWSLambdaTools for AWS Lambda operations.
Shows enable_ flag patterns for selective function access.
AWSLambdaTools is a small tool (<6 functions) so it uses enable_ flags.

Prerequisites:
- Run: `pip install boto3` to install dependencies
- Set up AWS credentials (AWS CLI, environment variables, or IAM roles)
- Ensure proper IAM permissions for Lambda operations
"""

from agno.agent import Agent
from agno.tools.aws_lambda import AWSLambdaTools

# Example 1: All functions enabled (default behavior)
agent_full = Agent(
    tools=[AWSLambdaTools(region_name="us-east-1")],  # All functions enabled
    name="Full AWS Lambda Agent",
    description="You are a comprehensive AWS Lambda specialist with all serverless capabilities.",
    instructions=[
        "Help users with all AWS Lambda operations including listing, invoking, and managing functions",
        "Provide clear explanations of Lambda operations and results",
        "Ensure proper error handling for AWS operations",
        "Format responses clearly using markdown",
    ],
    markdown=True,
)

# Example 2: Enable only function listing and invocation
agent_basic = Agent(
    tools=[
        AWSLambdaTools(
            region_name="us-east-1",
            enable_list_functions=True,
            enable_invoke_function=True,
            enable_create_function=False,  # Disable function creation
            enable_update_function=False,  # Disable function updates
        )
    ],
    name="Lambda Reader Agent",
    description="You are an AWS Lambda specialist focused on reading and invoking existing functions.",
    instructions=[
        "List and invoke existing Lambda functions",
        "Cannot create or modify Lambda functions",
        "Provide insights about function execution and performance",
        "Focus on function monitoring and execution",
    ],
    markdown=True,
)

# Example 3: Enable all functions using 'all=True' pattern
agent_comprehensive = Agent(
    tools=[AWSLambdaTools(region_name="us-east-1", all=True)],
    name="Comprehensive Lambda Agent",
    description="You are a full-featured AWS Lambda manager with all capabilities enabled.",
    instructions=[
        "Manage complete AWS Lambda lifecycle including creation, updates, and deployments",
        "Provide comprehensive serverless architecture guidance",
        "Support advanced Lambda configurations and optimizations",
        "Handle complex serverless workflows and integrations",
    ],
    markdown=True,
)

# Example 4: Invoke-only agent for testing
agent_tester = Agent(
    tools=[
        AWSLambdaTools(
            region_name="us-east-1",
            enable_list_functions=True,  # Enable listing for reference
            enable_invoke_function=True,  # Enable function testing
            enable_create_function=False,  # Disable creation
            enable_delete_function=False,  # Disable deletion (safety)
        )
    ],
    name="Lambda Tester Agent",
    description="You are an AWS Lambda testing specialist focused on safe function execution.",
    instructions=[
        "Test and validate Lambda function execution",
        "Cannot create or delete functions for safety",
        "Provide detailed execution results and performance metrics",
        "Focus on function testing and validation workflows",
    ],
    markdown=True,
)

# Example usage
print("=== Basic Lambda Operations Example ===")
agent_basic.print_response(
    "List all Lambda functions in our AWS account", markdown=True
)

print("\n=== Function Testing Example ===")
agent_tester.print_response(
    "Invoke the 'hello-world' Lambda function with an empty payload and analyze the results",
    markdown=True,
)

print("\n=== Comprehensive Management Example ===")
agent_comprehensive.print_response(
    "Provide an overview of our Lambda environment including function count, runtimes, and recent activity",
    markdown=True,
)

# Note: Make sure you have the necessary AWS credentials set up in your environment
# or use AWS CLI's configure command to set them up before running this script.
```

---

<a name="tools--aws_ses_toolspy"></a>

### `tools/aws_ses_tools.py`

```python
"""
AWS SES (Simple Email Service) Setup Instructions:

1. Go to AWS SES Console and verify your domain or email address:
   - For production:
     a. Go to AWS SES Console > Verified Identities > Create Identity
     b. Choose "Domain" and follow DNS verification steps
     c. Add DKIM and SPF records to your domain's DNS
   - For testing:
     a. Choose "Email Address" verification
     b. Click verification link sent to your email

2. Configure AWS Credentials:
   a. Create an IAM user:
      - Go to IAM Console > Users > Add User
      - Enable "Programmatic access"
      - Attach 'AmazonSESFullAccess' policy

   b. Set up credentials (choose one method):
      Method 1 - Using AWS CLI:
      ```
      aws configure
      # Enter your AWS Access Key ID
      # Enter your AWS Secret Access Key
      # Enter your default region
      ```

      Method 2 - Set environment variables:
      ```
      export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY
      export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
      ```

3. Install required Python packages:
   ```
   pip install boto3 agno
   ```

4. Update the variables below with your configuration:
   - sender_email: Your verified sender email address
   - sender_name: Display name that appears in email clients
   - region_name: AWS region where SES is set up (e.g., 'us-east-1', 'ap-south-1')

"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.aws_ses import AWSSESTool
from agno.tools.duckduckgo import DuckDuckGoTools

# Configure email settings
sender_email = "coolmusta@gmail.com"  # Your verified SES email
sender_name = "AI Research Updates"
region_name = "us-west-2"  # Your AWS region

# Create an agent that can research and send personalized email updates
agent = Agent(
    name="Research Newsletter Agent",
    model=OpenAIChat(id="gpt-4o"),
    description="""You are an AI research specialist who creates and sends personalized email 
    newsletters about the latest developments in artificial intelligence and technology.""",
    instructions=[
        "When given a prompt:",
        "1. Extract the recipient's email address carefully. Look for the complete email in format 'user@domain.com'.",
        "2. Research the latest AI developments using DuckDuckGo",
        "3. Compose a concise, engaging email with:",
        "   - A compelling subject line",
        "   - 3-4 key developments or news items",
        "   - Brief explanations of why they matter",
        "   - Links to sources",
        "4. Format the content in a clean, readable way",
        "5. Send the email using AWS SES. IMPORTANT: The receiver_email parameter must be the COMPLETE email address including the @ symbol and domain (e.g., if the user says 'send to mustafa@agno.com', you must use receiver_email='mustafa@agno.com', NOT 'mustafacom' or any other variation).",
    ],
    tools=[
        AWSSESTool(
            sender_email=sender_email, sender_name=sender_name, region_name=region_name
        ),
        DuckDuckGoTools(),
    ],
    markdown=True,
)

# Example 1: Send an email
agent.print_response(
    "Research AI developments in healthcare from the past week with a focus on practical applications in clinical settings. Send the summary via email to mustafa@agno.com"
)

"""
Troubleshooting:
- If emails aren't sending, check:
  * Both sender and recipient are verified (in sandbox mode)
  * AWS credentials are correctly configured
  * You're within sending limits
  * Your IAM user has correct SES permissions
- Use SES Console's 'Send Test Email' feature to verify setup
"""
```

---

<a name="tools--baidusearch_toolspy"></a>

### `tools/baidusearch_tools.py`

```python
from agno.agent import Agent
from agno.tools.baidusearch import BaiduSearchTools

agent = Agent(
    tools=[BaiduSearchTools()],
    description="You are a search agent that helps users find the most relevant information using Baidu.",
    instructions=[
        "Given a topic by the user, respond with the 3 most relevant search results about that topic.",
        "Search for 5 results and select the top 3 unique items.",
        "Search in both English and Chinese.",
    ],
)
agent.print_response("What are the latest advancements in AI?", markdown=True)
```

---

<a name="tools--bitbucket_toolspy"></a>

### `tools/bitbucket_tools.py`

```python
"""
Setup:
1. Generate an App Password:
   - Go to "Personal Bitbucket settings" -> "App passwords"
   - Create a new App password with the appropriate permissions

2. Set environment variables:
   - BITBUCKET_USERNAME: Your Bitbucket username
   - BITBUCKET_PASSWORD: Your generated App password
"""

from agno.agent import Agent
from agno.tools.bitbucket import BitbucketTools

repo_slug = "ai"
workspace = "MaximMFP"

agent = Agent(
    tools=[BitbucketTools(workspace=workspace, repo_slug=repo_slug)],
)

agent.print_response("List open pull requests", markdown=True)

# Example 1: Get specific pull request details
# agent.print_response("Get details of pull request #23", markdown=True)

# Example 2: Get the repo details
# agent.print_response("Get details of the repository", markdown=True)

# Example 3: List repositories
# agent.print_response("List 5 repositories for this workspace", markdown=True)

# Example 4: List commits
# agent.print_response("List the last 20 commits", markdown=True)
```

---

<a name="tools--brandfetch_toolspy"></a>

### `tools/brandfetch_tools.py`

```python
"""
You can use the Brandfetch API to retrieve the company's brand information.

Register an account at: https://developers.brandfetch.com/register

For the Brand API, you can use the `brand` parameter to True. (default is True)
For the Brand Search API, you can use the `search` parameter to True. (default is False)

-- Brand API

Export your API key as an environment variable:
export BRANDFETCH_API_KEY=your_api_key

-- Brand Search API

Export your Client ID as an environment variable:
export BRANDFETCH_CLIENT_KEY=your_client_id

You can find it on https://developers.brandfetch.com/dashboard/brand-search-api in the provided URL after `c=...`

"""

import asyncio

from agno.agent import Agent
from agno.tools.brandfetch import BrandfetchTools

# Brand API

# agent = Agent(
#     tools=[BrandfetchTools()],
#     description="You are a Brand research agent. Given a company name or company domain, you will use the Brandfetch API to retrieve the company's brand information.",
# )
# agent.print_response("What is the brand information of Google?", markdown=True)


# Brand Search API

agent = Agent(
    tools=[BrandfetchTools(async_tools=True)],
    description="You are a Brand research agent. Given a company name or company domain, you will use the Brandfetch API to retrieve the company's brand information.",
)
asyncio.run(
    agent.aprint_response("What is the brand information of Agno?", markdown=True)
)
```

---

<a name="tools--bravesearch_toolspy"></a>

### `tools/bravesearch_tools.py`

```python
from agno.agent import Agent
from agno.tools.bravesearch import BraveSearchTools

# Example 1: Enable specific Brave Search functions
agent = Agent(
    tools=[BraveSearchTools(enable_brave_search=True)],
    description="You are a news agent that helps users find the latest news.",
    instructions=[
        "Given a topic by the user, respond with 4 latest news items about that topic."
    ],
)

# Example 2: Enable all Brave Search functions
agent_all = Agent(
    tools=[BraveSearchTools(all=True)],
    description="You are a comprehensive search agent with full Brave Search capabilities.",
    instructions=[
        "Use Brave Search to find accurate, privacy-focused search results.",
        "Provide relevant and up-to-date information on any topic.",
    ],
)
agent.print_response("AI Agents", markdown=True)
```

---

<a name="tools--brightdata_toolspy"></a>

### `tools/brightdata_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.brightdata import BrightDataTools

# Example 1: Include specific BrightData functions for web scraping
scraping_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        BrightDataTools(include_tools=["web_scraper", "serp_google", "serp_amazon"])
    ],
    markdown=True,
)

# Example 2: Exclude screenshot functions for performance
no_screenshot_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[BrightDataTools(exclude_tools=["screenshot_generator"])],
    markdown=True,
)

# Example 3: Full BrightData functionality (default)
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[BrightDataTools()],
    markdown=True,
)

# Example 1: Scrape a webpage as Markdown
agent.print_response(
    "Scrape this webpage as markdown: https://docs.agno.com/introduction",
)

# Example 2: Take a screenshot of a webpage
# agent.print_response(
#     "Take a screenshot of this webpage: https://docs.agno.com/introduction",
# )

# response = agent.run_response
# if response.images:
#     save_base64_data(response.images[0].content, "tmp/agno_screenshot.png")

# Add a new SERP API zone: https://brightdata.com/cp/zones/new
# Example 3: Search using Google
# agent.print_response(
#     "Search Google for 'Python web scraping best practices' and give me the top 5 results",
# )

# Example 4: Get structured data from Amazon product
# agent.print_response(
#     "Get detailed product information from this Amazon product: https://www.amazon.com/dp/B0D2Q9397Y?th=1&psc=1",
# )

# Example 5: Get LinkedIn profile data
# agent.print_response(
#     "Search for Satya Nadella on LinkedIn and give me a summary of his profile"
# )
```

---

<a name="tools--browserbase_toolspy"></a>

### `tools/browserbase_tools.py`

```python
from agno.agent import Agent
from agno.tools.browserbase import BrowserbaseTools

# Browserbase Configuration
# -------------------------------
# These environment variables are required for the BrowserbaseTools to function properly.
# You can set them in your .env file or export them directly in your terminal.

# BROWSERBASE_API_KEY: Your API key from Browserbase dashboard
#   - Required for authentication
#   - Format: Starts with "bb_live_" or "bb_test_" followed by a unique string

# BROWSERBASE_PROJECT_ID: The project ID from your Browserbase dashboard
#   - Required to identify which project to use for browser sessions
#   - Format: UUID string (8-4-4-4-12 format)

# BROWSERBASE_BASE_URL: The Browserbase API endpoint
#   - Optional: Defaults to https://api.browserbase.com if not specified
#   - Only change this if you're using a custom API endpoint or proxy

agent = Agent(
    name="Web Automation Assistant",
    tools=[BrowserbaseTools()],
    instructions=[
        "You are a web automation assistant that can help with:",
        "1. Capturing screenshots of websites",
        "2. Extracting content from web pages",
        "3. Monitoring website changes",
        "4. Taking visual snapshots of responsive layouts",
        "5. Automated web testing and verification",
    ],
    markdown=True,
)

# Content Extraction and SS
# agent.print_response("""
#     Go to https://news.ycombinator.com and extract:
#     1. The page title
#     2. Take a screenshot of the top stories section
# """)

agent.print_response("""
    Visit https://quotes.toscrape.com and:
    1. Extract the first 5 quotes and their authors
    2. Navigate to page 2
    3. Extract the first 5 quotes from page 2
""")
```

---

<a name="tools--calcom_toolspy"></a>

### `tools/calcom_tools.py`

```python
from datetime import datetime

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.calcom import CalComTools

"""
Example showing how to use the Cal.com Tools with Agno.

Requirements:
- Cal.com API key (get from cal.com/settings/developer/api-keys)
- Event Type ID from Cal.com
- pip install requests pytz

Usage:
- Set the following environment variables:
    export CALCOM_API_KEY="your_api_key"
    export CALCOM_EVENT_TYPE_ID="your_event_type_id"

- Or provide them when creating the CalComTools instance
"""

INSTRUCTONS = f"""You're scheduing assistant. Today is {datetime.now()}.
You can help users by:
    - Finding available time slots
    - Creating new bookings
    - Managing existing bookings (view, reschedule, cancel)
    - Getting booking details
    - IMPORTANT: In case of rescheduling or cancelling booking, call the get_upcoming_bookings function to get the booking uid. check available slots before making a booking for given time
    Always confirm important details before making bookings or changes.
"""


# Example 1: Include specific Cal.com functions for booking management
booking_agent = Agent(
    name="Booking Assistant",
    instructions=[INSTRUCTONS],
    model=OpenAIChat(id="gpt-4"),
    tools=[
        CalComTools(
            user_timezone="America/New_York",
        )
    ],
    markdown=True,
)

# Example 2: Exclude cancellation functions for safety
safe_calendar_agent = Agent(
    name="Safe Calendar Assistant",
    instructions=[INSTRUCTONS],
    model=OpenAIChat(id="gpt-4"),
    tools=[
        CalComTools(
            user_timezone="America/New_York",
        )
    ],
    markdown=True,
)

# Example 3: Full Cal.com functionality (default)
agent = Agent(
    name="Full Calendar Assistant",
    instructions=[INSTRUCTONS],
    model=OpenAIChat(id="gpt-4"),
    tools=[CalComTools(user_timezone="America/New_York")],
    markdown=True,
)

# Example usage
agent.print_response("What are my bookings for tomorrow?")
```

---

<a name="tools--calculator_toolspy"></a>

### `tools/calculator_tools.py`

```python
from agno.agent import Agent
from agno.tools.calculator import CalculatorTools

# Example 1: Include specific calculator functions for basic operations
basic_calc_agent = Agent(
    tools=[CalculatorTools(include_tools=["add", "subtract", "multiply", "divide"])],
    markdown=True,
)

# Example 2: Exclude advanced functions for simple use cases
simple_calc_agent = Agent(
    tools=[CalculatorTools(exclude_tools=["factorial", "is_prime", "exponentiate"])],
    markdown=True,
)

# Example 3: Full calculator functionality (default)
agent = Agent(
    tools=[CalculatorTools()],
    markdown=True,
)
simple_calc_agent.print_response(
    "What is 10*5 then to the power of 2, do it step by step"
)
```

---

<a name="tools--cartesia_toolspy"></a>

### `tools/cartesia_tools.py`

```python
"""
Get an API key from https://play.cartesia.ai/keys

Run `pip install cartesia` to install the dependencies.
"""

from agno.agent import Agent
from agno.tools.cartesia import CartesiaTools
from agno.utils.audio import write_audio_to_file

# Initialize Agent with Cartesia tools
agent = Agent(
    name="Cartesia TTS Agent",
    description="An agent that uses Cartesia for text-to-speech.",
    tools=[CartesiaTools()],
)

response = agent.run(
    """Generate a simple greeting using Text-to-Speech:

    Say "Welcome to Cartesia, the advanced  speech synthesis platform. This speech is generated by an agent."
    """
)

# Save the generated audio
if response.audio:
    write_audio_to_file(audio=response.audio[0].content, filename="tmp/greeting.mp3")
```

---

<a name="tools--clickup_toolspy"></a>

### `tools/clickup_tools.py`

```python
"""
Steps to Get Your ClickUp API Key

Step 1: Log In to ClickUp
Step 2: Navigate to Settings (usually a circle with your initials) click on it
Step 3: Access the Apps Section: In the settings sidebar on the left, scroll down until you find Apps. Click on it to access the API settings.
Step 4: Generate Your API Key
In the Apps section, you should see an option labeled API Key. If its not already generated, look for a button that says Generate and click it.
Once generated, your API key will be displayed. Make sure to copy this key and store it as CLICKUP_API_KEY in .env file to use it.

Steps To find your MASTER_SPACE_ID :
clickup space url structure: https://app.clickup.com/{MASTER_SPACE_ID}/v/o/s/{SPACE_ID}
1. copy any space url from your clickup workspace all follow above url structure.
2. To use clickup tool copy the MASTER_SPACE_ID and store it .env file.

"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.clickup import ClickUpTools

clickup_agent = Agent(
    name="ClickUp Agent",
    role="Manage ClickUp tasks and spaces",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        ClickUpTools(
            exclude_tools=[
                "create_task",
                "get_task",
                "update_task",
                "delete_task",
            ]
        )
    ],
    instructions=[
        "You are a ClickUp assistant that helps users manage their tasks and spaces.",
        "You can:",
        "1. List all available spaces",
        "2. List tasks from a specific space",
        "3. List all lists in a space",
        "4. Create new tasks with title, description, and status",
        "When creating tasks:",
        "- Always get space name, task name, and description",
        "- Status can be: todo, in progress, or done",
        "- If status is not specified, use 'todo' as default",
        "Be helpful and guide users if they need more information.",
    ],
    markdown=True,
)

clickup_agent.print_response(
    "List all spaces i have",
    markdown=True,
)
clickup_agent.print_response(
    "Create a task (status 'To Do') called 'QA task' in Project 1 in the Team Space. The description should be about running basic QA checks on our Python codebase.",
    markdown=True,
)
```

---

<a name="tools--composio_toolspy"></a>

### `tools/composio_tools.py`

```python
from agno.agent import Agent
from composio_agno import Action, ComposioToolSet

toolset = ComposioToolSet()
composio_tools = toolset.get_tools(
    actions=[Action.GITHUB_STAR_A_REPOSITORY_FOR_THE_AUTHENTICATED_USER]
)
agent = Agent(tools=composio_tools)

agent.print_response("Can you star agno-agi/agno repo?")
```

---

<a name="tools--confluence_toolspy"></a>

### `tools/confluence_tools.py`

```python
from agno.agent import Agent
from agno.tools.confluence import ConfluenceTools

agent = Agent(
    name="Confluence agent",
    tools=[ConfluenceTools()],
    markdown=True,
)

## getting space details
agent.print_response("How many spaces are there and what are their names?")

## getting page_content
agent.print_response(
    "What is the content present in page 'Large language model in LLM space'"
)

## getting page details in a particular space
agent.print_response("Can you extract all the page names from 'LLM' space")

## creating a new page in a space
agent.print_response("Can you create a new page named 'TESTING' in 'LLM' space")
```

---

<a name="tools--crawl4ai_toolspy"></a>

### `tools/crawl4ai_tools.py`

```python
"""
Crawl4AI Tools - Web Scraping and Content Extraction

This example demonstrates how to use Crawl4aiTools for web crawling and content extraction.
Shows enable_ flag patterns for selective function access.
Crawl4aiTools is a small tool (<6 functions) so it uses enable_ flags.

Run: `pip install crawl4ai` to install the dependencies
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.crawl4ai import Crawl4aiTools

# Example 1: All functions enabled with pruning (default behavior)
agent_full = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        Crawl4aiTools(use_pruning=True)
    ],  # All functions enabled with content pruning
    description="You are a comprehensive web research assistant with all crawling capabilities.",
    instructions=[
        "Use Crawl4AI tools to extract information from web pages",
        "Provide detailed summaries and analysis of web content",
        "Handle various content types including articles, documentation, and repositories",
        "Use content pruning to focus on main content and reduce noise",
    ],
    markdown=True,
)

# Example 2: Enable specific crawling functions
agent_basic = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        Crawl4aiTools(
            use_pruning=True,
            enable_crawl_page=True,
            enable_extract_content=True,
            enable_extract_links=False,  # Disable link extraction
            enable_take_screenshot=False,  # Disable screenshot functionality
        )
    ],
    description="You are a basic web content extractor focused on page content only.",
    instructions=[
        "Extract and summarize main content from web pages",
        "Cannot extract links or take screenshots",
        "Focus on text content analysis and summarization",
        "Provide clean, well-structured content summaries",
    ],
    markdown=True,
)

# Example 3: Enable all functions using 'all=True' pattern
agent_comprehensive = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[Crawl4aiTools(use_pruning=True, all=True)],
    description="You are a full-featured web intelligence agent with all crawling capabilities.",
    instructions=[
        "Perform comprehensive web analysis using all Crawl4AI features",
        "Extract content, links, take screenshots, and analyze page structure",
        "Provide detailed insights about web pages and their relationships",
        "Support advanced web research and content analysis workflows",
    ],
    markdown=True,
)

# Example 4: Screenshot and visual analysis focused
agent_visual = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        Crawl4aiTools(
            use_pruning=False,  # Don't prune for visual analysis
            enable_crawl_page=True,
            enable_take_screenshot=True,
            enable_extract_content=False,  # Focus on visual, not text
            enable_analyze_layout=True,  # Assuming this function exists
        )
    ],
    description="You are a web visual analyst focused on page screenshots and layout analysis.",
    instructions=[
        "Take screenshots of web pages and analyze their visual layout",
        "Cannot extract detailed text content",
        "Focus on visual elements, design, and user interface analysis",
        "Provide insights about web page structure and visual design",
    ],
    markdown=True,
)

# Example usage
print("=== Comprehensive Web Analysis Example ===")
agent_full.print_response(
    "Give me a detailed summary of the Agno project from https://github.com/agno-agi/agno and what are its main features?"
)

print("\n=== Basic Content Extraction Example ===")
agent_basic.print_response(
    "Extract the main content and history from https://en.wikipedia.org/wiki/Python_(programming_language)"
)

print("\n=== Advanced Web Intelligence Example ===")
agent_comprehensive.print_response(
    "Analyze the structure, content, and key links from https://docs.python.org/3/ and provide insights about the documentation organization"
)

# Example 2: Extract main content only (remove navigation, ads, etc.)
# agent_clean = Agent(tools=[Crawl4aiTools(use_pruning=True)])
# agent_clean.print_response(
#     "Get the History from https://en.wikipedia.org/wiki/Python_(programming_language)"
# )

# Example 3: Search for specific content on a page
# agent_search = Agent(
#     instructions="You are a helpful assistant that can crawl the web and extract information. Use have access to crawl4ai tools to extract information from the web.",
#     tools=[Crawl4aiTools()],
# )
# agent_search.print_response(
#     "What are the diferent Techniques used in AI? https://en.wikipedia.org/wiki/Artificial_intelligence"
# )

# Example 4: Multiple URLs with clean extraction
# agent_multi = Agent(
#     tools=[Crawl4aiTools(use_pruning=True, headless=False)]
# )
# agent_multi.print_response(
#     "Compare the main content from https://en.wikipedia.org/wiki/Artificial_intelligence and https://en.wikipedia.org/wiki/Machine_learning"
# )
```

---

<a name="tools--csv_toolspy"></a>

### `tools/csv_tools.py`

```python
"""
CSV Tools - Data Analysis and Processing for CSV Files

This example demonstrates how to use CsvTools for CSV file operations.
Shows enable_ flag patterns for selective function access.
CsvTools is a small tool (<6 functions) so it uses enable_ flags.

Run: `pip install pandas` to install the dependencies
"""

from pathlib import Path

import httpx
from agno.agent import Agent
from agno.tools.csv_toolkit import CsvTools

# Download sample data
url = "https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv"
response = httpx.get(url)

imdb_csv = Path(__file__).parent.joinpath("imdb.csv")
imdb_csv.parent.mkdir(parents=True, exist_ok=True)
imdb_csv.write_bytes(response.content)

# Example 1: All functions enabled (default behavior)
agent_full = Agent(
    tools=[CsvTools(csvs=[imdb_csv])],  # All functions enabled by default
    description="You are a comprehensive CSV data analyst with all processing capabilities.",
    instructions=[
        "Help users with complete CSV data analysis and processing",
        "First always get the list of files",
        "Then check the columns in the file",
        "Run queries and provide detailed analysis",
        "Support all CSV operations and transformations",
    ],
    markdown=True,
)

# Example 2: Enable specific functions for read-only analysis
agent_readonly = Agent(
    tools=[
        CsvTools(
            csvs=[imdb_csv],
            enable_list_csv_files=True,
            enable_get_columns=True,
            enable_query_csv_file=True,
            enable_create_csv=False,  # Disable CSV creation
            enable_modify_csv=False,  # Disable CSV modification
        )
    ],
    description="You are a CSV data analyst focused on reading and analyzing existing data.",
    instructions=[
        "Analyze existing CSV files without modifications",
        "Provide insights and run analytical queries",
        "Cannot create or modify CSV files",
        "Focus on data exploration and reporting",
    ],
    markdown=True,
)

# Example 3: Enable all functions using 'all=True' pattern
agent_comprehensive = Agent(
    tools=[CsvTools(csvs=[imdb_csv], all=True)],
    description="You are a full-featured CSV processing expert with all capabilities.",
    instructions=[
        "Perform comprehensive CSV data operations",
        "Create, modify, analyze, and transform CSV files",
        "Support advanced data processing workflows",
        "Provide end-to-end CSV data management",
    ],
    markdown=True,
)

# Example 4: Query-focused agent
agent_query = Agent(
    tools=[
        CsvTools(
            csvs=[imdb_csv],
            enable_list_csv_files=True,
            enable_get_columns=True,
            enable_query_csv_file=True,
        )
    ],
    description="You are a CSV query specialist focused on data analysis and reporting.",
    instructions=[
        "Execute analytical queries on CSV data",
        "Provide statistical insights and summaries",
        "Generate reports based on data analysis",
        "Focus on extracting valuable insights from datasets",
    ],
    markdown=True,
)

print("=== Full CSV Analysis Example ===")
print("Using comprehensive agent for complete CSV operations")
agent_full.print_response(
    "Analyze the IMDB movie dataset. Show me the top 10 highest-rated movies and their directors.",
    markdown=True,
)

print("\n=== Read-Only Analysis Example ===")
print("Using read-only agent for data exploration")
agent_readonly.print_response(
    "What are the key statistics about the movie ratings and revenue in this dataset?",
    markdown=True,
)

print("\n=== Query-Focused Example ===")
print("Using query specialist for targeted analysis")
agent_query.print_response(
    "Find movies from the year 2016 with ratings above 8.0 and show their genres.",
    markdown=True,
)

# Optional: Interactive CLI mode
# agent_full.cli_app(stream=False)
```

---

<a name="tools--custom_api_toolspy"></a>

### `tools/custom_api_tools.py`

```python
from agno.agent import Agent
from agno.tools.api import CustomApiTools

"""
Args:
    base_url (Optional[str]): Base URL for API calls
    username (Optional[str]): Username for basic authentication
    password (Optional[str]): Password for basic authentication
    api_key (Optional[str]): API key for authentication
    headers (Optional[Dict[str, str]]): Default headers to include in requests
    verify_ssl (bool): Whether to verify SSL certificates
    timeout (int): Request timeout in seconds
"""
# Example 1: Enable specific API functions
agent = Agent(
    tools=[CustomApiTools(base_url="https://dog.ceo/api", enable_make_request=True)],
    markdown=True,
)

# Example 2: Enable all API functions
agent_all = Agent(
    tools=[CustomApiTools(base_url="https://dog.ceo/api", all=True)],
    markdown=True,
)

agent.print_response(
    'Make api calls to the following two different endpoints- /breeds/image/random and /breeds/list/all to get a random dog image and list of dog breeds respectively. Make sure that the method is "GET" for both the api calls.'
)
```

---

<a name="tools--custom_async_toolspy"></a>

### `tools/custom_async_tools.py`

```python
import asyncio
from dataclasses import dataclass

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from pydantic import BaseModel


async def dict_tool(name: str, age: int, city: str):
    """
    Return a dictionary with the name, age, and city of the person.
    """
    return {"name": name, "age": age, "city": city}


async def list_tool(items: list[str]):
    """
    Return a list of items.
    """
    return items


async def set_tool(items: list[str]):
    """
    Return a set of items.
    """
    return set(items)


async def tuple_tool(name: str, age: int, city: str):
    """
    Return a tuple with the name, age, and city of the person.
    """
    return (name, age, city)


async def generator_tool(items: list[str]):
    """
    Return a generator of items.
    """
    for item in items:
        yield item
        yield " "


async def pydantic_tool(name: str, age: int, city: str):
    """
    Return a Pydantic model with the name, age, and city of the person.
    """

    class CustomTool(BaseModel):
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


async def data_class_tool(name: str, age: int, city: str):
    """
    Return a data class with the name, age, and city of the person.
    """

    @dataclass
    class CustomTool:
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        dict_tool,
        list_tool,
        generator_tool,
        pydantic_tool,
        data_class_tool,
        set_tool,
        tuple_tool,
    ],
)

asyncio.run(
    agent.aprint_response("Call all the tools and make up interesting arguments")
)
```

---

<a name="tools--custom_tool_eventspy"></a>

### `tools/custom_tool_events.py`

```python
"""This example demonstrate how to yield custom events from a custom tool."""

import asyncio
from dataclasses import dataclass
from typing import Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.agent import CustomEvent
from agno.tools import tool


# Our custom event, extending the CustomEvent class
@dataclass
class CustomerProfileEvent(CustomEvent):
    """CustomEvent for customer profile."""

    customer_name: Optional[str] = None
    customer_email: Optional[str] = None
    customer_phone: Optional[str] = None


# Our custom tool
@tool()
async def get_customer_profile():
    """Example custom tool that simply yields a custom event."""

    yield CustomerProfileEvent(
        customer_name="John Doe",
        customer_email="john.doe@example.com",
        customer_phone="1234567890",
    )


# Setup an Agent with our custom tool.
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_customer_profile],
    instructions="Your task is to retrieve customer profiles for the user.",
)


async def run_agent():
    # Running the Agent: it should call our custom tool and yield the custom event
    async for event in agent.arun(
        "Hello, can you get me the customer profile for customer with ID 123?",
        stream=True,
    ):
        if isinstance(event, CustomEvent):
            print(f" Custom event emitted: {event}")


asyncio.run(run_agent())
```

---

<a name="tools--custom_toolspy"></a>

### `tools/custom_tools.py`

```python
from dataclasses import dataclass

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from pydantic import BaseModel


def dict_tool(name: str, age: int, city: str):
    """
    Return a dictionary with the name, age, and city of the person.
    """
    return {"name": name, "age": age, "city": city}


def list_tool(items: list[str]):
    """
    Return a list of items.
    """
    return items


def set_tool(items: list[str]):
    """
    Return a set of items.
    """
    return set(items)


def tuple_tool(name: str, age: int, city: str):
    """
    Return a tuple with the name, age, and city of the person.
    """
    return (name, age, city)


def generator_tool(items: list[str]):
    """
    Return a generator of items.
    """
    for item in items:
        yield item
        yield " "


def pydantic_tool(name: str, age: int, city: str):
    """
    Return a Pydantic model with the name, age, and city of the person.
    """

    class CustomTool(BaseModel):
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


def data_class_tool(name: str, age: int, city: str):
    """
    Return a data class with the name, age, and city of the person.
    """

    @dataclass
    class CustomTool:
        name: str
        age: int
        city: str

    return CustomTool(name=name, age=age, city=city)


agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        dict_tool,
        list_tool,
        generator_tool,
        pydantic_tool,
        data_class_tool,
        set_tool,
        tuple_tool,
    ],
)
agent.print_response("Call all the tools and make up interesting arguments")
```

---

<a name="tools--dalle_toolspy"></a>

### `tools/dalle_tools.py`

```python
"""Run `pip install openai` to install dependencies."""

from pathlib import Path

from agno.agent import Agent
from agno.tools.dalle import DalleTools
from agno.utils.media import download_image

# Example 1: Basic DALL-E agent with all functions enabled
agent = Agent(tools=[DalleTools(all=True)], name="DALL-E Image Generator")

# Example 2: Enable specific DALL-E functions
agent_specific = Agent(
    tools=[
        DalleTools(
            enable_create_image=True,
            model="dall-e-3",
            size="1024x1024",
            quality="standard",
        )
    ],
    name="Basic DALL-E Generator",
)

# Example 3: High-quality custom DALL-E generator
custom_dalle = DalleTools(
    all=True, model="dall-e-3", size="1792x1024", quality="hd", style="natural"
)

agent_custom = Agent(
    tools=[custom_dalle],
    name="Custom DALL-E Generator",
)

# Test basic generation
agent.print_response(
    "Generate an image of a futuristic city with flying cars and tall skyscrapers",
    markdown=True,
)

response = agent_custom.run(
    "Create a panoramic nature scene showing a peaceful mountain lake at sunset",
    markdown=True,
)
if response.images and response.images[0].url:
    download_image(
        url=response.images[0].url,
        output_path=str(Path(__file__).parent.joinpath("tmp/nature.jpg")),
    )
```

---

<a name="tools--daytona_toolspy"></a>

### `tools/daytona_tools.py`

```python
"""
 Agent with Daytona tools

This example shows how to use Agno's Daytona integration to run Agent-generated code in a remote, secure sandbox.

1. Get your Daytona API key and API URL: https://app.daytona.io/dashboard/keys
2. Set the API key and API URL as environment variables:
    export DAYTONA_API_KEY=<your_api_key>
    export DAYTONA_API_URL=<your_api_url> (optional)
3. Install the dependencies:
    pip install agno anthropic daytona
"""

from agno.agent import Agent
from agno.tools.daytona import DaytonaTools

agent = Agent(
    name="Coding Agent with Daytona tools",
    tools=[DaytonaTools()],
    markdown=True,
    instructions=[
        "You are an expert at writing and executing code. You have access to a remote, secure Daytona sandbox.",
        "Your primary purpose is to:",
        "1. Write clear, efficient code based on user requests",
        "2. ALWAYS execute the code in the Daytona sandbox using run_code",
        "3. Show the actual execution results to the user",
        "4. Provide explanations of how the code works and what the output means",
        "Guidelines:",
        "- NEVER just provide code without executing it",
        "- Execute all code using the run_code tool to show real results",
        "- Support Python, JavaScript, and TypeScript execution",
        "- Use file operations (create_file, read_file) when working with scripts",
        "- Install missing packages when needed using run_shell_command",
        "- Always show both the code AND the execution output",
        "- Handle errors gracefully and explain any issues encountered",
    ],
)

agent.print_response(
    "Write JavaScript code to generate 10 random numbers between 1 and 100, sort them in ascending order, and print each number"
)
```

---

<a name="tools--desi_vocal_toolspy"></a>

### `tools/desi_vocal_tools.py`

```python
"""
pip install requests
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.desi_vocal import DesiVocalTools

audio_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DesiVocalTools()],
    description="You are an AI agent that can generate audio using the DesiVocal API.",
    instructions=[
        "When the user asks you to generate audio, use the `text_to_speech` tool to generate the audio.",
        "You'll generate the appropriate prompt to send to the tool to generate audio.",
        "You don't need to find the appropriate voice first, I already specified the voice to user.",
        "Return the audio file name in your response. Don't convert it to markdown.",
        "Generate the text prompt we send in hindi language",
    ],
    markdown=True,
)

audio_agent.print_response(
    "Generate a very small audio of history of french revolution"
)
```

---

<a name="tools--discord_toolspy"></a>

### `tools/discord_tools.py`

```python
import os

from agno.agent import Agent
from agno.tools.discord import DiscordTools

# Get Discord token from environment
discord_token = os.getenv("DISCORD_BOT_TOKEN")
if not discord_token:
    raise ValueError("DISCORD_BOT_TOKEN not set")

# Example 1: Enable all Discord functions
discord_agent_all = Agent(
    name="Discord Agent - All Functions",
    instructions=[
        "You are a Discord bot with access to all Discord operations.",
        "You can send messages, manage channels, read history, and manage messages.",
    ],
    tools=[
        DiscordTools(
            bot_token=discord_token,
            all=True,  # Enable all Discord functions
        )
    ],
    markdown=True,
)

# Example 2: Enable specific Discord functions only
discord_agent_specific = Agent(
    name="Discord Agent - Specific Functions",
    instructions=[
        "You are a Discord bot with limited operations.",
        "You can only send messages and read message history.",
    ],
    tools=[
        DiscordTools(
            bot_token=discord_token,
            enable_send_message=True,
            enable_get_channel_messages=True,
            enable_get_channel_info=False,
            enable_list_channels=False,
            enable_delete_message=False,
        )
    ],
    markdown=True,
)

# Example 3: Default behavior with specific configurations
discord_agent = Agent(
    name="Discord Agent - Default",
    instructions=[
        "You are a Discord bot that can perform various operations.",
        "You can send messages, read message history, manage channels, and delete messages.",
    ],
    tools=[
        DiscordTools(
            bot_token=discord_token,
            enable_send_message=True,
            enable_get_channel_messages=True,
            enable_get_channel_info=True,
            enable_list_channels=True,
            enable_delete_message=True,
        )
    ],
    markdown=True,
)

# Replace with your Discord IDs
channel_id = "YOUR_CHANNEL_ID"
server_id = "YOUR_SERVER_ID"

# Example usage with all functions enabled
print("=== Example 1: Using all Discord functions ===")
discord_agent_all.print_response(
    f"Send a message 'Hello from Agno with all functions!' to channel {channel_id}",
    stream=True,
)

# Example usage with specific functions only
print("\n=== Example 2: Using specific Discord functions ===")
discord_agent_specific.print_response(
    f"Send a message 'Hello from limited bot!' to channel {channel_id}", stream=True
)

# Example usage with default configuration
print("\n=== Example 3: Default Discord agent usage ===")
discord_agent.print_response(
    f"Send a message 'Hello from Agno!' to channel {channel_id}", stream=True
)

discord_agent.print_response(f"Get information about channel {channel_id}", stream=True)

discord_agent.print_response(f"List all channels in server {server_id}", stream=True)

discord_agent.print_response(
    f"Get the last 5 messages from channel {channel_id}", stream=True
)

# Example: Delete a message (replace message_id with an actual message ID)
# message_id = 123456789
# discord_agent.print_response(
#     f"Delete message {message_id} from channel {channel_id}",
#     stream=True
# )
```

---

<a name="tools--docker_toolspy"></a>

### `tools/docker_tools.py`

```python
import sys

from agno.agent import Agent

try:
    from agno.tools.docker import DockerTools

    # Example 1: Include specific Docker functions for container management
    container_tools = DockerTools(
        include_tools=[
            "list_containers",
            "start_container",
            "stop_container",
            "get_container_logs",
            "inspect_container",
        ]
    )

    # Example 2: Exclude dangerous functions (like remove operations)
    safe_docker_tools = DockerTools(
        exclude_tools=[
            "remove_container",
            "remove_image",
            "remove_volume",
            "remove_network",
        ]
    )

    # Example 3: Include all functions (default behavior)
    full_docker_tools = DockerTools()

    # Create agents with different tool configurations
    container_agent = Agent(
        name="Docker Container Agent",
        instructions=[
            "You are a Docker container management assistant.",
            "You can list, start, stop, and inspect containers.",
        ],
        tools=[container_tools],
        markdown=True,
    )

    safe_agent = Agent(
        name="Safe Docker Agent",
        instructions=[
            "You are a Docker management assistant with safe operations only.",
            "You can view and manage Docker resources but cannot delete them.",
        ],
        tools=[safe_docker_tools],
        markdown=True,
    )

    docker_agent = Agent(
        name="Full Docker Agent",
        instructions=[
            "You are a comprehensive Docker management assistant.",
            "You can manage containers, images, volumes, and networks.",
        ],
        tools=[full_docker_tools],
        markdown=True,
    )

    # Example 1: List running containers
    docker_agent.print_response("List all running Docker containers", stream=True)

    # Example 2: List all images
    docker_agent.print_response("List all Docker images on this system", stream=True)

    # Example 3: Pull an image
    docker_agent.print_response("Pull the latest nginx image", stream=True)

    # Example 4: Run a container
    docker_agent.print_response(
        "Run an nginx container named 'web-server' on port 8080", stream=True
    )

    # Example 5: Get container logs
    docker_agent.print_response("Get logs from the 'web-server' container", stream=True)

    # # Example 6: List volumes
    docker_agent.print_response("List all Docker volumes", stream=True)

    # Example 7: Create a network
    docker_agent.print_response(
        "Create a new Docker network called 'test-network'", stream=True
    )

    # Example 8: Stop and remove container
    docker_agent.print_response(
        "Stop and remove the 'web-server' container", stream=True
    )

    # Example 9: Inspect an image
    docker_agent.print_response("Inspect the nginx image", stream=True)

    # # Example 10: Build an image (uncomment and modify path as needed)
    # docker_agent.print_response(
    #     "Build a Docker image from the Dockerfile in ./app with tag 'myapp:latest'",
    #     stream=True
    # )

except ValueError as e:
    print(f"\n Docker Tool Error: {e}")
    print("\n Troubleshooting steps:")

    if sys.platform == "darwin":  # macOS
        print("1. Ensure Docker Desktop is running")
        print("2. Check Docker Desktop settings")
        print("3. Try running 'docker ps' in terminal to verify access")

    elif sys.platform == "linux":
        print("1. Check if Docker service is running:")
        print("   systemctl status docker")
        print("2. Make sure your user has permissions to access Docker:")
        print("   sudo usermod -aG docker $USER")

    elif sys.platform == "win32":
        print("1. Ensure Docker Desktop is running")
        print("2. Check Docker Desktop settings")
```

---

<a name="tools--duckdb_toolspy"></a>

### `tools/duckdb_tools.py`

```python
from agno.agent import Agent
from agno.tools.duckdb import DuckDbTools

agent = Agent(
    tools=[DuckDbTools()],
    instructions="Use this file for Movies data: https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
)
agent.print_response(
    "What is the average rating of movies?", markdown=True, stream=False
)
```

---

<a name="tools--duckduckgo_toolspy"></a>

### `tools/duckduckgo_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

# Example 1: Enable specific DuckDuckGo functions
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(enable_search=True, enable_news=False)],
)

# Example 2: Enable all DuckDuckGo functions
agent_all = Agent(model=OpenAIChat(id="gpt-4o"), tools=[DuckDuckGoTools(all=True)])

# Example 3: Enable only news search
news_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(enable_search=False, enable_news=True)],
)

# Example 4: Specify the search engine
yandex_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(enable_search=True, enable_news=False, backend="yandex")],
    add_datetime_to_context=True,
)

# Test the agents
agent.print_response("What's the latest about GPT-5?", markdown=True)
# news_agent.print_response(
#     "Find recent news about artificial intelligence", markdown=True
# )
# yandex_agent.print_response("What's happening in AI?", markdown=True)
```

---

<a name="tools--e2b_toolspy"></a>

### `tools/e2b_tools.py`

```python
"""
E2B Tools Example - Demonstrates how to use the E2B toolkit for sandboxed code execution.

This example shows how to:
1. Set up authentication with E2B API
2. Initialize the E2BTools with proper configuration
3. Create an agent that can run Python code in a secure sandbox
4. Use the sandbox for data analysis, visualization, and more

Prerequisites:
1. Create an account and get your API key from E2B:
   - Visit https://e2b.dev/
   - Sign up for an account
   - Navigate to the Dashboard to get your API key

2. Install required packages:
   pip install e2b_code_interpreter pandas matplotlib

3. Set environment variable:
   export E2B_API_KEY=your_api_key

Features:
- Run Python code in a secure sandbox environment
- Upload and download files to/from the sandbox
- Create and download data visualizations
- Run servers within the sandbox with public URLs
- Manage sandbox lifecycle (timeout, shutdown)
- Access the internet from within the sandbox

Usage:
Run this script with the E2B_API_KEY environment variable set to interact
with the E2B sandbox through natural language commands.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.e2b import E2BTools

# Example 1: Include specific E2B functions for basic code execution
basic_e2b_tools = E2BTools(
    timeout=600,  # 10 minutes timeout (in seconds)
    include_tools=[
        "run_python_code",
        "list_files",
        "read_file_content",
        "write_file_content",
    ],
)

# Example 2: Exclude server-related functions for security
safe_e2b_tools = E2BTools(
    timeout=600, exclude_tools=["run_server", "get_public_url", "run_command"]
)

# Example 3: Full E2B functionality (default)
full_e2b_tools = E2BTools(
    timeout=600,  # 10 minutes timeout (in seconds)
)

# Create agents with different tool configurations
basic_agent = Agent(
    name="Basic Code Execution Sandbox",
    id="e2b-basic-sandbox",
    model=OpenAIChat(id="gpt-4o"),
    tools=[basic_e2b_tools],
    markdown=True,
    instructions=[
        "You are a Python code execution assistant with basic file operations.",
        "You can run Python code and manage files in a secure sandbox.",
    ],
)

agent = Agent(
    name="Full Code Execution Sandbox",
    id="e2b-sandbox",
    model=OpenAIChat(id="gpt-4o"),
    tools=[full_e2b_tools],
    markdown=True,
    instructions=[
        "You are an expert at writing and validating Python code using a secure E2B sandbox environment.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the E2B sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
        "",
        "You can use these tools:",
        "1. Run Python code (run_python_code)",
        "2. Upload files to the sandbox (upload_file)",
        "3. Download files from the sandbox (download_file_from_sandbox)",
        "4. Generate and add visualizations as image artifacts (download_png_result)",
        "5. List files in the sandbox (list_files)",
        "6. Read and write file content (read_file_content, write_file_content)",
        "7. Start web servers and get public URLs (run_server, get_public_url)",
        "8. Manage the sandbox lifecycle (set_sandbox_timeout, get_sandbox_status, shutdown_sandbox)",
        "",
        "Guidelines:",
        "- ALWAYS share the complete code with the user, properly formatted in code blocks",
        "- Verify code functionality by executing it in the sandbox before sharing",
        "- Iterate and debug code as needed to ensure it works correctly",
        "- Use pandas, matplotlib, and other Python libraries for data analysis when appropriate",
        "- Create proper visualizations when requested and add them as image artifacts to show inline",
        "- Handle file uploads and downloads properly",
        "- Explain your approach and the code's functionality in detail",
        "- Format responses with both code and explanations for maximum clarity",
        "- Handle errors gracefully and explain any issues encountered",
    ],
)


agent.print_response(
    "Write Python code to generate the first 10 Fibonacci numbers and calculate their sum and average"
)

# agent.print_response(
#     " upload file cookbook/tools/sample_data.csv and use it to create a matplotlib visualization of total sales by region and provide chart image or its downloaded path or any link  "
# )
# agent.print_response(" use dataset sample_data.csv and create a matplotlib visualization of total sales by region and provide chart image")
# agent.print_response(" run a server and Write a simple fast api web server that displays 'Hello from E2B Sandbox!' and run it , use run_command to get the data from the server and provide the  url of api swagger docs and host link")
# agent.print_response(
#     " run server and Create and run a Python script that fetch top 5 latest news from hackernews using hackernews api"
# )
# agent.print_response("Extend the sandbox timeout to 20 minutes")
# agent.print_response("list all sandboxes ")
```

---

<a name="tools--elevenlabs_toolspy"></a>

### `tools/elevenlabs_tools.py`

```python
"""
pip install elevenlabs
"""

import base64

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.eleven_labs import ElevenLabsTools
from agno.utils.media import save_base64_data

audio_agent = Agent(
    model=Gemini(id="gemini-2.5-pro"),
    tools=[
        ElevenLabsTools(
            voice_id="21m00Tcm4TlvDq8ikWAM",
            model_id="eleven_multilingual_v2",
            target_directory="audio_generations",
        )
    ],
    description="You are an AI agent that can generate audio using the ElevenLabs API.",
    instructions=[
        "When the user asks you to generate audio, use the `generate_audio` tool to generate the audio.",
        "You'll generate the appropriate prompt to send to the tool to generate audio.",
        "You don't need to find the appropriate voice first, I already specified the voice to user."
        "Return the audio file name in your response. Don't convert it to markdown.",
        "The audio should be long and detailed.",
    ],
    markdown=True,
)

response = audio_agent.run(
    "Generate a very long audio of history of french revolution and tell me which subject it belongs to.",
    debug_mode=True,
)

if response.audio:
    print("Agent response:", response.content)
    base64_audio = base64.b64encode(response.audio[0].content).decode("utf-8")
    save_base64_data(base64_audio, "tmp/french_revolution.mp3")
    print("Successfully saved generated speech to tmp/french_revolution.mp3")


audio_agent.print_response("Generate a kick sound effect")
```

---

<a name="tools--email_toolspy"></a>

### `tools/email_tools.py`

```python
from agno.agent import Agent
from agno.tools.email import EmailTools

receiver_email = "<receiver_email>"
sender_email = "<sender_email>"
sender_name = "<sender_name>"
sender_passkey = "<sender_passkey>"

# Example 1: Enable specific email functions
agent = Agent(
    tools=[
        EmailTools(
            receiver_email=receiver_email,
            sender_email=sender_email,
            sender_name=sender_name,
            sender_passkey=sender_passkey,
            enable_email_user=True,
        )
    ]
)

# Example 2: Enable all email functions
agent_all = Agent(
    tools=[
        EmailTools(
            receiver_email=receiver_email,
            sender_email=sender_email,
            sender_name=sender_name,
            sender_passkey=sender_passkey,
            all=True,
        )
    ]
)

# Test the agent
agent.print_response(
    "Send an email to the receiver with subject 'Test Email' and a friendly greeting message",
    markdown=True,
)
```

---

<a name="tools--evm_toolspy"></a>

### `tools/evm_tools.py`

```python
"""
EVM Tools Example

This example demonstrates how to use Agno's EVM integration to send ETH transactions
on any EVM-compatible blockchain.

1. Set your environment variables:
    export EVM_PRIVATE_KEY=0x<your-private-key>
    export EVM_RPC_URL=https://your-rpc-endpoint

2. Or pass them directly to the EvmTools constructor
3. Install dependencies:
    pip install agno web3
"""

from agno.agent import Agent
from agno.tools.evm import EvmTools

# Option 1: Use environment variables (recommended)
agent = Agent(
    tools=[EvmTools()],  # Will use EVM_PRIVATE_KEY and EVM_RPC_URL from env
)

# Option 2: Pass credentials directly (for testing only)
# private_key = "0x<private-key>"
# rpc_url = "https://0xrpc.io/sep"  # Sepolia testnet
# agent = Agent(
#     tools=[
#         EvmTools(
#             private_key=private_key,
#             rpc_url=rpc_url,
#         )
#     ],
# )

# Convert 0.001 ETH to wei (1 ETH = 10^18 wei)
# 0.001 ETH = 1,000,000,000,000,000 wei
agent.print_response(
    "Send 0.001 eth (which is 1000000000000000 wei) to address 0x3Dfc53E3C77bb4e30Ce333Be1a66Ce62558bE395"
)
```

---

<a name="tools--exa_toolspy"></a>

### `tools/exa_tools.py`

```python
from agno.agent import Agent
from agno.tools.exa import ExaTools

# Example 1: Enable all tools
agent_all = Agent(
    tools=[
        ExaTools(
            all=True,  # Enable all exa tools
            show_results=True,
        )
    ],
    markdown=True,
)

# Example 2: Enable specific tools only
agent_specific = Agent(
    tools=[
        ExaTools(
            enable_search=True,
            enable_answer=True,
            enable_get_contents=False,
            enable_find_similar=False,
            enable_research=False,
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com"],
            show_results=True,
            text=False,
            highlights=False,
        )
    ],
    markdown=True,
)

# Example 3: Default behavior (most functions enabled by default)
agent = Agent(
    tools=[
        ExaTools(
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com"],
            show_results=True,
            text=False,
            highlights=False,
        )
    ],
    markdown=True,
)

agent.print_response("Search for AAPL news", markdown=True)


agent = Agent(
    tools=[
        ExaTools(
            show_results=True,
        )
    ],
    markdown=True,
)

agent.print_response("Search for AAPL news", markdown=True)

agent.print_response(
    "What is the paper at https://arxiv.org/pdf/2307.06435 about?", markdown=True
)

agent.print_response(
    "Find me similar papers to https://arxiv.org/pdf/2307.06435 and provide a summary of what they contain",
    markdown=True,
)

agent.print_response(
    "What is the latest valuation of SpaceX?",
    markdown=True,
)
```

---

<a name="tools--fal_toolspy"></a>

### `tools/fal_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.fal import FalTools

fal_agent = Agent(
    name="Fal Video Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        FalTools(
            model="fal-ai/hunyuan-video",
            enable_generate_media=True,
        )
    ],
    description="You are an AI agent that can generate videos using the Fal API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "Return the URL as raw to the user.",
        "Don't convert video URL to markdown or anything else.",
    ],
    markdown=True,
)

fal_agent.print_response("Generate video of balloon in the ocean")
```

---

<a name="tools--file_generation_toolspy"></a>

### `tools/file_generation_tools.py`

```python
"""
File Generation Tool Example
This cookbook shows how to use the FileGenerationTool to generate various file types (JSON, CSV, PDF, TXT).
The tool can generate files from agent responses and make them available for download or further processing.
"""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.file_generation import FileGenerationTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=SqliteDb(db_file="tmp/test.db"),
    tools=[FileGenerationTools(output_directory="tmp")],
    description="You are a helpful assistant that can generate files in various formats.",
    instructions=[
        "When asked to create files, use the appropriate file generation tools.",
        "Always provide meaningful content and appropriate filenames.",
        "Explain what you've created and how it can be used.",
    ],
    markdown=True,
)


def example_json_generation():
    """Example: Generate a JSON file"""
    print("=== JSON File Generation Example ===")
    response = agent.run(
        "Create a JSON file containing information about 3 fictional employees with name, position, department, and salary."
    )
    print(response.content)
    if response.files:
        for file in response.files:
            print(f"Generated file: {file.filename} ({file.size} bytes)")
            if file.url:
                print(f"File location: {file.url}")
    print()


def example_csv_generation():
    """Example: Generate a CSV file"""
    print("=== CSV File Generation Example ===")
    response = agent.run(
        "Create a CSV file with sales data for the last 6 months. Include columns for month, product, units_sold, and revenue."
    )
    print(response.content)
    if response.files:
        for file in response.files:
            print(f"Generated file: {file.filename} ({file.size} bytes)")
            if file.url:
                print(f"File location: {file.url}")
    print()


def example_pdf_generation():
    """Example: Generate a PDF file"""
    print("=== PDF File Generation Example ===")
    response = agent.run(
        "Create a PDF report about renewable energy trends in 2024. Include sections on solar, wind, and hydroelectric power."
    )
    print(response.content)
    if response.files:
        for file in response.files:
            print(f"Generated file: {file.filename} ({file.size} bytes)")
            if file.url:
                print(f"File location: {file.url}")
    print()


def example_text_generation():
    """Example: Generate a text file"""
    print("=== Text File Generation Example ===")
    response = agent.run(
        "Create a text file with a list of best practices for remote work productivity."
    )
    print(response.content)
    if response.files:
        for file in response.files:
            print(f"Generated file: {file.filename} ({file.size} bytes)")
            if file.url:
                print(f"File location: {file.url}")
    print()


if __name__ == "__main__":
    print("File Generation Tool Cookbook Examples")
    print("=" * 50)

    example_json_generation()
    example_csv_generation()
    example_pdf_generation()
    example_text_generation()
```

---

<a name="tools--file_toolspy"></a>

### `tools/file_tools.py`

```python
"""
File Tools - File System Operations and Management

This example demonstrates how to use FileTools for file operations.
Shows enable_ flag patterns for selective function access.
FileTools is a small tool (<6 functions) so it uses enable_ flags.
"""

from pathlib import Path

from agno.agent import Agent
from agno.tools.file import FileTools

# Example 1: All functions enabled (default behavior)
agent_full = Agent(
    tools=[FileTools(Path("tmp/file"))],  # All functions enabled by default
    description="You are a comprehensive file management assistant with all file operation capabilities.",
    instructions=[
        "Help users with all file operations including read, write, search, and management",
        "Create, modify, and organize files and directories",
        "Provide clear feedback on file operations",
        "Ensure file paths and operations are valid",
    ],
    markdown=True,
)

# Example 2: Enable only file reading and searching
agent_readonly = Agent(
    tools=[
        FileTools(
            Path("tmp/file"),
            enable_read_file=True,
            enable_search_files=True,
            enable_list_files=True,
        )
    ],
    description="You are a file reader focused on accessing and searching existing files.",
    instructions=[
        "Read and search through existing files",
        "List file contents and directory structures",
        "Cannot create, modify, or delete files",
        "Focus on information retrieval and file exploration",
    ],
    markdown=True,
)

# Example 3: Enable all functions using 'all=True' pattern
agent_comprehensive = Agent(
    tools=[FileTools(Path("tmp/file"), all=True)],
    description="You are a full-featured file system manager with all capabilities enabled.",
    instructions=[
        "Perform comprehensive file system operations",
        "Manage complete file lifecycles including creation, modification, and deletion",
        "Support advanced file organization and processing workflows",
        "Provide end-to-end file management solutions",
    ],
    markdown=True,
)

# Example 4: Write-only operations (for content creation)
agent_writer = Agent(
    tools=[
        FileTools(
            Path("tmp/file"),
            enable_save_file=True,
            enable_read_file=False,  # Disable file reading
            enable_search_files=False,  # Disable file searching
        )
    ],
    description="You are a content creator focused on writing and organizing new files.",
    instructions=[
        "Create new files and directories",
        "Generate and save content to files",
        "Cannot read existing files or search directories",
        "Focus on content creation and file organization",
    ],
    markdown=True,
)

# Example usage
print("=== Full File Management Example ===")
agent_full.print_response(
    "What is the most advanced LLM currently? Save the answer to a file.", markdown=True
)

print("\n=== Read-Only File Operations Example ===")
agent_readonly.print_response(
    "Search for all files in the directory and list their names and sizes",
    markdown=True,
)

print("\n=== File Writing Example ===")
agent_writer.print_response(
    "Create a summary of Python best practices and save it to 'python_guide.txt'",
    markdown=True,
)

print("\n=== File Search Example ===")
agent_full.print_response(
    "Search for all files which have an extension '.txt' and save the answer to a new file named 'all_txt_files.txt'",
    markdown=True,
)
```

---

<a name="tools--financial_datasets_toolspy"></a>

### `tools/financial_datasets_tools.py`

```python
"""
Financial Datasets API Toolkit Example
This example demonstrates various Financial Datasets API functionalities including
financial statements, stock prices, news, insider trades, and more.

Prerequisites:
- Set the environment variable `FINANCIAL_DATASETS_API_KEY` with your Financial Datasets API key.
  You can obtain the API key by creating an account at https://financialdatasets.ai
"""

from agno.agent import Agent
from agno.tools.financial_datasets import FinancialDatasetsTools

agent = Agent(
    name="Financial Data Agent",
    tools=[
        FinancialDatasetsTools(),  # For accessing financial data
    ],
    description="You are a financial data specialist that helps analyze financial information for stocks and cryptocurrencies.",
    instructions=[
        "When given a financial query:",
        "1. Use appropriate Financial Datasets methods based on the query type",
        "2. Format financial data clearly and highlight key metrics",
        "3. For financial statements, compare important metrics with previous periods when relevant",
        "4. Calculate growth rates and trends when appropriate",
        "5. Handle errors gracefully and provide meaningful feedback",
    ],
    markdown=True,
)

# Example 1: Financial Statements
print("\n=== Income Statement Example ===")
agent.print_response(
    "Get the most recent income statement for AAPL and highlight key metrics",
    stream=True,
)

# Example 2: Balance Sheet Analysis
print("\n=== Balance Sheet Analysis Example ===")
agent.print_response(
    "Analyze the balance sheets for MSFT over the last 3 years. Focus on debt-to-equity ratio and cash position.",
    stream=True,
)

# # Example 3: Cash Flow Analysis
# print("\n=== Cash Flow Analysis Example ===")
# agent.print_response(
#     "Get the quarterly cash flow statements for TSLA for the past year and analyze their free cash flow trends",
#     stream=True,
# )

# # Example 4: Company Information
# print("\n=== Company Information Example ===")
# agent.print_response(
#     "Provide key information about NVDA including its business description, sector, and industry",
#     stream=True,
# )

# # Example 5: Stock Price Analysis
# print("\n=== Stock Price Analysis Example ===")
# agent.print_response(
#     "Analyze the daily stock prices for AMZN over the past 30 days. Calculate the average, high, low, and volatility.",
#     stream=True,
# )

# # Example 6: Earnings Comparison
# print("\n=== Earnings Comparison Example ===")
# agent.print_response(
#     "Compare the last 4 earnings reports for GOOG. Show the trend in EPS and revenue.",
#     stream=True,
# )

# # Example 7: Insider Trades Analysis
# print("\n=== Insider Trades Analysis Example ===")
# agent.print_response(
#     "Analyze recent insider trading activity for META. Are insiders buying or selling?",
#     stream=True,
# )

# # Example 8: Institutional Ownership
# print("\n=== Institutional Ownership Example ===")
# agent.print_response(
#     "Who are the largest institutional owners of INTC? Have they increased or decreased their positions recently?",
#     stream=True,
# )

# # Example 9: Financial News
# print("\n=== Financial News Example ===")
# agent.print_response(
#     "What are the latest news items about NFLX? Summarize the key stories.",
#     stream=True,
# )

# # Example 10: Multi-stock Comparison
# print("\n=== Multi-stock Comparison Example ===")
# agent.print_response(
#     """Compare the following tech companies: AAPL, MSFT, GOOG, AMZN, META
#     1. Revenue growth rate
#     2. Profit margins
#     3. P/E ratios
#     4. Debt levels
#     Present as a comparison table.""",
#     stream=True,
# )

# # Example 11: Cryptocurrency Analysis
# print("\n=== Cryptocurrency Analysis Example ===")
# agent.print_response(
#     "Analyze Bitcoin (BTC) price movements over the past week. Show daily price changes and calculate volatility.",
#     stream=True,
# )

# # Example 12: SEC Filings Analysis
# print("\n=== SEC Filings Analysis Example ===")
# agent.print_response(
#     "Get the most recent 10-K and 10-Q filings for AAPL and extract key risk factors mentioned.",
#     stream=True,
# )

# # Example 13: Financial Metrics and Ratios
# print("\n=== Financial Metrics Example ===")
# agent.print_response(
#     "Calculate and explain the following financial metrics for TSLA: P/E ratio, P/S ratio, EV/EBITDA, and ROE.",
#     stream=True,
# )

# # Example 14: Segmented Financials
# print("\n=== Segmented Financials Example ===")
# agent.print_response(
#     "Analyze AAPL's segmented financials. How much revenue comes from each product category and geographic region?",
#     stream=True,
# )

# # Example 15: Stock Ticker Search
# print("\n=== Stock Ticker Search Example ===")
# agent.print_response(
#     "Find all stock tickers related to 'artificial intelligence' and give me a brief overview of each company.",
#     stream=True,
# )

# # Example 16: Financial Statement Comparison
# print("\n=== Financial Statement Comparison Example ===")
# agent.print_response(
#     """Compare the financial statements of AAPL and MSFT for the most recent fiscal year:
#     1. Revenue and revenue growth
#     2. Net income and profit margins
#     3. Cash position and debt levels
#     4. R&D spending
#     Present the comparison in a well-formatted table.""",
#     stream=True,
# )

# # Example 17: Portfolio Analysis
# print("\n=== Portfolio Analysis Example ===")
# agent.print_response(
#     """Analyze a portfolio with the following stocks and weights:
#     - AAPL (25%)
#     - MSFT (25%)
#     - GOOG (20%)
#     - AMZN (15%)
#     - TSLA (15%)
#     Calculate the portfolio's overall financial metrics and recent performance.""",
#     stream=True,
# )

# # Example 18: Dividend Analysis
# print("\n=== Dividend Analysis Example ===")
# agent.print_response(
#     "Analyze the dividend history and dividend yield for JNJ over the past 5 years.",
#     stream=True,
# )

# # Example 19: Technical Indicator Analysis
# print("\n=== Technical Indicator Analysis Example ===")
# agent.print_response(
#     "Using daily stock prices for the past 30 days, calculate and interpret the 7-day and 21-day moving averages for AAPL.",
#     stream=True,
# )

# # Example 20: Financial Report Summary
# print("\n=== Financial Report Summary Example ===")
# agent.print_response(
#     """Create a comprehensive financial summary for NVDA including:
#     1. Company overview
#     2. Latest income statement highlights
#     3. Balance sheet strength
#     4. Cash flow analysis
#     5. Key financial ratios
#     6. Recent news affecting the stock""",
#     stream=True,
# )
```

---

<a name="tools--firecrawl_toolspy"></a>

### `tools/firecrawl_tools.py`

```python
"""
This is an example of how to use the FirecrawlTools.

Prerequisites:
- Create a Firecrawl account and get an API key
- Set the API key as an environment variable:
    export FIRECRAWL_API_KEY=<your-api-key>
"""

from agno.agent import Agent
from agno.tools.firecrawl import FirecrawlTools

agent = Agent(
    tools=[
        FirecrawlTools(
            enable_scrape=False, enable_crawl=True, enable_search=True, poll_interval=2
        )
    ],
    markdown=True,
)

# Should use search
agent.print_response(
    "Search for the web for the latest on 'web scraping technologies'",
    formats=["markdown", "links"],
)

# Should use crawl
agent.print_response("Summarize this https://docs.agno.com/introduction/")
```

---

<a name="tools--giphy_toolspy"></a>

### `tools/giphy_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.giphy import GiphyTools

"""Create an agent specialized in creating gifs using Giphy """

# Example 1: Enable specific Giphy functions
gif_agent = Agent(
    name="Gif Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GiphyTools(limit=5, enable_search_gifs=True)],
    description="You are an AI agent that can generate gifs using Giphy.",
    instructions=[
        "When the user asks you to create a gif, come up with the appropriate Giphy query and use the `search_gifs` tool to find the appropriate gif.",
    ],
)

# Example 2: Enable all Giphy functions
gif_agent_all = Agent(
    name="Full Giphy Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GiphyTools(limit=10, all=True)],
    description="You are an AI agent with full Giphy capabilities.",
    instructions=[
        "Use Giphy to find the perfect GIF for any situation or mood.",
        "Consider the user's context and preferences when searching.",
    ],
)

gif_agent.print_response("I want a gif to send to a friend for their birthday.")
```

---

<a name="tools--github_toolspy"></a>

### `tools/github_tools.py`

```python
"""
GitHub Authentication Setup Guide

1. Getting Personal Access Token (PAT):
   a. Navigate to GitHub Settings:
      - Log into GitHub
      - Click profile picture (top-right)
      - Select "Settings"
      - Go to "Developer settings"  "Personal access tokens"  "Tokens (classic)"

   b. Generate New Token:
      - Click "Generate new token (classic)"
      - Add descriptive note
      - Set expiration date
      - Select scopes (minimum 'repo' access)
      - Click "Generate token"
      - IMPORTANT: Save token immediately - only shown once!

2. Setting Environment Variables:

   # For Public GitHub
   export GITHUB_ACCESS_TOKEN="your_token_here"
   export GITHUB_BASE_URL="https://api.github.com"

   # For Enterprise GitHub
   export GITHUB_BASE_URL="https://YOUR-ENTERPRISE-HOSTNAME/api/v3"
"""

from agno.agent import Agent
from agno.tools.github import GithubTools

# Example 1: Include specific GitHub functions
agent = Agent(
    instructions=[
        "Use your tools to answer questions about the repo: agno-agi/agno",
        "Do not create any issues or pull requests unless explicitly asked to do so",
    ],
    tools=[
        GithubTools(
            include_tools=[
                "search_repositories",
                "get_repository",
                "list_repositories",
                "get_pull_requests",
                "list_issues",
            ]
        )
    ],
)

# Example 2: Exclude dangerous functions
agent_safe = Agent(
    instructions=[
        "Use your tools to answer questions about the repo: agno-agi/agno",
        "You can only read repository data, not modify anything",
    ],
    tools=[
        GithubTools(
            exclude_tools=[
                "delete_repository",
                "create_repository",
                "create_issue",
                "create_pull_request",
                "delete_file",
            ]
        )
    ],
)

# Example 3: Include all functions (default behavior)
agent_full = Agent(
    instructions=[
        "Use your tools to answer questions about the repo: agno-agi/agno",
        "You have full access to GitHub repository management",
    ],
    tools=[GithubTools()],
)

# Basic repository listing
agent.print_response("List open pull requests", markdown=True)

# Example: Get comprehensive repository stats
# agent.print_response(
#     "Get comprehensive stats for the agno-agi/agno repository", markdown=True
# )

# Example: Get detailed pull request information
# agent.print_response(
#     "Get comprehensive details for pull request #100 in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Working with issues
# agent.print_response(
#     "List all open issues in the agno-agi/agno repository", markdown=True
# )

# Example: Get specific issue details
# agent.print_response(
#     "Get details for issue #50 in the agno-agi/agno repository", markdown=True
# )

# Example: File operations - checking file content
# agent.print_response(
#     "Show me the content of the README.md file in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Directory listing
# agent.print_response(
#     "List all files in the docs directory of the agno-agi/agno repository",
#     markdown=True,
# )

# Example: List branch content
# agent.print_response(
#     "Show me the files in the main branch of the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Branch operations
# agent.print_response("List all branches in the agno-agi/agno repository", markdown=True)

# Example: Search code in repository
# agent.print_response(
#     "Search for 'Agent' class definitions in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Search issues and pull requests
# agent.print_response(
#     "Find all issues and PRs mentioning 'bug' in the agno-agi/agno repository",
#     markdown=True,
# )

# Example: Creating a pull request (commented out by default)
# agent.print_response("Create a pull request from 'feature-branch' to 'main' in agno-agi/agno titled 'New Feature' with description 'Implements the new feature'", markdown=True)

# Example: Creating a branch (commented out by default)
# agent.print_response("Create a new branch called 'feature-branch' from the main branch in the agno-agi/agno repository", markdown=True)

# Example: Setting default branch (commented out by default)
# agent.print_response("Set the default branch to 'develop' in the agno-agi/agno repository", markdown=True)

# Example: File creation (commented out by default)
# agent.print_response("Create a file called 'test.md' with content 'This is a test' in the agno-agi/agno repository", markdown=True)

# Example: Update file (commented out by default)
# agent.print_response("Update the README.md file in the agno-agi/agno repository to add a new section about installation", markdown=True)

# Example: Delete file (commented out by default)
# agent.print_response("Delete the file test.md from the agno-agi/agno repository", markdown=True)

# Example: Requesting a review for a pull request (commented out by default)
# agent.print_response("Request a review from user 'username' for pull request #100 in the agno-agi/agno repository", markdown=True)

# # Advanced examples (commented out by default)

# # Example usage: Search for python projects on github that have more than 1000 stars
# agent.print_response("Search for python projects on github that have more than 1000 stars", markdown=True, stream=True)

# # Example usage: Search for python projects on github that have more than 1000 stars, but return the 2nd page of results
# agent.print_response("Search for python projects on github that have more than 1000 stars, but return the 2nd page of results", markdown=True, stream=True)

# # Example usage: Get pull request details
# agent.print_response("Get details of #1239", markdown=True)

# # Example usage: Get pull request changes
# agent.print_response("Show changes for #1239", markdown=True)

# # Example usage: Get pull request count
# agent.print_response("How many pull requests are there in the agno-agi/agno repository?", markdown=True)

# # Example usage: Get pull request count by author
# agent.print_response("How many pull requests has user 'username' created in the agno-agi/agno repository?", markdown=True)

# # Example usage: List open issues
# agent.print_response("What is the latest opened issue?", markdown=True)

# # Example usage: Create an issue
# agent.print_response("Explain the comments for the most recent issue", markdown=True)

# # Example usage: Create a Repo
# agent.print_response("Create a repo called agno-test and add description hello", markdown=True)

# # Example usage: Get repository stars
# agent.print_response("How many stars does the agno-agi/agno repository have?", markdown=True)

# # Example usage: Get pull requests by query parameters
# agent.print_response("Get open pull requests from the agno-agi/agno repository on the main branch sorted by creation date", markdown=True)

# # Example usage: Get pull request comments
# agent.print_response("Show me all review comments on pull request #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Create a pull request comment
# agent.print_response("Add a comment 'Nice work!' to line 10 of file.py in the latest commit of PR #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Edit a pull request comment
# agent.print_response("Update comment #1057297855 in the agno-agi/agno repository to say 'Updated: This looks good now'", markdown=True)

# # Example usage: Get repository stars
# agent.print_response("How many stars does the agno-agi/agno repository have?", markdown=True)

# # Example usage: Get pull requests by query parameters
# agent.print_response("Get open pull requests from the agno-agi/agno repository on the main branch sorted by creation date", markdown=True)

# # Example usage: Get pull request comments
# agent.print_response("Show me all review comments on pull request #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Create a pull request comment
# agent.print_response("Add a comment 'Nice work!' to line 10 of file.py in the latest commit of PR #100 in the agno-agi/agno repository", markdown=True)

# # Example usage: Edit a pull request comment
# agent.print_response("Update comment #1057297855 in the agno-agi/agno repository to say 'Updated: This looks good now'", markdown=True)
```

---

<a name="tools--gmail_toolspy"></a>

### `tools/gmail_tools.py`

```python
"""
Gmail Agent that can read, draft and send emails using the Gmail.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.gmail import GmailTools
from pydantic import BaseModel, Field


class FindEmailOutput(BaseModel):
    message_id: str = Field(..., description="The message id of the email")
    thread_id: str = Field(..., description="The thread id of the email")
    references: str = Field(..., description="The references of the email")
    in_reply_to: str = Field(..., description="The in-reply-to of the email")
    subject: str = Field(..., description="The subject of the email")
    body: str = Field(..., description="The body of the email")


# Example 1: Include specific Gmail functions for reading only
read_only_agent = Agent(
    name="Gmail Reader Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        GmailTools(
            include_tools=["search_emails", "get_emails_by_thread", "get_email_body"]
        )
    ],
    description="You are a Gmail reading specialist that can search and read emails.",
    instructions=[
        "You can search and read Gmail messages but cannot send or draft emails.",
        "Summarize email contents and extract key details and dates.",
        "Show the email contents in a structured markdown format.",
    ],
    markdown=True,
    output_schema=FindEmailOutput,
)

# Example 2: Exclude dangerous functions (sending emails)
safe_gmail_agent = Agent(
    name="Safe Gmail Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GmailTools(exclude_tools=["send_email", "reply_to_email"])],
    description="You are a Gmail agent with safe operations only.",
    instructions=[
        "You can read and draft emails but cannot send them.",
        "Show the email contents in a structured markdown format.",
    ],
    markdown=True,
    output_schema=FindEmailOutput,
)

# Example 3: Full Gmail functionality (default)
agent = Agent(
    name="Full Gmail Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GmailTools()],
    description="You are an expert Gmail Agent that can read, draft and send emails using Gmail.",
    instructions=[
        "Based on user query, you can read, draft and send emails using Gmail.",
        "While showing email contents, you can summarize the email contents, extract key details and dates.",
        "Show the email contents in a structured markdown format.",
        "Attachments can be added to the email",
    ],
    markdown=True,
    output_schema=FindEmailOutput,
)

# Example 1: Find the last email from a specific sender
email = "<replace_with_email_address>"
response = agent.run(
    f"Find the last email from {email} along with the message id, references and in-reply-to",
    markdown=True,
    stream=True,
)
response_content: FindEmailOutput = response.content  # type: ignore

agent.print_response(
    f"""Send an email in order to reply to the last email from {email}.
    Use the thread_id {response_content.thread_id} and message_id {response_content.in_reply_to}. The subject should be 'Re: {response_content.subject}' and the body should be 'Hello'""",
    markdown=True,
    stream=True,
)

# Example 2: Send a new email with attachments
# agent.print_response(
#     """Send an email to user@example.com with subject 'Subject'
#     and body 'Body' and Attach the file 'tmp/attachment.pdf'""",
#     markdown=True,
#     stream=True,
# )
```

---

<a name="tools--google_bigquery_toolspy"></a>

### `tools/google_bigquery_tools.py`

```python
"""
You can set the following environment variables for your Google Cloud project:

export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="your-location"

Or you can set the following parameters in the BQTools class:

BQTools(
    project="<your-project-id>",
    location="<your-location>",
    dataset="<your-dataset>",
)

NOTE: Instruct the agent to prepend the table name with the project name and dataset name
Describe the table schemas in instructions and use thinking tools for better responses.
"""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.google_bigquery import GoogleBigQueryTools

agent = Agent(
    instructions=[
        "You are an expert Big query Writer",
        "Always prepend the table name with your_project_id.your_dataset_name when run_sql tool is invoked",
    ],
    tools=[GoogleBigQueryTools(dataset="test_dataset")],
    model=Gemini(id="gemini-2.0-flash", vertexai=True),
)

agent.print_response(
    "List the tables in the dataset. Tell me about contents of one of the tables",
    markdown=True,
)
```

---

<a name="tools--google_maps_toolspy"></a>

### `tools/google_maps_tools.py`

```python
"""
Google Maps Tools - Location and Business Information Agent

This example demonstrates various Google Maps API functionalities including business search,
directions, geocoding, address validation, and more. Shows how to use include_tools and
exclude_tools parameters for selective function access.

Prerequisites:
- Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
  You can obtain the API key from the Google Cloud Console:
  https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

- You also need to activate the Address Validation API for your project:
  https://console.developers.google.com/apis/api/addressvalidation.googleapis.com

"""

from agno.agent import Agent
from agno.tools.crawl4ai import Crawl4aiTools
from agno.tools.google_maps import GoogleMapTools

# Example 1: All functions available (default behavior)
agent_full = Agent(
    name="Full Maps API Agent",
    tools=[
        GoogleMapTools(),  # All functions enabled by default
        Crawl4aiTools(max_length=5000),
    ],
    description="You are a location and business information specialist with full Google Maps access.",
    instructions=[
        "Use any Google Maps function as needed for location-based queries",
        "Combine Maps data with website data when available",
        "Format responses clearly and provide relevant details",
        "Handle errors gracefully and provide meaningful feedback",
    ],
    markdown=True,
)

# Example 2: Include only specific functions
agent_search = Agent(
    name="Search-focused Maps Agent",
    tools=[
        GoogleMapTools(
            include_tools=[
                "search_places",
            ]
        ),
    ],
    description="You are a location search specialist focused only on finding places.",
    instructions=[
        "Focus on place searches and getting place details",
        "Use search_places for general queries",
        "Use find_place_from_text for specific place names",
        "Use get_nearby_places for proximity searches",
    ],
    markdown=True,
)

# Example 3: Exclude potentially expensive operations
agent_safe = Agent(
    name="Safe Maps API Agent",
    tools=[
        GoogleMapTools(
            exclude_tools=[
                "get_distance_matrix",  # Can be expensive with many origins/destinations
                "get_directions",  # Excludes detailed route calculations
            ]
        ),
        Crawl4aiTools(max_length=3000),
    ],
    description="You are a location specialist with restricted access to expensive operations.",
    instructions=[
        "Provide location information without detailed routing",
        "Use geocoding and place searches freely",
        "For directions, provide general guidance only",
    ],
    markdown=True,
)

# Using the full-featured agent for examples
agent = agent_full

# Example 1: Business Search
print("\n=== Business Search Example ===")
agent.print_response(
    "Find me highly rated Chinese restaurants in Phoenix, AZ with their contact details",
    stream=True,
)

# Example 2: Directions
print("\n=== Directions Example ===")
agent.print_response(
    """Get driving directions from 'Phoenix Sky Harbor Airport' to 'Desert Botanical Garden',
    avoiding highways if possible""",
    stream=True,
)

# Example 3: Address Validation and Geocoding
print("\n=== Address Validation and Geocoding Example ===")
agent.print_response(
    """Please validate and geocode this address:
    '1600 Amphitheatre Parkway, Mountain View, CA'""",
    stream=True,
)

# Example 4: Distance Matrix
print("\n=== Distance Matrix Example ===")
agent.print_response(
    """Calculate the travel time and distance between these locations in Phoenix:
    Origins: ['Phoenix Sky Harbor Airport', 'Downtown Phoenix']
    Destinations: ['Desert Botanical Garden', 'Phoenix Zoo']""",
    stream=True,
)

# Example 5: Nearby Places and Details
print("\n=== Nearby Places Example ===")
agent.print_response(
    """Find coffee shops near Arizona State University Tempe campus.
    Include ratings and opening hours if available.""",
    stream=True,
)

# Example 6: Reverse Geocoding and Timezone
print("\n=== Reverse Geocoding and Timezone Example ===")
agent.print_response(
    """Get the address and timezone information for these coordinates:
    Latitude: 33.4484, Longitude: -112.0740 (Phoenix)""",
    stream=True,
)

# Example 7: Multi-step Route Planning
print("\n=== Multi-step Route Planning Example ===")
agent.print_response(
    """Plan a route with multiple stops in Phoenix:
    Start: Phoenix Sky Harbor Airport
    Stops:
    1. Arizona Science Center
    2. Heard Museum
    3. Desert Botanical Garden
    End: Return to Airport
    Please include estimated travel times between each stop.""",
    stream=True,
)

# Example 8: Location Analysis
print("\n=== Location Analysis Example ===")
agent.print_response(
    """Analyze this location in Phoenix:
    Address: '2301 N Central Ave, Phoenix, AZ 85004'
    Please provide:
    1. Exact coordinates
    2. Nearby landmarks
    3. Elevation data
    4. Local timezone""",
    stream=True,
)

# Example 9: Business Hours and Accessibility
print("\n=== Business Hours and Accessibility Example ===")
agent.print_response(
    """Find museums in Phoenix that are:
    1. Open on Mondays
    2. Have wheelchair accessibility
    3. Within 5 miles of downtown
    Include their opening hours and contact information.""",
    stream=True,
)

# Example 10: Transit Options
print("\n=== Transit Options Example ===")
agent.print_response(
    """Compare different travel modes from 'Phoenix Convention Center' to 'Phoenix Art Museum':
    1. Driving
    2. Walking
    3. Transit (if available)
    Include estimated time and distance for each option.""",
    stream=True,
)
```

---

<a name="tools--googlecalendar_toolspy"></a>

### `tools/googlecalendar_tools.py`

```python
"""
Steps to get the Google OAuth Credentials (Reference : https://developers.google.com/calendar/api/quickstart/python)

1. Enable Google Calender API
    - Go To https://console.cloud.google.com/apis/enableflow?apiid=calendar-json.googleapis.com
    - Select Project and Enable The API

2. Go To API & Service -> OAuth Consent Screen

3.Select User Type .
    - If you are Google Workspace User select Internal
    - Else Select External

4.Fill in the app details (App name, logo, support email, etc.).

5. Select Scope
    - Click on Add or Remove Scope
    - Search for Google Calender API (Make Sure you've enabled Google calender API otherwise scopes wont be visible)
    - Select Scopes Accordingly
        - From the dropdown check on /auth/calendar scope
    - Save and Continue


6. Adding Test User
    - Click Add Users and enter the email addresses of the users you want to allow during testing.
    - NOTE : Only these users can access the app's OAuth functionality when the app is in "Testing" mode.
    If anyone else tries to authenticate, they'll see an error like: "Error 403: access_denied."
    - To make the app available to all users, you'll need to move the app's status to "In Production.".
    Before doing so, ensure the app is fully verified by Google if it uses sensitive or restricted scopes.
    - Click on Go back to Dashboard


7. Generate OAuth 2.0 Client ID
    - Go To Credentials
    - Click on Create Credentials -> OAuth Client ID
    - Select Application Type as Desktop app
    - Download JSON

8. Using Google Calender Tool
    - Pass the Path of downloaded credentials as credentials_path to Google Calender tool
"""

from agno.agent import Agent
from agno.tools.googlecalendar import GoogleCalendarTools

agent = Agent(
    tools=[
        GoogleCalendarTools(
            # credentials_path="credentials.json",  # Path to your downloaded OAuth credentials
            # token_path="token.json",  # Path to your downloaded OAuth credentials
            oauth_port=8080,  # port used for oauth authentication
            allow_update=True,
        )
    ],
    instructions=[
        """
    You are a scheduling assistant.
    You should help users to perform these actions in their Google calendar:
        - get their scheduled events from a certain date and time
        - create events based on provided details
        - update existing events
        - delete events
        - find available time slots for scheduling
    """
    ],
    add_datetime_to_context=True,
)

# Example 1: List calendar events
agent.print_response("Give me the list of tomorrow's events", markdown=True)

# Example 2: Create an event
# agent.print_response(
#     "create an event tomorrow from 9am to 10am, make the title as 'Team Meeting' and description as 'Weekly team sync'",
#     markdown=True,
# )

# Example 3: Find available time slots
# agent.print_response(
#     "Find available 1-hour time slots for tomorrow between 9 AM and 5 PM",
#     markdown=True,
# )

# Example 4: List available calendars
# agent.print_response(
#     "List all my calendars",
#     markdown=True,
# )

# Example 5: Update an event
# agent.print_response(
#     "update the 'Team Meeting' event to run from 5pm to 7pm and change description to 'Extended team sync'",
#     markdown=True,
# )

# Example 6: Delete an event
# agent.print_response("delete the 'Team Meeting' event", markdown=True)

# # Example 7: Find available time slots for a specific calendar
# agent.print_response(
#     "Find available 1-hour time slots for this week between 9 AM and 5 PM in the Appointments calendar",
#     markdown=True,
# )

# Example 9: Find available slots using locale-based working hours
# agent.print_response(
#     "Find available 60-minute slots for the next 3 days", markdown=True
# )
```

---

<a name="tools--googlesearch_toolspy"></a>

### `tools/googlesearch_tools.py`

```python
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools

# Example 1: Enable specific Google Search functions
agent = Agent(
    tools=[GoogleSearchTools(enable_google_search=True)],
    description="You are a news agent that helps users find the latest news.",
    instructions=[
        "Given a topic by the user, respond with 4 latest news items about that topic.",
        "Search for 10 news items and select the top 4 unique items.",
        "Search in English and in French.",
    ],
)

# Example 2: Enable all Google Search functions
agent_all = Agent(
    tools=[GoogleSearchTools(all=True)],
    description="You are a comprehensive search agent with all Google Search capabilities.",
    instructions=[
        "Use Google Search to find information on any topic requested by the user.",
        "Provide accurate and up-to-date results.",
    ],
)
agent.print_response("Mistral AI", markdown=True)
```

---

<a name="tools--googlesheets_toolspy"></a>

### `tools/googlesheets_tools.py`

```python
"""

Google Sheets Toolkit can be used to read, create, update and duplicate Google Sheets.

Example spreadsheet: https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/
The ID is the URL of the spreadsheet and the range is the sheet name and the range of cells to read.

Note: Add the complete auth URL as an Authorised redirect URIs for the Client ID in the Google Cloud Console.

e.g for Localhost and port 8080: http://localhost:8080/flowName=GeneralOAuthFlow and pass the oauth_port to the toolkit

"""

from agno.agent import Agent
from agno.tools.googlesheets import GoogleSheetsTools

SAMPLE_SPREADSHEET_ID = "1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"
SAMPLE_RANGE_NAME = "Class Data!A2:E"

google_sheets_tools = GoogleSheetsTools(
    spreadsheet_id=SAMPLE_SPREADSHEET_ID,
    spreadsheet_range=SAMPLE_RANGE_NAME,
    oauth_port=8080,  # or any other port
)

agent = Agent(
    tools=[google_sheets_tools],
    instructions=[
        "You help users interact with Google Sheets using tools that use the Google Sheets API",
        "Before asking for spreadsheet details, first attempt the operation as the user may have already configured the ID and range in the constructor",
    ],
)
agent.print_response("Please tell me about the contents of the spreadsheet")
```

---

<a name="tools--hackernews_toolspy"></a>

### `tools/hackernews_tools.py`

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-0"),
    tools=[HackerNewsTools()],
    markdown=True,
)
agent.print_response("Summarize the top 5 stories on hackernews", stream=True)
```

---

<a name="tools--jinareader_toolspy"></a>

### `tools/jinareader_tools.py`

```python
from agno.agent import Agent
from agno.tools.jina import JinaReaderTools

agent = Agent(tools=[JinaReaderTools()])
agent.print_response("Summarize: https://github.com/agno-agi/agno")
```

---

<a name="tools--jira_toolspy"></a>

### `tools/jira_tools.py`

```python
from agno.agent import Agent
from agno.tools.jira import JiraTools

# Example 1: Enable all Jira functions
agent_all = Agent(
    tools=[
        JiraTools(
            all=True,  # Enable all Jira functions
        )
    ],
    markdown=True,
)

# Example 2: Enable specific Jira functions only
agent_specific = Agent(
    tools=[
        JiraTools(
            enable_search_issues=True,
            enable_get_issue=True,
            enable_create_issue=False,
        )
    ],
    markdown=True,
)

# Example 3: Default behavior with all functions enabled
agent = Agent(
    tools=[
        JiraTools(
            enable_search_issues=True,
            enable_get_issue=True,
            enable_create_issue=True,
        )
    ],
    markdown=True,
)

# Example usage with all functions enabled
print("=== Example 1: Using all Jira functions ===")
agent_all.print_response(
    "Find all issues in project PROJ and create a summary report", markdown=True
)

# Example usage with specific functions only
print("\n=== Example 2: Using specific Jira functions (read-only) ===")
agent_specific.print_response("Find all issues in project PROJ", markdown=True)

# Example usage with default configuration
print("\n=== Example 3: Default Jira agent usage ===")
agent.print_response("Find all issues in project PROJ", markdown=True)

agent.print_response("Get details for issue PROJ-123", markdown=True)
```

---

<a name="tools--knowledge_toolpy"></a>

### `tools/knowledge_tool.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

kb = Knowledge(
    vector_db=PgVector(
        table_name="documents",
        db_url=db_url,
    ),
)

agent = Agent(
    knowledge=kb,
    update_knowledge=True,
)
agent.print_response(
    "Update your knowledge with the fact that cats and dogs are pets", markdown=True
)
```

---

<a name="tools--linear_toolspy"></a>

### `tools/linear_tools.py`

```python
from agno.agent import Agent
from agno.tools.linear import LinearTools

agent = Agent(
    name="Linear Tool Agent",
    tools=[LinearTools()],
    markdown=True,
)


user_id = "69069"
issue_id = "6969"
team_id = "73"
new_title = "updated title for issue"
new_issue_title = "title for new issue"
desc = "issue description"

agent.print_response("Get all the details of current user")
agent.print_response(f"Show the issue with the issue id: {issue_id}")
agent.print_response(
    f"Create a new issue with the title: {new_issue_title} with description: {desc} and team id: {team_id}"
)
agent.print_response(
    f"Update the issue with the issue id: {issue_id} with new title: {new_title}"
)
agent.print_response(f"Show all the issues assigned to user id: {user_id}")
agent.print_response("Show all the high priority issues")
```

---

<a name="tools--linkup_toolspy"></a>

### `tools/linkup_tools.py`

```python
from agno.agent import Agent
from agno.tools.linkup import LinkupTools

agent = Agent(tools=[LinkupTools()])
agent.print_response("What's the latest news in French politics?", markdown=True)
```

---

<a name="tools--lumalabs_toolspy"></a>

### `tools/lumalabs_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.lumalab import LumaLabTools

"""Create an agent specialized for Luma AI video generation"""

luma_agent = Agent(
    name="Luma Video Agent",
    id="luma-video-agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[LumaLabTools()],  # Using the LumaLab tool we created
    markdown=True,
    instructions=[
        "You are an agent designed to generate videos using the Luma AI API.",
        "You can generate videos in two ways:",
        "1. Text-to-Video Generation:",
        "   - Use the generate_video function for creating videos from text prompts",
        "   - Default parameters: loop=False, aspect_ratio='16:9', keyframes=None",
        "2. Image-to-Video Generation:",
        "   - Use the image_to_video function when starting from one or two images",
        "   - Required parameters: prompt, start_image_url",
        "   - Optional parameters: end_image_url, loop=False, aspect_ratio='16:9'",
        "   - The image URLs must be publicly accessible",
        "Choose the appropriate function based on whether the user provides image URLs or just a text prompt.",
        "The video will be displayed in the UI automatically below your response, so you don't need to show the video URL in your response.",
        "Politely and courteously let the user know that the video has been generated and will be displayed below as soon as its ready.",
        "After generating any video, if generation is async (wait_for_completion=False), inform about the generation ID",
    ],
    system_message=(
        "Use generate_video for text-to-video requests and image_to_video for image-based "
        "generation. Don't modify default parameters unless specifically requested. "
        "Always provide clear feedback about the video generation status."
    ),
)

luma_agent.run("Generate a video of a car in a sky")
# luma_agent.run("Transform this image into a video of a tiger walking: https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Walking_tiger_female.jpg/1920px-Walking_tiger_female.jpg")
# luma_agent.run("""
# Create a transition video between these two images:
# Start: https://img.freepik.com/premium-photo/car-driving-dark-forest-generative-ai_634053-6661.jpg?w=1380
# End: https://img.freepik.com/free-photo/front-view-black-luxury-sedan-road_114579-5030.jpg?t=st=1733821884~exp=1733825484~hmac=735ca584a9b985c53875fc1ad343c3fd394e1de4db49e5ab1a9ab37ac5f91a36&w=1380
# Make it a smooth, natural movement
# """)
```

---

<a name="tools--mcp--agno_mcppy"></a>

### `tools/mcp/agno_mcp.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    async with MCPTools(
        transport="streamable-http", url="https://docs.agno.com/mcp"
    ) as agno_mcp_server:
        agent = Agent(
            model=Claude(id="claude-sonnet-4-0"),
            tools=[agno_mcp_server],
            markdown=True,
        )
        await agent.aprint_response(input=message, stream=True)


if __name__ == "__main__":
    asyncio.run(run_agent("What is Agno?"))
```

---

<a name="tools--mcp--airbnbpy"></a>

### `tools/mcp/airbnb.py`

```python
""" MCP Airbnb Agent - Search for Airbnb listings!

This example shows how to create an agent that uses MCP and Gemini 2.5 Pro to search for Airbnb listings.

Run: `pip install google-genai mcp agno` to install the dependencies
"""

import asyncio

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.tools.mcp import MCPTools


async def run_mcp_agent(message: str):
    # Initialize the MCP tools
    mcp_tools = MCPTools("npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt")

    # Connect to the MCP server
    await mcp_tools.connect()

    # Use the MCP tools with an Agent
    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[mcp_tools],
        markdown=True,
    )
    await agent.aprint_response(message)

    # Close the MCP connection
    await mcp_tools.close()


if __name__ == "__main__":
    asyncio.run(run_mcp_agent("Show me listings in Barcelona, for 2 people."))
```

---

<a name="tools--mcp--bravepy"></a>

### `tools/mcp/brave.py`

```python
"""MCP Brave Agent - Search for Brave

This example shows how to create an agent that uses Anthropic to search for information using the Brave MCP server.

You can get the Brave API key from https://brave.com/search/api/

Run: `pip install anthropic mcp agno` to install the dependencies
"""

import asyncio
from os import getenv

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.mcp import MCPTools
from agno.utils.pprint import apprint_run_response


async def run_agent(message: str) -> None:
    async with MCPTools(
        "npx -y @modelcontextprotocol/server-brave-search",
        env={
            "BRAVE_API_KEY": getenv("BRAVE_API_KEY"),
        },
    ) as mcp_tools:
        agent = Agent(
            model=Claude(id="claude-sonnet-4-20250514"),
            tools=[mcp_tools],
            markdown=True,
        )

        response_stream = await agent.arun(message)
        await apprint_run_response(response_stream)


if __name__ == "__main__":
    asyncio.run(run_agent("What is the weather in Tokyo?"))
```

---

<a name="tools--mcp--clipy"></a>

### `tools/mcp/cli.py`

```python
"""Show how to run an interactive CLI to interact with an agent equipped with MCP tools.

This example uses the MCP GitHub Agent. Example prompts to try:
- "List open issues in the repository"
- "Show me recent pull requests"
- "What are the repository statistics?"
- "Find issues labeled as bugs"
- "Show me contributor activity"

Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    """Run an interactive CLI for the GitHub agent with the given message."""

    # Create a client session to connect to the MCP server
    async with MCPTools("npx -y @modelcontextprotocol/server-github") as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a GitHub assistant. Help users explore repositories and their activity.

                - Use headings to organize your responses
                - Be concise and focus on relevant information\
            """),
            markdown=True,
        )

        # Run an interactive command-line interface to interact with the agent.
        await agent.acli_app(input=message, stream=True)


if __name__ == "__main__":
    # Pull request example
    asyncio.run(
        run_agent(
            "Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information."
        )
    )
```

---

<a name="tools--mcp--filesystempy"></a>

### `tools/mcp/filesystem.py`

```python
""" MCP Filesystem Agent - Your Personal File Explorer!

This example shows how to create a filesystem agent that uses MCP to explore,
analyze, and provide insights about files and directories. The agent leverages the Model
Context Protocol (MCP) to interact with the filesystem, allowing it to answer questions
about file contents, directory structures, and more.

Example prompts to try:
- "What files are in the current directory?"
- "Show me the content of README.md"
- "What is the license for this project?"
- "Find all Python files in the project"
- "Summarize the main functionality of the codebase"

Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    """Run the filesystem agent with the given message."""
    # Initialize the MCP server
    file_path = str(Path(__file__).parent.parent.parent.parent)

    # Create a client session to connect to the MCP server
    async with MCPTools(
        f"npx -y @modelcontextprotocol/server-filesystem {file_path}"
    ) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a filesystem assistant. Help users explore files and directories.

                - Navigate the filesystem to answer questions
                - Use the list_allowed_directories tool to find directories that you can access
                - Provide clear context about files you examine
                - Use headings to organize your responses
                - Be concise and focus on relevant information\
            """),
            markdown=True,
        )

        # Run the agent
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Basic example - exploring project license
    asyncio.run(run_agent("What is the license for this project?"))

    # File content example
    asyncio.run(
        run_agent("Show me the content of README.md and explain what this project does")
    )


# More example prompts to explore:
"""
File exploration queries:
1. "What are the main Python packages used in this project?"
2. "Show me all configuration files and explain their purpose"
3. "Find all test files and summarize what they're testing"
4. "What's the project's entry point and how does it work?"
5. "Analyze the project's dependency structure"

Code analysis queries:
1. "Explain the architecture of this codebase"
2. "What design patterns are used in this project?"
3. "Find potential security issues in the codebase"
4. "How is error handling implemented across the project?"
5. "Analyze the API endpoints in this project"

Documentation queries:
1. "Generate a summary of the project documentation"
2. "What features are documented but not implemented?"
3. "Are there any TODOs or FIXMEs in the codebase?"
4. "Create a high-level overview of the project's functionality"
5. "What's missing from the documentation?"
"""
```

---

<a name="tools--mcp--gibsonaipy"></a>

### `tools/mcp/gibsonai.py`

```python
""" GibsonAI MCP Server - Create and manage databases with prompts

This example shows how to connect a local GibsonAI MCP to Agno agent.
You can instantly generate, modify database schemas
and chat with your relational database using natural language.
From prompt to a serverless database (MySQL, PostgresQL, etc.), auto-generated REST APIs for your data.

Example prompts to try:
- "Create a new GibsonAI project for my e-commerce app"
- "Show me the current schema for my project"
- "Add a 'products' table with name, price, and description fields"
- "Create a 'users' table with authentication fields"
- "Deploy my schema changes to production"

How to setup and run:

1. Install [UV](https://docs.astral.sh/uv/) package manager.
2. Install the GibsonAI CLI:
    ```bash
    uvx --from gibson-cli@latest gibson auth login
    ```
3. Install the required dependencies:
    ```bash
    pip install agno mcp openai
    ```
4. Export your API key:
    ```bash
    export OPENAI_API_KEY="your_openai_api_key"
    ```
5. Run the GibsonAI agent by running this file.
6. Check created database and schema on GibsonAI dashboard: https://app.gibsonai.com

This logs you into the [GibsonAI CLI](https://docs.gibsonai.com/reference/cli-quickstart)
so you can access all the features directly from your agent.

"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools


async def run_gibsonai_agent(message: str):
    """Run the GibsonAI agent with the given message."""
    mcp_tools = MCPTools(
        "uvx --from gibson-cli@latest gibson mcp run",
        timeout_seconds=300,  # Extended timeout for GibsonAI operations
    )

    # Connect to the MCP server
    await mcp_tools.connect()

    agent = Agent(
        name="GibsonAIAgent",
        model=OpenAIChat(id="gpt-4o"),
        tools=[mcp_tools],
        description="Agent for managing database projects and schemas",
        instructions=dedent("""\
            You are a GibsonAI database assistant. Help users manage their database projects and schemas.

            Your capabilities include:
            - Creating new GibsonAI projects
            - Managing database schemas (tables, columns, relationships)
            - Deploying schema changes to hosted databases
            - Querying database schemas and data
            - Providing insights about database structure and best practices
        """),
        markdown=True,
    )

    # Run the agent
    await agent.aprint_response(message, stream=True)

    # Close the MCP connection
    await mcp_tools.close()


# Example usage
if __name__ == "__main__":
    asyncio.run(
        run_gibsonai_agent(
            """
            Create a database for blog posts platform with users and posts tables.
            You can decide the schema of the tables without double checking with me.
            """
        )
    )
```

---

<a name="tools--mcp--githubpy"></a>

### `tools/mcp/github.py`

```python
""" MCP GitHub Agent - Your Personal GitHub Explorer!

This example shows how to create a GitHub agent that uses MCP to explore,
analyze, and provide insights about GitHub repositories. The agent leverages the Model
Context Protocol (MCP) to interact with GitHub, allowing it to answer questions
about issues, pull requests, repository details and more.

Example prompts to try:
- "List open issues in the repository"
- "Show me recent pull requests"
- "What are the repository statistics?"
- "Find issues labeled as bugs"
- "Show me contributor activity"

Run: `pip install agno mcp openai` to install the dependencies
Environment variables needed:
- Create a GitHub personal access token following these steps:
    - https://github.com/modelcontextprotocol/servers/tree/main/src/github#setup
- export GITHUB_TOKEN: Your GitHub personal access token
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters


async def run_agent(message: str) -> None:
    """Run the GitHub agent with the given message."""

    # Initialize the MCP server
    server_params = StdioServerParameters(
        command="npx",
        args=["-y", "@modelcontextprotocol/server-github"],
    )

    # Create a client session to connect to the MCP server
    async with MCPTools(server_params=server_params) as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a GitHub assistant. Help users explore repositories and their activity.

                - Use headings to organize your responses
                - Be concise and focus on relevant information\
            """),
            markdown=True,
        )

        # Run the agent
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Pull request example
    asyncio.run(
        run_agent(
            "Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information."
        )
    )


# More example prompts to explore:
"""
Issue queries:
1. "Find issues needing attention"
2. "Show me issues by label"
3. "What issues are being actively discussed?"
4. "Find related issues"
5. "Analyze issue resolution patterns"

Pull request queries:
1. "What PRs need review?"
2. "Show me recent merged PRs"
3. "Find PRs with conflicts"
4. "What features are being developed?"
5. "Analyze PR review patterns"

Repository queries:
1. "Show repository health metrics"
2. "What are the contribution guidelines?"
3. "Find documentation gaps"
4. "Analyze code quality trends"
5. "Show repository activity patterns"
"""
```

---

<a name="tools--mcp--graphitipy"></a>

### `tools/mcp/graphiti.py`

```python
"""
 MCP Graphiti Agent - A personal diary assistant

This example demonstrates how to use Agno's MCP integration together with Graphiti, to build a personal diary assistant.

- Run your Graphiti MCP server. Full instructions: https://github.com/getzep/graphiti/tree/main/mcp_server
- Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

mcp_server_url = "http://localhost:8000/sse"


async def run_agent(message: str) -> None:
    async with MCPTools(url=mcp_server_url, transport="sse") as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            model=OpenAIChat(id="o3-mini"),
            instructions=dedent(
                """
                You are an assistant with access to tools related to Graphiti's knowledge graph capabilities.
                You maintain a diary for the user.
                Your job is to help them add new entries and use the diary data to answer their questions.
                """
            ),
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    asyncio.run(
        # Using the agent to add new entries to the diary
        run_agent(
            "Add the following entry to the diary: 'Today I spent some time building agents with Agno'"
        )
    )

    asyncio.run(
        # Using the agent to answer questions about the diary
        run_agent("What have I been building recently?")
    )
```

---

<a name="tools--mcp--groq_mcppy"></a>

### `tools/mcp/groq_mcp.py`

```python
""" Groq + MCP = Lightning Fast Agents

This example demonstrates how to create a high-performance filesystem agent by combining
Groq's fast LLM inference with the Model Context Protocol (MCP). This combination delivers
exceptional speed while maintaining powerful filesystem exploration capabilities.

Example prompts to try:
- "What files are in the current directory?"
- "Show me the content of README.md"
- "What is the license for this project?"
- "Find all Python files in the project"
- "Analyze the performance benefits of using Groq with MCP"

Run: `pip install agno mcp openai` to install the dependencies
"""

import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


async def create_filesystem_agent(session):
    """Create and configure a high-performance filesystem agent with Groq and MCP."""
    # Initialize the MCP toolkit
    mcp_tools = MCPTools(session=session)
    await mcp_tools.initialize()

    # Create an agent with the MCP toolkit and Groq's fast LLM
    return Agent(
        model=Groq(id="llama-3.3-70b-versatile"),
        tools=[mcp_tools],
        instructions=dedent("""\
            You are a high-performance filesystem assistant powered by Groq and MCP.
            Your combination of Groq's fast inference and MCP's efficient context handling
            makes you exceptionally quick at exploring and analyzing files.

            - Navigate the filesystem with lightning speed to answer questions
            - Use the list_allowed_directories tool to find directories that you can access
            - Highlight the performance benefits of the Groq+MCP combination when relevant
            - Provide clear context about files you examine
            - Use headings to organize your responses
            - Be concise and focus on relevant information\
        """),
        markdown=True,
    )


async def run_agent(message: str) -> None:
    """Run the filesystem agent with the given message."""
    # Initialize the MCP server
    server_params = StdioServerParameters(
        command="npx",
        args=[
            "-y",
            "@modelcontextprotocol/server-filesystem",
            str(Path(__file__).parent.parent.parent.parent),
        ],
    )

    # Create a client session to connect to the MCP server
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            agent = await create_filesystem_agent(session)

            # Run the agent
            await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Basic example - exploring project license
    asyncio.run(run_agent("What is the license for this project?"))

    # Performance demonstration example
    asyncio.run(
        run_agent(
            "Show me the README.md and explain how Groq with MCP enables fast file analysis"
        )
    )


# More example prompts to explore:
"""
Performance-focused queries:
1. "Analyze a large Python file and explain how Groq+MCP makes this fast"
2. "Compare the directory structure and explain how MCP efficiently provides this information"
3. "Find all TODO comments in the codebase and demonstrate the speed advantage"
4. "Process multiple configuration files simultaneously and explain the performance benefits"
5. "Explain how the Groq+MCP combination optimizes context handling for large codebases"

File exploration queries:
1. "What are the main Python packages used in this project?"
2. "Show me all configuration files and explain their purpose"
3. "Find all test files and summarize what they're testing"
4. "What's the project's entry point and how does it work?"
5. "Analyze the project's dependency structure"

Code analysis queries:
1. "Explain the architecture of this codebase"
2. "What design patterns are used in this project?"
3. "Find potential security issues in the codebase"
4. "How is error handling implemented across the project?"
5. "Analyze the API endpoints in this project"
"""
```

---

<a name="tools--mcp--include_exclude_toolspy"></a>

### `tools/mcp/include_exclude_tools.py`

```python
"""
This example demonstrates how to use multiple MCP servers in a single agent.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API key from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com
"""

import asyncio

from agno.agent import Agent
from agno.tools.mcp import MultiMCPTools


async def run_agent(message: str) -> None:
    """Run the GitHub agent with the given message.

    Remember to set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    """

    # Initialize the MCP server
    async with MultiMCPTools(
        [
            "npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt",
            "npx -y @modelcontextprotocol/server-google-maps",
        ],
        include_tools=["airbnb_search"],
        exclude_tools=["maps_place_details"],
    ) as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            markdown=True,
        )

        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    asyncio.run(
        run_agent(
            "What listings are available in Cape Town for 2 people for 3 nights from 1 to 4 August 2025?"
        )
    )

    asyncio.run(run_agent("What restaurants are open right now in Cape Town?"))
```

---

<a name="tools--mcp--include_toolspy"></a>

### `tools/mcp/include_tools.py`

```python
import asyncio
from pathlib import Path
from textwrap import dedent

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    file_path = str(Path(__file__).parents[3] / "libs/agno")

    # Initialize the MCP server
    async with (
        MCPTools(
            f"npx -y @modelcontextprotocol/server-filesystem {file_path}",
            include_tools=[
                "list_allowed_directories",
                "list_directory",
                "read_file",
            ],
        ) as fs_tools,
    ):
        agent = Agent(
            model=Groq(id="llama-3.3-70b-versatile"),
            tools=[fs_tools],
            instructions=dedent("""\
                - First, ALWAYS use the list_allowed_directories tool to find directories that you can access
                - Use the list_directory tool to list the contents of a directory
                - Use the read_file tool to read the contents of a file
                - Be concise and focus on relevant information\
            """),
            markdown=True,
        )
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    asyncio.run(run_agent("What is the license for this project?"))
```

---

<a name="tools--mcp--local_server--clientpy"></a>

### `tools/mcp/local_server/client.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.mcp import MCPTools


async def run_agent(message: str) -> None:
    # Initialize the MCP server
    async with (
        MCPTools(
            "fastmcp run cookbook/tools/mcp/local_server/server.py",  # Supply the command to run the MCP server
        ) as mcp_tools,
    ):
        agent = Agent(
            model=Groq(id="llama-3.3-70b-versatile"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    asyncio.run(run_agent("What is the weather in San Francisco?"))
```

---

<a name="tools--mcp--local_server--serverpy"></a>

### `tools/mcp/local_server/server.py`

```python
"""
`fastmcp` is required for this demo.

```bash
pip install fastmcp
```

Run this with `fastmcp run cookbook/tools/mcp/local_server/server.py`
"""

from fastmcp import FastMCP

mcp = FastMCP("weather_tools")


@mcp.tool()
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"


@mcp.tool()
def get_temperature(city: str) -> str:
    return f"The temperature in {city} is 70 degrees"


if __name__ == "__main__":
    mcp.run(transport="stdio")
```

---

<a name="tools--mcp--mcp_toolbox_demo--agentpy"></a>

### `tools/mcp/mcp_toolbox_demo/agent.py`

```python
"""
Simple test script that connects to the MCP toolbox server
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp_toolbox import MCPToolbox

url = "http://127.0.0.1:5001"


async def run_agent(message: str = None) -> None:
    """Run an interactive CLI for the Hotel agent with the given message."""

    # Approach 1: Load specific toolset at initialization
    async with MCPToolbox(
        url=url, toolsets=["hotel-management", "booking-system"]
    ) as db_tools:
        # returns a list of tools from a toolset
        agent = Agent(
            model=OpenAIChat(),
            tools=[db_tools],
            instructions=dedent(
                """ \
                You're a helpful hotel assistant. You handle hotel searching, booking and
                cancellations. When the user searches for a hotel, mention it's name, id,
                location and price tier. Always mention hotel ids while performing any
                searches. This is very important for any operations. For any bookings or
                cancellations, please provide the appropriate confirmation. Be sure to
                update checkin or checkout dates if mentioned by the user.
                Don't ask for confirmations from the user.
            """
            ),
            markdown=True,
        )

        # Run an interactive command-line interface to interact with the agent.
        await agent.acli_app(message=message, stream=True)


async def run_agent_manual_loading(message: str) -> None:
    """Alternative approach: Manual loading with custom auth parameters."""

    # Approach 2: Manual loading with custom auth parameters
    async with MCPToolbox(url=url) as toolbox:  # No filter parameters
        # Load specific toolsets with custom auth
        hotel_tools = await toolbox.load_toolset(
            "hotel-management",
            auth_token_getters={"hotel_api": lambda: "your-hotel-api-key"},
            bound_params={"region": "us-east-1"},
        )

        booking_tools = await toolbox.load_toolset(
            "booking-system",
            auth_token_getters={"booking_api": lambda: "your-booking-api-key"},
            bound_params={"environment": "production"},
        )

        # Combine tools as needed
        selected_tools = []
        selected_tools.extend(hotel_tools)
        selected_tools.extend(booking_tools[:2])  # Only first 2 booking tools

        agent = Agent(
            tools=selected_tools,
            instructions=dedent(
                """ \
                You're a helpful hotel assistant. You handle hotel searching, booking and
                cancellations. When the user searches for a hotel, mention it's name, id,
                location and price tier. Always mention hotel ids while performing any
                searches. This is very important for any operations. For any bookings or
                cancellations, please provide the appropriate confirmation. Be sure to
                update checkin or checkout dates if mentioned by the user.
                Don't ask for confirmations from the user.
            """
            ),
            markdown=True,
            show_tool_calls=True,
            add_history_to_messages=True,
            debug_mode=True,
        )

        await agent.acli_app(message=message, stream=True)


if __name__ == "__main__":
    # Use the original approach
    asyncio.run(run_agent(message=None))

    # Or use the manual loading approach
    # asyncio.run(run_agent_manual_loading(message=None))
```

---

<a name="tools--mcp--mcp_toolbox_demo--agent_ospy"></a>

### `tools/mcp/mcp_toolbox_demo/agent_os.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.os import AgentOS
from agno.tools.mcp_toolbox import MCPToolbox

url = "http://127.0.0.1:5001"

mcp_database_tools = MCPToolbox(
    url=url, toolsets=["hotel-management", "booking-system"]
)

agent = Agent(
    tools=[mcp_database_tools],
    instructions=dedent(
        """ \
        You're a helpful hotel assistant. You handle hotel searching, booking and
        cancellations. When the user searches for a hotel, mention it's name, id,
        location and price tier. Always mention hotel ids while performing any
        searches. This is very important for any operations. For any bookings or
        cancellations, please provide the appropriate confirmation. Be sure to
        update checkin or checkout dates if mentioned by the user.
        Don't ask for confirmations from the user.
    """
    ),
    markdown=True,
)


agent_os = AgentOS(
    name="Hotel Assistant",
    description="An agent that helps users find and book hotels.",
    agents=[agent],
)

app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="agent_os:app", reload=True)
```

---

<a name="tools--mcp--mcp_toolbox_demo--hotel_management_typesafepy"></a>

### `tools/mcp/mcp_toolbox_demo/hotel_management_typesafe.py`

```python
import asyncio
from datetime import date
from textwrap import dedent
from typing import List, Literal

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp_toolbox import MCPToolbox
from pydantic import BaseModel, Field

url = "http://127.0.0.1:5001"

toolsets = ["hotel-management", "booking-system"]


class Hotel(BaseModel):
    id: int = Field(..., description="Unique identifier for the hotel")
    name: str = Field(..., description="Name of the hotel")
    location: str = Field(..., description="Location of the hotel")
    checkin_date: date = Field(..., description="Check-in date for the hotel stay")
    checkout_date: date = Field(..., description="Check-out date for the hotel stay")
    price_tier: Literal["Luxury", "Economy", "Boutique", "Extended-Stay"] = Field(
        description="The hotel tier/category - must be one of: Luxury, Economy, Boutique, or Extended-Stay"
    )
    booked: str = Field(
        description="Indicates if the hotel is booked (bit field from database)"
    )


class HotelSearch(BaseModel):
    location: str = Field(
        ...,
        description="The city, region, or specific location to search for hotels",
        min_length=1,
        max_length=100,
    )
    tier: Literal["Luxury", "Economy", "Boutique", "Extended-Stay"] = Field(
        description="The hotel tier/category to search for"
    )


class HotelSearchResult(BaseModel):
    hotels: List[Hotel] = Field(
        description="List of hotels matching the search criteria"
    )
    total_results: int = Field(description="Total number of hotels found")


agent = Agent(
    tools=[],
    instructions=dedent(
        """ \
        You're a helpful hotel assistant. You handle hotel searching, booking and
        cancellations. When the user searches for a hotel, mention it's name, id,
        location and price tier. Always mention hotel ids while performing any
        searches. This is very important for any operations. For any bookings or
        cancellations, please provide the appropriate confirmation. Be sure to
        update checkin or checkout dates if mentioned by the user.
        Don't ask for confirmations from the user.
    """
    ),
    markdown=True,
    input_schema=HotelSearch,
    output_schema=HotelSearchResult,
    parser_model=OpenAIChat("gpt-4o-mini"),
    debug_mode=True,
    debug_level=2,
)


async def run_agent(hotel_search: HotelSearch) -> None:
    async with MCPToolbox(url=url, toolsets=toolsets) as tools:
        agent.tools = [tools]
        await agent.aprint_response(hotel_search)


if __name__ == "__main__":
    hotel_search = HotelSearch(
        location="Zurich",
        tier="Boutique",
    )

    asyncio.run(run_agent(hotel_search))
```

---

<a name="tools--mcp--mcp_toolbox_demo--hotel_management_workflowspy"></a>

### `tools/mcp/mcp_toolbox_demo/hotel_management_workflows.py`

```python
#!/usr/bin/env python3
"""Sequential Workflow Demo: Hotel Search  Hotel Booking"""

import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.mcp_toolbox import MCPToolbox
from agno.workflow.condition import Step
from agno.workflow.workflow import Workflow

# Configuration
url = "http://127.0.0.1:5001"

# Database for workflow
db = SqliteDb(db_file="tmp/workflow_demo.db")

# Create agents with different toolsets
search_agent = Agent(
    name="Hotel Search Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a hotel search expert. Find hotels based on user requirements.",
        "Always provide hotel IDs, names, locations, and availability.",
        "Be specific about which hotels are available for booking.",
    ],
)

booking_agent = Agent(
    name="Hotel Booking Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a booking specialist. Book hotels using the hotel IDs provided.",
        "Always confirm successful bookings with hotel name and ID.",
        "If booking fails, explain the reason clearly.",
    ],
)

# Define workflow steps
search_step = Step(
    name="Search Hotels",
    agent=search_agent,
)

booking_step = Step(
    name="Book Hotel",
    agent=booking_agent,
)

# Create the workflow
workflow = Workflow(
    name="hotel-workflow",
    description="Search and book hotels sequentially",
    db=db,
    steps=[search_step, booking_step],
)


async def run_workflow_demo():
    """Run the hotel workflow with MCP toolboxes"""

    # Create separate toolboxes for each agent's role
    search_tools = MCPToolbox(url=url, toolsets=["hotel-management"])
    booking_tools = MCPToolbox(url=url, toolsets=["booking-system"])

    async with search_tools, booking_tools:
        # Assign tools to agents
        search_agent.tools = [search_tools]
        booking_agent.tools = [booking_tools]

        # Input for the workflow
        user_request = "Find luxury hotels in Zurich and book the first available one"

        print(" Hotel Search and Booking Workflow")
        print(f"Request: {user_request}")
        print("=" * 50)

        # Execute workflow
        result = await workflow.arun(user_request)

        print("\n Workflow Result:")
        print(f"Content: {result.content}")
        print(f"Steps executed: {len(result.step_results)}")

        return result


if __name__ == "__main__":
    asyncio.run(run_workflow_demo())
```

---

<a name="tools--mcp--mcp_toolbox_for_dbpy"></a>

### `tools/mcp/mcp_toolbox_for_db.py`

```python
"""Example code showcasing how to connect to the MCP toolbox server using the MCPToolbox Toolkit"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp_toolbox import MCPToolbox

url = "http://127.0.0.1:5001"


async def run_agent(message: str = None) -> None:
    """Run an interactive CLI for the GitHub agent with the given message."""

    # Approach 1: Load specific toolset at initialization
    async with MCPToolbox(
        url=url, toolsets=["hotel-management", "booking-system"]
    ) as db_tools:
        print(db_tools.functions)  # Print available tools for debugging
        # returns a list of tools from a toolset
        agent = Agent(
            tools=[db_tools],
            instructions=dedent(
                """ \
                You're a helpful hotel assistant. You handle hotel searching, booking and
                cancellations. When the user searches for a hotel, mention it's name, id,
                location and price tier. Always mention hotel ids while performing any
                searches. This is very important for any operations. For any bookings or
                cancellations, please provide the appropriate confirmation. Be sure to
                update checkin or checkout dates if mentioned by the user.
                Don't ask for confirmations from the user.
            """
            ),
            markdown=True,
            show_tool_calls=True,
            add_history_to_messages=True,
            debug_mode=True,
        )

        # Run an interactive command-line interface to interact with the agent.
        await agent.acli_app(message=message, stream=True)


async def run_agent_manual_loading(message: str) -> None:
    """Alternative approach: Manual loading with custom auth parameters."""

    # Approach 2: Manual loading with custom auth parameters
    async with MCPToolbox(url=url) as toolbox:  # No filter parameters
        # Load specific toolsets with custom auth
        hotel_tools = await toolbox.load_toolset(
            "hotel-management",
            auth_token_getters={"hotel_api": lambda: "your-hotel-api-key"},
            bound_params={"region": "us-east-1"},
        )

        booking_tools = await toolbox.load_toolset(
            "booking-system",
            auth_token_getters={"booking_api": lambda: "your-booking-api-key"},
            bound_params={"environment": "production"},
        )

        # Combine tools as needed
        selected_tools = []
        selected_tools.extend(hotel_tools)
        selected_tools.extend(booking_tools[:2])  # Only first 2 booking tools

        agent = Agent(
            tools=selected_tools,
            instructions=dedent(
                """ \
                You're a helpful hotel assistant. You handle hotel searching, booking and
                cancellations. When the user searches for a hotel, mention it's name, id,
                location and price tier. Always mention hotel ids while performing any
                searches. This is very important for any operations. For any bookings or
                cancellations, please provide the appropriate confirmation. Be sure to
                update checkin or checkout dates if mentioned by the user.
                Don't ask for confirmations from the user.
            """
            ),
            markdown=True,
            show_tool_calls=True,
            add_history_to_messages=True,
            debug_mode=True,
        )

        await agent.acli_app(message=message, stream=True)


async def run_agent_no_ctx_manager(message: str = None) -> None:
    """Run an interactive CLI for the GitHub agent with the given message."""

    # Approach 1: Load specific toolset at initialization
    toolbox = MCPToolbox(url=url, toolsets=["hotel-management", "booking-system"])

    await toolbox.connect()

    agent = Agent(
        tools=[toolbox],
        instructions=dedent(
            """ \
            You're a helpful hotel assistant. You handle hotel searching, booking and
            cancellations. When the user searches for a hotel, mention it's name, id,
                location and price tier. Always mention hotel ids while performing any
                searches. This is very important for any operations. For any bookings or
                cancellations, please provide the appropriate confirmation. Be sure to
                update checkin or checkout dates if mentioned by the user.
                Don't ask for confirmations from the user.
            """
        ),
        markdown=True,
        show_tool_calls=True,
        add_history_to_messages=True,
        debug_mode=True,
    )

    await agent.acli_app(message=message, stream=True)


if __name__ == "__main__":
    asyncio.run(run_agent(message=None))

    # Or use the manual loading approach
    # asyncio.run(run_agent_manual_loading(message=None))

    # Or use without context manager
    # asyncio.run(run_agent_no_ctx_manager(message=None))
```

---

<a name="tools--mcp--mem0py"></a>

### `tools/mcp/mem0.py`

```python
"""
 Mem0 MCP - Personalized Code Reviewer

This example demonstrates how to use Agno's MCP integration together with Mem0, to build a personalized code reviewer.

- Run your Mem0 MCP server. Full instructions: https://github.com/mem0ai/mem0-mcp
- Run: `pip install agno mcp` to install the dependencies
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

mcp_server_url = "http://localhost:8080/sse"


async def run_agent(message: str) -> None:
    async with MCPTools(url=mcp_server_url, transport="sse") as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            model=OpenAIChat(id="o4-mini"),
            instructions=dedent(
                """
                You are a professional code reviewer. You help users keep their code clean and on line with their preferences.
                You have access to some tools to keep track of coding preferences you need to enforce when reviewing code.
                You will be given a code snippet and you need to review it and provide feedback on it.
                """
            ),
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    # The agent will use mem0 memory to keep track of the user's preferences.
    asyncio.run(
        run_agent(
            "When possible, use the walrus operator to make the code more readable."
        )
    )
    # The agent will review your code and propose improvements based on your preferences.
    asyncio.run(
        run_agent(
            dedent(
                """
Please, review this Python snippet:

```python
def process_data(data):
    length = len(data)
    if length > 10:
        print(f"Processing {length} items")
        return data[:10]
    return data

# Example usage
items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
result = process_data(items)
```
"""
            )
        )
    )
```

---

<a name="tools--mcp--multiple_serverspy"></a>

### `tools/mcp/multiple_servers.py`

```python
"""
This example demonstrates how to use multiple MCP servers in a single agent.

Prerequisites:
- Set the environment variable "ACCUWEATHER_API_KEY" for the weather MCP tools.
- You can get the API key from the AccuWeather website: https://developer.accuweather.com/
"""

import asyncio
from os import getenv

from agno.agent import Agent
from agno.tools.mcp import MultiMCPTools


async def run_agent(message: str) -> None:
    # Initialize the MCP tools
    mcp_tools = MultiMCPTools(
        [
            "npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt",
            "npx -y @modelcontextprotocol/server-brave-search",
        ],
        env={
            "BRAVE_API_KEY": getenv("BRAVE_API_KEY"),
        },
        timeout_seconds=30,
    )

    # Connect to the MCP servers
    await mcp_tools.connect()

    # Use the MCP tools with an Agent
    agent = Agent(
        tools=[mcp_tools],
        markdown=True,
    )
    await agent.aprint_response(message)

    # Close the MCP connection
    await mcp_tools.close()


# Example usage
if __name__ == "__main__":
    asyncio.run(run_agent("What listings are available in Barcelona tonight?"))
    asyncio.run(run_agent("What's the fastest way to get to Barcelona from London?"))
```

---

<a name="tools--mcp--notion_mcp_agentpy"></a>

### `tools/mcp/notion_mcp_agent.py`

```python
"""
Notion MCP Agent - Manages your documents

This example shows how to use the Agno MCP tools to interact with your Notion workspace.

1. Start by setting up a new internal integration in Notion: https://www.notion.so/profile/integrations
2. Export your new Notion key: `export NOTION_API_KEY=ntn_****`
3. Connect your relevant Notion pages to the integration. To do this, you'll need to visit that page, and click on the 3 dots, and select "Connect to integration".

Dependencies: pip install agno mcp openai

Usage:
  python cookbook/tools/mcp/notion_mcp_agent.py
"""

import asyncio
import json
import os
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters


async def run_agent():
    token = os.getenv("NOTION_API_KEY")
    if not token:
        raise ValueError(
            "Missing Notion API key: provide --NOTION_API_KEY or set NOTION_API_KEY environment variable"
        )

    command = "npx"
    args = ["-y", "@notionhq/notion-mcp-server"]
    env = {
        "OPENAPI_MCP_HEADERS": json.dumps(
            {"Authorization": f"Bearer {token}", "Notion-Version": "2022-06-28"}
        )
    }
    server_params = StdioServerParameters(command=command, args=args, env=env)

    async with MCPTools(server_params=server_params) as mcp_tools:
        agent = Agent(
            name="NotionDocsAgent",
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            description="Agent to query and modify Notion docs via MCP",
            instructions=dedent("""\
                You have access to Notion documents through MCP tools.
                - Use tools to read, search, or update pages.
                - Confirm with the user before making modifications.
            """),
            markdown=True,
        )

        await agent.acli_app(
            input="You are a helpful assistant that can access Notion workspaces and pages.",
            stream=True,
            markdown=True,
            exit_on=["exit", "quit"],
        )


if __name__ == "__main__":
    asyncio.run(run_agent())
```

---

<a name="tools--mcp--oxylabspy"></a>

### `tools/mcp/oxylabs.py`

```python
import asyncio
import os

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.mcp import MCPTools


async def run_agent_prompt():
    async with MCPTools(
        command="uvx oxylabs-mcp",
        env={
            "OXYLABS_USERNAME": os.getenv("OXYLABS_USERNAME"),
            "OXYLABS_PASSWORD": os.getenv("OXYLABS_PASSWORD"),
        },
    ) as server:
        agent = Agent(
            model=Gemini(api_key=os.getenv("GEMINI_API_KEY")),
            tools=[server],
            instructions=["Use MCP tools to fulfill the requests"],
            markdown=True,
        )
        await agent.aprint_response(
            "Go to oxylabs.io, look for career page, "
            "go to it and return all job titles in markdown format. "
            "Don't invent URLs, start from one provided."
        )


if __name__ == "__main__":
    asyncio.run(run_agent_prompt())
```

---

<a name="tools--mcp--pipedream_authpy"></a>

### `tools/mcp/pipedream_auth.py`

```python
"""
 Using Pipedream MCP servers with authentication

This is an example of how to use Pipedream MCP servers with authentication.
This is useful if your app is interfacing with the MCP servers in behalf of your users.

1. Get your access token. You can check how in Pipedream's docs: https://pipedream.com/docs/connect/mcp/developers/
2. Get the URL of the MCP server. It will look like this: https://remote.mcp.pipedream.net/<External user id>/<MCP app slug>
3. Set the environment variables:
    - MCP_SERVER_URL: The URL of the MCP server you previously got
    - MCP_ACCESS_TOKEN: The access token you previously got
    - PIPEDREAM_PROJECT_ID: The project id of the Pipedream project you want to use
    - PIPEDREAM_ENVIRONMENT: The environment of the Pipedream project you want to use
3. Install dependencies: pip install agno mcp
"""

import asyncio
from os import getenv

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools, StreamableHTTPClientParams
from agno.utils.log import log_exception

mcp_server_url = getenv("MCP_SERVER_URL")
mcp_access_token = getenv("MCP_ACCESS_TOKEN")
pipedream_project_id = getenv("PIPEDREAM_PROJECT_ID")
pipedream_environment = getenv("PIPEDREAM_ENVIRONMENT")


server_params = StreamableHTTPClientParams(
    url=mcp_server_url,
    headers={
        "Authorization": f"Bearer {mcp_access_token}",
        "x-pd-project-id": pipedream_project_id,
        "x-pd-environment": pipedream_environment,
    },
)


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            server_params=server_params, transport="streamable-http", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(input=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    # The agent can read channels, users, messages, etc.
    asyncio.run(run_agent("Show me the latest message in the channel #general"))
```

---

<a name="tools--mcp--pipedream_google_calendarpy"></a>

### `tools/mcp/pipedream_google_calendar.py`

```python
"""
 Pipedream Google Calendar MCP

This example shows how to use Pipedream MCP servers (in this case the Google Calendar one) with Agno Agents.

1. Connect your Pipedream and Google Calendar accounts: https://mcp.pipedream.com/app/google_calendar
2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/google_calendar
3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above
4. Install dependencies: pip install agno mcp
"""

import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.utils.log import log_exception

mcp_server_url = os.getenv("MCP_SERVER_URL")


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            url=mcp_server_url, transport="sse", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(input=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    asyncio.run(
        run_agent("Tell me about all events I have in my calendar for tomorrow")
    )
```

---

<a name="tools--mcp--pipedream_linkedinpy"></a>

### `tools/mcp/pipedream_linkedin.py`

```python
"""
 Pipedream LinkedIn MCP

This example shows how to use Pipedream MCP servers (in this case the LinkedIn one) with Agno Agents.

1. Connect your Pipedream and LinkedIn accounts: https://mcp.pipedream.com/app/linkedin
2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/linkedin
3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above
4. Install dependencies: pip install agno mcp
"""

import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.utils.log import log_exception

mcp_server_url = os.getenv("MCP_SERVER_URL")


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            url=mcp_server_url, transport="sse", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(input=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    asyncio.run(
        run_agent("Check the Pipedream organization on LinkedIn and tell me about it")
    )
```

---

<a name="tools--mcp--pipedream_slackpy"></a>

### `tools/mcp/pipedream_slack.py`

```python
"""
 Pipedream Slack MCP

This example shows how to use Pipedream MCP servers (in this case the Slack one) with Agno Agents.

1. Connect your Pipedream and Slack accounts: https://mcp.pipedream.com/app/slack
2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/slack
3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above
4. Install dependencies: pip install agno mcp

"""

import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.utils.log import log_exception

mcp_server_url = os.getenv("MCP_SERVER_URL")


async def run_agent(task: str) -> None:
    try:
        async with MCPTools(
            url=mcp_server_url, transport="sse", timeout_seconds=20
        ) as mcp:
            agent = Agent(
                model=OpenAIChat(id="gpt-4o-mini"),
                tools=[mcp],
                markdown=True,
            )
            await agent.aprint_response(input=task, stream=True)
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    # The agent can read channels, users, messages, etc.
    asyncio.run(run_agent("Show me the latest message in the channel #general"))

    # Use your real Slack name for this one to work!
    asyncio.run(
        run_agent("Send a message to <YOUR_NAME> saying 'Hello, I'm your Agno Agent!'")
    )
```

---

<a name="tools--mcp--qdrantpy"></a>

### `tools/mcp/qdrant.py`

```python
import asyncio
from os import getenv

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.mcp import MCPTools
from agno.utils.pprint import apprint_run_response

QDRANT_URL = getenv("QDRANT_URL")
QDRANT_API_KEY = getenv("QDRANT_API_KEY")
COLLECTION_NAME = "qdrant_collection"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"


async def run_agent(message: str) -> None:
    async with MCPTools(
        "uvx mcp-server-qdrant",
        env={
            "QDRANT_URL": QDRANT_URL,
            "QDRANT_API_KEY": QDRANT_API_KEY,
            "COLLECTION_NAME": COLLECTION_NAME,
            "EMBEDDING_MODEL": EMBEDDING_MODEL,
        },
    ) as mcp_tools:
        agent = Agent(
            model=Gemini(id="gemini-2.5-flash-preview-05-20"),
            tools=[mcp_tools],
            instructions="""
            You are the storage agent for the Model Context Protocol (MCP) server.
            You need to save the files in the vector database and answer the user's questions.
            You can use the following tools:
            - qdrant-store: Store data/output in the Qdrant vector database.
            - qdrant-find: Retrieve data/output from the Qdrant vector database.
            """,
            markdown=True,
        )

        response = await agent.arun(message, stream=True)
        await apprint_run_response(response)


if __name__ == "__main__":
    query = """
    Tell me about the extinction event of dinosaurs in detail. Include all possible theories and evidence. Store the result in the vector database.
    """
    asyncio.run(run_agent(query))
```

---

<a name="tools--mcp--sequential_thinkingpy"></a>

### `tools/mcp/sequential_thinking.py`

```python
"""
This example demonstrates how to use multiple MCP servers in a single agent.

Prerequisites:
- Google Maps:
    - Set the environment variable `GOOGLE_MAPS_API_KEY` with your Google Maps API key.
    You can obtain the API key from the Google Cloud Console:
    https://console.cloud.google.com/projectselector2/google/maps-apis/credentials

    - You also need to activate the Address Validation API for your .
    https://console.developers.google.com/apis/api/addressvalidation.googleapis.com
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from agno.tools.yfinance import YFinanceTools


async def run_agent(message: str) -> None:
    """Run the GitHub agent with the given message."""

    async with (
        MCPTools(
            command="npx -y @modelcontextprotocol/server-sequential-thinking"
        ) as sequential_thinking_mcp_tools,
    ):
        agent = Agent(
            tools=[
                sequential_thinking_mcp_tools,
                YFinanceTools(),
            ],
            instructions=dedent("""\
                ## Using the think tool
                Before taking any action or responding to the user after receiving tool results, use the think tool as a scratchpad to:
                - List the specific rules that apply to the current request
                - Check if all required information is collected
                - Verify that the planned action complies with all policies
                - Iterate over tool results for correctness

                ## Rules
                - Its expected that you will use the think tool generously to jot down thoughts and ideas.
                - Use tables where possible\
                """),
            markdown=True,
        )

        await agent.aprint_response(message, stream=True)


# Example usage
if __name__ == "__main__":
    # Pull request example
    asyncio.run(run_agent("Write a report comparing NVDA to TSLA"))
```

---

<a name="tools--mcp--sse_transport--clientpy"></a>

### `tools/mcp/sse_transport/client.py`

```python
"""
Show how to connect to MCP servers that use the SSE transport using our MCPTools and MultiMCPTools classes.
Check the README.md file for instructions on how to run these examples.
"""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools, MultiMCPTools

# This is the URL of the MCP server we want to use.
server_url = "http://localhost:8000/sse"


async def run_agent(message: str) -> None:
    async with MCPTools(transport="sse", url=server_url) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(input=message, stream=True, markdown=True)


# Using MultiMCPTools, we can connect to multiple MCP servers at once, even if they use different transports.
# In this example we connect to both our example server (SSE transport), and a different server (stdio transport).
async def run_agent_with_multimcp(message: str) -> None:
    async with MultiMCPTools(
        commands=["npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt"],
        urls=[server_url],
        urls_transports=["sse"],
    ) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(input=message, stream=True, markdown=True)


if __name__ == "__main__":
    asyncio.run(run_agent("Do I have any birthdays this week?"))
    asyncio.run(
        run_agent_with_multimcp(
            "Can you check when is my mom's birthday, and if there are any AirBnb listings in SF for two people for that day?"
        )
    )
```

---

<a name="tools--mcp--sse_transport--serverpy"></a>

### `tools/mcp/sse_transport/server.py`

```python
"""Start an example MCP server that uses the SSE transport."""

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("calendar_assistant")


@mcp.tool()
def get_events(day: str) -> str:
    return f"There are no events scheduled for {day}."


@mcp.tool()
def get_birthdays_this_week() -> str:
    return "It is your mom's birthday tomorrow"


if __name__ == "__main__":
    mcp.run(transport="sse")
```

---

<a name="tools--mcp--stagehandpy"></a>

### `tools/mcp/stagehand.py`

```python
"""
 Stagehand MCP Agent - Hacker News Reader's Digest

This example demonstrates how to use Agno's agent to create a Hacker News content using the Stagehand MCP server.

Features:
- Scrapes current Hacker News headlines and metadata
- Extracts top comments from popular stories
- Creates a structured digest with key insights
- Respects rate limits and community guidelines

Prerequisites:
- Clone the Stagehand MCP server: git clone https://github.com/browserbase/mcp-server-browserbase
- Build the Stagehand MCP server: cd mcp-server-browserbase/stagehand && npm install && npm run build
  - This will create a dist/index.js file in the location where you cloned the repository
- Install dependencies: pip install agno mcp
- Set environment variables: BROWSERBASE_API_KEY, BROWSERBASE_PROJECT_ID, OPENAI_API_KEY
- Run this example: python cookbook/tools/mcp/stagehand.py
"""

import asyncio
from os import environ
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters


async def run_agent(message: str) -> None:
    server_params = StdioServerParameters(
        command="node",
        # Update this path to the location where you cloned the repository
        args=["mcp-server-browserbase/stagehand/dist/index.js"],
        env=environ.copy(),
    )

    async with MCPTools(server_params=server_params, timeout_seconds=60) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a web scraping assistant that creates concise reader's digests from Hacker News.

                CRITICAL INITIALIZATION RULES - FOLLOW EXACTLY:
                1. NEVER use screenshot tool until AFTER successful navigation
                2. ALWAYS start with stagehand_navigate first
                3. Wait for navigation success message before any other actions
                4. If you see initialization errors, restart with navigation only
                5. Use stagehand_observe and stagehand_extract to explore pages safely

                Available tools and safe usage order:
                - stagehand_navigate: Use FIRST to initialize browser
                - stagehand_extract: Use to extract structured data from pages
                - stagehand_observe: Use to find elements and understand page structure
                - stagehand_act: Use to click links and navigate to comments
                - screenshot: Use ONLY after navigation succeeds and page loads

                Your goal is to create a comprehensive but concise digest that includes:
                - Top headlines with brief summaries
                - Key themes and trends
                - Notable comments and insights
                - Overall tech news landscape overview

                Be methodical, extract structured data, and provide valuable insights.
            """),
            markdown=True,
        )
        await agent.aprint_response(message, stream=True)


if __name__ == "__main__":
    asyncio.run(
        run_agent(
            "Create a comprehensive Hacker News Reader's Digest from https://news.ycombinator.com"
        )
    )
```

---

<a name="tools--mcp--streamable_http_transport--clientpy"></a>

### `tools/mcp/streamable_http_transport/client.py`

```python
"""
Show how to connect to MCP servers that use either SSE or Streamable HTTP transport using our MCPTools and MultiMCPTools classes.

Check the README.md file for instructions on how to run these examples.
"""

import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools, MultiMCPTools

# This is the URL of the MCP server we want to use.
server_url = "http://localhost:8000/mcp"


async def run_agent(message: str) -> None:
    async with MCPTools(transport="streamable-http", url=server_url) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(input=message, stream=True, markdown=True)


# Using MultiMCPTools, we can connect to multiple MCP servers at once, even if they use different transports.
# In this example we connect to both our example server (Streamable HTTP transport), and a different server (stdio transport).
async def run_agent_with_multimcp(message: str) -> None:
    async with MultiMCPTools(
        commands=["npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt"],
        urls=[server_url],
        urls_transports=["streamable-http"],
    ) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-4o"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(input=message, stream=True, markdown=True)


if __name__ == "__main__":
    asyncio.run(run_agent("Do I have any birthdays this week?"))
    asyncio.run(
        run_agent_with_multimcp(
            "Can you check when is my mom's birthday, and if there are any AirBnb listings in SF for two people for that day?",
        )
    )
```

---

<a name="tools--mcp--streamable_http_transport--serverpy"></a>

### `tools/mcp/streamable_http_transport/server.py`

```python
"""Start an example MCP server that uses the Streamable HTTP transport."""

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("calendar_assistant")


@mcp.tool()
def get_events(day: str) -> str:
    return f"There are no events scheduled for {day}."


@mcp.tool()
def get_birthdays_this_week() -> str:
    return "It is your mom's birthday tomorrow"


if __name__ == "__main__":
    mcp.run(transport="streamable-http")
```

---

<a name="tools--mcp--stripepy"></a>

### `tools/mcp/stripe.py`

```python
""" Stripe MCP Agent - Manage Your Stripe Operations

This example demonstrates how to create an Agno agent that interacts with the Stripe API via the Model Context Protocol (MCP). This agent can create and manage Stripe objects like customers, products, prices, and payment links using natural language commands.


Setup:
2. Install Python dependencies:
   ```bash
   pip install agno mcp
   ```
3. Set Environment Variable: export STRIPE_SECRET_KEY=***.

Stripe MCP Docs: https://github.com/stripe/agent-toolkit
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from agno.utils.log import log_error, log_exception, log_info


async def run_agent(message: str) -> None:
    """
    Sets up the Stripe MCP server and initialize the Agno agent
    """
    # Verify Stripe API Key is available
    stripe_api_key = os.getenv("STRIPE_SECRET_KEY")
    if not stripe_api_key:
        log_error("STRIPE_SECRET_KEY environment variable not set.")
        return

    enabled_tools = "paymentLinks.create,products.create,prices.create,customers.create,customers.read"

    # handle different Operating Systems
    npx_command = "npx.cmd" if os.name == "nt" else "npx"

    try:
        # Initialize MCP toolkit with Stripe server
        async with MCPTools(
            command=f"{npx_command} -y @stripe/mcp --tools={enabled_tools} --api-key={stripe_api_key}"
        ) as mcp_toolkit:
            agent = Agent(
                name="StripeAgent",
                instructions=dedent("""\
                    You are an AI assistant specialized in managing Stripe operations.
                    You interact with the Stripe API using the available tools.

                    - Understand user requests to create or list Stripe objects (customers, products, prices, payment links).
                    - Clearly state the results of your actions, including IDs of created objects or lists retrieved.
                    - Ask for clarification if a request is ambiguous.
                    - Use markdown formatting, especially for links or code snippets.
                    - Execute the necessary steps sequentially if a request involves multiple actions (e.g., create product, then price, then link).
                """),
                tools=[mcp_toolkit],
                markdown=True,
            )

            # Run the agent with the provided task
            log_info(f"Running agent with assignment: '{message}'")
            await agent.aprint_response(message, stream=True)

    except FileNotFoundError:
        error_msg = f"Error: '{npx_command}' command not found. Please ensure Node.js and npm/npx are installed and in your system's PATH."
        log_error(error_msg)
    except Exception as e:
        log_exception(f"An unexpected error occurred during agent execution: {e}")


if __name__ == "__main__":
    task = "Create a new Stripe product named 'iPhone'. Then create a price of $999.99 USD for it. Finally, create a payment link for that price."
    asyncio.run(run_agent(task))


# Example prompts:
"""
Customer Management:
- "Create a customer. Name: ACME Corp, Email: billing@acme.example.com"
- "List my customers."
- "Find customer by email 'jane.doe@example.com'" # Note: Requires 'customers.retrieve' or search capability

Product and Price Management:
- "Create a new product called 'Basic Plan'."
- "Create a recurring monthly price of $10 USD for product 'Basic Plan'."
- "Create a product 'Ebook Download' and a one-time price of $19.95 USD."
- "List all products." # Note: Requires 'products.list' capability
- "List all prices." # Note: Requires 'prices.list' capability

Payment Links:
- "Create a payment link for the $10 USD monthly 'Basic Plan' price."
- "Generate a payment link for the '$19.95 Ebook Download'."

Combined Tasks:
- "Create a product 'Pro Service', add a price $150 USD (one-time), and give me the payment link."
- "Register a new customer 'support@example.com' named 'Support Team'."
"""
```

---

<a name="tools--mcp--supabasepy"></a>

### `tools/mcp/supabase.py`

```python
""" Supabase MCP Agent - Showcase Supabase MCP Capabilities

This example demonstrates how to use the Supabase MCP server to create create projects, database schemas, edge functions, and more.

Setup:
1. Install Python dependencies:

```bash
pip install agno mcp
```

2. Create a Supabase Access Token: https://supabase.com/dashboard/account/tokens and set it as the SUPABASE_ACCESS_TOKEN environment variable.
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools
from agno.utils.log import log_error, log_exception, log_info


async def run_agent(task: str) -> None:
    token = os.getenv("SUPABASE_ACCESS_TOKEN")
    if not token:
        log_error("SUPABASE_ACCESS_TOKEN environment variable not set.")
        return

    npx_cmd = "npx.cmd" if os.name == "nt" else "npx"

    try:
        async with MCPTools(
            f"{npx_cmd} -y @supabase/mcp-server-supabase@latest --access-token={token}"
        ) as mcp:
            instructions = dedent(f"""
                You are an expert Supabase MCP architect. Given the project description:
                {task}

                Automatically perform the following steps :
                1. Plan the entire database schema based on the project description.
                2. Call `list_organizations` and select the first organization in the response.
                3. Use `get_cost(type='project')` to estimate project creation cost and mention the cost in your response.
                4. Create a new Supabase project with `create_project`, passing the confirmed cost ID.
                5. Poll project status with `get_project` until the status is `ACTIVE_HEALTHY`.
                6. Analyze the project requirements and propose a complete, normalized SQL schema (tables,  columns, data types, indexes, constraints, triggers, and functions) as DDL statements.
                7. Apply the schema using `apply_migration`, naming the migration `initial_schema`.
                8. Validate the deployed schema via `list_tables` and `list_extensions`.
                8. Deploy a simple health-check edge function with `deploy_edge_function`.
                9. Retrieve and print the project URL (`get_project_url`) and anon key (`get_anon_key`).
            """)
            agent = Agent(
                model=OpenAIChat(id="o4-mini"),
                instructions=instructions,
                tools=[mcp, ReasoningTools(add_instructions=True)],
                markdown=True,
            )

            log_info(f"Running Supabase project agent for: {task}")
            await agent.aprint_response(
                input=task,
                stream=True,
                stream_intermediate_steps=True,
                show_full_reasoning=True,
            )
    except Exception as e:
        log_exception(f"Unexpected error: {e}")


if __name__ == "__main__":
    demo_description = (
        "Develop a cloud-based SaaS platform with AI-powered task suggestions, calendar syncing, predictive prioritization, "
        "team collaboration, and project analytics."
    )
    asyncio.run(run_agent(demo_description))


# Example prompts to try:
"""
A SaaS tool that helps businesses automate document processing using AI. Users can upload invoices, contracts, or PDFs and get structured data, smart summaries, and red flag alerts for compliance or anomalies. Ideal for legal teams, accountants, and enterprise back offices.

An AI-enhanced SaaS platform for streamlining the recruitment process. Features include automated candidate screening using NLP, AI interview scheduling, bias detection in job descriptions, and pipeline analytics. Designed for fast-growing startups and mid-sized HR teams.

An internal SaaS tool for HR departments to monitor employee wellbeing. Combines weekly mood check-ins, anonymous feedback, and AI-driven burnout detection models. Integrates with Slack and HR systems to support a healthier workplace culture.
"""
```

---

<a name="tools--mcp_toolspy"></a>

### `tools/mcp_tools.py`

```python
import asyncio
import sys
from pathlib import Path

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


async def main(prompt: str) -> None:
    # Initialize the MCP server
    server_params = StdioServerParameters(
        command="npx",
        args=[
            "-y",
            "@modelcontextprotocol/server-filesystem",
            str(Path(__file__).parent.parent),
        ],
    )
    # Create a client session to connect to the MCP server
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Initialize the MCP toolkit
            mcp_tools = MCPTools(session=session)
            await mcp_tools.initialize()

            # Create an agent with the MCP toolkit
            agent = Agent(tools=[mcp_tools])

            # Run the agent
            await agent.aprint_response(prompt, stream=True)


if __name__ == "__main__":
    prompt = (
        sys.argv[1] if len(sys.argv) > 1 else "Read and summarize the file ./LICENSE"
    )
    asyncio.run(main(prompt))
```

---

<a name="tools--mem0_toolspy"></a>

### `tools/mem0_tools.py`

```python
"""
This example demonstrates how to use the Mem0 toolkit with Agno agents.

To get started, please export your Mem0 API key as an environment variable. You can get your Mem0 API key from https://app.mem0.ai/dashboard/api-keys

export MEM0_API_KEY=<your-mem0-api-key>
export MEM0_ORG_ID=<your-mem0-org-id> (Optional)
export MEM0_PROJECT_ID=<your-mem0-project-id> (Optional)
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mem0 import Mem0Tools

USER_ID = "jane_doe"
SESSION_ID = "agno_session"

# Example 1: Enable all Mem0 functions
agent_all = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        Mem0Tools(
            all=True,  # Enable all Mem0 memory functions
        )
    ],
    user_id=USER_ID,
    session_id=SESSION_ID,
    markdown=True,
    instructions=dedent(
        """
        You have full access to memory operations. You can create, search, update, and delete memories.
        Proactively manage memories to provide the best user experience.
        """
    ),
)

# Example 2: Enable specific Mem0 functions only
agent_specific = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        Mem0Tools(
            enable_add_memory=True,
            enable_search_memory=True,
            enable_get_all_memories=False,
            enable_delete_all_memories=False,
        )
    ],
    user_id=USER_ID,
    session_id=SESSION_ID,
    markdown=True,
    instructions=dedent(
        """
        You can add new memories and search existing ones, but cannot delete or view all memories.
        Focus on learning and recalling information about the user.
        """
    ),
)

# Example 3: Default behavior with full memory access
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        Mem0Tools(
            enable_add_memory=True,
            enable_search_memory=True,
            enable_get_all_memories=True,
            enable_delete_all_memories=True,
        )
    ],
    user_id=USER_ID,
    session_id=SESSION_ID,
    markdown=True,
    instructions=dedent(
        """
        You have an evolving memory of this user. Proactively capture new personal details,
        preferences, plans, and relevant context the user shares, and naturally bring them up
        in later conversation. Before answering questions about past details, recall from your memory
        to provide precise and personalized responses. Keep your memory concise: store only
        meaningful information that enhances long-term dialogue. If the user asks to start fresh,
        clear all remembered information and proceed anew.
        """
    ),
)

# Example usage with all functions enabled
print("=== Example 1: Using all Mem0 functions ===")
agent_all.print_response("I live in NYC and work as a software engineer")
agent_all.print_response("Summarize all my memories and delete outdated ones if needed")

# Example usage with specific functions only
print("\n=== Example 2: Using specific Mem0 functions (add + search only) ===")
agent_specific.print_response("I love Italian food, especially pasta")
agent_specific.print_response("What do you remember about my food preferences?")

# Example usage with default configuration
print("\n=== Example 3: Default Mem0 agent usage ===")
agent.print_response("I live in NYC")
agent.print_response("I lived in San Francisco for 5 years previously")
agent.print_response("I'm going to a Taylor Swift concert tomorrow")

agent.print_response("Summarize all the details of the conversation")

# More examples:
# agent.print_response("NYC has a famous Brooklyn Bridge")
# agent.print_response("Delete all my memories")
# agent.print_response("I moved to LA")
# agent.print_response("What is the name of the concert I am going to?")
```

---

<a name="tools--memori_toolspy"></a>

### `tools/memori_tools.py`

```python
"""
This example demonstrates how to use the Memori ToolKit with Agno Agents,
for persistent memory across conversations.

Run: `pip install memorisdk` to install dependencies
"""

from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.memori import MemoriTools

# Setup the Memori ToolKit
memori_tools = MemoriTools(
    database_connect="sqlite:///memori_cookbook_memory.db",
    namespace="cookbook_agent",
    # enable_* flags are optional; defaults enable all small functions
    enable_search_memory=True,
    enable_record_conversation=True,
    enable_get_memory_stats=True,
)

# Setup your Agent
agent = Agent(
    # Add the Memori ToolKit to the Agent
    tools=[memori_tools],
    model=OpenAIChat(id="gpt-4o"),
    markdown=True,
    instructions=dedent(
        """\
        Instructions:
        1. First, search your memory for relevant past conversations using the memori tool
        2. Use any relevant memories to provide a personalized response
        3. Provide a helpful and contextual answer
        4. Be conversational and friendly

        If this is the first conversation, introduce yourself and explain that you'll remember our conversations.
    """
    ),
)

# Run your Agent
agent.print_response("I'm a Python developer and I love building web applications")

# Thanks to the Memori ToolKit, your Agent can now remember the conversation:
agent.print_response("What do you remember about my programming background?")

# Using the Memori ToolKit, your Agent also gains access to memory statistics:
agent.print_response("Show me your memory statistics")


# More examples:
#
# agent.print_response("I prefer working in the morning hours, around 8-11 AM")
# agent.print_response("What were my productivity preferences again?")
# agent.print_response("I just learned React and really enjoyed it!")
# agent.print_response("Search your memory for all my technology preferences")
```

---

<a name="tools--mlx_transcribe_toolspy"></a>

### `tools/mlx_transcribe_tools.py`

```python
"""
MLX Transcribe: A tool for transcribing audio files using MLX Whisper

Requirements:
1. ffmpeg - Install using:
   - macOS: `brew install ffmpeg`
   - Ubuntu: `sudo apt-get install ffmpeg`
   - Windows: Download from https://ffmpeg.org/download.html

2. mlx-whisper library:
   pip install mlx-whisper

Example Usage:
- Place your audio files in the 'storage/audio' directory
    Eg: download https://www.ted.com/talks/reid_hoffman_and_kevin_scott_the_evolution_of_ai_and_how_it_will_impact_human_creativity
- Run this script to transcribe audio files
- Supports various audio formats (mp3, mp4, wav, etc.)
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mlx_transcribe import MLXTranscribeTools

# Get audio files from storage/audio directory
agno_root_dir = Path(__file__).parent.parent.parent.resolve()
audio_storage_dir = agno_root_dir.joinpath("storage/audio")
if not audio_storage_dir.exists():
    audio_storage_dir.mkdir(exist_ok=True, parents=True)

agent = Agent(
    name="Transcription Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[MLXTranscribeTools(base_dir=audio_storage_dir)],
    instructions=[
        "To transcribe an audio file, use the `transcribe` tool with the name of the audio file as the argument.",
        "You can find all available audio files using the `read_files` tool.",
    ],
    markdown=True,
)

agent.print_response(
    "Summarize the reid hoffman ted talk, split into sections", stream=True
)
```

---

<a name="tools--models--azure_openai_toolspy"></a>

### `tools/models/azure_openai_tools.py`

```python
"""Example showing how to use Azure OpenAI Tools with Agno.

Requirements:
1. Azure OpenAI service setup with DALL-E deployment and chat model deployment
2. Environment variables:
   - AZURE_OPENAI_API_KEY - Your Azure OpenAI API key
   - AZURE_OPENAI_ENDPOINT - The Azure OpenAI endpoint URL
   - AZURE_OPENAI_DEPLOYMENT - The deployment name for the language model
   - AZURE_OPENAI_IMAGE_DEPLOYMENT - The deployment name for an image generation model
   - OPENAI_API_KEY (for standard OpenAI example)

The script will automatically run only the examples for which you have the necessary
environment variables set.
"""

import sys
from os import getenv

from agno.agent import Agent
from agno.models.azure import AzureOpenAI
from agno.models.openai import OpenAIChat
from agno.tools.models.azure_openai import AzureOpenAITools

# Check for base requirements first - needed for all examples
# Exit early if base requirements aren't met
if not bool(
    getenv("AZURE_OPENAI_API_KEY")
    and getenv("AZURE_OPENAI_ENDPOINT")
    and getenv("AZURE_OPENAI_IMAGE_DEPLOYMENT")
):
    print("Error: Missing base Azure OpenAI requirements.")
    print("Required for all examples:")
    print("- AZURE_OPENAI_API_KEY")
    print("- AZURE_OPENAI_ENDPOINT")
    print("- AZURE_OPENAI_IMAGE_DEPLOYMENT")
    sys.exit(1)


print("Running Example 1: Standard OpenAI model with Azure OpenAI Tools")
print(
    "This approach uses OpenAI for the agent's model but Azure for image generation.\n"
)

standard_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),  # Using standard OpenAI for the agent
    tools=[AzureOpenAITools()],  # Using Azure OpenAI for image generation
    name="Mixed OpenAI Generator",
    description="An AI assistant that uses standard OpenAI for chat and Azure OpenAI for image generation",
    instructions=[
        "You are an AI artist specializing in creating images based on user descriptions.",
        "Use the generate_image tool to create detailed visualizations of user requests.",
        "Provide creative suggestions to enhance the images if needed.",
    ],
    debug_mode=True,
)

# Generate an image with the standard OpenAI model and Azure tools
standard_agent.print_response(
    "Generate an image of a futuristic city with flying cars and tall skyscrapers",
    markdown=True,
)

print("\nRunning Example 2: Full Azure OpenAI setup")
print(
    "This approach uses Azure OpenAI for both the agent's model and image generation.\n"
)

# Create an AzureOpenAI model using Azure credentials
azure_endpoint = getenv("AZURE_OPENAI_ENDPOINT")
azure_api_key = getenv("AZURE_OPENAI_API_KEY")
azure_deployment = getenv("AZURE_OPENAI_DEPLOYMENT")

# Explicitly pass all parameters to make debugging easier
azure_model = AzureOpenAI(
    azure_endpoint=azure_endpoint,
    azure_deployment=azure_deployment,
    api_key=azure_api_key,
    id=azure_deployment,  # Using the deployment name as the model ID
)

# Create an agent with Azure OpenAI model and tools
azure_agent = Agent(
    model=azure_model,  # Using Azure OpenAI for the agent
    tools=[AzureOpenAITools()],  # Using Azure OpenAI for image generation
    name="Full Azure OpenAI Generator",
    description="An AI assistant that uses Azure OpenAI for both chat and image generation",
    instructions=[
        "You are an AI artist specializing in creating images based on user descriptions.",
        "Use the generate_image tool to create detailed visualizations of user requests.",
        "Provide creative suggestions to enhance the images if needed.",
    ],
)

# Generate an image with the full Azure setup
azure_agent.print_response(
    "Generate an image of a serene Japanese garden with cherry blossoms",
    markdown=True,
)
```

---

<a name="tools--models--gemini_image_generationpy"></a>

### `tools/models/gemini_image_generation.py`

```python
""" Example: Using the GeminiTools Toolkit for Image Generation

An Agent using the Gemini image generation tool.

Example prompts to try:
- "Generate an image of a dog and tell me the color of the dog"
- "Create an image of a cat driving a car"

Run `pip install google-genai agno` to install the necessary dependencies.
"""

import base64

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[GeminiTools()],
    debug_mode=True,
)

response = agent.run(
    "Generate an image of a dog and tell me the color of the dog",
)

if response and response.images:
    for image in response.images:
        if image.content:
            image_base64 = base64.b64encode(image.content).decode("utf-8")
            save_base64_data(
                base64_data=image_base64,
                output_path=f"tmp/dog_{image.id}.png",
            )
            print(f"Image saved to tmp/dog_{image.id}.png")
```

---

<a name="tools--models--gemini_video_generationpy"></a>

### `tools/models/gemini_video_generation.py`

```python
""" Example: Using the GeminiTools Toolkit for Video Generation

An Agent using the Gemini video generation tool.

Video generation only works with Vertex AI.
Make sure you have set the GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION environment variables.

Example prompts to try:
- "Generate a 5-second video of a kitten playing a piano"
- "Create a short looping animation of a neon city skyline at dusk"

Run `pip install google-genai agno` to install the necessary dependencies.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[GeminiTools(vertexai=True)],  # Video Generation only works on VertexAI mode
    debug_mode=True,
)

agent.print_response(
    "create a video of a cat driving at top speed",
)
response = agent.get_last_run_output()
if response and response.videos:
    for video in response.videos:
        if video.content:
            save_base64_data(
                base64_data=str(video.content),
                output_path=f"tmp/cat_driving_{video.id}.mp4",
            )
```

---

<a name="tools--models--morphpy"></a>

### `tools/models/morph.py`

```python
"""
Simple example showing Morph Fast Apply with file creation and editing.
"""

from pathlib import Path

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.morph import MorphTools


def create_sample_file():
    """Create a simple Python file in tmp directory for testing"""
    # Create tmp directory if it doesn't exist
    tmp_dir = Path("tmp")
    tmp_dir.mkdir(exist_ok=True)

    # Create a simple Python file
    sample_file = tmp_dir / "calculator.py"

    sample_code = """
def add(a, b):
    return a + b

def multiply(x, y):
    result = x * y
    return result

class Calculator:
    def __init__(self):
        self.history = []
    
    def calculate(self, operation, a, b):
        if operation == "add":
            result = add(a, b)
        elif operation == "multiply":
            result = multiply(a, b)
        else:
            result = None
        return result
"""

    with open(sample_file, "w") as f:
        f.write(sample_code)

    return str(sample_file)


def main():
    target_file = create_sample_file()

    code_editor = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[MorphTools(model="morph-v3-large")],
        debug_mode=True,
        markdown=True,
    )

    # Request to improve the code
    improvement_request = f"""
        Please improve the Python code in "{target_file}" by adding:

        1. Type hints for all functions and methods
        2. Docstrings for all functions and the Calculator class
        3. Error handling and input validation
    """  # <-- Or directly provide the code here

    code_editor.print_response(improvement_request)


if __name__ == "__main__":
    main()
```

---

<a name="tools--models--nebius_toolspy"></a>

### `tools/models/nebius_tools.py`

```python
"""Run `pip install openai agno` to install dependencies.

This example demonstrates how to use NebiusTools for text-to-image generation with Nebius AI Studio.
"""

import base64
import os
from pathlib import Path
from uuid import uuid4

from agno.agent import Agent
from agno.tools.models.nebius import NebiusTools
from agno.utils.media import save_base64_data

# Create an Agent with the Nebius text-to-image tool
agent = Agent(
    tools=[
        NebiusTools(
            # You can provide your API key here or set the NEBIUS_API_KEY environment variable
            api_key=os.getenv("NEBIUS_API_KEY"),
            image_model="black-forest-labs/flux-schnell",  # Fastest model
            image_size="1024x1024",
            image_quality="standard",
        )
    ],
    name="Nebius Image Generator",
    markdown=True,
)

# Example 1: Generate a basic image
response = agent.run(
    "Generate an image of a futuristic city with flying cars and tall skyscrapers",
)

if response.images:
    image_path = Path("tmp") / f"nebius_futuristic_city_{uuid4()}.png"
    Path("tmp").mkdir(exist_ok=True)
    image_base64 = base64.b64encode(response.images[0].content).decode("utf-8")
    save_base64_data(
        base64_data=image_base64,
        output_path=str(image_path),
    )
    print(f"Image saved to {image_path}")

# Example 2: Generate an image with the higher quality model
high_quality_agent = Agent(
    tools=[
        NebiusTools(
            api_key=os.getenv("NEBIUS_API_KEY"),
            image_model="black-forest-labs/flux-dev",  # Better quality model
            image_size="1024x1024",
            image_quality="hd",  # Higher quality setting
        )
    ],
    name="Nebius High-Quality Image Generator",
    markdown=True,
)

response = high_quality_agent.run(
    "Create a detailed portrait of a cyberpunk character with neon lights",
)

# Save the generated image
if response.images:
    image_path = Path("tmp") / f"nebius_cyberpunk_character_{uuid4()}.png"
    Path("tmp").mkdir(exist_ok=True)
    image_base64 = base64.b64encode(response.images[0].content).decode("utf-8")
    save_base64_data(
        base64_data=image_base64,
        output_path=str(image_path),
    )
    print(f"High-quality image saved to {image_path}")

# Example 3: Generate an image with the SDXL (Stability Diffusion XL model) model
sdxl_agent = Agent(
    tools=[
        NebiusTools(
            api_key=os.getenv("NEBIUS_API_KEY"),
            image_model="stability-ai/sdxl",  # Stability Diffusion XL model
            image_size="1024x1024",
        )
    ],
    name="Nebius SDXL Image Generator",
    markdown=True,
)

response = sdxl_agent.run(
    "Create a fantasy landscape with a castle on a floating island",
)

# Save the generated image
if response.images:
    image_path = Path("tmp") / f"nebius_fantasy_landscape_{uuid4()}.png"
    Path("tmp").mkdir(exist_ok=True)
    image_base64 = base64.b64encode(response.images[0].content).decode("utf-8")
    save_base64_data(
        base64_data=image_base64,
        output_path=str(image_path),
    )
    print(f"SDXL image saved to {image_path}")
```

---

<a name="tools--models--openai_toolspy"></a>

### `tools/models/openai_tools.py`

```python
"""
This example demonstrates how to use the OpenAITools to transcribe an audio file.
"""

import base64
from pathlib import Path

from agno.agent import Agent
from agno.run.agent import RunOutput
from agno.tools.openai import OpenAITools
from agno.utils.media import download_file, save_base64_data

# Example 1: Transcription
url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

local_audio_path = Path("tmp/sample_conversation.wav")
print(f"Downloading file to local path: {local_audio_path}")
download_file(url, local_audio_path)

transcription_agent = Agent(
    tools=[OpenAITools(transcription_model="gpt-4o-transcribe")],
    markdown=True,
)
transcription_agent.print_response(
    f"Transcribe the audio file for this file: {local_audio_path}"
)

# Example 2: Image Generation
agent = Agent(
    tools=[OpenAITools(image_model="gpt-image-1")],
    markdown=True,
)

response = agent.run(
    "Generate an image of a sports car and tell me its color.", debug_mode=True
)

if isinstance(response, RunOutput):
    print("Agent response:", response.content)
    if response.images:
        image_base64 = base64.b64encode(response.images[0].content).decode("utf-8")
        save_base64_data(image_base64, "tmp/sports_car.png")
```

---

<a name="tools--models_lab_toolspy"></a>

### `tools/models_lab_tools.py`

```python
"""Run `pip install requests` to install dependencies."""

from agno.agent import Agent
from agno.tools.models_labs import ModelsLabTools

# Create an Agent with the ModelsLabs tool
agent = Agent(tools=[ModelsLabTools()], name="ModelsLabs Agent")

agent.print_response(
    "Generate a video of a beautiful sunset over the ocean", markdown=True
)
```

---

<a name="tools--moviepy_video_toolspy"></a>

### `tools/moviepy_video_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.moviepy_video import MoviePyVideoTools
from agno.tools.openai import OpenAITools

video_tools = MoviePyVideoTools(
    process_video=True, generate_captions=True, embed_captions=True
)

openai_tools = OpenAITools()

video_caption_agent = Agent(
    name="Video Caption Generator Agent",
    model=OpenAIChat(
        id="gpt-4o",
    ),
    tools=[video_tools, openai_tools],
    description="You are an AI agent that can generate and embed captions for videos.",
    instructions=[
        "When a user provides a video, process it to generate captions.",
        "Use the video processing tools in this sequence:",
        "1. Extract audio from the video using extract_audio",
        "2. Transcribe the audio using transcribe_audio",
        "3. Generate SRT captions using create_srt",
        "4. Embed captions into the video using embed_captions",
    ],
    markdown=True,
)


video_caption_agent.print_response(
    "Generate captions for {video with location} and embed them in the video"
)
```

---

<a name="tools--multiple_toolspy"></a>

### `tools/multiple_tools.py`

```python
"""Run `pip install openai ddgs yfinance` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(), YFinanceTools(enable_all=True)],
    instructions=["Use tables to display data"],
    markdown=True,
)
agent.print_response(
    "Write a thorough report on NVDA, get all financial information and latest news",
    stream=True,
)
```

---

<a name="tools--neo4j_toolspy"></a>

### `tools/neo4j_tools.py`

```python
"""
Example script demonstrating the use of Neo4jTools with an Agno agent.
This script sets up an agent that can interact with a Neo4j database using natural language queries,
such as listing node labels or executing Cypher queries.

## Setting up Neo4j Locally

### Option 1: Using Docker (Recommended)

1. **Install Docker** if you haven't already from https://www.docker.com/

2. **Run Neo4j in Docker:**
   ```bash
   docker run \
       --name neo4j \
       -p 7474:7474 -p 7687:7687 \
       -d \
       -v $HOME/neo4j/data:/data \
       -v $HOME/neo4j/logs:/logs \
       -v $HOME/neo4j/import:/var/lib/neo4j/import \
       -v $HOME/neo4j/plugins:/plugins \
       --env NEO4J_AUTH=neo4j/password \
       neo4j:latest
   ```

3. **Access Neo4j Browser:** Open http://localhost:7474 in your browser
   - Username: `neo4j`
   - Password: `password`

### Option 2: Native Installation

1. **Download Neo4j Desktop** from https://neo4j.com/download/
2. **Install and create a new database**
3. **Start the database** and note the connection details

### Option 3: Using Neo4j Community Edition

1. **Download** from https://neo4j.com/download-center/#community
2. **Extract and run:**
   ```bash
   tar -xf neo4j-community-*-unix.tar.gz
   cd neo4j-community-*
   ./bin/neo4j start
   ```

## Python Setup

1. **Install required packages:**
   ```bash
   pip install neo4j python-dotenv
   ```

2. **Set environment variables** (create a `.env` file in your project root):
   ```env
   NEO4J_URI=bolt://localhost:7687
   NEO4J_USERNAME=neo4j
   NEO4J_PASSWORD=password
   ```

## Usage

1. **Ensure Neo4j is running** (check http://localhost:7474)
2. **Run this script** to create an agent that can interact with your Neo4j database
3. **Test with queries** like "What are the node labels in my graph?" or "Show me the database schema"

## Troubleshooting

- **Connection refused:** Make sure Neo4j is running on the correct port (7687)
- **Authentication failed:** Verify your username/password in the Neo4j browser first
- **Import errors:** Install the neo4j driver with `pip install neo4j`
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.neo4j import Neo4jTools
from dotenv import load_dotenv

load_dotenv()

# Optionally load from environment or hardcode here
uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
user = os.getenv("NEO4J_USERNAME", "neo4j")
password = os.getenv("NEO4J_PASSWORD", "password")

# Example 1: All functions enabled (default)
neo4j_toolkit_all = Neo4jTools(
    uri=uri,
    user=user,
    password=password,
    all=True,
)

# Example 2: Specific functions only
neo4j_toolkit_specific = Neo4jTools(
    uri=uri,
    user=user,
    password=password,
    enable_list_labels=True,
    enable_get_schema=True,
    enable_list_relationships=False,
    enable_run_cypher=False,
)

# Example 3: Default behavior
neo4j_toolkit = Neo4jTools(
    uri=uri,
    user=user,
    password=password,
)

description = """You are a Neo4j expert assistant who can help with all operations in a Neo4j database by understanding natural language context and translating it into Cypher queries."""

instructions = [
    "Analyze the user's context and convert it into Cypher queries that respect the database's current schema.",
    "Before performing any operation, query the current schema (e.g., check for existing nodes or relationships).",
    "If the necessary schema elements are missing, dynamically create or extend the schema using best practices, ensuring data integrity and consistency.",
    "If properties are required or provided for nodes or relationships, ensure that they are added correctly do not overwrite existing ones and do not create duplicates and do not create extra nodes.",
    "Optionally, use or implement a dedicated function to retrieve the current schema (e.g., via a 'get_schema' function).",
    "Ensure that all operations maintain data integrity and follow best practices.",
    "Intelligently create relationships if bi-directional relationships are required, and understand the users intent and create relationships accordingly.",
    "Intelligently handle queries that involve multiple nodes and relationships, understand has to be nodes, properties, and relationships and maintain best practices.",
    "Handle errors gracefully and provide clear feedback to the user.",
]

# Example: Use with AGNO Agent
agent = Agent(
    model=OpenAIChat(id="o3-mini"),
    tools=[neo4j_toolkit],
    markdown=True,
    description=description,
    instructions=instructions,
)

# Agent handles tool usage automatically via LLM reasoning
agent.print_response(
    "Add some nodes in my graph to represent a person with the name John Doe and a person with the name Jane Doe, and they belong to company 'X' and they are friends."
)

agent.print_response("What is the schema of my graph?")
```

---

<a name="tools--newspaper4k_toolspy"></a>

### `tools/newspaper4k_tools.py`

```python
from agno.agent import Agent
from agno.tools.newspaper4k import Newspaper4kTools

agent = Agent(tools=[Newspaper4kTools(enable_read_article=True)])
agent.print_response(
    "Please summarize https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime"
)
```

---

<a name="tools--newspaper_toolspy"></a>

### `tools/newspaper_tools.py`

```python
from agno.agent import Agent
from agno.tools.newspaper import NewspaperTools

agent = Agent(tools=[NewspaperTools()])
agent.print_response("Please summarize https://en.wikipedia.org/wiki/Language_model")
```

---

<a name="tools--openbb_toolspy"></a>

### `tools/openbb_tools.py`

```python
from agno.agent import Agent
from agno.tools.openbb import OpenBBTools

# Example 1: Enable all OpenBB functions
agent_all = Agent(
    tools=[
        OpenBBTools(
            all=True,  # Enable all OpenBB financial data functions
        )
    ],
    markdown=True,
)

# Example 2: Enable specific OpenBB functions only
agent_specific = Agent(
    tools=[
        OpenBBTools(
            enable_get_stock_price=True,
            enable_search_company_symbol=True,
            enable_get_company_news=True,
            enable_get_company_profile=True,
            enable_get_price_targets=True,
        )
    ],
    markdown=True,
)

# Example 3: Default behavior with all functions enabled
agent = Agent(
    tools=[
        OpenBBTools(
            enable_get_stock_price=True,
            enable_search_company_symbol=True,
            enable_get_company_news=False,
            enable_get_company_profile=False,
            enable_get_price_targets=False,
        )
    ],
    markdown=True,
)

# Example usage with all functions enabled
print("=== Example 1: Using all OpenBB functions ===")
agent_all.print_response(
    "Provide a comprehensive analysis of Apple (AAPL) including current price, historical data, news, and ratios"
)

# Example usage with specific functions only
print(
    "\n=== Example 2: Using specific OpenBB functions (company info + historical data) ==="
)
agent_specific.print_response(
    "Get company information and historical stock data for Tesla (TSLA)"
)

# Example usage with default configuration
print("\n=== Example 3: Default OpenBB agent usage ===")
agent.print_response(
    "Get me the current stock price and key information for Apple (AAPL)"
)

agent.print_response("What are the top gainers in the market today?")

agent.print_response(
    "Show me the latest GDP growth rate and inflation numbers for the US"
)
```

---

<a name="tools--opencv_toolspy"></a>

### `tools/opencv_tools.py`

```python
"""
OpenCV Tools - Computer Vision and Image Processing

This example demonstrates how to use OpenCVTools for computer vision tasks.
Shows enable_ flag patterns for selective function access.
OpenCVTools is a small tool (<6 functions) so it uses enable_ flags.

Steps to use OpenCV Tools:

1. Install OpenCV
   - Run: pip install opencv-python

2. Camera Permissions (macOS)
   - Go to System Settings > Privacy & Security > Camera
   - Enable camera access for Terminal or your IDE

3. Camera Permissions (Linux)
   - Ensure your user is in the video group: sudo usermod -a -G video $USER
   - Restart your session after adding to the group

4. Camera Permissions (Windows)
   - Go to Settings > Privacy > Camera
   - Enable "Allow apps to access your camera"

Note: Make sure your webcam is connected and not being used by other applications.
"""

import base64

from agno.agent import Agent
from agno.tools.opencv import OpenCVTools
from agno.utils.media import save_base64_data

# Example 1: All functions enabled with live preview (default behavior)
agent_full = Agent(
    name="Full OpenCV Agent",
    tools=[OpenCVTools(show_preview=True)],  # All functions enabled with preview
    description="You are a comprehensive computer vision specialist with all OpenCV capabilities.",
    instructions=[
        "Use all OpenCV tools for complete image processing and camera operations",
        "With live preview enabled, users can see real-time camera feed",
        "For images: show preview window, press 'c' to capture, 'q' to quit",
        "For videos: show live recording with countdown timer",
        "Provide detailed analysis of captured content",
    ],
    markdown=True,
)

# Example 2: Enable specific camera functions
agent_camera = Agent(
    name="Camera Specialist",
    tools=[
        OpenCVTools(
            show_preview=True,
            enable_capture_image=True,
            enable_capture_video=True,
        )
    ],
    description="You are a camera specialist focused on capturing images and videos.",
    instructions=[
        "Specialize in capturing images and videos from webcam",
        "Cannot perform advanced image processing or object detection",
        "Focus on high-quality image and video capture",
        "Provide clear instructions for camera operations",
    ],
    markdown=True,
)

# Example 3: Enable all functions using 'all=True' pattern
agent_comprehensive = Agent(
    name="Comprehensive Vision Agent",
    tools=[OpenCVTools(show_preview=True, all=True)],
    description="You are a full-featured computer vision expert with all capabilities enabled.",
    instructions=[
        "Perform advanced computer vision analysis and processing",
        "Use all available OpenCV functions for complex tasks",
        "Combine camera capture with real-time processing",
        "Provide comprehensive image analysis and insights",
    ],
    markdown=True,
)

# Example 4: Processing-focused agent (no camera capture)
agent_processor = Agent(
    name="Image Processor",
    tools=[
        OpenCVTools(
            show_preview=False,  # Disable live preview
            enable_capture_image=False,  # Disable camera capture
            enable_capture_video=False,  # Disable video capture
        )
    ],
    description="You are an image processing specialist focused on analyzing existing images.",
    instructions=[
        "Process and analyze existing images without camera operations",
        "Cannot capture new images or videos",
        "Focus on image enhancement, filtering, and analysis",
        "Provide detailed insights about image content and properties",
    ],
    markdown=True,
)

# Use the full agent for main examples
agent = agent_full

# Example 1: Interactive mode with live preview
print("Example 1: Interactive mode with live preview using full agent")

response = agent.run(
    "Take a quick test of camera, capture the photo and tell me what you see in the photo."
)

if response and response.images:
    print("Agent response:", response.content)
    image_base64 = base64.b64encode(response.images[0].content).decode("utf-8")
    save_base64_data(image_base64, "tmp/test.png")

# Example 2: Capture a video
response = agent.run("Capture a 5 second webcam video.")

if response and response.videos:
    save_base64_data(
        base64_data=str(response.videos[0].content),
        output_path="tmp/captured_test_video.mp4",
    )
```

---

<a name="tools--openweather_toolspy"></a>

### `tools/openweather_tools.py`

```python
"""
OpenWeatherMap API Integration Example

This example demonstrates how to use the OpenWeatherTools to get weather data
from the OpenWeatherMap API.

Prerequisites:
1. Get an API key from https://openweathermap.org/api
2. Set the OPENWEATHER_API_KEY environment variable or pass it directly to the tool

Usage:
- Get current weather for a location
- Get weather forecast for a location
- Get air pollution data for a location
- Geocode a location name to coordinates
"""

from agno.agent import Agent
from agno.tools.openweather import OpenWeatherTools

# Example 1: Enable all OpenWeather functions
agent_all = Agent(
    tools=[
        OpenWeatherTools(
            all=True,  # Enable all OpenWeather functions
            units="imperial",  # Options: 'standard', 'metric', 'imperial'
        )
    ],
    markdown=True,
)

# Example 2: Enable specific OpenWeather functions only
agent_specific = Agent(
    tools=[
        OpenWeatherTools(
            enable_current_weather=True,
            enable_forecast=True,
            enable_air_pollution=True,
            enable_geocoding=True,
            units="metric",
        )
    ],
    markdown=True,
)

# Example 3: Default behavior with all functions enabled
agent = Agent(
    tools=[
        OpenWeatherTools(
            enable_current_weather=True,
            enable_forecast=True,
            enable_air_pollution=True,
            enable_geocoding=True,
            units="imperial",  # Options: 'standard', 'metric', 'imperial'
        )
    ],
    markdown=True,
)

# Example usage with all functions enabled
print("=== Example 1: Using all OpenWeather functions ===")
agent_all.print_response(
    "Give me a comprehensive weather report for Tokyo including current weather, forecast, and air quality",
    markdown=True,
)

# Example usage with specific functions only
print(
    "\n=== Example 2: Using specific OpenWeather functions (current weather + geocoding) ==="
)
agent_specific.print_response(
    "What's the current weather in Tokyo?",
    markdown=True,
)

# Example usage with default configuration
print("\n=== Example 3: Default OpenWeather agent usage ===")
agent.print_response(
    "What's the current weather in Tokyo?",
    markdown=True,
)

# Additional examples (commented out to avoid API calls)
# agent.print_response(
#     "Give me a 3-day weather forecast for New York City",
#     markdown=True,
# )

# agent.print_response(
#     "What's the air quality in Beijing right now?",
#     markdown=True,
# )

# agent.print_response(
#     "Compare the current weather between London, Paris, and Rome",
#     markdown=True,
# )
```

---

<a name="tools--other--add_tool_after_initializationpy"></a>

### `tools/other/add_tool_after_initialization.py`

```python
import random

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool


@tool(stop_after_tool_call=True)
def get_weather(city: str) -> str:
    """Get the weather for a city."""
    # In a real implementation, this would call a weather API
    weather_conditions = ["sunny", "cloudy", "rainy", "snowy", "windy"]
    random_weather = random.choice(weather_conditions)

    return f"The weather in {city} is {random_weather}."


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    markdown=True,
)

agent.print_response("What can you do?", stream=True)

agent.add_tool(get_weather)

agent.print_response("What is the weather in San Francisco?", stream=True)
```

---

<a name="tools--other--cache_tool_callspy"></a>

### `tools/other/cache_tool_calls.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools(cache_results=True), YFinanceTools(cache_results=True)],
)

asyncio.run(
    agent.aprint_response(
        "What is the current stock price of AAPL and latest news on 'Apple'?",
        markdown=True,
    )
)
```

---

<a name="tools--other--complex_input_typespy"></a>

### `tools/other/complex_input_types.py`

```python
"""
This example shows how to use complex input types with tools.

Recommendations:
- Specify fields with descriptions, these will be used in the JSON schema sent to the model and will increase accuracy.
- Try not to nest the structures too deeply, the model will have a hard time understanding them.
"""

from datetime import datetime
from enum import Enum
from typing import List, Optional

from agno.agent import Agent
from agno.tools.decorator import tool
from pydantic import BaseModel, Field


# Define Pydantic models for our tools
class UserProfile(BaseModel):
    """User profile information."""

    name: str = Field(..., description="Full name of the user")
    email: str = Field(..., description="Valid email address")
    age: int = Field(..., ge=0, le=120, description="Age of the user")
    interests: List[str] = Field(
        default_factory=list, description="List of user interests"
    )
    created_at: datetime = Field(
        default_factory=datetime.now, description="Account creation timestamp"
    )


class TaskPriority(str, Enum):
    """Priority levels for tasks."""

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    URGENT = "urgent"


class Task(BaseModel):
    """Task information."""

    title: str = Field(..., min_length=1, max_length=100, description="Task title")
    description: Optional[str] = Field(None, description="Detailed task description")
    priority: TaskPriority = Field(
        default=TaskPriority.MEDIUM, description="Task priority level"
    )
    due_date: Optional[datetime] = Field(None, description="Task due date")
    assigned_to: Optional[UserProfile] = Field(
        None, description="User assigned to the task"
    )


# Custom tools using Pydantic models
@tool
def create_user(user_data: UserProfile) -> str:
    """Create a new user profile with validated information."""
    # In a real application, this would save to a database
    return f"Created user profile for {user_data.name} with email {user_data.email}"


@tool
def create_task(task_data: Task) -> str:
    """Create a new task with priority and assignment."""
    # In a real application, this would save to a database
    return f"Created task '{task_data.title}' with priority {task_data.priority}"


# Create the agent
agent = Agent(
    name="task_manager",
    description="An agent that manages users and tasks with proper validation",
    tools=[create_user, create_task],
)

# Example usage
if __name__ == "__main__":
    # Example 1: Create a user
    agent.print_response(
        "Create a new user named John Doe with email john@example.com, age 30, and interests in Python and AI"
    )

    # Example 2: Create a task
    agent.print_response(
        "Create a high priority task titled 'Implement API endpoints' due tomorrow"
    )
```

---

<a name="tools--other--custom_tool_manipulate_session_statepy"></a>

### `tools/other/custom_tool_manipulate_session_state.py`

```python
from agno.agent import Agent
from agno.tools import tool
from agno.tools.duckduckgo import DuckDuckGoTools
from pydantic import BaseModel
from rich.pretty import pprint


@tool()
def answer_from_known_questions(session_state: dict, question: str) -> str:
    """Answer a question from a list of known questions

    Args:
        question: The question to answer

    Returns:
        The answer to the question
    """

    class Answer(BaseModel):
        answer: str
        original_question: str

    faq = {
        "What is the capital of France?": "Paris",
        "What is the capital of Germany?": "Berlin",
        "What is the capital of Italy?": "Rome",
        "What is the capital of Spain?": "Madrid",
        "What is the capital of Portugal?": "Lisbon",
        "What is the capital of Greece?": "Athens",
        "What is the capital of Turkey?": "Ankara",
    }
    if session_state is None:
        session_state = {}

    if "last_answer" in session_state:
        del session_state["last_answer"]

    if question in faq:
        answer = Answer(answer=faq[question], original_question=question)
        session_state["last_answer"] = answer
        return answer.answer
    else:
        return "I don't know the answer to that question."


q_and_a_agent = Agent(
    name="Q & A Agent",
    tools=[answer_from_known_questions, DuckDuckGoTools()],
    markdown=True,
    instructions="You are a Q & A agent that can answer questions from a list of known questions. If you don't know the answer, you can search the web.",
)

q_and_a_agent.print_response("What is the capital of France?", stream=True)

session_state = q_and_a_agent.get_session_state()
if session_state and "last_answer" in session_state:
    pprint(session_state["last_answer"])


q_and_a_agent.print_response("What is the capital of South Africa?", stream=True)
```

---

<a name="tools--other--human_in_the_looppy"></a>

### `tools/other/human_in_the_loop.py`

```python
""" Human-in-the-Loop: Adding User Confirmation to Tool Calls

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:
- Add pre-hooks to tools for user confirmation
- Handle user input during tool execution
- Gracefully cancel operations based on user choice

Some practical applications:
- Confirming sensitive operations before execution
- Reviewing API calls before they're made
- Validating data transformations
- Approving automated actions in critical systems

Run `pip install openai httpx rich agno` to install dependencies.
"""

import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.exceptions import StopAgentRun
from agno.models.openai import OpenAIChat
from agno.tools import FunctionCall, tool
from rich.console import Console
from rich.prompt import Prompt

# This is the console instance used by the print_response method
# We can use this to stop and restart the live display and ask for user confirmation
console = Console()


def pre_hook(fc: FunctionCall):
    # Get the live display instance from the console
    live = console._live

    # Stop the live display temporarily so we can ask for user confirmation
    live.stop()  # type: ignore

    # Ask for confirmation
    console.print(f"\nAbout to run [bold blue]{fc.function.name}[/]")
    message = (
        Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
        .strip()
        .lower()
    )

    # Restart the live display
    live.start()  # type: ignore

    # If the user does not want to continue, raise a StopExecution exception
    if message != "y":
        raise StopAgentRun(
            "Tool call cancelled by user",
            agent_message="Stopping execution as permission was not granted.",
        )


@tool(pre_hook=pre_hook)
def get_top_hackernews_stories(num_stories: int) -> Iterator[str]:
    """Fetch top stories from Hacker News after user confirmation.

    Args:
        num_stories (int): Number of stories to retrieve

    Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        yield json.dumps(story)


# Initialize the agent with a tech-savvy personality and clear instructions
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

agent.print_response(
    "Fetch the top 2 hackernews stories?", stream=True, console=console
)
```

---

<a name="tools--other--include_exclude_toolspy"></a>

### `tools/other/include_exclude_tools.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[
        CalculatorTools(
            enable_all=True,
            exclude_tools=["exponentiate", "factorial", "is_prime", "square_root"],
        ),
        DuckDuckGoTools(include_tools=["duckduckgo_search"]),
    ],
)

asyncio.run(
    agent.aprint_response(
        "Search the web for a difficult sum that can be done with normal arithmetic and solve it.",
        markdown=True,
    )
)
```

---

<a name="tools--other--include_exclude_tools_custom_toolkitpy"></a>

### `tools/other/include_exclude_tools_custom_toolkit.py`

```python
import asyncio
import json

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            name="customer_db",
            tools=[self.retrieve_customer_profile, self.delete_customer_profile],
            *args,
            **kwargs,
        )

    async def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[CustomerDBTools(include_tools=["retrieve_customer_profile"])],
)

asyncio.run(
    agent.aprint_response(
        "Retrieve the customer profile for customer ID 123 and delete it.",  # The agent shouldn't be able to delete the profile
        markdown=True,
    )
)
```

---

<a name="tools--other--retry_tool_callpy"></a>

### `tools/other/retry_tool_call.py`

```python
from agno.agent import Agent
from agno.exceptions import RetryAgentRun
from agno.models.openai import OpenAIChat
from agno.utils.log import logger


def add_item(session_state: dict, item: str) -> str:
    """Add an item to the shopping list."""
    if session_state:
        session_state["shopping_list"].append(item)
        len_shopping_list = len(session_state["shopping_list"])
    if len_shopping_list < 3:
        logger.info(
            f"Asking the model to add {3 - len_shopping_list} more items to the shopping list."
        )
        raise RetryAgentRun(
            f"Shopping list is: {session_state['shopping_list']}. Minimum 3 items in the shopping list. "  # type: ignore
            + f"Add {3 - len_shopping_list} more items.",
        )

    logger.info(f"The shopping list is now: {session_state.get('shopping_list')}")  # type: ignore
    return f"The shopping list is now: {session_state.get('shopping_list')}"  # type: ignore


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item],
    markdown=True,
)
agent.print_response("Add milk", stream=True)
print(f"Final session state: {agent.get_session_state()}")
```

---

<a name="tools--other--retry_tool_call_from_post_hookpy"></a>

### `tools/other/retry_tool_call_from_post_hook.py`

```python
from agno.agent import Agent
from agno.exceptions import RetryAgentRun
from agno.models.openai import OpenAIChat
from agno.tools import FunctionCall, tool
from agno.utils.log import logger


def post_hook(session_state: dict, fc: FunctionCall):
    logger.info(f"Post-hook: {fc.function.name}")
    logger.info(f"Arguments: {fc.arguments}")
    shopping_list = session_state.get("shopping_list", []) if session_state else []
    if len(shopping_list) < 3:
        raise RetryAgentRun(
            f"Shopping list is: {shopping_list}. Minimum 3 items in the shopping list. "
            + f"Add {3 - len(shopping_list)} more items."
        )


@tool(post_hook=post_hook)
def add_item(session_state: dict, item: str) -> str:
    """Add an item to the shopping list."""
    if session_state:
        session_state["shopping_list"].append(item)
        return f"The shopping list is now {session_state['shopping_list']}"
    return ""


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item],
    markdown=True,
)
agent.print_response("Add milk", stream=True)
print(f"Final session state: {agent.get_session_state()}")
```

---

<a name="tools--other--stop_after_tool_callpy"></a>

### `tools/other/stop_after_tool_call.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIChat(id="gpt-4.5-preview"),
    tools=[
        GoogleSearchTools(
            stop_after_tool_call_tools=["google_search"],
            show_result_tools=["google_search"],
        )
    ],
)

agent.print_response("Whats the latest about gpt 4.5?", markdown=True)
```

---

<a name="tools--other--stop_agent_exceptionpy"></a>

### `tools/other/stop_agent_exception.py`

```python
from agno.agent import Agent
from agno.exceptions import StopAgentRun
from agno.models.openai import OpenAIChat
from agno.utils.log import logger


def add_item(session_state: dict, item: str) -> str:
    """Add an item to the shopping list."""
    if session_state:
        session_state["shopping_list"].append(item)
        len_shopping_list = len(session_state["shopping_list"])
    if len_shopping_list < 3:
        raise StopAgentRun(
            f"Shopping list is: {session_state['shopping_list']}. We must stop the agent."  # type: ignore
        )

    logger.info(f"The shopping list is now: {session_state.get('shopping_list')}")  # type: ignore
    return f"The shopping list is now: {session_state.get('shopping_list')}"  # type: ignore


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item],
    markdown=True,
)
agent.print_response("Add milk", stream=True)
print(f"Final session state: {agent.get_session_state()}")
```

---

<a name="tools--oxylabs_toolspy"></a>

### `tools/oxylabs_tools.py`

```python
from agno.agent import Agent
from agno.tools.oxylabs import OxylabsTools

agent = Agent(
    tools=[OxylabsTools()],
    markdown=True,
)

# Example 1: Google Search
agent.print_response(
    "Let's search for 'latest iPhone reviews' and provide a summary of the top 3 results. ",
)

# Example 2: Amazon Product Search
# agent.print_response(
#     "Let's search for an Amazon product with ASIN 'B07FZ8S74R' (Echo Dot). ",
# )

# Example 3: Multi-Domain Amazon Search
# agent.print_response(
#     "Use search_amazon_products to search for 'gaming keyboards' on both:\n"
#     "1. Amazon US (domain='com')\n"
#     "2. Amazon UK (domain='co.uk')\n"
#     "Compare the top 3 results from each region including pricing and availability."
# )
```

---

<a name="tools--pandas_toolspy"></a>

### `tools/pandas_tools.py`

```python
"""
Pandas Tools - Data Analysis and DataFrame Operations

This example demonstrates how to use PandasTools for data manipulation and analysis.
Shows enable_ flag patterns for selective function access.
PandasTools is a small tool (<6 functions) so it uses enable_ flags.

Run: `pip install pandas` to install the dependencies
"""

from agno.agent import Agent
from agno.tools.pandas import PandasTools

agent_full = Agent(
    tools=[PandasTools()],  # All functions enabled by default
    description="You are a data analyst with full pandas capabilities for comprehensive data analysis.",
    instructions=[
        "Help users with all aspects of pandas data manipulation",
        "Create, modify, analyze, and visualize DataFrames",
        "Provide detailed explanations of data operations",
        "Suggest best practices for data analysis workflows",
    ],
    markdown=True,
)

print("=== DataFrame Creation and Analysis Example ===")
agent_full.print_response("""
Please perform these tasks:
1. Create a pandas dataframe named 'sales_data' using DataFrame() with this sample data:
   {'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],
    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],
    'quantity': [10, 15, 8, 12, 20],
    'price': [9.99, 15.99, 9.99, 12.99, 15.99]}
2. Show me the first 5 rows of the sales_data dataframe
3. Calculate the total revenue (quantity * price) for each row
""")
```

---

<a name="tools--postgres_toolspy"></a>

### `tools/postgres_tools.py`

```python
from agno.agent import Agent
from agno.tools.postgres import PostgresTools

# Example 1: Include specific Postgres functions (default behavior - all functions included)
agent = Agent(
    tools=[
        PostgresTools(
            host="localhost",
            port=5532,
            db_name="ai",
            user="ai",
            password="ai",
            table_schema="ai",
        )
    ]
)

# Example 2: Include only read-only operations
agent_readonly = Agent(
    tools=[
        PostgresTools(
            host="localhost",
            port=5532,
            db_name="ai",
            user="ai",
            password="ai",
            table_schema="ai",
            include_tools=[
                "show_tables",
                "describe_table",
                "summarize_table",
                "inspect_query",
            ],
        )
    ]
)

# Example 3: Exclude dangerous operations
agent_safe = Agent(
    tools=[
        PostgresTools(
            host="localhost",
            port=5532,
            db_name="ai",
            user="ai",
            password="ai",
            table_schema="ai",
            exclude_tools=["run_query"],  # Exclude direct query execution
        )
    ]
)

agent.print_response(
    "List the tables in the database and summarize one of the tables", markdown=True
)

agent.print_response("""
Please run a SQL query to get all sessions in `agent_sessions` or `team_sessions` created in the last 24 hours and summarize the table.
""")
```

---

<a name="tools--pubmed_toolspy"></a>

### `tools/pubmed_tools.py`

```python
from agno.agent import Agent
from agno.tools.pubmed import PubmedTools

# Example 1: Enable all PubMed functions
agent_all = Agent(
    tools=[
        PubmedTools(
            all=True,  # Enable all PubMed search functions
        )
    ],
    markdown=True,
)

# Example 2: Enable specific PubMed functions only
agent_specific = Agent(
    tools=[
        PubmedTools(
            enable_search_pubmed=True,  # Only enable the main search function
        )
    ],
    markdown=True,
)

# Example 3: Default behavior with search enabled
agent = Agent(
    tools=[
        PubmedTools(
            enable_search_pubmed=True,
        )
    ],
    markdown=True,
)

# Example usage with all functions enabled
print("=== Example 1: Using all PubMed functions ===")
agent_all.print_response(
    "Tell me about ulcerative colitis and find the latest research."
)

# Example usage with specific functions only
print("\n=== Example 2: Using specific PubMed functions (search only) ===")
agent_specific.print_response("Search for recent studies on diabetes treatment.")

# Example usage with default configuration
print("\n=== Example 3: Default PubMed agent usage ===")
agent.print_response("Tell me about ulcerative colitis.")

agent.print_response("Find research papers on machine learning in healthcare.")
```

---

<a name="tools--python_functionpy"></a>

### `tools/python_function.py`

```python
import json

import httpx
from agno.agent import Agent


def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to return. Defaults to 10.

    Returns:
        str: JSON string of top stories.
    """

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


agent = Agent(tools=[get_top_hackernews_stories], markdown=True)
agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)
```

---

<a name="tools--python_function_as_toolpy"></a>

### `tools/python_function_as_tool.py`

```python
import json

import httpx
from agno.agent import Agent


def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to return. Defaults to 10.

    Returns:
        str: JSON string of top stories.
    """

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


agent = Agent(tools=[get_top_hackernews_stories], markdown=True)
agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)
```

---

<a name="tools--python_toolspy"></a>

### `tools/python_tools.py`

```python
from pathlib import Path

from agno.agent import Agent
from agno.tools.python import PythonTools

# Example 1: All functions available (default behavior)
agent_all = Agent(
    name="Python Agent - All Functions",
    tools=[PythonTools(base_dir=Path("tmp/python"))],
    instructions=["You have access to all Python execution capabilities."],
    markdown=True,
)

# Example 2: Include specific functions only
agent_specific = Agent(
    name="Python Agent - Specific Functions",
    tools=[
        PythonTools(
            base_dir=Path("tmp/python"),
            include_tools=["save_to_file_and_run", "run_python_code"],
        )
    ],
    instructions=["You can only save and run Python code, no package installation."],
    markdown=True,
)

# Example 3: Exclude dangerous functions
agent_safe = Agent(
    name="Python Agent - Safe Mode",
    tools=[
        PythonTools(
            base_dir=Path("tmp/python"),
            exclude_tools=["pip_install_package", "uv_pip_install_package"],
        )
    ],
    instructions=["You can run Python code but cannot install packages."],
    markdown=True,
)

# Use the default agent for examples
agent = agent_all

agent.print_response(
    "Write a python script for fibonacci series and display the result till the 10th number"
)
```

---

<a name="tools--reddit_toolspy"></a>

### `tools/reddit_tools.py`

```python
"""
Steps to get Reddit credentials:

1. Create/Login to Reddit account
   - Go to https://www.reddit.com

2. Create a Reddit App
   - Go to https://www.reddit.com/prefs/apps
   - Click "Create App" or "Create Another App" button
   - Fill in required details:
     * Name: Your app name
     * App type: Select "script"
     * Description: Brief description
     * About url: Your website (can be http://localhost)
     * Redirect uri: http://localhost:8080
   - Click "Create app" button

3. Get credentials
   - client_id: Found under your app name (looks like a random string)
   - client_secret: Listed as "secret"
   - user_agent: Format as: "platform:app_id:version (by /u/username)"
   - username: Your Reddit username
   - password: Your Reddit account password

"""

from agno.agent import Agent
from agno.tools.reddit import RedditTools

agent = Agent(
    instructions=[
        "Use your tools to answer questions about Reddit content and statistics",
        "Respect Reddit's content policies and NSFW restrictions",
        "When analyzing subreddits, provide relevant statistics and trends",
    ],
    tools=[RedditTools()],
)

agent.print_response("What are the top 5 posts on r/SAAS this week ?", stream=True)
```

---

<a name="tools--replicate_toolspy"></a>

### `tools/replicate_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.replicate import ReplicateTools

"""Create an agent specialized for Replicate AI content generation"""

# Example 1: Enable specific Replicate functions
image_agent = Agent(
    name="Image Generator Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReplicateTools(model="luma/photon-flash", enable_generate_media=True)],
    description="You are an AI agent that can generate images using the Replicate API.",
    instructions=[
        "When the user asks you to create an image, use the `generate_media` tool to create the image.",
        "Return the URL as raw to the user.",
        "Don't convert image URL to markdown or anything else.",
    ],
    markdown=True,
)

# Example 2: Enable all Replicate functions
full_agent = Agent(
    name="Full Replicate Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[ReplicateTools(model="minimax/video-01", all=True)],
    description="You are an AI agent that can generate various media using Replicate models.",
    instructions=[
        "Use the Replicate API to generate images or videos based on user requests.",
        "Return the generated media URL to the user.",
    ],
    markdown=True,
)

image_agent.print_response("Generate an image of a horse in the dessert.")
```

---

<a name="tools--resend_toolspy"></a>

### `tools/resend_tools.py`

```python
from agno.agent import Agent
from agno.tools.resend import ResendTools

from_email = "<enter_from_email>"
to_email = "<enter_to_email>"

agent = Agent(tools=[ResendTools(from_email=from_email)])
agent.print_response(f"Send an email to {to_email} greeting them with hello world")
```

---

<a name="tools--scrapegraph_toolspy"></a>

### `tools/scrapegraph_tools.py`

```python
"""
ScrapeGraphTools

This script demonstrates the various capabilities of ScrapeGraphTools toolkit:

1. smartscraper: Extract structured data using natural language prompts
2. markdownify: Convert web pages to markdown format
3. searchscraper: Search the web and extract information
4. crawl: Crawl websites with structured data extraction
5. scrape: Get raw HTML content from websites (NEW!)

The scrape method is particularly useful when you need:
- Complete HTML source code
- Raw content for further processing
- HTML structure analysis
- Content that needs to be parsed differently

All methods support heavy JavaScript rendering when needed.
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.scrapegraph import ScrapeGraphTools

agent_model = OpenAIChat(id="gpt-4.1")
scrapegraph_smartscraper = ScrapeGraphTools(enable_smartscraper=True)

agent = Agent(
    tools=[scrapegraph_smartscraper], model=agent_model, markdown=True, stream=True
)

# Example 1: Use smartscraper tool
agent.print_response("""
Use smartscraper to extract the following from https://www.wired.com/category/science/:
- News articles
- Headlines
- Images
- Links
- Author
""")

# Example 2: Only markdownify enabled (by setting smartscraper=False)
# scrapegraph_md = ScrapeGraphTools(enable_smartscraper=False)

# md_agent = Agent(tools=[scrapegraph_md], model=agent_model, markdown=True)

# md_agent.print_response(
#     "Fetch and convert https://www.wired.com/category/science/ to markdown format"
# )

# # Example 3: Enable crawl
# scrapegraph_crawl = ScrapeGraphTools(enable_crawl=True)

# crawl_agent = Agent(tools=[scrapegraph_crawl], model=agent_model, markdown=True)

# crawl_agent.print_response(
#     "Use crawl to extract what the company does and get text content from privacy and terms from https://scrapegraphai.com/ with a suitable schema."
# )

# # Example 4: Enable scrape method for raw HTML content
# scrapegraph_scrape = ScrapeGraphTools(enable_scrape=True, enable_smartscraper=False)

# scrape_agent = Agent(
#     tools=[scrapegraph_scrape],
#     model=agent_model,
#     markdown=True,
#     stream=True,
# )

# scrape_agent.print_response(
#     "Use the scrape tool to get the complete raw HTML content from https://en.wikipedia.org/wiki/2025_FIFA_Club_World_Cup"
# )

# # Example 5: Enable all ScrapeGraph functions
# scrapegraph_all = Agent(
#     tools=[
#         ScrapeGraphTools(all=True, render_heavy_js=True)
#     ],  # render_heavy_js=True scrapes all JavaScript
#     model=agent_model,
#     markdown=True,
#     stream=True,
# )

# scrapegraph_all.print_response("""
# Use any appropriate scraping method to extract comprehensive information from https://www.wired.com/category/science/:
# - News articles and headlines
# - Convert to markdown if needed
# - Search for specific information
# """)
```

---

<a name="tools--searxng_toolspy"></a>

### `tools/searxng_tools.py`

```python
from agno.agent import Agent
from agno.tools.searxng import SearxngTools

# Initialize Searxng with your Searxng instance URL
searxng = SearxngTools(
    host="http://localhost:53153",
    engines=[],
    fixed_max_results=5,
    news=True,
    science=True,
)

# Create an agent with Searxng
agent = Agent(tools=[searxng])

# Example: Ask the agent to search using Searxng
agent.print_response("""
Please search for information about artificial intelligence 
and summarize the key points from the top results
""")
```

---

<a name="tools--serpapi_toolspy"></a>

### `tools/serpapi_tools.py`

```python
from agno.agent import Agent
from agno.tools.serpapi import SerpApiTools

# Example 1: Enable specific SerpAPI functions
agent = Agent(
    tools=[SerpApiTools(enable_search_google=True, enable_search_youtube=False)]
)

# Example 2: Enable all SerpAPI functions
agent_all = Agent(tools=[SerpApiTools(all=True)])

# Example 3: Enable only YouTube search
youtube_agent = Agent(
    tools=[SerpApiTools(enable_search_google=False, enable_search_youtube=True)]
)

# Test the agents
agent.print_response("What's happening in the USA?", markdown=True)
youtube_agent.print_response("Search YouTube for 'python tutorial'", markdown=True)
```

---

<a name="tools--serper_toolspy"></a>

### `tools/serper_tools.py`

```python
"""
This is a example of an agent using the Serper Toolkit.

You can obtain an API key from https://serper.dev/

 - Set your API key as an environment variable: export SERPER_API_KEY="your_api_key_here"
 - or pass api_key to the SerperTools class
"""

from agno.agent import Agent
from agno.tools.serper import SerperTools

agent = Agent(
    tools=[SerperTools()],
)

agent.print_response(
    "Search for the latest news about artificial intelligence developments",
    markdown=True,
)

# Example 2: Google Scholar Search
# agent.print_response(
#     "Find 2 recent academic papers about large language model safety and alignment",
#     markdown=True,
# )

# Example 3: Web Scraping
# agent.print_response(
#     "Scrape and summarize the main content from this OpenAI blog post: https://openai.com/index/gpt-4/",
#     markdown=True
# )
```

---

<a name="tools--shell_toolspy"></a>

### `tools/shell_tools.py`

```python
from agno.agent import Agent
from agno.tools.shell import ShellTools

agent = Agent(tools=[ShellTools()])
agent.print_response("Show me the contents of the current directory", markdown=True)
```

---

<a name="tools--slack_toolspy"></a>

### `tools/slack_tools.py`

```python
"""Run `pip install openai slack-sdk` to install dependencies."""

from agno.agent import Agent
from agno.tools.slack import SlackTools

# Example 1: Enable all Slack functions
agent_all = Agent(
    tools=[
        SlackTools(
            all=True,  # Enable all Slack functions
        )
    ],
    markdown=True,
)

# Example 2: Enable specific Slack functions only
agent_specific = Agent(
    tools=[
        SlackTools(
            enable_send_message=True,
            enable_list_channels=True,
            enable_get_channel_history=False,
            enable_send_message_thread=False,
        )
    ],
    markdown=True,
)


# Example usage with all functions enabled
print("=== Example 1: Using all Slack functions ===")
agent_all.print_response(
    "Send a message 'Hello from Agno with all functions!' to the channel #bot-test and then list all channels",
    markdown=True,
)

# Example usage with specific functions only
print(
    "\n=== Example 2: Using specific Slack functions (send message + list channels) ==="
)
agent_specific.print_response(
    "Send a message 'Hello from limited bot!' to the channel #bot-test", markdown=True
)
```

---

<a name="tools--sleep_toolspy"></a>

### `tools/sleep_tools.py`

```python
from agno.agent import Agent
from agno.tools.sleep import SleepTools

# Example 1: Enable specific sleep functions
agent = Agent(tools=[SleepTools(enable_sleep=True)], name="Sleep Agent")

# Example 2: Enable all sleep functions
agent_all = Agent(tools=[SleepTools(all=True)], name="Full Sleep Agent")

# Test the agents
agent.print_response("Sleep for 2 seconds")
agent_all.print_response("Sleep for 5 seconds")
```

---

<a name="tools--spider_toolspy"></a>

### `tools/spider_tools.py`

```python
from agno.agent import Agent
from agno.tools.spider import SpiderTools

# Example 1: All functions available (default behavior)
agent_all = Agent(
    name="Spider Agent - All Functions",
    tools=[SpiderTools(optional_params={"proxy_enabled": True})],
    instructions=["You have access to all Spider web scraping capabilities."],
    markdown=True,
)

# Example 2: Include specific functions only
agent_specific = Agent(
    name="Spider Agent - Search Only",
    tools=[SpiderTools(enable_crawl=False, optional_params={"proxy_enabled": True})],
    instructions=["You can only search the web, no scraping or crawling."],
    markdown=True,
)


# Use the default agent for examples
agent = agent_all

agent.print_response(
    'Can you scrape the first search result from a search on "news in USA"?'
)
```

---

<a name="tools--sql_toolspy"></a>

### `tools/sql_tools.py`

```python
from agno.agent import Agent
from agno.tools.sql import SQLTools

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(tools=[SQLTools(db_url=db_url)])
agent.print_response(
    "List the tables in the database. Tell me about contents of one of the tables",
    markdown=True,
)
```

---

<a name="tools--tavily_toolspy"></a>

### `tools/tavily_tools.py`

```python
from agno.agent import Agent
from agno.tools.tavily import TavilyTools

# Example 1: default TavilyTools
agent = Agent(tools=[TavilyTools()])

# Example 2: Enable all Tavily functions
agent_all = Agent(tools=[TavilyTools(all=True)])

# Example 3: Use advanced search with context
context_agent = Agent(
    tools=[
        TavilyTools(
            enable_web_search_using_tavily=False,
            enable_web_search_with_tavily=True,
        )
    ]
)

# Test the agents
agent.print_response(
    "Search for 'language models' and recent developments", markdown=True
)
context_agent.print_response(
    "Get detailed context about artificial intelligence trends", markdown=True
)
```

---

<a name="tools--telegram_toolspy"></a>

### `tools/telegram_tools.py`

```python
"""
Telegram Tools - Bot Communication and Messaging

This example demonstrates how to use TelegramTools for Telegram bot operations.
Shows enable_ flag patterns for selective function access.
TelegramTools is a small tool (<6 functions) so it uses enable_ flags.

Prerequisites:
- Create a new bot with BotFather on Telegram: https://core.telegram.org/bots/features#creating-a-new-bot
- Get the token from BotFather
- Send a message to the bot
- Get the chat_id by going to: https://api.telegram.org/bot<your-bot-token>/getUpdates
"""

from agno.agent import Agent
from agno.tools.telegram import TelegramTools

telegram_token = "<enter-your-bot-token>"
chat_id = "<enter-your-chat-id>"

# Example 1: All functions enabled (default behavior)
agent = Agent(
    name="telegram-full",
    tools=[
        TelegramTools(token=telegram_token, chat_id=chat_id)
    ],  # All functions enabled
    description="You are a comprehensive Telegram bot assistant with all messaging capabilities.",
    instructions=[
        "Help users with all Telegram bot operations",
        "Send messages, handle media, and manage bot interactions",
        "Provide clear feedback on bot operations",
        "Follow Telegram bot best practices",
    ],
    markdown=True,
)

agent.print_response("Send a message to the bot")
```

---

<a name="tools--todoist_toolspy"></a>

### `tools/todoist_tools.py`

```python
"""
Example showing how to use the Todoist Tools with Agno

Requirements:
- Sign up/login to Todoist and get a Todoist API Token (get from https://app.todoist.com/app/settings/integrations/developer)
- pip install todoist-api-python

Usage:
- Set the following environment variables:
    export TODOIST_API_TOKEN="your_api_token"

- Or provide them when creating the TodoistTools instance
"""

from agno.agent import Agent
from agno.models.google.gemini import Gemini
from agno.tools.todoist import TodoistTools

# Example 1: All functions available (default behavior)
todoist_agent_all = Agent(
    name="Todoist Agent - All Functions",
    role="Manage your todoist tasks with full capabilities",
    instructions=[
        "You have access to all Todoist operations.",
        "You can create, read, update, delete tasks and manage projects.",
    ],
    id="todoist-agent-all",
    model=Gemini("gemini-2.0-flash-exp"),
    tools=[TodoistTools()],
    markdown=True,
)


# Example 3: Exclude dangerous functions
todoist_agent = Agent(
    name="Todoist Agent - Safe Mode",
    role="Manage your todoist tasks safely",
    instructions=[
        "You can create and update tasks but cannot delete anything.",
        "You have read access to all tasks and projects.",
    ],
    id="todoist-agent-safe",
    model=Gemini("gemini-2.0-flash-exp"),
    tools=[TodoistTools(exclude_tools=["delete_task"])],
    markdown=True,
)


# Example 1: Create a task
print("\n=== Create a task ===")
todoist_agent_all.print_response(
    "Create a todoist task to buy groceries tomorrow at 10am"
)


# Example 2: Delete a task
print("\n=== Delete a task ===")
todoist_agent.print_response(
    "Delete the todoist task to buy groceries tomorrow at 10am"
)
```

---

<a name="tools--tool_calls_accesing_agentpy"></a>

### `tools/tool_calls_accesing_agent.py`

```python
import json

import httpx
from agno.agent import Agent


def get_top_hackernews_stories(agent: Agent) -> str:
    num_stories = agent.dependencies.get("num_stories", 5) if agent.dependencies else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)


agent = Agent(
    dependencies={
        "num_stories": 3,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)
```

---

<a name="tools--tool_decorator--async_tool_decoratorpy"></a>

### `tools/tool_decorator/async_tool_decorator.py`

```python
import asyncio
import json
from typing import AsyncIterator

import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(show_result=True)
async def get_top_hackernews_stories(agent: Agent) -> AsyncIterator[str]:
    num_stories = agent.dependencies.get("num_stories", 5) if agent.dependencies else 5

    async with httpx.AsyncClient() as client:
        # Fetch top story IDs
        response = await client.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        )
        story_ids = response.json()

        # Yield story details
        for story_id in story_ids[:num_stories]:
            story_response = await client.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            if "text" in story:
                story.pop("text", None)
            yield json.dumps(story)


agent = Agent(
    dependencies={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
)
if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response("What are the top hackernews stories?", stream=True)
    )
```

---

<a name="tools--tool_decorator--cache_tool_callspy"></a>

### `tools/tool_decorator/cache_tool_calls.py`

```python
import json

import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(stop_after_tool_call=True, cache_results=True)
def get_top_hackernews_stories(num_stories: int = 5) -> str:
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(json.dumps(story))

    return "\n".join(stories)


agent = Agent(
    tools=[get_top_hackernews_stories],
    markdown=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)
```

---

<a name="tools--tool_decorator--stop_after_tool_callpy"></a>

### `tools/tool_decorator/stop_after_tool_call.py`

```python
from agno.agent import Agent
from agno.tools import tool


@tool(stop_after_tool_call=True)
def get_answer_to_life_universe_and_everything() -> str:
    """
    This returns the answer to the life, the universe and everything.
    """
    return "42"


agent = Agent(
    tools=[get_answer_to_life_universe_and_everything],
    markdown=True,
)
agent.print_response("What is the answer to life, the universe and everything?")
```

---

<a name="tools--tool_decorator--tool_decoratorpy"></a>

### `tools/tool_decorator/tool_decorator.py`

```python
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(show_result=True)
def get_top_hackernews_stories(agent: Agent) -> Iterator[str]:
    num_stories = agent.dependencies.get("num_stories", 5) if agent.dependencies else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        yield json.dumps(story)


agent = Agent(
    dependencies={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)
```

---

<a name="tools--tool_decorator--tool_decorator_asyncpy"></a>

### `tools/tool_decorator/tool_decorator_async.py`

```python
import asyncio
import json

import httpx
from agno.agent import Agent
from agno.tools import tool


class DemoTools:
    @tool(description="Get the top hackernews stories")
    @staticmethod
    async def get_top_hackernews_stories(agent: Agent):
        num_stories = (
            agent.dependencies.get("num_stories", 5) if agent.dependencies else 5
        )

        # Fetch top story IDs
        response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
        story_ids = response.json()

        # Get story details
        for story_id in story_ids[:num_stories]:
            async with httpx.AsyncClient() as client:
                story_response = await client.get(
                    f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                )
            story = story_response.json()
            if "text" in story:
                story.pop("text", None)
            return json.dumps(story)

    @tool(
        description="Get the current weather for a city using the MetaWeather public API"
    )
    @staticmethod
    async def get_current_weather(agent: Agent):
        city = (
            agent.dependencies.get("city", "San Francisco")
            if agent.dependencies
            else "San Francisco"
        )

        async with httpx.AsyncClient() as client:
            # Geocode city to get latitude and longitude
            geo_resp = await client.get(
                "https://geocoding-api.open-meteo.com/v1/search",
                params={"name": city, "count": 1, "language": "en", "format": "json"},
            )
            geo_data = geo_resp.json()
            if not geo_data.get("results"):
                return json.dumps({"error": f"City '{city}' not found."})
            location = geo_data["results"][0]
            lat, lon = location["latitude"], location["longitude"]

            # Get current weather
            weather_resp = await client.get(
                "https://api.open-meteo.com/v1/forecast",
                params={
                    "latitude": lat,
                    "longitude": lon,
                    "current_weather": True,
                    "timezone": "auto",
                },
            )
            weather_data = weather_resp.json()
            current_weather = weather_data.get("current_weather")
            if not current_weather:
                return json.dumps({"error": f"No weather data found for '{city}'."})

            result = {
                "city": city,
                "weather_state": f"{current_weather['weathercode']}",  # Open-Meteo uses weather codes
                "temp_celsius": current_weather["temperature"],
                "humidity": None,  # Open-Meteo current_weather does not provide humidity
                "date": current_weather["time"],
            }
            return json.dumps(result)


agent = Agent(
    name="HackerNewsAgent",
    dependencies={
        "num_stories": 2,
    },
    tools=[DemoTools.get_top_hackernews_stories],
)
asyncio.run(agent.aprint_response("What are the top hackernews stories?"))


agent = Agent(
    name="WeatherAgent",
    dependencies={
        "city": "San Francisco",
    },
    tools=[DemoTools().get_current_weather],
)
asyncio.run(agent.aprint_response("What is the weather like?"))
```

---

<a name="tools--tool_decorator--tool_decorator_with_hookpy"></a>

### `tools/tool_decorator/tool_decorator_with_hook.py`

```python
"""Show how to decorate a custom hook with a tool execution hook."""

import json
import time
from typing import Any, Callable, Dict

import httpx
from agno.agent import Agent
from agno.tools import tool
from agno.utils.log import logger


def duration_logger_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    """Log the duration of the function call"""
    start_time = time.time()

    result = function_call(**arguments)

    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"Function {function_name} took {duration:.2f} seconds to execute")
    return result


@tool(tool_hooks=[duration_logger_hook])
def get_top_hackernews_stories(agent: Agent) -> str:
    num_stories = agent.dependencies.get("num_stories", 5) if agent.dependencies else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    final_stories = {}
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        final_stories[story_id] = story

    return json.dumps(final_stories)


agent = Agent(
    dependencies={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)
```

---

<a name="tools--tool_decorator--tool_decorator_with_instructionspy"></a>

### `tools/tool_decorator/tool_decorator_with_instructions.py`

```python
import httpx
from agno.agent import Agent
from agno.tools import tool


@tool(
    name="fetch_hackernews_stories",
    description="Get top stories from Hacker News",
    show_result=True,
    instructions="""
        Use this tool when:
          1. The user wants to see recent popular tech news or discussions
          2. You need examples of trending technology topics
          3. The user asks for Hacker News content or tech industry stories

        The tool will return titles and URLs for the specified number of top stories. When presenting results:
          - Highlight interesting or unusual stories
          - Summarize key themes if multiple stories are related
          - If summarizing, mention the original source is Hacker News
    """,
)
def get_top_hackernews_stories(num_stories: int = 5) -> str:
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Get story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        stories.append(f"{story.get('title')} - {story.get('url', 'No URL')}")

    return "\n".join(stories)


agent = Agent(
    tools=[get_top_hackernews_stories],
    markdown=True,
)

agent.print_response(
    "Show me the top news from Hacker News and summarize them", stream=True
)
```

---

<a name="tools--tool_hooks--async_pre_and_post_hookspy"></a>

### `tools/tool_hooks/async_pre_and_post_hooks.py`

```python
import asyncio
import json
from typing import AsyncIterator

import httpx
from agno.agent import Agent
from agno.tools import FunctionCall, tool


async def pre_hook(fc: FunctionCall):
    print(f"About to run: {fc.function.name}")


async def post_hook(fc: FunctionCall):
    print("After running: ", fc.function.name)


@tool(show_result=True, pre_hook=pre_hook, post_hook=post_hook)
async def get_top_hackernews_stories(agent: Agent) -> AsyncIterator[str]:
    num_stories = agent.dependencies.get("num_stories", 5) if agent.dependencies else 5

    async with httpx.AsyncClient() as client:
        # Fetch top story IDs
        response = await client.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        )
        story_ids = response.json()

        # Yield story details
        for story_id in story_ids[:num_stories]:
            story_response = await client.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            )
            story = story_response.json()
            if "text" in story:
                story.pop("text", None)
            yield json.dumps(story)


agent = Agent(
    dependencies={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
)
if __name__ == "__main__":
    asyncio.run(agent.aprint_response("What are the top hackernews stories?"))
```

---

<a name="tools--tool_hooks--pre_and_post_hookspy"></a>

### `tools/tool_hooks/pre_and_post_hooks.py`

```python
import json
from typing import Iterator

import httpx
from agno.agent import Agent
from agno.tools import FunctionCall, tool


def pre_hook(fc: FunctionCall):
    print(f"Pre-hook: {fc.function.name}")
    print(f"Arguments: {fc.arguments}")
    print(f"Result: {fc.result}")


def post_hook(fc: FunctionCall):
    print(f"Post-hook: {fc.function.name}")
    print(f"Arguments: {fc.arguments}")
    print(f"Result: {fc.result}")


@tool(pre_hook=pre_hook, post_hook=post_hook)
def get_top_hackernews_stories(agent: Agent) -> Iterator[str]:
    num_stories = agent.dependencies.get("num_stories", 5) if agent.dependencies else 5

    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

    # Yield story details
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        yield json.dumps(story)


agent = Agent(
    dependencies={
        "num_stories": 2,
    },
    tools=[get_top_hackernews_stories],
    markdown=True,
)
agent.print_response("What are the top hackernews stories?", stream=True)
```

---

<a name="tools--tool_hooks--tool_hookpy"></a>

### `tools/tool_hooks/tool_hook.py`

```python
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.log import logger


def logger_hook(function_name: str, function_call: Callable, arguments: Dict[str, Any]):
    # Pre-hook logic: this runs before the tool is called
    logger.info(f"Running {function_name} with arguments {arguments}")

    # Call the tool
    result = function_call(**arguments)

    # Post-hook logic: this runs after the tool is called
    logger.info(f"Result of {function_name} is {result}")
    return result


agent = Agent(
    model=OpenAIChat(id="gpt-4o"), tools=[DuckDuckGoTools()], tool_hooks=[logger_hook]
)

agent.print_response("What's happening in the world?", stream=True, markdown=True)
```

---

<a name="tools--tool_hooks--tool_hook_asyncpy"></a>

### `tools/tool_hooks/tool_hook_async.py`

```python
"""Show how to use a tool execution hook with async functions, to run logic before and after a tool is called."""

import asyncio
from inspect import iscoroutinefunction
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.log import logger


async def logger_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    # Pre-hook logic: this runs before the tool is called
    logger.info(f"Running {function_name} with arguments {arguments}")

    # Call the tool
    if iscoroutinefunction(function_call):
        result = await function_call(**arguments)
    else:
        result = function_call(**arguments)

    # Post-hook logic: this runs after the tool is called
    logger.info(f"Result of {function_name} is {result}")
    return result


agent = Agent(tools=[DuckDuckGoTools()], tool_hooks=[logger_hook])

asyncio.run(agent.aprint_response("What is currently trending on Twitter?"))
```

---

<a name="tools--tool_hooks--tool_hook_in_toolkitpy"></a>

### `tools/tool_hooks/tool_hook_in_toolkit.py`

```python
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


def validation_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    if function_name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    if function_name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    result = function_call(**arguments)

    logger.info(
        f"Validation hook: {function_name} with arguments {arguments} returned {result}"
    )

    return result


agent = Agent(tools=[CustomerDBTools()], tool_hooks=[validation_hook])

# This should work
agent.print_response("I am customer 456, please retrieve my profile.")

# This should fail
agent.print_response("I am customer 123, please delete my profile.")
```

---

<a name="tools--tool_hooks--tool_hook_in_toolkit_asyncpy"></a>

### `tools/tool_hooks/tool_hook_in_toolkit_async.py`

```python
"""Show how to use a tool execution hook with async functions, to run logic before and after a tool is called."""

import asyncio
import json
from inspect import iscoroutinefunction
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    async def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


async def validation_hook(
    function_name: str, function_call: Callable, arguments: Dict[str, Any]
):
    if function_name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    if function_name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    if iscoroutinefunction(function_call):
        result = await function_call(**arguments)
    else:
        result = function_call(**arguments)

    logger.info(
        f"Validation hook: {function_name} with arguments {arguments} returned {result}"
    )

    return result


agent = Agent(tools=[CustomerDBTools()], tool_hooks=[validation_hook])

asyncio.run(agent.aprint_response("I am customer 456, please retrieve my profile."))
asyncio.run(agent.aprint_response("I am customer 456, please delete my profile."))
```

---

<a name="tools--tool_hooks--tool_hook_in_toolkit_with_statepy"></a>

### `tools/tool_hooks/tool_hook_in_toolkit_with_state.py`

```python
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools import Toolkit


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)

    def retrieve_customer_profile(self, customer: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        return customer


# When used as a tool hook, this function will receive the contextual Agent, function_name, etc as parameters
def grab_customer_profile_hook(
    session_state: dict,
    function_name: str,
    function_call: Callable,
    arguments: Dict[str, Any],
):
    cust_id = arguments.get("customer")
    if cust_id not in session_state["customer_profiles"]:  # type: ignore
        raise ValueError(f"Customer profile for {cust_id} not found")
    customer_profile = session_state["customer_profiles"][cust_id]  # type: ignore

    # Replace the customer with the customer_profile
    arguments["customer"] = json.dumps(customer_profile)
    # Call the function with the updated arguments
    result = function_call(**arguments)

    return result


agent = Agent(
    tools=[CustomerDBTools()],
    tool_hooks=[grab_customer_profile_hook],
    session_state={
        "customer_profiles": {
            "123": {"name": "Jane Doe", "email": "jane.doe@example.com"},
            "456": {"name": "John Doe", "email": "john.doe@example.com"},
        }
    },
)

# This should work
agent.print_response("I am customer 456, please retrieve my profile.")

# This should fail
agent.print_response("I am customer 789, please retrieve my profile.")
```

---

<a name="tools--tool_hooks--tool_hook_in_toolkit_with_state_nestedpy"></a>

### `tools/tool_hooks/tool_hook_in_toolkit_with_state_nested.py`

```python
"""Show how to use a tool execution hook, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)

    def retrieve_customer_profile(self, customer: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        return customer


# When used as a tool hook, this function will receive the contextual Agent, function_name, etc as parameters
def grab_customer_profile_hook(
    session_state: dict,
    function_name: str,
    function_call: Callable,
    arguments: Dict[str, Any],
):
    cust_id = arguments.get("customer")
    if cust_id not in session_state["customer_profiles"]:  # type: ignore
        raise ValueError(f"Customer profile for {cust_id} not found")
    customer_profile = session_state["customer_profiles"][cust_id]  # type: ignore

    # Replace the customer with the customer_profile
    arguments["customer"] = json.dumps(customer_profile)
    # Call the function with the updated arguments
    result = function_call(**arguments)

    return result


def logger_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    logger.info("Before Logger Hook")
    result = func(**arguments)
    logger.info("After Logger Hook")
    return result


agent = Agent(
    tools=[CustomerDBTools()],
    tool_hooks=[grab_customer_profile_hook, logger_hook],
    session_state={
        "customer_profiles": {
            "123": {"name": "Jane Doe", "email": "jane.doe@example.com"},
            "456": {"name": "John Doe", "email": "john.doe@example.com"},
        }
    },
)

# This should work
agent.print_response("I am customer 456, please retrieve my profile.")

# This should fail
agent.print_response("I am customer 789, please retrieve my profile.")
```

---

<a name="tools--tool_hooks--tool_hooks_in_toolkit_nestedpy"></a>

### `tools/tool_hooks/tool_hooks_in_toolkit_nested.py`

```python
"""Show how to use multiple tool execution hooks, to run logic before and after a tool is called."""

import json
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


def validation_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    if name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    if name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    logger.info("Before Validation Hook")
    result = func(**arguments)
    logger.info("After Validation Hook")
    # Remove name from result to sanitize the output
    result = json.loads(result)
    result.pop("name")
    return json.dumps(result)


def logger_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    logger.info("Before Logger Hook")
    result = func(**arguments)
    logger.info("After Logger Hook")
    return result


agent = Agent(
    tools=[CustomerDBTools()],
    # Hooks are executed in order of the list
    tool_hooks=[validation_hook, logger_hook],
)

if __name__ == "__main__":
    agent.print_response("I am customer 456, please retrieve my profile.")
```

---

<a name="tools--tool_hooks--tool_hooks_in_toolkit_nested_asyncpy"></a>

### `tools/tool_hooks/tool_hooks_in_toolkit_nested_async.py`

```python
"""Show how to use multiple tool execution hooks with async functions, to run logic before and after a tool is called."""

import asyncio
import json
from inspect import iscoroutinefunction
from typing import Any, Callable, Dict

from agno.agent import Agent
from agno.tools import Toolkit
from agno.utils.log import logger


class CustomerDBTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.register(self.retrieve_customer_profile)
        self.register(self.delete_customer_profile)

    async def retrieve_customer_profile(self, customer_id: str):
        """
        Retrieves a customer profile from the database.

        Args:
            customer_id: The ID of the customer to retrieve.

        Returns:
            A string containing the customer profile.
        """
        logger.info(f"Looking up customer profile for {customer_id}")
        return json.dumps(
            {
                "customer_id": customer_id,
                "name": "John Doe",
                "email": "john.doe@example.com",
            }
        )

    def delete_customer_profile(self, customer_id: str):
        """
        Deletes a customer profile from the database.

        Args:
            customer_id: The ID of the customer to delete.
        """
        logger.info(f"Deleting customer profile for {customer_id}")
        return f"Customer profile for {customer_id}"


async def validation_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    if name == "retrieve_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot retrieve customer profile for ID 123")

    if name == "delete_customer_profile":
        cust_id = arguments.get("customer_id")
        if cust_id == "123":
            raise ValueError("Cannot delete customer profile for ID 123")

    logger.info("Before Validation Hook")
    if iscoroutinefunction(func):
        result = await func(**arguments)
    else:
        result = func(**arguments)
    logger.info("After Validation Hook")
    # Remove name from result to sanitize the output
    if name == "retrieve_customer_profile":
        result = json.loads(result)
        result.pop("name")
        return json.dumps(result)
    return result


async def logger_hook(name: str, func: Callable, arguments: Dict[str, Any]):
    logger.info("Before Logger Hook")
    if iscoroutinefunction(func):
        result = await func(**arguments)
    else:
        result = func(**arguments)
    logger.info("After Logger Hook")
    return result


agent = Agent(
    tools=[CustomerDBTools()],
    # Hooks are executed in order of the list
    tool_hooks=[validation_hook, logger_hook],
)

if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response(
            "I am customer 456, please retrieve my profile.", stream=True
        )
    )
    asyncio.run(
        agent.aprint_response(
            "I am customer 456, please delete my profile.", stream=True
        )
    )
```

---

<a name="tools--trafilatura_toolspy"></a>

### `tools/trafilatura_tools.py`

```python
"""
TrafilaturaTools Cookbook

This cookbook demonstrates various ways to use TrafilaturaTools for web scraping and text extraction.
TrafilaturaTools provides powerful capabilities for extracting clean, readable text from web pages
and converting raw HTML into structured, meaningful data.

Prerequisites:
- Install trafilatura: pip install trafilatura
- No API keys required
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.trafilatura import TrafilaturaTools

# =============================================================================
# Example 1: Basic Text Extraction
# =============================================================================


def basic_text_extraction():
    """
    Basic text extraction from a single URL.
    Perfect for simple content extraction tasks.
    """
    print("=== Example 1: Basic Text Extraction ===")

    agent = Agent(
        tools=[TrafilaturaTools()],  # Default configuration
        markdown=True,
    )

    agent.print_response(
        "Please extract and summarize the main content from https://github.com/agno-agi/agno"
    )


# =============================================================================
# Example 2: JSON Output with Metadata
# =============================================================================


def json_with_metadata():
    """
    Extract content in JSON format with metadata.
    Useful when you need structured data including titles, authors, dates, etc.
    """
    print("\n=== Example 2: JSON Output with Metadata ===")

    # Configure tool for JSON output with metadata
    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_comments=True,
                include_tables=True,
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Extract the article content from https://en.wikipedia.org/wiki/Web_scraping in JSON format with metadata"
    )


# =============================================================================
# Example 3: Markdown Output with Formatting
# =============================================================================


def markdown_with_formatting():
    """
    Extract content in Markdown format preserving structure.
    Great for maintaining document structure and readability.
    """
    print("\n=== Example 3: Markdown with Formatting ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="markdown",
                include_formatting=True,
                include_links=True,
                with_metadata=True,
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Convert https://docs.python.org/3/tutorial/introduction.html to markdown format while preserving the structure and links"
    )


# =============================================================================
# Example 4: Metadata-Only Extraction
# =============================================================================


def metadata_only_extraction():
    """
    Extract only metadata without main content.
    Perfect for getting quick information about pages.
    """
    print("\n=== Example 4: Metadata-Only Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                include_tools=["extract_metadata_only"],
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Get the metadata (title, author, date, etc.) from https://techcrunch.com/2024/01/15/ai-news-update/"
    )


# =============================================================================
# Example 5: High Precision Extraction
# =============================================================================


def high_precision_extraction():
    """
    Extract with high precision settings.
    Use when you need clean, accurate content and don't mind missing some text.
    """
    print("\n=== Example 5: High Precision Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                favor_precision=True,
                include_comments=False,  # Skip comments for cleaner output
                include_tables=True,
                output_format="txt",
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Extract the main article content from https://www.bbc.com/news with high precision, excluding comments and ads"
    )


# =============================================================================
# Example 6: High Recall Extraction
# =============================================================================


def high_recall_extraction():
    """
    Extract with high recall settings.
    Use when you want to capture as much content as possible.
    """
    print("\n=== Example 6: High Recall Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                favor_recall=True,
                include_comments=True,
                include_tables=True,
                include_formatting=True,
                output_format="markdown",
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Extract comprehensive content from https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags including all comments and discussions"
    )


# =============================================================================
# Example 7: Language-Specific Extraction
# =============================================================================


def language_specific_extraction():
    """
    Extract content with language filtering.
    Useful for multilingual websites or language-specific content.
    """
    print("\n=== Example 7: Language-Specific Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                target_language="en",  # Filter for English content
                output_format="json",
                with_metadata=True,
                deduplicate=True,
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Extract English content from https://www.reddit.com/r/MachineLearning/ and provide a summary"
    )


# =============================================================================
# Example 8: Website Crawling (if spider available)
# =============================================================================


def website_crawling():
    """
    Crawl a website to discover and extract content from multiple pages.
    Note: Requires trafilatura spider module to be available.
    """
    print("\n=== Example 8: Website Crawling ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                enable_crawl_website=True,
                max_crawl_urls=5,  # Limit for demo
                output_format="json",
                with_metadata=True,
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Crawl https://example.com and extract content from up to 5 internal pages"
    )


# =============================================================================
# Example 9: HTML to Text Conversion
# =============================================================================


def html_to_text_conversion():
    """
    Convert raw HTML content to clean text.
    Useful when you already have HTML content that needs cleaning.
    """
    print("\n=== Example 9: HTML to Text Conversion ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                enable_html_to_text=True,
            )
        ],
        markdown=True,
    )

    # Example with HTML content
    html_content = """
    <html>
    <body>
        <h1>Sample Article</h1>
        <p>This is a paragraph with <strong>bold</strong> and <em>italic</em> text.</p>
        <ul>
            <li>List item 1</li>
            <li>List item 2</li>
        </ul>
        <div class="advertisement">This is an ad</div>
    </body>
    </html>
    """

    agent.print_response(f"Convert this HTML to clean text: {html_content}")


# =============================================================================
# Example 10: Workflow Integration Example
# =============================================================================


def research_assistant_agent():
    """
    Create a specialized research assistant using TrafilaturaTools.
    This agent is optimized for extracting and analyzing research content.
    """
    research_agent = Agent(
        name="Research Assistant",
        model=OpenAIChat(id="gpt-4"),
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_tables=True,
                include_links=True,
                favor_recall=True,
                target_language="en",
            )
        ],
        instructions="""
        You are a research assistant specialized in gathering and analyzing information from web sources.
        
        When extracting content:
        1. Always include source metadata (title, author, date, URL)
        2. Preserve important structural elements like tables and lists
        3. Maintain links for citation purposes
        4. Focus on comprehensive content extraction
        5. Provide structured analysis of the extracted content
        
        Format your responses with:
        - Executive Summary
        - Key Findings
        - Important Data/Statistics
        - Source Information
        - Recommendations for further research
        """,
        markdown=True,
    )

    research_agent.print_response("""
        Research the current state of AI in healthcare by analyzing:
        https://www.nature.com/articles/s41591-021-01614-0
        
        Provide a comprehensive analysis including key findings, 
        methodologies mentioned, and implications for future research.
    """)


# =============================================================================
# Example 11: Multiple URLs with Different Configurations
# =============================================================================


def multiple_urls_different_configs():
    """
    Process multiple URLs with different extraction strategies.
    Demonstrates flexibility in handling various content types.
    """
    print("\n=== Example 10: Multiple URLs with Different Configurations ===")

    # Different agents for different content types
    news_agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_comments=False,
                favor_precision=True,
            )
        ],
        markdown=True,
    )

    documentation_agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="markdown",
                include_formatting=True,
                include_links=True,
                include_tables=True,
                favor_recall=True,
            )
        ],
        markdown=True,
    )

    print("Processing news article...")
    news_agent.print_response(
        "Extract and summarize this news article: https://techcrunch.com"
    )

    print("\nProcessing documentation...")
    documentation_agent.print_response(
        "Extract the documentation content from https://docs.python.org/3/tutorial/ preserving structure"
    )


# =============================================================================
# Example 12: Advanced Customization
# =============================================================================


def advanced_customization():
    """
    Advanced configuration with all customization options.
    Shows how to fine-tune extraction for specific needs.
    """
    print("\n=== Example 11: Advanced Customization ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="xml",
                include_comments=False,
                include_tables=True,
                include_images=True,
                include_formatting=True,
                include_links=True,
                with_metadata=True,
                favor_precision=True,
                target_language="en",
                deduplicate=True,
                max_tree_size=10000,
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Extract comprehensive structured content from https://en.wikipedia.org/wiki/Artificial_intelligence in XML format with all metadata and structural elements"
    )


# =============================================================================
# Example 13: Comparative Analysis
# =============================================================================


def comparative_analysis():
    """
    Compare content from multiple sources using different extraction strategies.
    Useful for research and content analysis tasks.
    """
    print("\n=== Example 12: Comparative Analysis ===")

    agent = Agent(
        model=OpenAIChat(id="gpt-4"),
        tools=[
            TrafilaturaTools(
                output_format="json",
                with_metadata=True,
                include_tables=True,
                favor_precision=True,
            )
        ],
        markdown=True,
    )

    agent.print_response("""
        Compare and analyze the content about artificial intelligence from these sources:
        1. https://en.wikipedia.org/wiki/Artificial_intelligence
        2. https://www.ibm.com/cloud/learn/what-is-artificial-intelligence
        
        Provide a comparative analysis highlighting the key differences in how they present AI concepts.
    """)


# =============================================================================
# Example 14: Content Research Pipeline
# =============================================================================


def content_research_pipeline():
    """
    Create a content research pipeline using TrafilaturaTools.
    Demonstrates how to use the tool for systematic content research.
    """
    print("\n=== Example 13: Content Research Pipeline ===")

    agent = Agent(
        model=OpenAIChat(id="gpt-4"),
        tools=[
            TrafilaturaTools(
                output_format="markdown",
                with_metadata=True,
                include_links=True,
                include_tables=True,
                favor_recall=True,
            )
        ],
        instructions="""
        You are a research assistant that helps gather and analyze information from web sources.
        Use TrafilaturaTools to extract content and provide comprehensive analysis.
        Always include source metadata in your analysis.
        """,
        markdown=True,
    )

    agent.print_response("""
        Research the topic of "web scraping best practices" by:
        1. Extracting content from https://blog.apify.com/web-scraping-best-practices/
        2. Analyzing the main points and recommendations
        3. Providing a summary with key takeaways
        
        Include metadata about the source and structure your response with clear sections.
    """)


# =============================================================================
# Example 15: Performance Optimized Extraction
# =============================================================================


def performance_optimized():
    """
    Optimized configuration for fast, efficient extraction.
    Best for high-volume processing or when speed is critical.
    """
    print("\n=== Example 14: Performance Optimized Extraction ===")

    agent = Agent(
        tools=[
            TrafilaturaTools(
                output_format="txt",
                include_comments=False,
                include_tables=False,
                include_images=False,
                include_formatting=False,
                include_links=False,
                with_metadata=False,
                favor_precision=True,  # Faster processing
                deduplicate=False,  # Skip deduplication for speed
            )
        ],
        markdown=True,
    )

    agent.print_response(
        "Quickly extract just the main text content from https://news.ycombinator.com optimized for speed"
    )


# =============================================================================
# Main Execution
# =============================================================================

if __name__ == "__main__":
    """
    Run specific examples or all examples.
    Uncomment the examples you want to test.
    """
    print("TrafilaturaTools Cookbook - Web Scraping and Text Extraction Examples")
    print("=" * 80)

    # Basic examples
    basic_text_extraction()

    # Format-specific examples
    # json_with_metadata()
    # markdown_with_formatting()

    # Extraction strategy examples
    # high_precision_extraction()
    # high_recall_extraction()

    # Advanced examples
    # language_specific_extraction()
    # website_crawling()
    # html_to_text_conversion()
    # research_assistant_agent()

    # Complex workflows
    # multiple_urls_different_configs()
    # advanced_customization()
    # comparative_analysis()
    # content_research_pipeline()
    # performance_optimized()

    print("\n" + "=" * 80)
    print("Cookbook execution completed!")
    print("\n")
```

---

<a name="tools--trello_toolspy"></a>

### `tools/trello_tools.py`

```python
"""
Setting Up Authentication for Trello Tools
Step 1: Get Your API Key
1. Visit the Trello Power-Ups Administration Page https://trello.com/power-ups/admin
2. (Optional) Create a Workspace
3. Create a Power Up (this is required. Its like a "App" connector)
   - If you don't already have a power-ups, create one by clicking the "New" button.
   - If you have an existing Power-Up, select it from the list.
Step 2: Generate API Key and Secret
1. On the left sidebar, click on the "API Key" option.
2. Generate a new API Key:
   - Click the button to generate your API Key.
   - Copy the generated API Key and Secret. Store as TRELLO_API_KEY and TRELLO_API_SECRET.
Step 3: Generate a Token
1. On the same page where your API Key is shown, locate the option to manually generate a Token.
2. Authorize your Trello account:
   - Follow the on-screen instructions to authorize the application.
3. Copy the generated Token. Store as TRELLO_TOKEN.
"""

from agno.agent import Agent
from agno.tools.trello import TrelloTools

agent = Agent(
    instructions=[
        "You are a Trello management assistant that helps organize and manage Trello boards, lists, and cards",
        "Help users with tasks like:",
        "- Creating and organizing boards, lists, and cards",
        "- Moving cards between lists",
        "- Retrieving board and list information",
        "- Managing card details and descriptions",
        "Always confirm successful operations and provide relevant board/list/card IDs and URLs",
        "When errors occur, provide clear explanations and suggest solutions",
    ],
    tools=[TrelloTools()],
)

agent.print_response(
    "Create a board called ai-agent and inside it create list called 'todo' and 'doing' and inside each of them create card called 'create agent'",
    stream=True,
)
```

---

<a name="tools--twilio_toolspy"></a>

### `tools/twilio_tools.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.twilio import TwilioTools

"""
Example showing how to use the Twilio Tools with Agno.

Requirements:
- Twilio Account SID and Auth Token (get from console.twilio.com)
- A Twilio phone number
- pip install twilio

Usage:
- Set the following environment variables:
    export TWILIO_ACCOUNT_SID="your_account_sid"
    export TWILIO_AUTH_TOKEN="your_auth_token"

- Or provide them when creating the TwilioTools instance
"""


# Example 1: Enable specific Twilio functions
agent = Agent(
    name="Twilio Agent",
    instructions=[
        """You can help users by:
        - Sending SMS messages
        - Checking message history
        - getting call details
        """
    ],
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        TwilioTools(
            enable_send_sms=True,
            enable_get_call_details=True,
            enable_list_messages=True,
        )
    ],
    markdown=True,
)

# Example 2: Enable all Twilio functions
agent_all = Agent(
    name="Twilio Agent All",
    model=OpenAIChat(id="gpt-4o"),
    tools=[TwilioTools(all=True)],
    markdown=True,
)

# Example 3: Enable only SMS functionality
sms_agent = Agent(
    name="SMS Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        TwilioTools(
            enable_send_sms=True,
            enable_get_call_details=False,
            enable_list_messages=False,
        )
    ],
    markdown=True,
)

sender_phone_number = "+1234567890"
receiver_phone_number = "+1234567890"

agent.print_response(
    f"Can you send an SMS saying 'Your package has arrived' to {receiver_phone_number} from {sender_phone_number}?"
)
```

---

<a name="tools--valyu_toolspy"></a>

### `tools/valyu_tools.py`

```python
"""
This cookbook demonstrates how to use the Valyu Toolkit for academic and web search.

Prerequisites:
- Install: pip install valyu
- Get API key: https://platform.valyu.network
- Set environment variable: export VALYU_API_KEY with your api key or pass the api key while initializing the toolkit
"""

from agno.agent import Agent
from agno.tools.valyu import ValyuTools

agent = Agent(
    tools=[ValyuTools()],
    markdown=True,
)

# Example 1: Basic Academic Paper Search
agent.print_response(
    "What are the latest safety mechanisms and mitigation strategies for CRISPR off-target effects?",
    markdown=True,
)

# Example 2: Focused ArXiv Search with Date Filtering
agent.print_response(
    "Search for transformer architecture papers published between June 2023 and January 2024, focusing on attention mechanisms",
    markdown=True,
)

# Example 3: Search Within Specific Paper
agent.print_response(
    "Search within the paper https://arxiv.org/abs/1706.03762 for details about the multi-head attention mechanism architecture",
    markdown=True,
)

# Example 4: Search Web
agent.print_response(
    "What are the main developments in large language model reasoning capabilities published in 2024?",
    markdown=True,
)
```

---

<a name="tools--visualization_toolspy"></a>

### `tools/visualization_tools.py`

```python
""" Data Visualization Tools - Create Charts and Graphs with AI Agents

This example shows how to use the VisualizationTools to create various types of charts
and graphs for data visualization. Demonstrates include_tools/exclude_tools patterns
for selective visualization function access.

Run: `pip install matplotlib` to install the dependencies
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.visualization import VisualizationTools

# Example 1: Enable all visualization functions
viz_agent_all = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        VisualizationTools(
            all=True,  # Enable all visualization functions
            output_dir="business_charts",
        )
    ],
    instructions=[
        "You are a data visualization expert with access to all chart types.",
        "Use appropriate visualization functions for the data presented.",
        "Always provide meaningful titles, axis labels, and context.",
        "Suggest insights based on the data visualized.",
        "Format data appropriately for each chart type.",
    ],
    markdown=True,
)

# Example 1b: All visualization functions available (explicit flags)
viz_agent_full = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        VisualizationTools(
            enable_create_bar_chart=True,
            enable_create_line_chart=True,
            enable_create_scatter_plot=True,
            enable_create_pie_chart=True,
            enable_create_histogram=True,
            output_dir="business_charts",
        )
    ],
    instructions=[
        "You are a data visualization expert with access to all chart types.",
        "Use appropriate visualization functions for the data presented.",
        "Always provide meaningful titles, axis labels, and context.",
        "Suggest insights based on the data visualized.",
        "Format data appropriately for each chart type.",
    ],
    markdown=True,
)

# Example 2: Enable only basic chart types
viz_agent_basic = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        VisualizationTools(
            enable_create_bar_chart=True,
            enable_create_line_chart=True,
            enable_create_pie_chart=True,
            enable_create_scatter_plot=False,
            enable_create_histogram=False,
            output_dir="basic_charts",
        )
    ],
    instructions=[
        "You are a data visualization specialist focused on basic chart types.",
        "Use bar charts for categorical comparisons.",
        "Use line charts for trends over time.",
        "Use pie charts for part-to-whole relationships.",
        "Keep visualizations simple and clear.",
    ],
    markdown=True,
)

# Example 3: Enable standard visualization functions (avoid complex ones)
viz_agent_safe = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        VisualizationTools(
            enable_create_bar_chart=True,
            enable_create_line_chart=True,
            enable_create_scatter_plot=True,
            enable_create_pie_chart=True,
            enable_create_histogram=True,
            # Note: Complex functions like create_3d_plot, create_heatmap would be False
            output_dir="safe_charts",
        )
    ],
    instructions=[
        "You are a business analyst creating straightforward visualizations.",
        "Focus on clear, easy-to-interpret charts.",
        "Avoid overly complex visualization types.",
        "Ensure charts are suitable for business presentations.",
    ],
    markdown=True,
)

# Example 4: Statistical analysis focused agent
viz_agent_stats = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        VisualizationTools(
            enable_create_scatter_plot=True,
            enable_create_histogram=True,
            enable_create_bar_chart=False,
            enable_create_line_chart=False,
            enable_create_pie_chart=False,
            # Note: Would also enable box_plot, violin_plot if available
            output_dir="stats_charts",
        )
    ],
    instructions=[
        "You are a statistical analyst focused on data distribution and correlation.",
        "Use scatter plots to show relationships between variables.",
        "Use histograms to show data distributions.",
        "Provide statistical insights based on the visualizations.",
    ],
    markdown=True,
)

# Use the all-enabled agent for the main examples
viz_agent = viz_agent_all

# Example 1: Sales Performance Analysis
print(" Example 1: Creating a Sales Performance Chart")
viz_agent.print_response(
    """
Create a bar chart showing our Q4 sales performance:
- December: $45,000
- November: $38,000  
- October: $42,000
- September: $35,000

Title it "Q4 Sales Performance" and provide insights about the trend.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 2: Market Share Analysis
print(" Example 2: Market Share Pie Chart")
viz_agent.print_response(
    """
Create a pie chart showing our market share compared to competitors:
- Our Company: 35%
- Competitor A: 25%
- Competitor B: 20%
- Competitor C: 15%
- Others: 5%

Title it "Market Share Analysis 2024" and analyze our position.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 3: Growth Trend Analysis
print(" Example 3: Revenue Growth Trend")
viz_agent.print_response(
    """
Create a line chart showing our monthly revenue growth over the past 6 months:
- January: $120,000
- February: $135,000
- March: $128,000
- April: $145,000
- May: $158,000
- June: $162,000

Title it "Monthly Revenue Growth" and identify trends and growth rate.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 4: Advanced Data Analysis
print(" Example 4: Customer Satisfaction vs Sales Correlation")
viz_agent.print_response(
    """
Create a scatter plot to analyze the relationship between customer satisfaction scores and sales:

Customer satisfaction scores (x-axis): [7.2, 8.1, 6.9, 8.5, 7.8, 9.1, 6.5, 8.3, 7.6, 8.9, 7.1, 8.7]
Sales in thousands (y-axis): [45, 62, 38, 71, 53, 85, 32, 68, 48, 79, 41, 75]

Title it "Customer Satisfaction vs Sales Performance" and analyze the correlation.
""",
    stream=True,
)

print("\n" + "=" * 60 + "\n")

# Example 5: Distribution Analysis
print(" Example 5: Score Distribution Histogram")
viz_agent.print_response(
    """
Create a histogram showing the distribution of customer review scores:
Data: [4.1, 4.5, 3.8, 4.7, 4.2, 4.9, 3.9, 4.6, 4.3, 4.8, 4.0, 4.4, 3.7, 4.5, 4.1, 4.6, 4.2, 4.7, 3.9, 4.3]

Use 6 bins, title it "Customer Review Score Distribution" and analyze the distribution pattern.
""",
    stream=True,
)

print(
    "\n All examples completed! Check the 'business_charts' folder for generated visualizations."
)

# More advanced example with business context
print("\n" + "=" * 60)
print(" ADVANCED EXAMPLE: Business Intelligence Dashboard")
print("=" * 60 + "\n")

bi_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        VisualizationTools(
            all=True,  # Enable all visualization functions
            output_dir="dashboard_charts",
        )
    ],
    instructions=[
        "You are a Business Intelligence analyst.",
        "Create comprehensive visualizations for executive dashboards.",
        "Provide actionable insights and recommendations.",
        "Use appropriate chart types for different data scenarios.",
        "Always explain what the data reveals about business performance.",
    ],
    markdown=True,
)

# Multi-chart business analysis
bi_agent.print_response(
    """
I need to create a comprehensive quarterly business review. Please help me with these visualizations:

1. First, create a bar chart showing revenue by product line:
   - Software Licenses: $2.3M
   - Support Services: $1.8M
   - Consulting: $1.2M
   - Training: $0.7M

2. Then create a line chart showing our customer acquisition over the past 12 months:
   - Jan: 45, Feb: 52, Mar: 48, Apr: 61, May: 58, Jun: 67
   - Jul: 73, Aug: 69, Sep: 78, Oct: 84, Nov: 81, Dec: 89

3. Finally, create a pie chart showing our expense breakdown:
   - Personnel: 45%
   - Technology: 25%
   - Marketing: 15%
   - Operations: 10%
   - Other: 5%

For each chart, provide business insights and recommendations for next quarter.
""",
    stream=True,
)
```

---

<a name="tools--web_toolspy"></a>

### `tools/web_tools.py`

```python
from agno.agent import Agent
from agno.tools.webtools import WebTools

agent = Agent(tools=[WebTools()])
agent.print_response("Tell me about https://tinyurl.com/57bmajz4")
```

---

<a name="tools--webbrowser_toolspy"></a>

### `tools/webbrowser_tools.py`

```python
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.webbrowser import WebBrowserTools

# Example 1: Enable specific WebBrowser functions
agent = Agent(
    model=Gemini("gemini-2.0-flash"),
    tools=[WebBrowserTools(enable_open_page=True), DuckDuckGoTools(all=True)],
    instructions=[
        "Find related websites and pages using DuckDuckGo",
        "Use web browser to open the site",
    ],
    markdown=True,
)

# Example 2: Enable all WebBrowser functions
agent_all = Agent(
    model=Gemini("gemini-2.0-flash"),
    tools=[WebBrowserTools(all=True), DuckDuckGoTools(all=True)],
    instructions=[
        "Find related websites and pages using DuckDuckGo",
        "Use web browser to open the site with full functionality",
    ],
    markdown=True,
)
agent.print_response("Find an article explaining MCP and open it in the web browser.")
```

---

<a name="tools--webex_toolspy"></a>

### `tools/webex_tools.py`

```python
"""
Run `pip install openai webexpythonsdk` to install dependencies.
To get the Webex Teams Access token refer to - https://developer.webex.com/docs/bots

Steps:

1. Sign up for Webex Teams and go to the Webex [Developer Portal](https://developer.webex.com/)
2. Create the Bot
    2.1 Click in the top-right on your profile  My Webex Apps  Create a Bot.
    2.2 Enter Bot Name, Username, Icon, and Description, then click Add Bot.
3. Get the Access Token
    3.1 Copy the Access Token shown on the confirmation page (displayed once).
    3.2 If lost, regenerate it via My Webex Apps  Edit Bot  Regenerate Access Token.
4. Set the WEBEX_ACCESS_TOKEN environment variable
5. Launch Webex itself and add your bot to a space like the Welcome space. Use the bot's email address (e.g. test@webex.bot)
"""

from agno.agent import Agent
from agno.tools.webex import WebexTools

agent = Agent(tools=[WebexTools()])

# List all space in Webex
agent.print_response("List all space on our Webex", markdown=True)

# Send a message to a Space in Webex
agent.print_response(
    "Send a funny ice-breaking message to the webex Welcome space", markdown=True
)
```

---

<a name="tools--website_toolspy"></a>

### `tools/website_tools.py`

```python
"""
Website Tools - Web Scraping and Content Analysis

This example demonstrates how to use WebsiteTools for web scraping and analysis.
Shows enable_ flag patterns for selective function access.
WebsiteTools is a small tool (<6 functions) so it uses enable_ flags.
"""

from agno.agent import Agent
from agno.tools.website import WebsiteTools

agent = Agent(
    tools=[WebsiteTools()],  # All functions enabled by default
    description="You are a comprehensive web scraping specialist with all website analysis capabilities.",
    instructions=[
        "Help users scrape and analyze website content",
        "Provide detailed summaries and insights from web pages",
        "Handle various website formats and structures",
        "Ensure respectful scraping practices",
    ],
    markdown=True,
)

# Example usage
print("=== Basic Web Content Search Example ===")
agent.print_response(
    "Search web page: 'https://docs.agno.com/introduction' and summarize the key concepts",
    markdown=True,
)
```

---

<a name="tools--website_tools_knowledgepy"></a>

### `tools/website_tools_knowledge.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.tools.website import WebsiteTools
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

# Create PDF URL knowledge base
kb = Knowledge(
    vector_db=PgVector(
        table_name="documents",
        db_url=db_url,
    ),
)

kb.add_contents(
    urls=[
        "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        "https://docs.agno.com/introduction",
    ]
)

# Initialize the Agent with the combined knowledge base
agent = Agent(
    knowledge=kb,
    search_knowledge=True,
    tools=[
        WebsiteTools(knowledge=kb)  # Set combined or website knowledge base
    ],
)

# Use the agent
agent.print_response(
    "How do I get started on Mistral: https://docs.mistral.ai/getting-started/models/models_overview",
    markdown=True,
    stream=True,
)
```

---

<a name="tools--whatsapp_toolspy"></a>

### `tools/whatsapp_tools.py`

```python
"""
WhatsApp Cookbook
----------------

This cookbook demonstrates how to use WhatsApp integration with Agno. Before running this example,
you'll need to complete these setup steps:

1. Create Meta Developer Account
   - Go to [Meta Developer Portal](https://developers.facebook.com/) and create a new account
   - Create a new app at [Meta Apps Dashboard](https://developers.facebook.com/apps/)
   - Enable WhatsApp integration for your app [here](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

2. Set Up WhatsApp Business API
   You can get your WhatsApp Business Account ID from [Business Settings](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

3. Configure Environment
   - Set these environment variables:
     WHATSAPP_ACCESS_TOKEN=your_access_token          # Access Token
     WHATSAPP_PHONE_NUMBER_ID=your_phone_number_id    # Phone Number ID
     WHATSAPP_RECIPIENT_WAID=your_recipient_waid      # Recipient WhatsApp ID (e.g. 1234567890)
     WHATSAPP_VERSION=your_whatsapp_version           # WhatsApp API Version (e.g. v22.0)

Important Notes:
- For first-time outreach, you must use pre-approved message templates
  [here](https://developers.facebook.com/docs/whatsapp/cloud-api/guides/send-message-templates)
- Test messages can only be sent to numbers that are registered in your test environment

The example below shows how to send a template message using Agno's WhatsApp tools.
For more complex use cases, check out the WhatsApp Cloud API documentation:
[here](https://developers.facebook.com/docs/whatsapp/cloud-api/overview)
"""

from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.whatsapp import WhatsAppTools

agent = Agent(
    name="whatsapp",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[WhatsAppTools()],
)

# Example: Send a template message
# Note: Replace 'hello_world' with your actual template name
agent.print_response(
    "Send a template message using the 'hello_world' template in English to +1 123456789"
)
```

---

<a name="tools--wikipedia_toolspy"></a>

### `tools/wikipedia_tools.py`

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.tools.wikipedia import WikipediaTools

# Example 1: Basic Wikipedia search (without knowledge base)
agent = Agent(
    tools=[
        WikipediaTools(
            enable_search_wikipedia=True,
            enable_search_wikipedia_and_update_knowledge_base=False,
        )
    ]
)

# Example 2: Enable all Wikipedia functions
agent_all = Agent(tools=[WikipediaTools(all=True)])

# Example 3: Wikipedia with knowledge base integration
knowledge_base = Knowledge()
kb_agent = Agent(
    tools=[
        WikipediaTools(
            knowledge=knowledge_base,
            enable_search_wikipedia=False,
            enable_search_wikipedia_and_update_knowledge_base=True,
        )
    ]
)

# Test the agents
agent.print_response("Search Wikipedia for 'artificial intelligence'", markdown=True)
kb_agent.print_response(
    "Find information about machine learning and add it to knowledge base",
    markdown=True,
)
```

---

<a name="tools--x_toolspy"></a>

### `tools/x_tools.py`

```python
from agno.agent import Agent
from agno.tools.x import XTools

"""
To set up an X developer account and obtain the necessary keys, follow these steps:

1. **Create an X Developer Account:**
   - Go to the X Developer website: https://developer.x.com/
   - Sign in with your X account or create a new one if you don't have an account.
   - Apply for a developer account by providing the required information about your intended use of the X API.

2. **Create a Project and App:**
   - Once your developer account is approved, log in to the X Developer portal.
   - Navigate to the "Projects & Apps" section and create a new project.
   - Within the project, create a new app. This app will be used to generate the necessary API keys and tokens.
   - You'll get a client id and client secret, but you can ignore them.

3. **Generate API Keys, Tokens, and Client Credentials:**
   - After creating the app, navigate to the "Keys and tokens" tab.
   - Generate the following keys, tokens, and client credentials:
     - **API Key (Consumer Key)**
     - **API Secret Key (Consumer Secret)**
     - **Bearer Token**
     - **Access Token**
     - **Access Token Secret**

4. **Set Environment Variables:**
   - Export the generated keys, tokens, and client credentials as environment variables in your system or provide them as arguments to the `XTools` constructor.
     - `X_CONSUMER_KEY`
     - `X_CONSUMER_SECRET`
     - `X_ACCESS_TOKEN`
     - `X_ACCESS_TOKEN_SECRET`
     - `X_BEARER_TOKEN`
"""


# Initialize the x toolkit
x_tools = XTools()

# Create an agent with the X toolkit
agent = Agent(
    instructions=[
        "Use your tools to interact with X (Twitter) as the authorized user @AgnoAgi",
        "When asked to create a post, generate appropriate content based on the request",
        "Do not actually post content unless explicitly instructed to do so",
        "Provide informative responses about the user's timeline and posts",
        "Respect X's usage policies and rate limits",
    ],
    tools=[x_tools],
)

# Example usage: Get your details
agent.print_response(
    "Can you return my x profile with my home timeline?", markdown=True
)

# # Example usage: Get information about a user
# agent.print_response(
#     "Can you retrieve information about this user https://x.com/AgnoAgi ",
#     markdown=True,
# )

# # Example usage: Reply To a Post
# agent.print_response(
#     "Can you reply to this [post ID] post as a general message as to how great this project is: https://x.com/AgnoAgi",
#     markdown=True,
# )

# # Example usage: Send a direct message
# agent.print_response(
#     "Send direct message to the user @AgnoAgi telling them I want to learn more about them and a link to their community.",
#     markdown=True,
# )

# # Example usage: Create a new post
# agent.print_response("Create & post content about how 2025 is the year of the AI agent", markdown=True)
```

---

<a name="tools--yfinance_toolspy"></a>

### `tools/yfinance_tools.py`

```python
"""
YFinance Tools - Stock Market Analysis and Financial Data

This example demonstrates how to use YFinanceTools for financial analysis,
showing include_tools/exclude_tools patterns for selective function access.
YFinanceTools is a large tool (6 functions) so it uses include_tools/exclude_tools.

Run: `pip install yfinance` to install the dependencies
"""

from agno.agent import Agent
from agno.tools.yfinance import YFinanceTools

# Example 1: All financial functions available (default behavior)
agent_full = Agent(
    tools=[YFinanceTools()],  # All functions enabled by default
    description="You are a comprehensive investment analyst with access to all financial data functions.",
    instructions=[
        "Use any financial function as needed for investment analysis",
        "Format your response using markdown and use tables to display data",
        "Provide detailed analysis and insights based on the data",
        "Include relevant financial metrics and recommendations",
    ],
    markdown=True,
)

# Example 2: Include only basic stock information
agent_basic = Agent(
    tools=[
        YFinanceTools(
            include_tools=[
                "get_current_stock_price",
                "get_company_info",
                "get_historical_stock_prices",
            ]
        )
    ],
    description="You are a basic stock information specialist focused on price and historical data.",
    instructions=[
        "Provide current stock prices and basic company information",
        "Show historical price trends when requested",
        "Keep analysis focused on price movements and basic metrics",
        "Format data clearly using tables",
    ],
    markdown=True,
)

# Example 3: Exclude complex financial analysis functions
agent_simple = Agent(
    tools=[
        YFinanceTools(
            exclude_tools=[
                "get_income_statements",  # Complex financial statements
                "get_key_financial_ratios",  # Detailed financial ratios
            ]
        )
    ],
    description="You are a stock analyst focused on market data without complex financial statements.",
    instructions=[
        "Provide stock prices, recommendations, and market trends",
        "Avoid complex financial statement analysis",
        "Focus on actionable market information",
        "Keep analysis accessible to general investors",
    ],
    markdown=True,
)

# Example 4: Include only analysis and recommendation functions
agent_analyst = Agent(
    tools=[
        YFinanceTools(
            include_tools=[
                "get_analyst_recommendations",
                "get_company_news",
                "get_current_stock_price",
            ]
        )
    ],
    description="You are an equity research analyst focused on recommendations and market sentiment.",
    instructions=[
        "Provide analyst recommendations and price targets",
        "Include relevant news and market sentiment",
        "Focus on forward-looking analysis and earnings expectations",
        "Present information suitable for investment decisions",
    ],
    markdown=True,
)

# Using the basic agent for the main example
print("=== Basic Stock Analysis Example ===")
agent_basic.print_response(
    "Share the NVDA stock price and recent historical performance", markdown=True
)

print("\n=== Analyst Recommendations Example ===")
agent_analyst.print_response(
    "Get analyst recommendations and recent news for AAPL", markdown=True
)

print("\n=== Full Analysis Example ===")
agent_full.print_response(
    "Provide a comprehensive analysis of TSLA including price, fundamentals, and analyst views",
    markdown=True,
)

print("\n=== Full Analysis Example ===")
agent_simple.print_response(
    "Provide a comprehensive analysis of TSLA including price, fundamentals, and analyst views",
    markdown=True,
)
```

---

<a name="tools--youtube_toolspy"></a>

### `tools/youtube_tools.py`

```python
from agno.agent import Agent
from agno.tools.youtube import YouTubeTools

agent = Agent(
    tools=[YouTubeTools()],
    description="You are a YouTube agent. Obtain the captions of a YouTube video and answer questions.",
)
agent.print_response(
    "Summarize this video https://www.youtube.com/watch?v=Iv9dewmcFbs&t", markdown=True
)
```

---

<a name="tools--zendesk_toolspy"></a>

### `tools/zendesk_tools.py`

```python
from agno.agent import Agent
from agno.tools.zendesk import ZendeskTools

agent = Agent(tools=[ZendeskTools()])
agent.print_response("How do I login?", markdown=True)
```

---

<a name="tools--zep_async_toolspy"></a>

### `tools/zep_async_tools.py`

```python
"""
This example demonstrates how to use the ZepAsyncTools class to interact with memories stored in Zep.

To get started, please export your Zep API key as an environment variable. You can get your Zep API key from https://app.getzep.com/

export ZEP_API_KEY=<your-zep-api-key>
"""

import asyncio
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zep import ZepAsyncTools


async def main():
    # Initialize the ZepAsyncTools
    zep_tools = ZepAsyncTools(
        user_id="agno", session_id="agno-async-session", add_instructions=True
    )

    # Initialize the Agent
    agent = Agent(
        model=OpenAIChat(),
        tools=[zep_tools],
        dependencies={
            "memory": lambda: zep_tools.get_zep_memory(memory_type="context"),
        },
        add_dependencies_to_context=True,
    )

    # Interact with the Agent
    await agent.aprint_response("My name is John Billings")
    await agent.aprint_response("I live in NYC")
    await agent.aprint_response("I'm going to a concert tomorrow")

    # Allow the memories to sync with Zep database
    time.sleep(10)

    # Refresh the context
    agent.dependencies["memory"] = await zep_tools.get_zep_memory(memory_type="context")

    # Ask the Agent about the user
    await agent.aprint_response("What do you know about me?")


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="tools--zep_toolspy"></a>

### `tools/zep_tools.py`

```python
"""
This example demonstrates how to use the ZepTools class to interact with memories stored in Zep.

To get started, please export your Zep API key as an environment variable. You can get your Zep API key from https://app.getzep.com/

export ZEP_API_KEY=<your-zep-api-key>
"""

import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zep import ZepTools

# Initialize the ZepTools
zep_tools = ZepTools(user_id="agno", session_id="agno-session", add_instructions=True)

# Initialize the Agent
agent = Agent(
    model=OpenAIChat(),
    tools=[zep_tools],
    dependencies={"memory": zep_tools.get_zep_memory(memory_type="context")},
    add_dependencies_to_context=True,
)

# Interact with the Agent so that it can learn about the user
agent.print_response("My name is John Billings")
agent.print_response("I live in NYC")
agent.print_response("I'm going to a concert tomorrow")

# Allow the memories to sync with Zep database
time.sleep(10)


if agent.dependencies:
    # Refresh the context
    agent.dependencies["memory"] = zep_tools.get_zep_memory(memory_type="context")

    # Ask the Agent about the user
    agent.print_response("What do you know about me?")
```

---

<a name="tools--zoom_toolspy"></a>

### `tools/zoom_tools.py`

```python
"""
Zoom Tools Example - Demonstrates how to use the Zoom toolkit for meeting management.

This example shows how to:
1. Set up authentication with Zoom API
2. Initialize the ZoomTools with proper credentials
3. Create an agent that can manage Zoom meetings
4. Use various Zoom API functionalities through natural language

Prerequisites:
-------------
1. Create a Server-to-Server OAuth app in Zoom Marketplace:
   - Visit https://marketplace.zoom.us/
   - Create a new app. Go to Develop -> Build App -> Server-to-Server OAuth.
   - Add required scopes:
     * meeting:write:admin
     * meeting:read:admin
     * cloud_recording:read:admin
   - Copy Account ID, Client ID, and Client Secret

2. Set environment variables:
   export ZOOM_ACCOUNT_ID=your_account_id
   export ZOOM_CLIENT_ID=your_client_id
   export ZOOM_CLIENT_SECRET=your_client_secret

Features:
---------
- Schedule new meetings
- Get meeting details
- List all meetings
- Get upcoming meetings
- Delete meetings
- Get meeting recordings

Usage:
------
Run this script with proper environment variables set to interact with
the Zoom API through natural language commands.
"""

import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zoom import ZoomTools

# Get environment variables
ACCOUNT_ID = os.getenv("ZOOM_ACCOUNT_ID")
CLIENT_ID = os.getenv("ZOOM_CLIENT_ID")
CLIENT_SECRET = os.getenv("ZOOM_CLIENT_SECRET")

# Initialize Zoom tools with credentials
zoom_tools = ZoomTools(
    account_id=ACCOUNT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET
)

# Create an agent with Zoom capabilities
agent = Agent(
    name="Zoom Meeting Manager",
    id="zoom-meeting-manager",
    model=OpenAIChat(id="gpt-4"),
    tools=[zoom_tools],
    markdown=True,
    instructions=[
        "You are an expert at managing Zoom meetings using the Zoom API.",
        "You can:",
        "1. Schedule new meetings (schedule_meeting)",
        "2. Get meeting details (get_meeting)",
        "3. List all meetings (list_meetings)",
        "4. Get upcoming meetings (get_upcoming_meetings)",
        "5. Delete meetings (delete_meeting)",
        "6. Get meeting recordings (get_meeting_recordings)",
        "",
        "For recordings, you can:",
        "- Retrieve recordings for any past meeting using the meeting ID",
        "- Include download tokens if needed",
        "- Get recording details like duration, size, download link and file types",
        "",
        "Guidelines:",
        "- Use ISO 8601 format for dates (e.g., '2024-12-28T10:00:00Z')",
        "- Accept and use user's timezone (e.g., 'America/New_York', 'Asia/Tokyo', 'UTC')",
        "- If no timezone is specified, default to UTC",
        "- Ensure meeting times are in the future",
        "- Provide meeting details after scheduling (ID, URL, time)",
        "- Handle errors gracefully",
        "- Confirm successful operations",
    ],
)

# Example usage - uncomment the ones you want to try
agent.print_response(
    "Schedule a meeting titled 'Team Sync' for tomorrow at 2 PM UTC for 45 minutes"
)

# More examples (uncomment to use):
# agent.print_response("What meetings do I have coming up?")
# agent.print_response("List all my scheduled meetings")
# agent.print_response("Get details for my most recent meeting")
# agent.print_response("Get the recordings for my last team meeting")
# agent.print_response("Delete the meeting titled 'Team Sync'")
# agent.print_response("Schedule daily standup meetings for next week at 10 AM UTC")
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_functions_and_agentspy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_functions_and_agents.py`

```python
import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


async def prepare_input_for_web_search(step_input: StepInput) -> StepOutput:
    topic = step_input.input
    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic
	<topic>
	{topic}
	</topic>

	Search the web for atleast 10 articles\
	""")
    )


async def prepare_input_for_writer(step_input: StepInput) -> StepOutput:
    topic = step_input.input
    research_team_output = step_input.previous_step_content

    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic:
	<topic>
	{topic}
	</topic>

	Here is information from the web:
	<research_results>
	{research_team_output}
	<research_results>\
	""")
    )


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )
    asyncio.run(
        content_creation_workflow.aprint_response(
            input="AI trends in 2024",
            markdown=True,
        )
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_functions_and_agents_streampy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_functions_and_agents_stream.py`

```python
import asyncio
from textwrap import dedent
from typing import AsyncIterator

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


async def prepare_input_for_web_search(
    step_input: StepInput,
) -> AsyncIterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.input

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic
        <topic>
        {topic}
        </topic>

        Search the web for atleast 10 articles\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


async def prepare_input_for_writer(step_input: StepInput) -> AsyncIterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.input
    research_team_output = step_input.previous_step_content

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic:
        <topic>
        {topic}
        </topic>

        Here is information from the web:
        <research_results>
        {research_team_output}
        </research_results>\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )

    asyncio.run(
        content_creation_workflow.aprint_response(
            input="AI trends in 2024",
            markdown=True,
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_stepspy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_steps.py`

```python
"""
This example shows a basic sequential sequence of steps that run agents and teams.

It is for a content writer that creates posts about tech trends from Hackernews and the web.
"""

import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[research_step, content_planning_step],
)


# Create and use workflow
async def main():
    await content_creation_workflow.aprint_response(
        input="AI agent frameworks 2025",
        markdown=True,
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--async--sequence_of_steps_streampy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/async/sequence_of_steps_stream.py`

```python
"""
This example shows a basic sequential sequence of steps that run agents and teams.

This shows how to stream the response from the steps.
"""

import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)
content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)


# Create and use workflow
async def main():
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
    )
    await content_creation_workflow.aprint_response(
        input="AI agent frameworks 2025",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--async--workflow_using_stepspy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/async/workflow_using_steps.py`

```python
import asyncio

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

# Define individual steps
research_step = Step(
    name="research",
    agent=researcher,
    description="Research the topic and gather information",
)

writing_step = Step(
    name="writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="editing",
    agent=editor,
    description="Edit and polish the article",
)

# Create a Steps sequence that chains these above steps together
article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to final edit",
    steps=[research_step, writing_step, editing_step],
)

# Create and use workflow
if __name__ == "__main__":
    article_workflow = Workflow(
        name="Article Creation Workflow",
        description="Automated article creation from research to publication",
        steps=[article_creation_sequence],
    )

    asyncio.run(
        article_workflow.aprint_response(
            input="Write an article about the benefits of renewable energy",
            markdown=True,
        )
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_functions_and_agentspy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_functions_and_agents.py`

```python
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


def prepare_input_for_web_search(step_input: StepInput) -> StepOutput:
    topic = step_input.input
    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic
	<topic>
	{topic}
	</topic>

	Search the web for atleast 10 articles\
	""")
    )


def prepare_input_for_writer(step_input: StepInput) -> StepOutput:
    topic = step_input.input
    research_team_output = step_input.previous_step_content

    return StepOutput(
        content=dedent(f"""\
	I'm writing a blog post on the topic:
	<topic>
	{topic}
	</topic>

	Here is information from the web:
	<research_results>
	{research_team_output}
	<research_results>\
	""")
    )


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_functions_and_agents_streampy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_functions_and_agents_stream.py`

```python
from textwrap import dedent
from typing import Iterator

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


def prepare_input_for_web_search(step_input: StepInput) -> Iterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.input

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic
        <topic>
        {topic}
        </topic>

        Search the web for atleast 10 articles\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


def prepare_input_for_writer(step_input: StepInput) -> Iterator[StepOutput]:
    """Generator function that yields StepOutput"""
    topic = step_input.input
    research_team_output = step_input.previous_step_content

    # Create proper StepOutput content
    content = dedent(f"""\
        I'm writing a blog post on the topic:
        <topic>
        {topic}
        </topic>

        Here is information from the web:
        <research_results>
        {research_team_output}
        </research_results>\
        """)

    # Yield a StepOutput as the final result
    yield StepOutput(content=content)


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Blog Post Workflow",
        description="Automated blog post creation from Hackernews and the web",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[
            prepare_input_for_web_search,
            research_team,
            prepare_input_for_writer,
            writer_agent,
        ],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_stepspy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_steps.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    # Define the sequence of steps
    # First run the research team, then the content planner Agent
    # You can mix and match agents, teams, and even regular python functions as steps
    steps=[research_step, content_planning_step],
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--sequence_of_steps_streampy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/sequence_of_steps_stream.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        # Define the sequence of steps
        # First run the research team, then the content planner Agent
        # You can mix and match agents, teams, and even regular python functions as steps
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_using_stepspy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_using_steps.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

# Define individual steps
research_step = Step(
    name="research",
    agent=researcher,
    description="Research the topic and gather information",
)

writing_step = Step(
    name="writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="editing",
    agent=editor,
    description="Edit and polish the article",
)

# Create a Steps sequence that chains these above steps together
article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to final edit",
    steps=[research_step, writing_step, editing_step],
)

# Create and use workflow
if __name__ == "__main__":
    article_workflow = Workflow(
        name="Article Creation Workflow",
        description="Automated article creation from research to publication",
        steps=[article_creation_sequence],
    )

    article_workflow.print_response(
        input="Write an article about the benefits of renewable energy",
        markdown=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_using_steps_nestedpy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_using_steps_nested.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

tech_researcher = Agent(
    name="Tech Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    instructions="Research tech-related topics from Hacker News and provide latest developments.",
)

news_researcher = Agent(
    name="News Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[ExaTools()],
    instructions="Research current news and trends using Exa search.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

content_agent = Agent(
    name="Content Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Prepare and format content for writing based on research inputs.",
)

# Define individual steps
initial_research_step = Step(
    name="InitialResearch",
    agent=researcher,
    description="Initial research on the topic",
)


# Condition evaluator function
def is_tech_topic(step_input) -> bool:
    """Check if the topic is tech-related and needs specialized research"""
    message = step_input.input.lower() if step_input.input else ""
    tech_keywords = [
        "ai",
        "machine learning",
        "technology",
        "software",
        "programming",
        "tech",
        "startup",
        "blockchain",
    ]
    return any(keyword in message for keyword in tech_keywords)


# Define parallel research steps
tech_research_step = Step(
    name="TechResearch",
    agent=tech_researcher,
    description="Research tech developments from Hacker News",
)

news_research_step = Step(
    name="NewsResearch",
    agent=news_researcher,
    description="Research current news and trends",
)

# Define content preparation step
content_prep_step = Step(
    name="ContentPreparation",
    agent=content_agent,
    description="Prepare and organize all research for writing",
)

writing_step = Step(
    name="Writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="Editing",
    agent=editor,
    description="Edit and polish the article",
)

# Create a Steps sequence with a Condition containing Parallel steps
article_creation_sequence = Steps(
    name="ArticleCreation",
    description="Complete article creation workflow from research to final edit",
    steps=[
        initial_research_step,
        # Condition with Parallel steps inside
        Condition(
            name="TechResearchCondition",
            description="If topic is tech-related, do specialized parallel research",
            evaluator=is_tech_topic,
            steps=[
                Parallel(
                    tech_research_step,
                    news_research_step,
                    name="SpecializedResearch",
                    description="Parallel tech and news research",
                ),
                content_prep_step,
            ],
        ),
        writing_step,
        editing_step,
    ],
)

# Create and use workflow
if __name__ == "__main__":
    article_workflow = Workflow(
        name="Enhanced Article Creation Workflow",
        description="Automated article creation with conditional parallel research",
        steps=[article_creation_sequence],
    )

    article_workflow.print_response(
        input="Write an article about the latest AI developments in machine learning",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_with_file_inputpy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_with_file_input.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.media import File
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
read_agent = Agent(
    name="Agent",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Read the contents of the attached file.",
)

summarize_agent = Agent(
    name="Summarize Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Summarize the contents of the attached file.",
    ],
)

# Define steps
read_step = Step(
    name="Read Step",
    agent=read_agent,
)

summarize_step = Step(
    name="Summarize Step",
    agent=summarize_agent,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(
        session_table="workflow",
        db_file="tmp/workflow.db",
    ),
    steps=[read_step, summarize_step],
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow.print_response(
        input="Summarize the contents of the attached file.",
        files=[
            File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")
        ],
        markdown=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_01_sequence_of_steps--sync--workflow_with_session_metricspy"></a>

### `workflows/_01_basic_workflows/_01_sequence_of_steps/sync/workflow_with_session_metrics.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from rich.pretty import pprint

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow_session_metrics.db",
    ),
    steps=[research_step, content_planning_step],
)

# Create and use workflow
if __name__ == "__main__":
    # Run the workflow
    response = content_creation_workflow.run(
        input="AI trends in 2024",
    )

    print("=" * 50)
    print("WORKFLOW RESPONSE")
    print("=" * 50)
    pprint_run_response(response, markdown=True)

    # Get and display session metrics
    print("\n" + "=" * 50)
    print("SESSION METRICS")
    print("=" * 50)

    session_metrics = content_creation_workflow.get_session_metrics()
    pprint(session_metrics)
```

---

<a name="workflows--_01_basic_workflows--_02_step_with_function--async--step_with_function_additional_datapy"></a>

### `workflows/_01_basic_workflows/_02_step_with_function/async/step_with_function_additional_data.py`

```python
import asyncio
from typing import AsyncIterator, Union

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowRunOutputEvent
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


async def custom_content_planning_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness
    Now also uses additional_data for extra context
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Access additional_data that was passed with the workflow
    additional_data = step_input.additional_data or {}
    user_email = additional_data.get("user_email", "No email provided")
    priority = additional_data.get("priority", "normal")
    client_type = additional_data.get("client_type", "standard")

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Additional Context:
        - Client Type: {client_type}
        - Priority Level: {priority}
        - Contact Email: {user_email}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies
        {"6. Mark as HIGH PRIORITY delivery" if priority == "high" else "6. Standard delivery timeline"}

        Please create a detailed, actionable content plan.
    """

    try:
        response_iterator = content_planner.arun(
            planning_prompt, stream=True, stream_intermediate_steps=True
        )
        async for event in response_iterator:
            yield event
        response = content_planner.get_last_run_output()

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Client Details:**
            - Type: {client_type}
            - Priority: {priority.upper()}
            - Contact: {user_email}

            **Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
            - Priority Level: {priority.upper()}
        """.strip()

        yield StepOutput(content=enhanced_content, response=response)

    except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
    )

    # Run workflow with additional_data
    asyncio.run(
        content_creation_workflow.aprint_response(
            input="AI trends in 2024",
            additional_data={
                "user_email": "kaustubh@agno.com",
                "priority": "high",
                "client_type": "enterprise",
            },
            markdown=True,
            stream=True,
            stream_intermediate_steps=True,
        )
    )

    print("\n" + "=" * 60 + "\n")
```

---

<a name="workflows--_01_basic_workflows--_02_step_with_function--async--step_with_function_streampy"></a>

### `workflows/_01_basic_workflows/_02_step_with_function/async/step_with_function_stream.py`

```python
import asyncio
from typing import AsyncIterator, Union

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowRunOutputEvent
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[GoogleSearchTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
    db=InMemoryDb(),
)


async def custom_content_planning_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        response_iterator = content_planner.arun(
            planning_prompt, stream=True, stream_intermediate_steps=True
        )
        async for event in response_iterator:
            yield event

        response = content_planner.get_last_run_output()

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

        yield StepOutput(content=enhanced_content)

    except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


async def main():
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
    )
    await content_creation_workflow.aprint_response(
        input="AI agent frameworks 2025",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="workflows--_01_basic_workflows--_02_step_with_function--sync--step_with_functionpy"></a>

### `workflows/_01_basic_workflows/_02_step_with_function/sync/step_with_function.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """
    Custom function that does intelligent content planning with context awareness
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        response = content_planner.run(planning_prompt)

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

        return StepOutput(content=enhanced_content)

    except Exception as e:
        return StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        # Define the sequence of steps
        # First run the research_step, then the content_planning_step
        # You can mix and match agents, teams, and even regular python functions directly as steps
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )

    print("\n" + "=" * 60 + "\n")
```

---

<a name="workflows--_01_basic_workflows--_02_step_with_function--sync--step_with_function_additional_datapy"></a>

### `workflows/_01_basic_workflows/_02_step_with_function/sync/step_with_function_additional_data.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """
    Custom function that does intelligent content planning with context awareness
    Now also uses additional_data for extra context
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Access additional_data that was passed with the workflow
    additional_data = step_input.additional_data or {}
    user_email = additional_data.get("user_email", "No email provided")
    priority = additional_data.get("priority", "normal")
    client_type = additional_data.get("client_type", "standard")

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Additional Context:
        - Client Type: {client_type}
        - Priority Level: {priority}
        - Contact Email: {user_email}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies
        {"6. Mark as HIGH PRIORITY delivery" if priority == "high" else "6. Standard delivery timeline"}

        Please create a detailed, actionable content plan.
    """

    try:
        response = content_planner.run(planning_prompt)

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Client Details:**
            - Type: {client_type}
            - Priority: {priority.upper()}
            - Contact: {user_email}

            **Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
            - Priority Level: {priority.upper()}
        """.strip()

        return StepOutput(content=enhanced_content, response=response)

    except Exception as e:
        return StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
    )

    # Run workflow with additional_data
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        additional_data={
            "user_email": "kaustubh@agno.com",
            "priority": "high",
            "client_type": "enterprise",
        },
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )

    print("\n" + "=" * 60 + "\n")
```

---

<a name="workflows--_01_basic_workflows--_02_step_with_function--sync--step_with_function_streampy"></a>

### `workflows/_01_basic_workflows/_02_step_with_function/sync/step_with_function_stream.py`

```python
from typing import Iterator, Union

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowRunOutputEvent
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
    db=InMemoryDb(),
)


def custom_content_planning_function(
    step_input: StepInput,
) -> Iterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        response_iterator = content_planner.run(
            planning_prompt, stream=True, stream_intermediate_steps=True
        )
        for event in response_iterator:
            yield event

        response = content_planner.get_last_run_output()

        enhanced_content = f"""
            ## Strategic Content Plan

            **Planning Topic:** {message}

            **Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

        yield StepOutput(content=enhanced_content)

    except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)


# Define and use examples
if __name__ == "__main__":
    streaming_content_workflow = Workflow(
        name="Streaming Content Creation Workflow",
        description="Automated content creation with streaming custom execution functions",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        # Define the sequence of steps
        # First run the research_step, then the content_planning_step
        # You can mix and match agents, teams, and even regular python functions directly as steps
        steps=[
            research_step,
            content_planning_step,
        ],
    )

    streaming_content_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_01_basic_workflows--_03_function_instead_of_steps--async--function_instead_of_stepspy"></a>

### `workflows/_01_basic_workflows/_03_function_instead_of_steps/async/function_instead_of_steps.py`

```python
import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


async def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the research team
    run_response = research_team.run(execution_input.input)
    research_content = run_response.content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {execution_input.input}

        Research Results: {research_content[:500]}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """
    content_plan = await content_planner.arun(planning_prompt)

    # Return the content plan
    return content_plan.content


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=custom_execution_function,
    )

    asyncio.run(
        content_creation_workflow.aprint_response(
            input="AI trends in 2024",
        )
    )
```

---

<a name="workflows--_01_basic_workflows--_03_function_instead_of_steps--async--function_instead_of_steps_streampy"></a>

### `workflows/_01_basic_workflows/_03_function_instead_of_steps/async/function_instead_of_steps_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Research key insights and content from Hackernews posts",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


async def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the Hackernews agent to gather research content
    research_content = ""
    async for response in hackernews_agent.arun(
        execution_input.input, stream=True, stream_intermediate_steps=True
    ):
        if hasattr(response, "content") and response.content:
            research_content += str(response.content)

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {execution_input.input}

        Research Results: {research_content[:500]}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    async for response in content_planner.arun(
        planning_prompt, stream=True, stream_intermediate_steps=True
    ):
        yield response


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=custom_execution_function,
    )
    asyncio.run(
        content_creation_workflow.aprint_response(
            input="AI trends in 2024",
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="workflows--_01_basic_workflows--_03_function_instead_of_steps--sync--function_instead_of_stepspy"></a>

### `workflows/_01_basic_workflows/_03_function_instead_of_steps/sync/function_instead_of_steps.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the research team
    run_response = research_team.run(execution_input.input)
    research_content = run_response.content

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {execution_input.input}

        Research Results: {research_content[:500]}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """
    content_plan = content_planner.run(planning_prompt)

    # Return the content plan
    return content_plan.content


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=custom_execution_function,
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
    )
```

---

<a name="workflows--_01_basic_workflows--_03_function_instead_of_steps--sync--function_instead_of_steps_streampy"></a>

### `workflows/_01_basic_workflows/_03_function_instead_of_steps/sync/function_instead_of_steps_stream.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Research key insights and content from Hackernews posts",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_execution_function(
    workflow: Workflow, execution_input: WorkflowExecutionInput
):
    print(f"Executing workflow: {workflow.name}")

    # Run the Hackernews agent to gather research content
    research_content = ""
    for response in hackernews_agent.run(
        execution_input.input, stream=True, stream_intermediate_steps=True
    ):
        if hasattr(response, "content") and response.content:
            research_content += str(response.content)

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {execution_input.input}

        Research Results: {research_content[:500]}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """
    yield from content_planner.run(
        planning_prompt, stream=True, stream_intermediate_steps=True
    )


# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=custom_execution_function,
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_02_workflows_conditional_execution--async--condition_and_parallel_stepspy"></a>

### `workflows/_02_workflows_conditional_execution/async/condition_and_parallel_steps.py`

```python
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.input or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.input or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.input or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(input="Latest AI developments in machine learning")
        )
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_02_workflows_conditional_execution--async--condition_and_parallel_steps_streampy"></a>

### `workflows/_02_workflows_conditional_execution/async/condition_and_parallel_steps_stream.py`

```python
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.input or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.input or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.input or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(
                input="Latest AI developments in machine learning",
                stream=True,
                stream_intermediate_steps=True,
            )
        )
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_02_workflows_conditional_execution--async--condition_steps_workflow_streampy"></a>

### `workflows/_02_workflows_conditional_execution/async/condition_steps_workflow_stream.py`

```python
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.condition import Condition
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === BASIC AGENTS ===
researcher = Agent(
    name="Researcher",
    instructions="Research the given topic and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

summarizer = Agent(
    name="Summarizer",
    instructions="Create a clear summary of the research findings.",
)

fact_checker = Agent(
    name="Fact Checker",
    instructions="Verify facts and check for accuracy in the research.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Write a comprehensive article based on all available research and verification.",
)

# === CONDITION EVALUATOR ===


def needs_fact_checking(step_input: StepInput) -> bool:
    """Determine if the research contains claims that need fact-checking"""
    summary = step_input.previous_step_content or ""

    # Look for keywords that suggest factual claims
    fact_indicators = [
        "study shows",
        "research indicates",
        "according to",
        "statistics",
        "data shows",
        "survey",
        "report",
        "million",
        "billion",
        "percent",
        "%",
        "increase",
        "decrease",
    ]

    return any(indicator in summary.lower() for indicator in fact_indicators)


# === WORKFLOW STEPS ===
research_step = Step(
    name="research",
    description="Research the topic",
    agent=researcher,
)

summarize_step = Step(
    name="summarize",
    description="Summarize research findings",
    agent=summarizer,
)

# Conditional fact-checking step
fact_check_step = Step(
    name="fact_check",
    description="Verify facts and claims",
    agent=fact_checker,
)

write_article = Step(
    name="write_article",
    description="Write final article",
    agent=writer,
)

# === BASIC LINEAR WORKFLOW ===
basic_workflow = Workflow(
    name="Basic Linear Workflow",
    description="Research -> Summarize -> Condition(Fact Check) -> Write Article",
    steps=[
        research_step,
        summarize_step,
        Condition(
            name="fact_check_condition",
            description="Check if fact-checking is needed",
            evaluator=needs_fact_checking,
            steps=[fact_check_step],
        ),
        write_article,
    ],
)

if __name__ == "__main__":
    print(" Running Basic Linear Workflow Example")
    print("=" * 50)

    try:
        asyncio.run(
            basic_workflow.aprint_response(
                input="Recent breakthroughs in quantum computing",
                stream=True,
                stream_intermediate_steps=True,
            )
        )
    except Exception as e:
        print(f" Error: {e}")
        import traceback

        traceback.print_exc()
```

---

<a name="workflows--_02_workflows_conditional_execution--async--condition_with_list_of_stepspy"></a>

### `workflows/_02_workflows_conditional_execution/async/condition_with_list_of_steps.py`

```python
import asyncio

from agno.agent.agent import Agent
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# Additional agents for multi-step condition
trend_analyzer_agent = Agent(
    name="Trend Analyzer",
    instructions="Analyze trends and patterns from research data",
)

fact_checker_agent = Agent(
    name="Fact Checker",
    instructions="Verify facts and cross-reference information",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

# === MULTI-STEP CONDITION STEPS ===
deep_exa_analysis_step = Step(
    name="DeepExaAnalysis",
    description="Conduct deep analysis using Exa search capabilities",
    agent=exa_agent,
)

trend_analysis_step = Step(
    name="TrendAnalysis",
    description="Analyze trends and patterns from the research data",
    agent=trend_analyzer_agent,
)

fact_verification_step = Step(
    name="FactVerification",
    description="Verify facts and cross-reference information",
    agent=fact_checker_agent,
)

# === FINAL STEPS ===
write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_comprehensive_research_needed(step_input: StepInput) -> bool:
    """Check if comprehensive multi-step research is needed"""
    topic = step_input.input or step_input.previous_step_content or ""
    comprehensive_keywords = [
        "comprehensive",
        "detailed",
        "thorough",
        "in-depth",
        "complete analysis",
        "full report",
        "extensive research",
    ]
    return any(keyword in topic.lower() for keyword in comprehensive_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow with Multi-Step Condition",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],  # Single step
                ),
                Condition(
                    name="ComprehensiveResearchCondition",
                    description="Check if comprehensive multi-step research is needed",
                    evaluator=check_if_comprehensive_research_needed,
                    steps=[  # Multiple steps
                        deep_exa_analysis_step,
                        trend_analysis_step,
                        fact_verification_step,
                    ],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(
                input="Comprehensive analysis of climate change research",
            )
        )
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_02_workflows_conditional_execution--sync--condition_and_parallel_stepspy"></a>

### `workflows/_02_workflows_conditional_execution/sync/condition_and_parallel_steps.py`

```python
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.input or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.input or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.input or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        workflow.print_response(input="Latest AI developments in machine learning")
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_02_workflows_conditional_execution--sync--condition_and_parallel_steps_streampy"></a>

### `workflows/_02_workflows_conditional_execution/sync/condition_and_parallel_steps_stream.py`

```python
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_we_should_search_web(step_input: StepInput) -> bool:
    """Check if we should search the web"""
    topic = step_input.input or step_input.previous_step_content or ""
    general_keywords = ["news", "information", "research", "facts", "data"]
    return any(keyword in topic.lower() for keyword in general_keywords)


def check_if_we_should_search_x(step_input: StepInput) -> bool:
    """Check if we should search X/Twitter"""
    topic = step_input.input or step_input.previous_step_content or ""
    social_keywords = [
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "twitter",
        "x",
    ]
    return any(keyword in topic.lower() for keyword in social_keywords)


def check_if_we_should_search_exa(step_input: StepInput) -> bool:
    """Check if we should use Exa search"""
    topic = step_input.input or step_input.previous_step_content or ""
    advanced_keywords = ["deep", "academic", "research", "analysis", "comprehensive"]
    return any(keyword in topic.lower() for keyword in advanced_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],
                ),
                Condition(
                    name="WebSearchCondition",
                    description="Check if we should search the web for general information",
                    evaluator=check_if_we_should_search_web,
                    steps=[research_web_step],
                ),
                Condition(
                    name="ExaSearchCondition",
                    description="Check if we should use Exa for advanced search",
                    evaluator=check_if_we_should_search_exa,
                    steps=[research_exa_step],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        workflow.print_response(
            input="Latest AI developments in machine learning",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_02_workflows_conditional_execution--sync--condition_steps_workflow_streampy"></a>

### `workflows/_02_workflows_conditional_execution/sync/condition_steps_workflow_stream.py`

```python
from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.condition import Condition
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === BASIC AGENTS ===
researcher = Agent(
    name="Researcher",
    instructions="Research the given topic and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

summarizer = Agent(
    name="Summarizer",
    instructions="Create a clear summary of the research findings.",
)

fact_checker = Agent(
    name="Fact Checker",
    instructions="Verify facts and check for accuracy in the research.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Write a comprehensive article based on all available research and verification.",
)

# === CONDITION EVALUATOR ===


def needs_fact_checking(step_input: StepInput) -> bool:
    """Determine if the research contains claims that need fact-checking"""
    summary = step_input.previous_step_content or ""

    # Look for keywords that suggest factual claims
    fact_indicators = [
        "study shows",
        "research indicates",
        "according to",
        "statistics",
        "data shows",
        "survey",
        "report",
        "million",
        "billion",
        "percent",
        "%",
        "increase",
        "decrease",
    ]

    return any(indicator in summary.lower() for indicator in fact_indicators)


# === WORKFLOW STEPS ===
research_step = Step(
    name="research",
    description="Research the topic",
    agent=researcher,
)

summarize_step = Step(
    name="summarize",
    description="Summarize research findings",
    agent=summarizer,
)

# Conditional fact-checking step
fact_check_step = Step(
    name="fact_check",
    description="Verify facts and claims",
    agent=fact_checker,
)

write_article = Step(
    name="write_article",
    description="Write final article",
    agent=writer,
)

# === BASIC LINEAR WORKFLOW ===
basic_workflow = Workflow(
    name="Basic Linear Workflow",
    description="Research -> Summarize -> Condition(Fact Check) -> Write Article",
    steps=[
        research_step,
        summarize_step,
        Condition(
            name="fact_check_condition",
            description="Check if fact-checking is needed",
            evaluator=needs_fact_checking,
            steps=[fact_check_step],
        ),
        write_article,
    ],
)

if __name__ == "__main__":
    print(" Running Basic Linear Workflow Example")
    print("=" * 50)

    try:
        basic_workflow.print_response(
            input="Recent breakthroughs in quantum computing",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f" Error: {e}")
        import traceback

        traceback.print_exc()
```

---

<a name="workflows--_02_workflows_conditional_execution--sync--condition_with_list_of_stepspy"></a>

### `workflows/_02_workflows_conditional_execution/sync/condition_with_list_of_steps.py`

```python
from agno.agent.agent import Agent
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# Additional agents for multi-step condition
trend_analyzer_agent = Agent(
    name="Trend Analyzer",
    instructions="Analyze trends and patterns from research data",
)

fact_checker_agent = Agent(
    name="Fact Checker",
    instructions="Verify facts and cross-reference information",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

# === MULTI-STEP CONDITION STEPS ===
deep_exa_analysis_step = Step(
    name="DeepExaAnalysis",
    description="Conduct deep analysis using Exa search capabilities",
    agent=exa_agent,
)

trend_analysis_step = Step(
    name="TrendAnalysis",
    description="Analyze trends and patterns from the research data",
    agent=trend_analyzer_agent,
)

fact_verification_step = Step(
    name="FactVerification",
    description="Verify facts and cross-reference information",
    agent=fact_checker_agent,
)

# === FINAL STEPS ===
write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


def check_if_comprehensive_research_needed(step_input: StepInput) -> bool:
    """Check if comprehensive multi-step research is needed"""
    topic = step_input.input or step_input.previous_step_content or ""
    comprehensive_keywords = [
        "comprehensive",
        "detailed",
        "thorough",
        "in-depth",
        "complete analysis",
        "full report",
        "extensive research",
    ]
    return any(keyword in topic.lower() for keyword in comprehensive_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow with Multi-Step Condition",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],  # Single step
                ),
                Condition(
                    name="ComprehensiveResearchCondition",
                    description="Check if comprehensive multi-step research is needed",
                    evaluator=check_if_comprehensive_research_needed,
                    steps=[  # Multiple steps
                        deep_exa_analysis_step,
                        trend_analysis_step,
                        fact_verification_step,
                    ],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            write_step,
        ],
    )

    try:
        workflow.print_response(
            input="Comprehensive analysis of climate change research",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_03_workflows_loop_execution--async--loop_steps_workflowpy"></a>

### `workflows/_03_workflows_loop_execution/async/loop_steps_workflow.py`

```python
import asyncio
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Step, Workflow
from agno.workflow.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Simple check - if any output contains substantial content, we're good
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f" Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            input="Research the latest trends in AI and machine learning, then create a summary",
        )
    )
```

---

<a name="workflows--_03_workflows_loop_execution--async--loop_steps_workflow_streampy"></a>

### `workflows/_03_workflows_loop_execution/async/loop_steps_workflow_stream.py`

```python
import asyncio
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Step, Workflow
from agno.workflow.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Simple check - if any output contains substantial content, we're good
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f" Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            input="Research the latest trends in AI and machine learning, then create a summary",
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="workflows--_03_workflows_loop_execution--async--loop_with_parallel_steps_streampy"></a>

### `workflows/_03_workflows_loop_execution/async/loop_with_parallel_steps_stream.py`

```python
import asyncio
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Parallel, Step, Workflow
from agno.workflow.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

analysis_agent = Agent(
    name="Analysis Agent",
    role="Data analyst",
    instructions="You are a data analyst. Analyze and summarize research findings.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

# Create analysis steps
trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

    # Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f" Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

    print(
        f" Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False


# Create workflow with loop containing parallel steps
workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            input="Research the latest trends in AI and machine learning, then create a summary",
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="workflows--_03_workflows_loop_execution--sync--loop_steps_workflowpy"></a>

### `workflows/_03_workflows_loop_execution/sync/loop_steps_workflow.py`

```python
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Step, Workflow
from agno.workflow.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if any outputs are present
    if not outputs:
        return False

    # Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f" Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    # Test the workflow
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
    )
```

---

<a name="workflows--_03_workflows_loop_execution--sync--loop_steps_workflow_streampy"></a>

### `workflows/_03_workflows_loop_execution/sync/loop_steps_workflow_stream.py`

```python
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Step, Workflow
from agno.workflow.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

# End condition function


def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Simple check - if any output contains substantial content, we're good
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f" Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop
workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_03_workflows_loop_execution--sync--loop_with_parallel_stepspy"></a>

### `workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps.py`

```python
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Parallel, Step, Workflow
from agno.workflow.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

analysis_agent = Agent(
    name="Analysis Agent",
    role="Data analyst",
    instructions="You are a data analyst. Analyze and summarize research findings.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

# Create analysis steps
trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

    # Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f" Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

    print(
        f" Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False


# Create workflow with loop containing parallel steps
workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
    )
```

---

<a name="workflows--_03_workflows_loop_execution--sync--loop_with_parallel_steps_streampy"></a>

### `workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps_stream.py`

```python
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Parallel, Step, Workflow
from agno.workflow.types import StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

analysis_agent = Agent(
    name="Analysis Agent",
    role="Data analyst",
    instructions="You are a data analyst. Analyze and summarize research findings.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

# Create analysis steps
trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)


# End condition function
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

    # Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

    # Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f" Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

    print(
        f" Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False


# Create workflow with loop containing parallel steps
workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_04_workflows_parallel_execution--async--parallel_and_condition_steps_streampy"></a>

### `workflows/_04_workflows_parallel_execution/async/parallel_and_condition_steps_stream.py`

```python
import asyncio

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def should_conduct_research(step_input: StepInput) -> bool:
    """Check if we should conduct comprehensive research"""
    topic = step_input.input or step_input.previous_step_content or ""

    # Keywords that indicate research is needed
    research_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
        "news",
        "information",
        "research",
        "facts",
        "data",
        "analysis",
        "comprehensive",
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "developments",
    ]

    # If the topic contains any research-worthy keywords, conduct research
    return any(keyword in topic.lower() for keyword in research_keywords)


def is_tech_related(step_input: StepInput) -> bool:
    """Check if the topic is tech-related for additional tech research"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Research Workflow",
        description="Conditionally execute parallel research based on topic relevance",
        steps=[
            # Main research condition - if topic needs research, run parallel research steps
            Condition(
                name="ResearchCondition",
                description="Check if comprehensive research is needed for this topic",
                evaluator=should_conduct_research,
                steps=[
                    Parallel(
                        research_hackernews_step,
                        research_web_step,
                        name="ComprehensiveResearch",
                        description="Run multiple research sources in parallel",
                    ),
                    research_exa_step,
                ],
            ),
            # # Additional tech-specific research if needed
            Condition(
                name="TechResearchCondition",
                description="Additional tech-focused research if topic is tech-related",
                evaluator=is_tech_related,
                steps=[
                    Step(
                        name="TechAnalysis",
                        description="Deep dive tech analysis and trend identification",
                        agent=content_agent,
                    ),
                ],
            ),
            # Content preparation and writing
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        asyncio.run(
            workflow.aprint_response(
                input="Latest AI developments in machine learning",
                stream=True,
                stream_intermediate_steps=True,
            )
        )
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_04_workflows_parallel_execution--async--parallel_steps_workflowpy"></a>

### `workflows/_04_workflows_parallel_execution/async/parallel_steps_workflow.py`

```python
import asyncio

from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.workflow.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

if __name__ == "__main__":
    asyncio.run(workflow.aprint_response("Write about the latest AI developments"))
```

---

<a name="workflows--_04_workflows_parallel_execution--async--parallel_steps_workflow_streampy"></a>

### `workflows/_04_workflows_parallel_execution/async/parallel_steps_workflow_stream.py`

```python
import asyncio

from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.workflow.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

asyncio.run(
    workflow.aprint_response(
        "Write about the latest AI developments",
        stream=True,
        stream_intermediate_steps=True,
    )
)
```

---

<a name="workflows--_04_workflows_parallel_execution--sync--parallel_and_condition_steps_streampy"></a>

### `workflows/_04_workflows_parallel_execution/sync/parallel_and_condition_steps_stream.py`

```python
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# === AGENTS ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="Research tech news and trends from Hacker News",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="Research general information from the web",
    tools=[DuckDuckGoTools()],
)

exa_agent = Agent(
    name="Exa Search Researcher",
    instructions="Research using Exa advanced search capabilities",
    tools=[ExaTools()],
)

content_agent = Agent(
    name="Content Creator",
    instructions="Create well-structured content from research data",
)

# === RESEARCH STEPS ===
research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_web_step = Step(
    name="ResearchWeb",
    description="Research general information from web",
    agent=web_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

prepare_input_for_write_step = Step(
    name="PrepareInput",
    description="Prepare and organize research data for writing",
    agent=content_agent,
)

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)


# === CONDITION EVALUATORS ===
def should_conduct_research(step_input: StepInput) -> bool:
    """Check if we should conduct comprehensive research"""
    topic = step_input.input or step_input.previous_step_content or ""

    # Keywords that indicate research is needed
    research_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
        "news",
        "information",
        "research",
        "facts",
        "data",
        "analysis",
        "comprehensive",
        "trending",
        "viral",
        "social",
        "discussion",
        "opinion",
        "developments",
    ]

    # If the topic contains any research-worthy keywords, conduct research
    return any(keyword in topic.lower() for keyword in research_keywords)


def is_tech_related(step_input: StepInput) -> bool:
    """Check if the topic is tech-related for additional tech research"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)


if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Research Workflow",
        description="Conditionally execute parallel research based on topic relevance",
        steps=[
            # Main research condition - if topic needs research, run parallel research steps
            Condition(
                name="ResearchCondition",
                description="Check if comprehensive research is needed for this topic",
                evaluator=should_conduct_research,
                steps=[
                    Parallel(
                        research_hackernews_step,
                        research_web_step,
                        name="ComprehensiveResearch",
                        description="Run multiple research sources in parallel",
                    ),
                    research_exa_step,
                ],
            ),
            # # Additional tech-specific research if needed
            Condition(
                name="TechResearchCondition",
                description="Additional tech-focused research if topic is tech-related",
                evaluator=is_tech_related,
                steps=[
                    Step(
                        name="TechAnalysis",
                        description="Deep dive tech analysis and trend identification",
                        agent=content_agent,
                    ),
                ],
            ),
            # Content preparation and writing
            prepare_input_for_write_step,
            write_step,
        ],
    )

    try:
        workflow.print_response(
            input="Latest AI developments in machine learning",
            stream=True,
            stream_intermediate_steps=True,
        )
    except Exception as e:
        print(f" Error: {e}")
    print()
```

---

<a name="workflows--_04_workflows_parallel_execution--sync--parallel_steps_workflowpy"></a>

### `workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow.py`

```python
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.workflow.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

workflow.print_response("Write about the latest AI developments")
```

---

<a name="workflows--_04_workflows_parallel_execution--sync--parallel_steps_workflow_streampy"></a>

### `workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow_stream.py`

```python
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.workflow.parallel import Parallel

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with direct execution
workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

workflow.print_response(
    "Write about the latest AI developments",
    stream=True,
    stream_intermediate_steps=True,
)
```

---

<a name="workflows--_05_workflows_conditional_branching--async--router_steps_workflowpy"></a>

### `workflows/_05_workflows_conditional_branching/async/router_steps_workflow.py`

```python
import asyncio
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f" Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f" General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            "Latest developments in artificial intelligence and machine learning"
        )
    )
```

---

<a name="workflows--_05_workflows_conditional_branching--async--router_steps_workflow_streampy"></a>

### `workflows/_05_workflows_conditional_branching/async/router_steps_workflow_stream.py`

```python
import asyncio
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f" Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f" General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    asyncio.run(
        workflow.aprint_response(
            "Latest developments in artificial intelligence and machine learning",
            stream=True,
            stream_intermediate_steps=True,
        )
    )
```

---

<a name="workflows--_05_workflows_conditional_branching--async--router_with_loop_stepspy"></a>

### `workflows/_05_workflows_conditional_branching/async/router_with_loop_steps.py`

```python
import asyncio
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.loop import Loop
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)

# End condition function for the loop


def research_quality_check(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    if not outputs:
        return False

    # Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 300:
            print(
                f" Research quality check passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research quality check failed - need more substantial research")
    return False


# Create a Loop step for deep tech research
deep_tech_research_loop = Loop(
    name="Deep Tech Research Loop",
    steps=[research_hackernews],
    end_condition=research_quality_check,
    max_iterations=3,
    description="Perform iterative deep research on tech topics",
)

# Router function that selects between simple web research or deep tech research loop


def research_strategy_router(step_input: StepInput) -> List[Step]:
    """
    Decide between simple web research or deep tech research loop based on the input topic.
    Returns either a single web research step or a tech research loop.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic requires deep tech research
    deep_tech_keywords = [
        "startup trends",
        "ai developments",
        "machine learning research",
        "programming languages",
        "developer tools",
        "silicon valley",
        "venture capital",
        "cryptocurrency analysis",
        "blockchain technology",
        "open source projects",
        "github trends",
        "tech industry",
        "software engineering",
    ]

    # Check if it's a complex tech topic that needs deep research
    if any(keyword in topic for keyword in deep_tech_keywords) or (
        "tech" in topic and len(topic.split()) > 3
    ):
        print(
            f" Deep tech topic detected: Using iterative research loop for '{topic}'"
        )
        return [deep_tech_research_loop]
    else:
        print(f" Simple topic detected: Using basic web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Adaptive Research Workflow",
    description="Intelligently selects between simple web research or deep iterative tech research based on topic complexity",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_strategy_router,
            choices=[research_web, deep_tech_research_loop],
            description="Chooses between simple web research or deep tech research loop",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    print("=== Testing with deep tech topic ===")
    asyncio.run(
        workflow.aprint_response(
            "Latest developments in artificial intelligence and machine learning and deep tech research trends"
        )
    )
```

---

<a name="workflows--_05_workflows_conditional_branching--async--selector_for_image_video_generation_pipelinepy"></a>

### `workflows/_05_workflows_conditional_branching/async/selector_for_image_video_generation_pipeline.py`

```python
import asyncio
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.tools.openai import OpenAITools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel


# Define the structured message data
class MediaRequest(BaseModel):
    topic: str
    content_type: str  # "image" or "video"
    prompt: str
    style: Optional[str] = "realistic"
    duration: Optional[int] = None  # For video, duration in seconds
    resolution: Optional[str] = "1024x1024"  # For image resolution


# Define specialized agents for different media types
image_generator = Agent(
    name="Image Generator",
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    instructions="""You are an expert image generation specialist.
    When users request image creation, you should ACTUALLY GENERATE the image using your available image generation tools.

    Always use the generate_image tool to create the requested image based on the user's specifications.
    Include detailed, creative prompts that incorporate style, composition, lighting, and mood details.

    After generating the image, provide a brief description of what you created.""",
)

image_describer = Agent(
    name="Image Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert image analyst and describer.
    When you receive an image (either as input or from a previous step), analyze and describe it in vivid detail, including:
    - Visual elements and composition
    - Colors, lighting, and mood
    - Artistic style and technique
    - Emotional impact and narrative

    If no image is provided, work with the image description or prompt from the previous step.
    Provide rich, engaging descriptions that capture the essence of the visual content.""",
)

video_generator = Agent(
    name="Video Generator",
    model=OpenAIChat(id="gpt-4o"),
    # Video Generation only works on VertexAI mode
    tools=[GeminiTools(vertexai=True)],
    instructions="""You are an expert video production specialist.
    Create detailed video generation prompts and storyboards based on user requests.
    Include scene descriptions, camera movements, transitions, and timing.
    Consider pacing, visual storytelling, and technical aspects like resolution and duration.
    Format your response as a comprehensive video production plan.""",
)

video_describer = Agent(
    name="Video Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert video analyst and critic.
    Analyze and describe videos comprehensively, including:
    - Scene composition and cinematography
    - Narrative flow and pacing
    - Visual effects and production quality
    - Audio-visual harmony and mood
    - Technical execution and artistic merit
    Provide detailed, professional video analysis.""",
)

# Define steps for image pipeline
generate_image_step = Step(
    name="generate_image",
    agent=image_generator,
    description="Generate a detailed image creation prompt based on the user's request",
)

describe_image_step = Step(
    name="describe_image",
    agent=image_describer,
    description="Analyze and describe the generated image concept in vivid detail",
)

# Define steps for video pipeline
generate_video_step = Step(
    name="generate_video",
    agent=video_generator,
    description="Create a comprehensive video production plan and storyboard",
)

describe_video_step = Step(
    name="describe_video",
    agent=video_describer,
    description="Analyze and critique the video production plan with professional insights",
)

# Define the two distinct pipelines
image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[generate_image_step, describe_image_step],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[generate_video_step, describe_video_step],
)


def media_sequence_selector(step_input: StepInput) -> List[Step]:
    """
    Simple pipeline selector based on keywords in the message.

    Args:
        step_input: StepInput containing message

    Returns:
        List of Steps to execute
    """

    # Check if message exists and is a string
    if not step_input.input or not isinstance(step_input.input, str):
        return [image_sequence]  # Default to image sequence

    # Convert message to lowercase for case-insensitive matching
    message_lower = step_input.input.lower()

    # Check for video keywords
    if "video" in message_lower:
        return [video_sequence]
    # Check for image keywords
    elif "image" in message_lower:
        return [image_sequence]
    else:
        # Default to image for any other case
        return [image_sequence]


# Usage examples
if __name__ == "__main__":
    # Create the media generation workflow
    media_workflow = Workflow(
        name="AI Media Generation Workflow",
        description="Generate and analyze images or videos using AI agents",
        steps=[
            Router(
                name="Media Type Router",
                description="Routes to appropriate media generation pipeline based on content type",
                selector=media_sequence_selector,
                choices=[image_sequence, video_sequence],
            )
        ],
    )

    print("=== Example 1: Image Generation (using message_data) ===")
    image_request = MediaRequest(
        topic="Create an image of magical forest for a movie scene",
        content_type="image",
        prompt="A mystical forest with glowing mushrooms",
        style="fantasy art",
        resolution="1920x1080",
    )

    asyncio.run(
        media_workflow.aprint_response(
            input="Create an image of magical forest for a movie scene",
            markdown=True,
        )
    )

    # print("\n=== Example 2: Video Generation (using message_data) ===")
    # video_request = MediaRequest(
    #     topic="Create a cinematic video city timelapse",
    #     content_type="video",
    #     prompt="A time-lapse of a city skyline from day to night",
    #     style="cinematic",
    #     duration=30,
    #     resolution="4K"
    # )

    # asyncio.run(media_workflow.aprint_response(
    #     input="Create a cinematic video city timelapse",
    #     markdown=True,
    # ))
```

---

<a name="workflows--_05_workflows_conditional_branching--sync--router_steps_workflowpy"></a>

### `workflows/_05_workflows_conditional_branching/sync/router_steps_workflow.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f" Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f" General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning"
    )
```

---

<a name="workflows--_05_workflows_conditional_branching--sync--router_steps_workflow_streampy"></a>

### `workflows/_05_workflows_conditional_branching/sync/router_steps_workflow_stream.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Now returns Step(s) to execute
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f" Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f" General topic detected: Using web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_05_workflows_conditional_branching--sync--router_with_loop_stepspy"></a>

### `workflows/_05_workflows_conditional_branching/sync/router_with_loop_steps.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.loop import Loop
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)

# End condition function for the loop


def research_quality_check(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    if not outputs:
        return False

    # Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 300:
            print(
                f" Research quality check passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research quality check failed - need more substantial research")
    return False


# Create a Loop step for deep tech research
deep_tech_research_loop = Loop(
    name="Deep Tech Research Loop",
    steps=[research_hackernews],
    end_condition=research_quality_check,
    max_iterations=3,
    description="Perform iterative deep research on tech topics",
)

# Router function that selects between simple web research or deep tech research loop


def research_strategy_router(step_input: StepInput) -> List[Step]:
    """
    Decide between simple web research or deep tech research loop based on the input topic.
    Returns either a single web research step or a tech research loop.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic requires deep tech research
    deep_tech_keywords = [
        "startup trends",
        "ai developments",
        "machine learning research",
        "programming languages",
        "developer tools",
        "silicon valley",
        "venture capital",
        "cryptocurrency analysis",
        "blockchain technology",
        "open source projects",
        "github trends",
        "tech industry",
        "software engineering",
    ]

    # Check if it's a complex tech topic that needs deep research
    if any(keyword in topic for keyword in deep_tech_keywords) or (
        "tech" in topic and len(topic.split()) > 3
    ):
        print(
            f" Deep tech topic detected: Using iterative research loop for '{topic}'"
        )
        return [deep_tech_research_loop]
    else:
        print(f" Simple topic detected: Using basic web research for '{topic}'")
        return [research_web]


workflow = Workflow(
    name="Adaptive Research Workflow",
    description="Intelligently selects between simple web research or deep iterative tech research based on topic complexity",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_strategy_router,
            choices=[research_web, deep_tech_research_loop],
            description="Chooses between simple web research or deep tech research loop",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    print("=== Testing with deep tech topic ===")
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning and deep tech research trends"
    )
```

---

<a name="workflows--_05_workflows_conditional_branching--sync--selector_for_image_video_generation_pipelinespy"></a>

### `workflows/_05_workflows_conditional_branching/sync/selector_for_image_video_generation_pipelines.py`

```python
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.tools.openai import OpenAITools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel


# Define the structured message data
class MediaRequest(BaseModel):
    topic: str
    content_type: str  # "image" or "video"
    prompt: str
    style: Optional[str] = "realistic"
    duration: Optional[int] = None  # For video, duration in seconds
    resolution: Optional[str] = "1024x1024"  # For image resolution


# Define specialized agents for different media types
image_generator = Agent(
    name="Image Generator",
    model=OpenAIChat(id="gpt-4o"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    instructions="""You are an expert image generation specialist.
    When users request image creation, you should ACTUALLY GENERATE the image using your available image generation tools.

    Always use the generate_image tool to create the requested image based on the user's specifications.
    Include detailed, creative prompts that incorporate style, composition, lighting, and mood details.

    After generating the image, provide a brief description of what you created.""",
)

image_describer = Agent(
    name="Image Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert image analyst and describer.
    When you receive an image (either as input or from a previous step), analyze and describe it in vivid detail, including:
    - Visual elements and composition
    - Colors, lighting, and mood
    - Artistic style and technique
    - Emotional impact and narrative

    If no image is provided, work with the image description or prompt from the previous step.
    Provide rich, engaging descriptions that capture the essence of the visual content.""",
)

video_generator = Agent(
    name="Video Generator",
    model=OpenAIChat(id="gpt-4o"),
    # Video Generation only works on VertexAI mode
    tools=[GeminiTools(vertexai=True)],
    instructions="""You are an expert video production specialist.
    Create detailed video generation prompts and storyboards based on user requests.
    Include scene descriptions, camera movements, transitions, and timing.
    Consider pacing, visual storytelling, and technical aspects like resolution and duration.
    Format your response as a comprehensive video production plan.""",
)

video_describer = Agent(
    name="Video Describer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="""You are an expert video analyst and critic.
    Analyze and describe videos comprehensively, including:
    - Scene composition and cinematography
    - Narrative flow and pacing
    - Visual effects and production quality
    - Audio-visual harmony and mood
    - Technical execution and artistic merit
    Provide detailed, professional video analysis.""",
)

# Define steps for image pipeline
generate_image_step = Step(
    name="generate_image",
    agent=image_generator,
    description="Generate a detailed image creation prompt based on the user's request",
)

describe_image_step = Step(
    name="describe_image",
    agent=image_describer,
    description="Analyze and describe the generated image concept in vivid detail",
)

# Define steps for video pipeline
generate_video_step = Step(
    name="generate_video",
    agent=video_generator,
    description="Create a comprehensive video production plan and storyboard",
)

describe_video_step = Step(
    name="describe_video",
    agent=video_describer,
    description="Analyze and critique the video production plan with professional insights",
)

# Define the two distinct pipelines
image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[generate_image_step, describe_image_step],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[generate_video_step, describe_video_step],
)


def media_sequence_selector(step_input: StepInput) -> List[Step]:
    """
    Simple pipeline selector based on keywords in the message.

    Args:
        step_input: StepInput containing message

    Returns:
        List of Steps to execute
    """

    # Check if message exists and is a string
    if not step_input.input or not isinstance(step_input.input, str):
        return [image_sequence]  # Default to image sequence

    # Convert message to lowercase for case-insensitive matching
    message_lower = step_input.input.lower()

    # Check for video keywords
    if "video" in message_lower:
        return [video_sequence]
    # Check for image keywords
    elif "image" in message_lower:
        return [image_sequence]
    else:
        # Default to image for any other case
        return [image_sequence]


# Usage examples
if __name__ == "__main__":
    # Create the media generation workflow
    media_workflow = Workflow(
        name="AI Media Generation Workflow",
        description="Generate and analyze images or videos using AI agents",
        steps=[
            Router(
                name="Media Type Router",
                description="Routes to appropriate media generation pipeline based on content type",
                selector=media_sequence_selector,
                choices=[image_sequence, video_sequence],
            )
        ],
    )

    print("=== Example 1: Image Generation (using message_data) ===")
    image_request = MediaRequest(
        topic="Create an image of magical forest for a movie scene",
        content_type="image",
        prompt="A mystical forest with glowing mushrooms",
        style="fantasy art",
        resolution="1920x1080",
    )

    media_workflow.print_response(
        input="Create an image of magical forest for a movie scene",
        markdown=True,
    )

    # print("\n=== Example 2: Video Generation (using message_data) ===")
    # video_request = MediaRequest(
    #     topic="Create a cinematic video city timelapse",
    #     content_type="video",
    #     prompt="A time-lapse of a city skyline from day to night",
    #     style="cinematic",
    #     duration=30,
    #     resolution="4K"
    # )

    # media_workflow.print_response(
    #     input="Create a cinematic video city timelapse",
    #     markdown=True,
    # )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--pydantic_model_as_inputpy"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/pydantic_model_as_input.py`

```python
from typing import List

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
    )

    print("=== Example: Research with Structured Topic ===")
    research_topic = ResearchTopic(
        topic="AI trends in 2024",
        focus_areas=[
            "Machine Learning",
            "Natural Language Processing",
            "Computer Vision",
            "AI Ethics",
        ],
        target_audience="Tech professionals and business leaders",
        sources_required=8,
    )
    content_creation_workflow.print_response(
        input=research_topic,
        markdown=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_agentpy"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_agent.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow
structured_workflow = Workflow(
    name="Structured Content Creation Pipeline",
    description="AI-powered content creation with structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Steps ===")

    # Test with simple string input
    structured_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning"
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_agent_streampy"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_agent_stream.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow
structured_workflow = Workflow(
    name="Structured Content Creation Pipeline",
    description="AI-powered content creation with structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Steps ===")

    # Test with simple string input
    structured_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_functionpy"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


def data_analysis_function(step_input: StepInput) -> StepOutput:
    """
    Custom function to analyze the structured data received from previous step
    This will help us see what format the data is in when received
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    print("\n" + "=" * 60)
    print(" CUSTOM FUNCTION DATA ANALYSIS")
    print("=" * 60)

    print(f"\n Input Message Type: {type(message)}")
    print(f" Input Message Value: {message}")

    print(f"\n Previous Step Content Type: {type(previous_step_content)}")

    # Try to parse if it's structured data
    analysis_results = []

    if previous_step_content:
        print("\n Previous Step Content Preview:")
        print("Topic: ", previous_step_content.topic, "\n")
        print("Key Insights: ", previous_step_content.key_insights, "\n")
        print(
            "Trending Technologies: ", previous_step_content.trending_technologies, "\n"
        )

        analysis_results.append(" Received structured data (BaseModel)")

        # If it's a BaseModel, try to access its fields
        analysis_results.append(
            f" BaseModel type: {type(previous_step_content).__name__}"
        )
        try:
            model_dict = previous_step_content.model_dump()
            analysis_results.append(f" Model fields: {list(model_dict.keys())}")

            # If it's ResearchFindings, extract specific data
            if hasattr(previous_step_content, "topic"):
                analysis_results.append(
                    f" Research Topic: {previous_step_content.topic}"
                )
            if hasattr(previous_step_content, "confidence_score"):
                analysis_results.append(
                    f" Confidence Score: {previous_step_content.confidence_score}"
                )

        except Exception as e:
            analysis_results.append(f" Error accessing BaseModel: {e}")

    # Create enhanced analysis
    enhanced_analysis = f"""
        ## Data Flow Analysis Report

        **Input Analysis:**
        - Message Type: {type(message).__name__}
        - Previous Content Type: {type(previous_step_content).__name__}

        **Structure Analysis:**
        {chr(10).join(analysis_results)}

        **Recommendations for Next Step:**
        Based on the data analysis, the content planning step should receive this processed information.
    """.strip()

    print("\n Analysis Results:")
    for result in analysis_results:
        print(f"   {result}")

    print("=" * 60)

    return StepOutput(content=enhanced_analysis, success=True)


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

# Custom function step to analyze data flow
analysis_step = Step(
    name="data_analysis",
    executor=data_analysis_function,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow with custom function in between
structured_workflow = Workflow(
    name="Structured Content Creation Pipeline with Analysis",
    description="AI-powered content creation with data flow analysis",
    steps=[research_step, analysis_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow with Custom Function Analysis ===")

    # Test with simple string input
    structured_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning",
        # stream=True,
        # stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_function_1py"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function_1.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


def data_analysis_function(step_input: StepInput) -> StepOutput:
    """
    Custom function to analyze the structured data received from previous step
    This will help us see what format the data is in when received
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    print("\n" + "=" * 60)
    print(" CUSTOM FUNCTION DATA ANALYSIS")
    print("=" * 60)

    print(f"\n Input Message Type: {type(message)}")
    print(f" Input Message Value: {message}")

    print(f"\n Previous Step Content Type: {type(previous_step_content)}")

    # Try to parse if it's structured data
    analysis_results = []

    if previous_step_content:
        print("\n Previous Step Content Preview:")
        print("Topic: ", previous_step_content.topic, "\n")
        print("Key Insights: ", previous_step_content.key_insights, "\n")
        print(
            "Trending Technologies: ", previous_step_content.trending_technologies, "\n"
        )

        analysis_results.append(" Received structured data (BaseModel)")

        # If it's a BaseModel, try to access its fields
        analysis_results.append(
            f" BaseModel type: {type(previous_step_content).__name__}"
        )
        try:
            model_dict = previous_step_content.model_dump()
            analysis_results.append(f" Model fields: {list(model_dict.keys())}")

            # If it's ResearchFindings, extract specific data
            if hasattr(previous_step_content, "topic"):
                analysis_results.append(
                    f" Research Topic: {previous_step_content.topic}"
                )
            if hasattr(previous_step_content, "confidence_score"):
                analysis_results.append(
                    f" Confidence Score: {previous_step_content.confidence_score}"
                )

        except Exception as e:
            analysis_results.append(f" Error accessing BaseModel: {e}")

    # Create enhanced analysis
    enhanced_analysis = f"""
        ## Data Flow Analysis Report

        **Input Analysis:**
        - Message Type: {type(message).__name__}
        - Previous Content Type: {type(previous_step_content).__name__}

        **Structure Analysis:**
        {chr(10).join(analysis_results)}

        **Recommendations for Next Step:**
        Based on the data analysis, the content planning step should receive this processed information.
    """.strip()

    print("\n Analysis Results:")
    for result in analysis_results:
        print(f"   {result}")

    print("=" * 60)

    return StepOutput(content=enhanced_analysis, success=True)


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

# Custom function step to analyze data flow
analysis_step = Step(
    name="data_analysis",
    executor=data_analysis_function,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow with custom function in between
structured_workflow = Workflow(
    name="Structured Content Creation Pipeline with Analysis",
    description="AI-powered content creation with data flow analysis",
    steps=[research_step, analysis_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow with Custom Function Analysis ===")

    # Test with simple string input
    structured_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning",
        # stream=True,
        # stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_function_2py"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function_2.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class AnalysisReport(BaseModel):
    """Enhanced analysis report with BaseModel output"""

    analysis_type: str = Field(description="Type of analysis performed")
    input_data_type: str = Field(description="Type of input data received")
    structured_data_detected: bool = Field(
        description="Whether structured data was found"
    )
    key_findings: List[str] = Field(description="Key findings from the analysis")
    recommendations: List[str] = Field(description="Recommendations for next steps")
    confidence_score: float = Field(
        description="Analysis confidence (0.0-1.0)", ge=0.0, le=1.0
    )
    data_quality_score: float = Field(
        description="Quality of input data (0.0-1.0)", ge=0.0, le=1.0
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


def enhanced_analysis_function(step_input: StepInput) -> StepOutput:
    """
    Enhanced custom function that returns a BaseModel instead of just a string.
    This demonstrates the new capability of StepOutput.content supporting structured data.
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    print("\n" + "=" * 60)
    print(" ENHANCED CUSTOM FUNCTION WITH STRUCTURED OUTPUT")
    print("=" * 60)

    print(f"\n Input Message Type: {type(message)}")
    print(f" Input Message Value: {message}")

    print(f"\n Previous Step Content Type: {type(previous_step_content)}")

    # Analysis results
    key_findings = []
    recommendations = []
    structured_data_detected = False
    confidence_score = 0.8
    data_quality_score = 0.9

    if previous_step_content:
        print("\n Previous Step Content Analysis:")

        if isinstance(previous_step_content, ResearchFindings):
            structured_data_detected = True
            print(" Detected ResearchFindings BaseModel")
            print(f"   Topic: {previous_step_content.topic}")
            print(
                f"   Key Insights: {len(previous_step_content.key_insights)} insights"
            )
            print(f"   Confidence: {previous_step_content.confidence_score}")

            # Extract findings from the structured data
            key_findings.extend(
                [
                    f"Research topic identified: {previous_step_content.topic}",
                    f"Found {len(previous_step_content.key_insights)} key insights",
                    f"Identified {len(previous_step_content.trending_technologies)} trending technologies",
                    f"Research confidence level: {previous_step_content.confidence_score}",
                    "Market impact assessment available",
                ]
            )

            recommendations.extend(
                [
                    "Leverage high-confidence research findings for content strategy",
                    "Focus on trending technologies identified in research",
                    "Use market impact insights for audience targeting",
                    "Build content around key insights with strong evidence",
                ]
            )

            confidence_score = previous_step_content.confidence_score
            data_quality_score = 0.95  # High quality due to structured input

        else:
            key_findings.append(
                "Received unstructured data - converted to string format"
            )
            recommendations.append(
                "Consider implementing structured data models for better processing"
            )
            confidence_score = 0.6
            data_quality_score = 0.7

    else:
        key_findings.append("No previous step content available")
        recommendations.append("Ensure data flow between steps is properly configured")
        confidence_score = 0.4
        data_quality_score = 0.5

    # Create structured analysis report using BaseModel
    analysis_report = AnalysisReport(
        analysis_type="Structured Data Flow Analysis",
        input_data_type=type(previous_step_content).__name__,
        structured_data_detected=structured_data_detected,
        key_findings=key_findings,
        recommendations=recommendations,
        confidence_score=confidence_score,
        data_quality_score=data_quality_score,
    )

    print("\n Analysis Results (BaseModel):")
    print(f"   Analysis Type: {analysis_report.analysis_type}")
    print(f"   Structured Data: {analysis_report.structured_data_detected}")
    print(f"   Confidence: {analysis_report.confidence_score}")
    print(f"   Data Quality: {analysis_report.data_quality_score}")
    print("=" * 60)

    # Return StepOutput with BaseModel content
    return StepOutput(content=analysis_report, success=True)


def simple_data_processor(step_input: StepInput) -> StepOutput:
    """
    Simple function that demonstrates accessing different content types
    """
    print("\n SIMPLE DATA PROCESSOR")
    print(f"Previous step content type: {type(step_input.previous_step_content)}")

    # Access the structured data directly
    if isinstance(step_input.previous_step_content, AnalysisReport):
        report = step_input.previous_step_content
        print(f"Processing analysis report with confidence: {report.confidence_score}")

        summary = {
            "processor": "simple_data_processor",
            "input_confidence": report.confidence_score,
            "input_quality": report.data_quality_score,
            "processed_findings": len(report.key_findings),
            "processed_recommendations": len(report.recommendations),
            "status": "processed_successfully",
        }

        return StepOutput(content=summary, success=True)
    else:
        return StepOutput(
            content="Unable to process - expected AnalysisReport", success=False
        )


# Define agents with response models
research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create content strategies based on research findings",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed content plans and calendars",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

# Define steps
research_step = Step(
    name="research_insights",
    agent=research_agent,
)

# Custom function step that returns a BaseModel
analysis_step = Step(
    name="enhanced_analysis",
    executor=enhanced_analysis_function,
)

# Another custom function that processes the BaseModel
processor_step = Step(
    name="data_processor",
    executor=simple_data_processor,
)

strategy_step = Step(
    name="content_strategy",
    agent=strategy_agent,
)

planning_step = Step(
    name="final_planning",
    agent=planning_agent,
)

# Create workflow with custom functions that demonstrate structured output
enhanced_workflow = Workflow(
    name="Enhanced Structured Content Creation Pipeline",
    description="AI-powered content creation with BaseModel outputs from custom functions",
    steps=[research_step, analysis_step, processor_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    # Test with simple string input
    enhanced_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_teampy"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_team.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Create individual agents for teams
research_specialist = Agent(
    name="Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Find and analyze the latest AI trends and developments",
    instructions=[
        "Search for recent AI developments using available tools",
        "Focus on breakthrough technologies and market trends",
        "Provide detailed analysis with credible sources",
    ],
)

data_analyst = Agent(
    name="Data Analyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Analyze research data and extract key insights",
    instructions=[
        "Process research findings to identify patterns",
        "Quantify market impact and confidence levels",
        "Structure insights for strategic planning",
    ],
)

content_strategist = Agent(
    name="Content Strategist",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Develop content strategies based on research insights",
    instructions=[
        "Create comprehensive content strategies",
        "Focus on audience targeting and engagement",
        "Recommend optimal posting schedules and content pillars",
    ],
)

marketing_expert = Agent(
    name="Marketing Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Provide marketing insights and hashtag recommendations",
    instructions=[
        "Suggest effective hashtags and engagement tactics",
        "Analyze target audience preferences",
        "Recommend proven marketing strategies",
    ],
)

project_manager = Agent(
    name="Project Manager",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed project plans and timelines",
    instructions=[
        "Develop comprehensive implementation plans",
        "Set realistic timelines and budget estimates",
        "Identify potential risks and mitigation strategies",
    ],
)

budget_analyst = Agent(
    name="Budget Analyst",
    model=OpenAIChat(id="gpt-4o"),
    role="Analyze costs and provide budget recommendations",
    instructions=[
        "Estimate project costs and resource requirements",
        "Provide budget ranges and cost optimization suggestions",
        "Consider ROI and success metrics",
    ],
)

# Create teams with structured outputs
research_team = Team(
    name="AI Research Team",
    members=[research_specialist, data_analyst],
    delegate_task_to_all_members=True,
    model=OpenAIChat(id="gpt-4o"),
    description="A collaborative team that researches AI trends and extracts structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Work together to research the given topic thoroughly",
        "Combine research findings with data analysis",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
    ],
)

strategy_team = Team(
    name="Content Strategy Team",
    members=[content_strategist, marketing_expert],
    delegate_task_to_all_members=True,
    model=OpenAIChat(id="gpt-4o"),
    description="A strategic team that creates comprehensive content strategies",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings from the previous step",
        "Collaborate to create a comprehensive content strategy",
        "Focus on audience engagement and brand building",
        "Combine content strategy with marketing expertise",
    ],
)

planning_team = Team(
    name="Content Planning Team",
    members=[project_manager, budget_analyst],
    delegate_task_to_all_members=True,
    model=OpenAIChat(id="gpt-4o"),
    description="A planning team that creates detailed implementation plans",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy to create a detailed implementation plan",
        "Combine project management with budget analysis",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
    ],
)

# Define steps using teams
research_step = Step(
    name="Research Insights",
    team=research_team,  # Using team instead of agent
)

strategy_step = Step(
    name="Content Strategy",
    team=strategy_team,  # Using team instead of agent
)

planning_step = Step(
    name="Final Planning",
    team=planning_team,  # Using team instead of agent
)

# Create workflow with teams
structured_workflow = Workflow(
    name="Team-Based Structured Content Creation Pipeline",
    description="AI-powered content creation with teams and structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Teams ===")

    structured_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning",
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--structured_io_at_each_level_team_streampy"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_team_stream.py`

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# Define structured models for each step
class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

    topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )


class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

    target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )


class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

    campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )


# Create individual agents for teams
research_specialist = Agent(
    name="Research Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Find and analyze the latest AI trends and developments",
    instructions=[
        "Search for recent AI developments using available tools",
        "Focus on breakthrough technologies and market trends",
        "Provide detailed analysis with credible sources",
    ],
)

data_analyst = Agent(
    name="Data Analyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Analyze research data and extract key insights",
    instructions=[
        "Process research findings to identify patterns",
        "Quantify market impact and confidence levels",
        "Structure insights for strategic planning",
    ],
)

content_strategist = Agent(
    name="Content Strategist",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Develop content strategies based on research insights",
    instructions=[
        "Create comprehensive content strategies",
        "Focus on audience targeting and engagement",
        "Recommend optimal posting schedules and content pillars",
    ],
)

marketing_expert = Agent(
    name="Marketing Expert",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Provide marketing insights and hashtag recommendations",
    instructions=[
        "Suggest effective hashtags and engagement tactics",
        "Analyze target audience preferences",
        "Recommend proven marketing strategies",
    ],
)

project_manager = Agent(
    name="Project Manager",
    model=OpenAIChat(id="gpt-4o"),
    role="Create detailed project plans and timelines",
    instructions=[
        "Develop comprehensive implementation plans",
        "Set realistic timelines and budget estimates",
        "Identify potential risks and mitigation strategies",
    ],
)

budget_analyst = Agent(
    name="Budget Analyst",
    model=OpenAIChat(id="gpt-4o"),
    role="Analyze costs and provide budget recommendations",
    instructions=[
        "Estimate project costs and resource requirements",
        "Provide budget ranges and cost optimization suggestions",
        "Consider ROI and success metrics",
    ],
)

# Create teams with structured outputs
research_team = Team(
    name="AI Research Team",
    members=[research_specialist, data_analyst],
    model=OpenAIChat(id="gpt-4o"),
    description="A collaborative team that researches AI trends and extracts structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Work together to research the given topic thoroughly",
        "Combine research findings with data analysis",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
    ],
)

strategy_team = Team(
    name="Content Strategy Team",
    members=[content_strategist, marketing_expert],
    model=OpenAIChat(id="gpt-4o"),
    description="A strategic team that creates comprehensive content strategies",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings from the previous step",
        "Collaborate to create a comprehensive content strategy",
        "Focus on audience engagement and brand building",
        "Combine content strategy with marketing expertise",
    ],
)

planning_team = Team(
    name="Content Planning Team",
    members=[project_manager, budget_analyst],
    delegate_task_to_all_members=True,
    model=OpenAIChat(id="gpt-4o"),
    description="A planning team that creates detailed implementation plans",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy to create a detailed implementation plan",
        "Combine project management with budget analysis",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
    ],
)

# Define steps using teams
research_step = Step(
    name="Research Insights",
    team=research_team,  # Using team instead of agent
)

strategy_step = Step(
    name="Content Strategy",
    team=strategy_team,  # Using team instead of agent
)

planning_step = Step(
    name="Final Planning",
    team=planning_team,  # Using team instead of agent
)

# Create workflow with teams
structured_workflow = Workflow(
    name="Team-Based Structured Content Creation Pipeline",
    description="AI-powered content creation with teams and structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Teams ===")

    structured_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_01_structured_io_at_each_level--workflow_with_input_schemapy"></a>

### `workflows/_06_advanced_concepts/_01_structured_io_at_each_level/workflow_with_input_schema.py`

```python
from typing import List

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


class DifferentModel(BaseModel):
    name: str


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
        input_schema=ResearchTopic,
    )

    print("=== Example: Research with Structured Topic ===")
    research_topic = ResearchTopic(
        topic="AI trends in 2024",
        focus_areas=[
            "Machine Learning",
            "Natural Language Processing",
            "Computer Vision",
            "AI Ethics",
        ],
        target_audience="Tech professionals and business leaders",
    )

    # 1. Should work properly, as its in sync with input schema
    content_creation_workflow.print_response(
        input=research_topic,
        markdown=True,
    )

    # 2. Should fail, as some fields present in input schema are missing here
    # content_creation_workflow.print_response(
    #     input=ResearchTopic(
    #         topic="AI trends in 2024",
    #         focus_areas=[
    #             "Machine Learning",
    #             "Natural Language Processing",
    #             "Computer Vision",
    #             "AI Ethics",
    #         ],
    #     ),
    #     markdown=True,
    # )

    # 3. Should fail, as its not in sync with input schema, diff pydantic model provided
    # content_creation_workflow.print_response(
    #     input=DifferentModel(name="test"),
    #     markdown=True,
    # )

    # 4. Pass a valid dict that matches ResearchTopic
    # content_creation_workflow.print_response(
    #     input={
    #         "topic": "AI trends in 2024",
    #         "focus_areas": ["Machine Learning", "Computer Vision"],
    #         "target_audience": "Tech professionals",
    #         "sources_required": 8
    #     },
    #     markdown=True,
    # )
```

---

<a name="workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_agentspy"></a>

### `workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_agents.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.workflow import Workflow
from agno.workflow.types import StepInput, StepOutput

# Create agents with more specific validation criteria
data_validator = Agent(
    name="Data Validator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a data validator. Analyze the provided data and determine if it's valid.",
        "For data to be VALID, it must meet these criteria:",
        "- user_count: Must be a positive number (> 0)",
        "- revenue: Must be a positive number (> 0)",
        "- date: Must be in a reasonable date format (YYYY-MM-DD)",
        "",
        "Return exactly 'VALID' if all criteria are met.",
        "Return exactly 'INVALID' if any criteria fail.",
        "Also briefly explain your reasoning.",
    ],
)

data_processor = Agent(
    name="Data Processor",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Process and transform the validated data.",
)

report_generator = Agent(
    name="Report Generator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Generate a final report from processed data.",
)


def early_exit_validator(step_input: StepInput) -> StepOutput:
    """
    Custom function that checks data quality and stops workflow early if invalid
    """
    # Get the validation result from previous step
    validation_result = step_input.previous_step_content or ""

    if "INVALID" in validation_result.upper():
        return StepOutput(
            content=" Data validation failed. Workflow stopped early to prevent processing invalid data.",
            stop=True,  # Stop the entire workflow here
        )
    else:
        return StepOutput(
            content=" Data validation passed. Continuing with processing...",
            stop=False,  # Continue normally
        )


# Create workflow with conditional early termination
workflow = Workflow(
    name="Data Processing with Early Exit",
    description="Process data but stop early if validation fails",
    steps=[
        data_validator,  # Step 1: Validate data
        early_exit_validator,  # Step 2: Check validation and possibly stop early
        data_processor,  # Step 3: Process data (only if validation passed)
        report_generator,  # Step 4: Generate report (only if processing completed)
    ],
)

if __name__ == "__main__":
    print("\n=== Testing with INVALID data ===")
    workflow.print_response(
        input="Process this data: {'user_count': -50, 'revenue': 'invalid_amount', 'date': 'bad_date'}"
    )

    print("=== Testing with VALID data ===")
    workflow.print_response(
        input="Process this data: {'user_count': 1000, 'revenue': 50000, 'date': '2024-01-15'}"
    )
```

---

<a name="workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_conditionpy"></a>

### `workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_condition.py`

```python
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow import Step, Workflow
from agno.workflow.condition import Condition
from agno.workflow.types import StepInput, StepOutput

# Create agents
researcher = Agent(
    name="Researcher",
    instructions="Research the given topic thoroughly and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Create engaging content based on research findings.",
)

reviewer = Agent(
    name="Reviewer",
    instructions="Review and improve the written content.",
)


# Custom compliance checker function
def compliance_checker(step_input: StepInput) -> StepOutput:
    """Compliance checker that can stop the condition if violations are found"""
    content = step_input.previous_step_content or ""

    # Simulate detecting compliance violations
    if "violation" in content.lower() or "illegal" in content.lower():
        return StepOutput(
            step_name="Compliance Checker",
            content=" COMPLIANCE VIOLATION DETECTED! Content contains material that violates company policies. Stopping content creation workflow immediately.",
            stop=True,  #  Request early termination from condition
        )
    else:
        return StepOutput(
            step_name="Compliance Checker",
            content=" Compliance check passed. Content meets all company policy requirements.",
            stop=False,
        )


# Custom quality assurance function
def quality_assurance(step_input: StepInput) -> StepOutput:
    """Quality assurance that runs after compliance check"""
    _ = step_input.previous_step_content or ""

    return StepOutput(
        step_name="Quality Assurance",
        content=" Quality assurance completed. Content meets quality standards and is ready for publication.",
        stop=False,
    )


# Condition evaluator function
def should_run_compliance_check(step_input: StepInput) -> bool:
    """Evaluate if compliance check should run based on content type"""
    content = step_input.input or ""

    # Run compliance check for sensitive topics
    sensitive_keywords = ["legal", "financial", "medical", "violation", "illegal"]
    return any(keyword in content.lower() for keyword in sensitive_keywords)


# Create workflow steps
research_step = Step(name="Research Content", agent=researcher)
compliance_check_step = Step(
    name="Compliance Check", executor=compliance_checker
)  #  Can stop workflow
quality_assurance_step = Step(name="Quality Assurance", executor=quality_assurance)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with conditional compliance checks
workflow = Workflow(
    name="Content Creation with Conditional Compliance",
    description="Creates content with conditional compliance checks that can stop the workflow",
    steps=[
        research_step,  # Always runs first
        Condition(
            name="Compliance and QA Gate",
            evaluator=should_run_compliance_check,  # Only runs for sensitive content
            steps=[
                compliance_check_step,  # Step 1: Compliance check (may stop here)
                quality_assurance_step,  # Step 2: QA (only if compliance passes)
            ],
        ),
        write_step,  # This should NOT execute if compliance check stops
        review_step,  # This should NOT execute if compliance check stops
    ],
)

if __name__ == "__main__":
    print("=== Testing Condition Early Termination with Compliance Check ===")
    print(
        "Expected: Compliance check should detect 'violation' and stop the entire workflow"
    )
    print(
        "Note: Condition will evaluate to True (sensitive content), then compliance check will stop"
    )
    print()

    workflow.print_response(
        input="Research legal violation cases and create content about illegal financial practices",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_looppy"></a>

### `workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_loop.py`

```python
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Step, Workflow
from agno.workflow.types import StepInput, StepOutput

# Create agents for research
research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)


# Custom function that will trigger early termination
def safety_checker(step_input: StepInput) -> StepOutput:
    """Safety checker that stops the loop if certain keywords are detected"""
    content = step_input.previous_step_content or ""

    # Simulate finding problematic content that requires stopping
    if (
        "AI" in content or "machine learning" in content
    ):  # Will trigger on our test message
        return StepOutput(
            step_name="Safety Checker",
            content=" SAFETY CONCERN DETECTED! Content contains sensitive AI-related information. Stopping research loop for review.",
            stop=True,  #  Request early termination
        )
    else:
        return StepOutput(
            step_name="Safety Checker",
            content=" Safety check passed. Content is safe to continue.",
            stop=False,
        )


# Create research steps
research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

safety_check_step = Step(
    name="Safety Check",
    executor=safety_checker,  #  Custom function that can stop the loop
    description="Check if research content is safe to continue",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)


# Normal end condition (keeps the original logic) + early termination check
def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient or if early termination was requested
    Returns True to break the loop, False to continue
    """
    if not outputs:
        print(" No research outputs - continuing loop")
        return False

    # Original logic: Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 200:
            print(
                f" Research evaluation passed - found substantial content ({len(output.content)} chars)"
            )
            return True

    print(" Research evaluation failed - need more substantial research")
    return False


# Create workflow with loop that includes safety checker
workflow = Workflow(
    name="Research with Safety Check Workflow",
    description="Research topics in loop with safety checks, stop if safety issues found",
    steps=[
        Loop(
            name="Research Loop with Safety",
            steps=[
                research_hackernews_step,  # Step 1: Research
                safety_check_step,  # Step 2: Safety check (may stop here)
                research_web_step,  # Step 3: More research (only if safety passes)
            ],
            end_condition=research_evaluator,
            max_iterations=3,
        ),
        content_agent,  # This should NOT execute if safety check stops the loop
    ],
)

if __name__ == "__main__":
    print("=== Testing Loop Early Termination with Safety Check ===")
    print("Expected: Safety check should detect 'AI' and stop the entire workflow")
    print()

    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_parallelpy"></a>

### `workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_parallel.py`

```python
from agno.agent import Agent
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.workflow.parallel import Parallel
from agno.workflow.types import StepInput, StepOutput

# Create agents
researcher = Agent(name="Researcher", tools=[HackerNewsTools(), GoogleSearchTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")


# Custom safety checker function that can stop the entire workflow
def content_safety_checker(step_input: StepInput) -> StepOutput:
    """Safety checker that runs in parallel and can stop the workflow"""
    content = step_input.input or ""

    # Simulate detecting unsafe content that requires immediate stopping
    if "unsafe" in content.lower() or "dangerous" in content.lower():
        return StepOutput(
            step_name="Safety Checker",
            content=" UNSAFE CONTENT DETECTED! Content contains dangerous material. Stopping entire workflow immediately for safety review.",
            stop=True,  #  Request early termination from parallel execution
        )
    else:
        return StepOutput(
            step_name="Safety Checker",
            content=" Content safety verification passed. Material is safe to proceed.",
            stop=False,
        )


# Custom quality checker function
def quality_checker(step_input: StepInput) -> StepOutput:
    """Quality checker that runs in parallel"""
    content = step_input.input or ""

    # Simulate quality check
    if len(content) < 10:
        return StepOutput(
            step_name="Quality Checker",
            content=" Quality check failed: Content too short for processing.",
            stop=False,
        )
    else:
        return StepOutput(
            step_name="Quality Checker",
            content=" Quality check passed. Content meets processing standards.",
            stop=False,
        )


# Create individual steps
research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
safety_check_step = Step(
    name="Safety Check", executor=content_safety_checker
)  #  Can stop workflow
quality_check_step = Step(name="Quality Check", executor=quality_checker)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

# Create workflow with parallel safety/quality checks
workflow = Workflow(
    name="Content Creation with Parallel Safety Checks",
    description="Creates content with parallel safety and quality checks that can stop the workflow",
    steps=[
        Parallel(
            research_hn_step,  # Research task 1
            research_web_step,  # Research task 2
            safety_check_step,  # Safety check (may stop here)
            quality_check_step,  # Quality check
            name="Research and Validation Phase",
        ),
        write_step,  # This should NOT execute if safety check stops
        review_step,  # This should NOT execute if safety check stops
    ],
)

if __name__ == "__main__":
    print("=== Testing Parallel Early Termination with Safety Check ===")
    print("Expected: Safety check should detect 'unsafe' and stop the entire workflow")
    print(
        "Note: All parallel steps run concurrently, but safety check will stop the workflow"
    )
    print()

    workflow.print_response(
        input="Write about unsafe and dangerous AI developments that could harm society",
    )
```

---

<a name="workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_routerpy"></a>

### `workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_router.py`

```python
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)


# Custom safety checker function
def content_safety_checker(step_input: StepInput) -> StepOutput:
    """Safety checker that can stop the router if inappropriate content is detected"""
    content = step_input.previous_step_content or ""

    # Simulate detecting inappropriate content that requires stopping
    if "controversial" in content.lower() or "sensitive" in content.lower():
        return StepOutput(
            step_name="Content Safety Checker",
            content=" CONTENT SAFETY VIOLATION! Research contains controversial or sensitive material. Stopping workflow for manual review.",
            stop=True,  #  Request early termination
        )
    else:
        return StepOutput(
            step_name="Content Safety Checker",
            content=" Content safety check passed. Material is appropriate for publication.",
            stop=False,
        )


# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

safety_check = Step(
    name="safety_check",
    executor=content_safety_checker,  #  Custom function that can stop the router
    description="Check if research content is safe for publication",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)


# Router function that returns multiple steps including safety check
def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute including safety check.
    """
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

    # Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

    if any(keyword in topic for keyword in tech_keywords):
        print(f" Tech topic detected: Using HackerNews research for '{topic}'")
        return [
            research_hackernews,  # Step 1: Research
            safety_check,  # Step 2: Safety check (may stop here)
            research_web,  # Step 3: Additional research (only if safety passes)
        ]
    else:
        print(f" General topic detected: Using web research for '{topic}'")
        return [
            research_web,  # Step 1: Research
            safety_check,  # Step 2: Safety check (may stop here)
        ]


workflow = Workflow(
    name="Research with Safety Router Workflow",
    description="Intelligently routes research methods with safety checks that can stop the workflow",
    steps=[
        Router(
            name="research_safety_router",
            selector=research_router,
            choices=[
                research_hackernews,
                safety_check,
                research_web,
            ],  # Available choices
            description="Intelligently selects research method with safety checks",
        ),
        publish_content,  # This should NOT execute if safety check stops the router
    ],
)

if __name__ == "__main__":
    print("=== Testing Router Early Termination with Safety Check ===")
    print(
        "Expected: Safety check should detect 'controversial' and stop the entire workflow"
    )
    print()

    workflow.print_response(
        input="Research the latest controversial trends in AI and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_steppy"></a>

### `workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_step.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.workflow import Step, Workflow
from agno.workflow.types import StepInput, StepOutput

# Create agents
security_scanner = Agent(
    name="Security Scanner",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a security scanner. Analyze the provided code or system for security vulnerabilities.",
        "Return 'SECURE' if no critical vulnerabilities found.",
        "Return 'VULNERABLE' if critical security issues are detected.",
        "Explain your findings briefly.",
    ],
)

code_deployer = Agent(
    name="Code Deployer",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Deploy the security-approved code to production environment.",
)

monitoring_agent = Agent(
    name="Monitoring Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Set up monitoring and alerts for the deployed application.",
)


def security_gate(step_input: StepInput) -> StepOutput:
    """
    Security gate that stops deployment if vulnerabilities found
    """
    security_result = step_input.previous_step_content or ""
    print(f" Security scan result: {security_result}")

    if "VULNERABLE" in security_result.upper():
        return StepOutput(
            content=" SECURITY ALERT: Critical vulnerabilities detected. Deployment blocked for security reasons.",
            stop=True,  # Stop the entire workflow to prevent insecure deployment
        )
    else:
        return StepOutput(
            content=" Security check passed. Proceeding with deployment...",
            stop=False,
        )


# Create deployment workflow with security gate
workflow = Workflow(
    name="Secure Deployment Pipeline",
    description="Deploy code only if security checks pass",
    steps=[
        Step(name="Security Scan", agent=security_scanner),
        Step(name="Security Gate", executor=security_gate),  # May stop here
        Step(name="Deploy Code", agent=code_deployer),  # Only if secure
        Step(name="Setup Monitoring", agent=monitoring_agent),  # Only if deployed
    ],
)

if __name__ == "__main__":
    print("\n=== Testing VULNERABLE code deployment ===")
    workflow.print_response(input="Scan this code: exec(input('Enter command: '))")

    print("=== Testing SECURE code deployment ===")
    workflow.print_response(input="Scan this code: def hello(): return 'Hello World'")
```

---

<a name="workflows--_06_advanced_concepts--_02_early_stopping--early_stop_workflow_with_stepspy"></a>

### `workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_steps.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Create agents
content_creator = Agent(
    name="Content Creator",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Create engaging content on the given topic. Research and write comprehensive articles.",
)

fact_checker = Agent(
    name="Fact Checker",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Verify facts and check accuracy of content. Flag any misinformation.",
)

editor = Agent(
    name="Editor",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Edit and polish content for publication. Ensure clarity and flow.",
)

publisher = Agent(
    name="Publisher",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Prepare content for publication and handle final formatting.",
)


# Custom content quality check function
def content_quality_gate(step_input: StepInput) -> StepOutput:
    """Quality gate that checks content and may stop the workflow"""
    content = step_input.previous_step_content or ""

    # Simulate quality check - stop if content is too short or mentions certain topics
    if len(content) < 100:
        return StepOutput(
            step_name="content_quality_gate",
            content=" QUALITY CHECK FAILED: Content too short. Stopping workflow.",
            stop=True,  #  Early termination
        )

    # Check for problematic content
    problematic_keywords = ["fake", "misinformation", "unverified", "conspiracy"]
    if any(keyword in content.lower() for keyword in problematic_keywords):
        return StepOutput(
            step_name="content_quality_gate",
            content=" QUALITY CHECK FAILED: Problematic content detected. Stopping workflow.",
            stop=True,  #  Early termination
        )

    return StepOutput(
        step_name="content_quality_gate",
        content=" QUALITY CHECK PASSED: Content meets quality standards.",
        stop=False,  # Continue workflow
    )


# Create Steps sequence with early termination
content_pipeline = Steps(
    name="content_pipeline",
    description="Content creation pipeline with quality gates",
    steps=[
        Step(name="create_content", agent=content_creator),
        Step(
            name="quality_gate", executor=content_quality_gate
        ),  #  Can stop workflow
        Step(name="fact_check", agent=fact_checker),  #  Won't execute if stopped
        Step(name="edit_content", agent=editor),  #  Won't execute if stopped
        Step(name="publish", agent=publisher),  #  Won't execute if stopped
    ],
)

# Create workflow
if __name__ == "__main__":
    workflow = Workflow(
        name="Content Creation with Quality Gate",
        description="Content creation workflow with early termination on quality issues",
        steps=[
            content_pipeline,
            Step(
                name="final_review", agent=editor
            ),  #  Won't execute if pipeline stopped
        ],
    )

    print("\n=== Test 2: Short content (should stop early) ===")
    workflow.print_response(
        input="Write a short note about conspiracy theories",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_03_access_previous_step_outputs--access_multiple_previous_step_output_stream_2py"></a>

### `workflows/_06_advanced_concepts/_03_access_previous_step_outputs/access_multiple_previous_step_output_stream_2.py`

```python
"""
This example shows how to access the output of multiple previous steps in a workflow.

The workflow is defined as a list of steps, where each step is directly an agent or a function.
We dont use Step objects in this example.
"""

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

reasoning_agent = Agent(
    instructions="You are an expert analyst who creates comprehensive reports by analyzing and synthesizing information from multiple sources. Create well-structured, insightful reports.",
)


# Custom function step that has access to ALL previous step outputs
def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

    # Access original workflow input
    original_topic = step_input.input or ""

    # Access specific step outputs by name
    hackernews_data = step_input.get_step_content("step_1") or ""
    web_data = step_input.get_step_content("step_2") or ""

    # Or access ALL previous content
    _ = step_input.get_all_previous_content()

    # Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

        ## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

        ## HackerNews Insights
        {hackernews_data[:500]}...

        ## Web Research Findings  
        {web_data[:500]}...
    """

    return StepOutput(content=report.strip(), success=True)


# Custom function to print the comprehensive report
def print_final_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that receives the comprehensive report and prints it.
    """

    # Get the output from the comprehensive_report step
    comprehensive_report = step_input.get_step_content("create_comprehensive_report")

    # Print the report
    print("=" * 80)
    print("FINAL COMPREHENSIVE REPORT")
    print("=" * 80)
    print(comprehensive_report)
    print("=" * 80)

    # Also print all previous step outputs for debugging
    print("\nDEBUG: All previous step outputs:")
    if step_input.previous_step_outputs:
        for step_name, output in step_input.previous_step_outputs.items():
            print(f"- {step_name}: {len(str(output.content))} characters")

    return StepOutput(
        step_name="print_final_report",
        content=f"Printed comprehensive report ({len(comprehensive_report)} characters)",
        success=True,
    )


# Final reasoning step using reasoning agent
reasoning_step = Step(
    name="final_reasoning",
    agent=reasoning_agent,
    description="Apply reasoning to create final insights and recommendations",
)

workflow = Workflow(
    name="Enhanced Research Workflow",
    description="Multi-source research with custom data flow and reasoning",
    steps=[
        hackernews_agent,
        web_agent,
        create_comprehensive_report,  # Has access to both previous steps
        print_final_report,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
    )
```

---

<a name="workflows--_06_advanced_concepts--_03_access_previous_step_outputs--access_multiple_previous_steps_output_streampy"></a>

### `workflows/_06_advanced_concepts/_03_access_previous_step_outputs/access_multiple_previous_steps_output_stream.py`

```python
from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

reasoning_agent = Agent(
    name="Reasoning Agent",
    instructions="You are an expert analyst who creates comprehensive reports by analyzing and synthesizing information from multiple sources. Create well-structured, insightful reports.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

# Custom function step that has access to ALL previous step outputs


def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

    # Access original workflow input
    original_topic = step_input.input or ""

    # Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

    # Or access ALL previous content
    _ = step_input.get_all_previous_content()

    # Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

        ## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

        ## HackerNews Insights
        {hackernews_data[:500]}...

        ## Web Research Findings  
        {web_data[:500]}...
    """

    return StepOutput(
        step_name="comprehensive_report", content=report.strip(), success=True
    )


comprehensive_report_step = Step(
    name="comprehensive_report",
    executor=create_comprehensive_report,
    description="Create comprehensive report from all research sources",
)

# Final reasoning step using reasoning agent
reasoning_step = Step(
    name="final_reasoning",
    agent=reasoning_agent,
    description="Apply reasoning to create final insights and recommendations",
)

workflow = Workflow(
    name="Enhanced Research Workflow",
    description="Multi-source research with custom data flow and reasoning",
    steps=[
        research_hackernews,
        research_web,
        comprehensive_report_step,  # Has access to both previous steps
        reasoning_step,  # Gets the last step output (comprehensive report)
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_03_access_previous_step_outputs--access_multiple_previous_steps_output_stream_1py"></a>

### `workflows/_06_advanced_concepts/_03_access_previous_step_outputs/access_multiple_previous_steps_output_stream_1.py`

```python
"""
This example shows how to access the output of multiple previous steps in a workflow.

The workflow is defined as a list of Step objects. Where each Step is an agent or a function.
"""

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define the research agents
hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

reasoning_agent = Agent(
    name="Reasoning Agent",
    instructions="You are an expert analyst who creates comprehensive reports by analyzing and synthesizing information from multiple sources. Create well-structured, insightful reports.",
)

# Create the research steps
research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

# Custom function step that has access to ALL previous step outputs


def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

    # Access original workflow input
    original_topic = step_input.input or ""

    # Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

    # Or access ALL previous content
    _ = step_input.get_all_previous_content()

    # Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

        ## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

        ## HackerNews Insights
        {hackernews_data[:500]}...

        ## Web Research Findings  
        {web_data[:500]}...
    """

    return StepOutput(
        step_name="comprehensive_report", content=report.strip(), success=True
    )


comprehensive_report_step = Step(
    name="comprehensive_report",
    executor=create_comprehensive_report,
    description="Create comprehensive report from all research sources",
)

# Final reasoning step using reasoning agent
reasoning_step = Step(
    name="final_reasoning",
    agent=reasoning_agent,
    description="Apply reasoning to create final insights and recommendations",
)

workflow = Workflow(
    name="Enhanced Research Workflow",
    description="Multi-source research with custom data flow and reasoning",
    steps=[
        research_hackernews,
        research_web,
        comprehensive_report_step,  # Has access to both previous steps
        reasoning_step,  # Gets the last step output (comprehensive report)
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )
```

---

<a name="workflows--_06_advanced_concepts--_04_shared_session_state--access_session_state_in_custom_function_step_streampy"></a>

### `workflows/_06_advanced_concepts/_04_shared_session_state/access_session_state_in_custom_function_step_stream.py`

```python
from typing import Iterator, Union

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowRunOutputEvent
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
    db=InMemoryDb(),
)


def custom_content_planning_function(
    step_input: StepInput, session_state: dict
) -> Iterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom streaming function that does intelligent content planning with context awareness
    and maintains a content plan history in session_state
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Initialize content history if not present
    if "content_plans" not in session_state:
        session_state["content_plans"] = []

    if "plan_counter" not in session_state:
        session_state["plan_counter"] = 0

    # Increment plan counter
    session_state["plan_counter"] += 1
    current_plan_id = session_state["plan_counter"]

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}
        Plan ID: #{current_plan_id}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Previous Plans Count: {len(session_state["content_plans"])}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        # Stream the agent's response
        response_iterator = content_planner.run(
            planning_prompt, stream=True, stream_intermediate_steps=True
        )

        # Forward all agent events
        for event in response_iterator:
            yield event

        # Get the final response
        response = content_planner.get_last_run_output()

        # Store this plan in session state
        plan_data = {
            "id": current_plan_id,
            "topic": message,
            "content": response.content if response else "No content generated",
            "timestamp": f"Plan #{current_plan_id}",
            "has_research": bool(previous_step_content),
        }
        session_state["content_plans"].append(plan_data)

        enhanced_content = f"""
## Strategic Content Plan #{current_plan_id}

**Planning Topic:** {message}

**Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}
**Total Plans Created:** {len(session_state["content_plans"])}

**Content Strategy:**
{response.content if response else "Content generation failed"}

**Custom Planning Enhancements:**
- Research Integration: {"High" if previous_step_content else "Baseline"}
- Strategic Alignment: Optimized for multi-channel distribution
- Execution Ready: Detailed action items included
- Session History: {len(session_state["content_plans"])} plans stored

**Plan ID:** #{current_plan_id}
        """.strip()

        yield StepOutput(content=enhanced_content)

    except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


def content_summary_function(
    step_input: StepInput, session_state: dict
) -> Iterator[StepOutput]:
    """
    Custom streaming function that summarizes all content plans created in the session
    """

    plans = session_state["content_plans"]

    summary = f"""
## Content Planning Session Summary

**Total Plans Created:** {len(plans)}
**Session Statistics:**
- Plans with research: {len([p for p in plans if p["has_research"]])}
- Plans without research: {len([p for p in plans if not p["has_research"]])}

**Plan Overview:**
    """

    for plan in plans:
        summary += f"""

### Plan #{plan["id"]} - {plan["topic"]}
- Research Available: {"" if plan["has_research"] else ""}
- Status: Completed
        """

    # Update session state with summary info
    session_state["session_summarized"] = True
    session_state["total_plans_summarized"] = len(plans)

    yield StepOutput(content=summary.strip())


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)

content_summary_step = Step(
    name="Content Summary Step",
    executor=content_summary_function,
)


# Define and use examples
if __name__ == "__main__":
    streaming_content_workflow = Workflow(
        name="Streaming Content Creation Workflow",
        description="Automated content creation with streaming custom execution functions and session state",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step, content_summary_step],
        # Initialize session state with empty content plans
        session_state={"content_plans": [], "plan_counter": 0},
    )

    print("=== First Streaming Workflow Run ===")
    streaming_content_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )

    print(
        f"\nSession State After First Run: {streaming_content_workflow.get_session_state()}"
    )

    print("\n" + "=" * 60 + "\n")

    print("=== Second Streaming Workflow Run (Same Session) ===")
    streaming_content_workflow.print_response(
        input="Machine Learning automation tools",
        markdown=True,
        stream=True,
        stream_intermediate_steps=True,
    )

    print(f"\nFinal Session State: {streaming_content_workflow.get_session_state()}")
```

---

<a name="workflows--_06_advanced_concepts--_04_shared_session_state--access_session_state_in_custom_python_function_steppy"></a>

### `workflows/_06_advanced_concepts/_04_shared_session_state/access_session_state_in_custom_python_function_step.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[hackernews_agent, web_agent],
    instructions="Analyze content and create comprehensive social media strategy",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)


def custom_content_planning_function(
    step_input: StepInput, session_state: dict
) -> StepOutput:
    """
    Custom function that does intelligent content planning with context awareness
    and maintains a content plan history in session_state
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

    # Initialize content history if not present
    if "content_plans" not in session_state:
        session_state["content_plans"] = []

    if "plan_counter" not in session_state:
        session_state["plan_counter"] = 0

    # Increment plan counter
    session_state["plan_counter"] += 1
    current_plan_id = session_state["plan_counter"]

    # Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

        Core Topic: {message}
        Plan ID: #{current_plan_id}

        Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

        Previous Plans Count: {len(session_state["content_plans"])}

        Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

        Please create a detailed, actionable content plan.
    """

    try:
        response = content_planner.run(planning_prompt)

        # Store this plan in session state
        plan_data = {
            "id": current_plan_id,
            "topic": message,
            "content": response.content,
            "timestamp": f"Plan #{current_plan_id}",
            "has_research": bool(previous_step_content),
        }
        session_state["content_plans"].append(plan_data)

        enhanced_content = f"""
            ## Strategic Content Plan #{current_plan_id}

            **Planning Topic:** {message}

            **Research Integration:** {" Research-based" if previous_step_content else " No research foundation"}
            **Total Plans Created:** {len(session_state["content_plans"])}

            **Content Strategy:**
            {response.content}

            **Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
            - Session History: {len(session_state["content_plans"])} plans stored
            
            **Plan ID:** #{current_plan_id}
        """.strip()

        return StepOutput(content=enhanced_content)

    except Exception as e:
        return StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )


def content_summary_function(step_input: StepInput, session_state: dict) -> StepOutput:
    """
    Custom function that summarizes all content plans created in the session
    """
    if "content_plans" not in session_state or not session_state["content_plans"]:
        return StepOutput(
            content="No content plans found in session state.", success=False
        )

    plans = session_state["content_plans"]
    summary = f"""
        ## Content Planning Session Summary
        
        **Total Plans Created:** {len(plans)}
        **Session Statistics:**
        - Plans with research: {len([p for p in plans if p["has_research"]])}
        - Plans without research: {len([p for p in plans if not p["has_research"]])}
        
        **Plan Overview:**
    """

    for plan in plans:
        summary += f"""
        
        ### Plan #{plan["id"]} - {plan["topic"]}
        - Research Available: {"" if plan["has_research"] else ""}
        - Status: Completed
        """

    # Update session state with summary info
    session_state["session_summarized"] = True
    session_state["total_plans_summarized"] = len(plans)

    return StepOutput(content=summary.strip())


# Define steps using different executor types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)

content_summary_step = Step(
    name="Content Summary Step",
    executor=content_summary_function,
)


# Define and use examples
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options and session state",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        # Define the sequence of steps
        # First run the research_step, then the content_planning_step, then the summary_step
        # You can mix and match agents, teams, and even regular python functions directly as steps
        steps=[research_step, content_planning_step, content_summary_step],
        # Initialize session state with empty content plans
        session_state={"content_plans": [], "plan_counter": 0},
    )

    print("=== First Workflow Run ===")
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )

    print(
        f"\nSession State After First Run: {content_creation_workflow.get_session_state()}"
    )

    print("\n" + "=" * 60 + "\n")

    print("=== Second Workflow Run (Same Session) ===")
    content_creation_workflow.print_response(
        input="Machine Learning automation tools",
        markdown=True,
    )

    print(f"\nFinal Session State: {content_creation_workflow.get_session_state()}")
```

---

<a name="workflows--_06_advanced_concepts--_04_shared_session_state--shared_session_state_with_agentpy"></a>

### `workflows/_06_advanced_concepts/_04_shared_session_state/shared_session_state_with_agent.py`

```python
from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db = SqliteDb(db_file="tmp/workflow.db")


# Define tools to manage a shopping list in workflow session state
def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list in workflow session state.

    Args:
        item (str): The item to add to the shopping list
    """
    # Check if item already exists (case-insensitive)
    existing_items = [
        existing_item.lower() for existing_item in session_state["shopping_list"]
    ]
    if item.lower() not in existing_items:
        session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list."
    else:
        return f"'{item}' is already in the shopping list."


def remove_item(session_state, item: str) -> str:
    """Remove an item from the shopping list in workflow session state.

    Args:
        item (str): The item to remove from the shopping list
    """
    if len(session_state["shopping_list"]) == 0:
        return f"Shopping list is empty. Cannot remove '{item}'."

    # Find and remove item (case-insensitive)
    shopping_list = session_state["shopping_list"]
    for i, existing_item in enumerate(shopping_list):
        if existing_item.lower() == item.lower():
            removed_item = shopping_list.pop(i)
            return f"Removed '{removed_item}' from the shopping list."

    return f"'{item}' not found in the shopping list."


def remove_all_items(session_state) -> str:
    """Remove all items from the shopping list in workflow session state."""
    session_state["shopping_list"] = []
    return "Removed all items from the shopping list."


def list_items(session_state) -> str:
    """List all items in the shopping list from workflow session state."""
    if len(session_state["shopping_list"]) == 0:
        return "Shopping list is empty."

    items = session_state["shopping_list"]
    items_str = "\n".join([f"- {item}" for item in items])
    return f"Shopping list:\n{items_str}"


# Create agents with tools that use workflow session state
shopping_assistant = Agent(
    name="Shopping Assistant",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[add_item, remove_item, list_items],
    instructions=[
        "You are a helpful shopping assistant.",
        "You can help users manage their shopping list by adding, removing, and listing items.",
        "Always use the provided tools to interact with the shopping list.",
        "Be friendly and helpful in your responses.",
    ],
)

list_manager = Agent(
    name="List Manager",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[list_items, remove_all_items],
    instructions=[
        "You are a list management specialist.",
        "You can view the current shopping list and clear it when needed.",
        "Always show the current list when asked.",
        "Confirm actions clearly to the user.",
    ],
)

# Create steps
manage_items_step = Step(
    name="manage_items",
    description="Help manage shopping list items (add/remove)",
    agent=shopping_assistant,
)

view_list_step = Step(
    name="view_list",
    description="View and manage the complete shopping list",
    agent=list_manager,
)

# Create workflow with workflow_session_state
shopping_workflow = Workflow(
    name="Shopping List Workflow",
    db=db,
    steps=[manage_items_step, view_list_step],
    session_state={"shopping_list": []},
)

if __name__ == "__main__":
    # Example 1: Add items to the shopping list
    print("=== Example 1: Adding Items ===")
    shopping_workflow.print_response(
        input="Please add milk, bread, and eggs to my shopping list."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

    # Example 2: Add more items and view list
    print("\n=== Example 2: Adding More Items ===")
    shopping_workflow.print_response(
        input="Add apples and bananas to the list, then show me the complete list."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

    # Example 3: Remove items
    print("\n=== Example 3: Removing Items ===")
    shopping_workflow.print_response(
        input="Remove bread from the list and show me what's left."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

    # Example 4: Clear the entire list
    print("\n=== Example 4: Clearing List ===")
    shopping_workflow.print_response(
        input="Clear the entire shopping list and confirm it's empty."
    )
    print("Final workflow session state:", shopping_workflow.get_session_state())
```

---

<a name="workflows--_06_advanced_concepts--_04_shared_session_state--shared_session_state_with_teampy"></a>

### `workflows/_06_advanced_concepts/_04_shared_session_state/shared_session_state_with_team.py`

```python
from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.team.team import Team
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db = SqliteDb(db_file="tmp/workflow.db")


# === TEAM TOOLS FOR STEP MANAGEMENT ===
def add_step(
    session_state, step_name: str, assignee: str, priority: str = "medium"
) -> str:
    """Add a step to the team's workflow session state."""
    if session_state is None:
        session_state = {}

    if "steps" not in session_state:
        session_state["steps"] = []

    step = {
        "name": step_name,
        "assignee": assignee,
        "status": "pending",
        "priority": priority,
        "created_at": "now",
    }
    session_state["steps"].append(step)

    result = f" Successfully added step '{step_name}' assigned to {assignee} (priority: {priority}). Total steps: {len(session_state['steps'])}"
    return result


def delete_step(session_state, step_name: str) -> str:
    """Delete a step from the team's workflow session state."""
    if session_state is None or "steps" not in session_state:
        return " No steps found to delete"

    steps = session_state["steps"]
    for i, step in enumerate(steps):
        if step["name"] == step_name:
            deleted_step = steps.pop(i)
            result = f" Successfully deleted step '{step_name}' (was assigned to {deleted_step['assignee']}). Remaining steps: {len(steps)}"
            return result

    result = f" Step '{step_name}' not found in the list"
    return result


# === AGENT TOOLS FOR STATUS MANAGEMENT ===
def update_step_status(
    session_state, step_name: str, new_status: str, notes: str = ""
) -> str:
    """Update the status of a step in the workflow session state."""
    if session_state is None or "steps" not in session_state:
        return " No steps found in workflow session state"

    steps = session_state["steps"]
    for step in steps:
        if step["name"] == step_name:
            old_status = step["status"]
            step["status"] = new_status
            if notes:
                step["notes"] = notes
            step["last_updated"] = "now"

            result = f" Updated step '{step_name}' status from '{old_status}' to '{new_status}'"
            if notes:
                result += f" with notes: {notes}"

            return result

    result = f" Step '{step_name}' not found in the list"
    return result


def assign_step(session_state, step_name: str, new_assignee: str) -> str:
    """Reassign a step to a different person."""
    if session_state is None or "steps" not in session_state:
        return " No steps found in workflow session state"

    steps = session_state["steps"]
    for step in steps:
        if step["name"] == step_name:
            old_assignee = step["assignee"]
            step["assignee"] = new_assignee
            step["last_updated"] = "now"

            result = f" Reassigned step '{step_name}' from {old_assignee} to {new_assignee}"
            return result

    result = f" Step '{step_name}' not found in the list"
    return result


# === CREATE AGENTS ===
step_manager = Agent(
    name="StepManager",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You are a precise step manager. Your ONLY job is to use the provided tools.",
        "When asked to add a step: ALWAYS use add_step(step_name, assignee, priority).",
        "When asked to delete a step: ALWAYS use delete_step(step_name).",
        "Do NOT create imaginary steps or lists.",
        "Do NOT provide explanations beyond what the tool returns.",
        "Be direct and use the tools immediately.",
    ],
)

step_coordinator = Agent(
    name="StepCoordinator",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=[
        "You coordinate with the StepManager to ensure tasks are completed.",
        "Support the team by confirming actions and helping with coordination.",
        "Be concise and focus on the specific request.",
    ],
)

status_manager = Agent(
    name="StatusManager",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[update_step_status, assign_step],
    instructions=[
        "You manage step statuses and assignments using the provided tools.",
        "Use update_step_status(step_name, new_status, notes) to change step status.",
        "Use assign_step(step_name, new_assignee) to reassign steps.",
        "Valid statuses: 'pending', 'in_progress', 'completed', 'blocked', 'cancelled'.",
        "Be precise and only use the tools provided.",
    ],
)

# === CREATE TEAMS ===
management_team = Team(
    name="ManagementTeam",
    members=[step_manager, step_coordinator],
    tools=[add_step, delete_step],
    instructions=[
        "You are a step management team that ONLY uses the provided tools for adding and deleting steps.",
        "CRITICAL: Use add_step(step_name, assignee, priority) to add steps.",
        "CRITICAL: Use delete_step(step_name) to delete steps.",
        "IMPORTANT: You do NOT handle status updates - that's handled by the status manager in the next step.",
        "IMPORTANT: Do NOT delete steps when asked to mark them as completed - only delete when explicitly asked to delete.",
        "If asked to mark a step as completed, respond that status updates are handled by the status manager.",
        "Do NOT create fictional content or step lists.",
        "Execute only the requested add/delete actions using tools and report the result.",
    ],
)

# === CREATE STEPS ===
manage_steps_step = Step(
    name="manage_steps",
    description="Management team uses tools to add/delete steps in the workflow session state",
    team=management_team,
)

update_status_step = Step(
    name="update_status",
    description="Status manager updates step statuses and assignments",
    agent=status_manager,
)

# === CREATE WORKFLOW ===
project_workflow = Workflow(
    name="Project Management Workflow",
    db=db,
    steps=[manage_steps_step, update_status_step],
    session_state={"steps": []},
)

# === HELPER FUNCTION TO DISPLAY CURRENT STATE ===


def print_current_steps(workflow):
    """Helper function to display current workflow state"""

    session_state = workflow.get_session_state()
    if not session_state or "steps" not in session_state:
        print(" No steps in workflow")
        return

    steps = session_state["steps"]
    if not steps:
        print(" Step list is empty")
        return

    print(" **Current Project Steps:**")
    for i, step in enumerate(steps, 1):
        status_emoji = {
            "pending": "",
            "in_progress": "",
            "completed": "",
            "blocked": "",
            "cancelled": "",
        }.get(step["status"], "")

        priority_emoji = {"high": "", "medium": "", "low": ""}.get(
            step.get("priority", "medium"), ""
        )

        print(
            f"  {i}. {status_emoji} {priority_emoji} **{step['name']}** (assigned to: {step['assignee']}, status: {step['status']})"
        )
        if "notes" in step:
            print(f"      Notes: {step['notes']}")
    print()


if __name__ == "__main__":
    print(" Starting Project Management Workflow Tests")

    # Example 1: Add multiple steps with different priorities
    print("=" * 60)
    print(" Example 1: Add Multiple Steps")
    print("=" * 60)
    project_workflow.print_response(
        input="Add a high priority step called 'Setup Database' assigned to Alice, and a medium priority step called 'Create API' assigned to Bob"
    )
    print_current_steps(project_workflow)
    print(f" Workflow Session State: {project_workflow.get_session_state()}")
    print()

    # Example 2: Update step status
    print("=" * 60)
    print(" Example 2: Update Step Status")
    print("=" * 60)
    project_workflow.print_response(
        input="Mark 'Setup Database' as in_progress with notes 'Started database schema design'"
    )
    print_current_steps(project_workflow)
    print(f" Workflow Session State: {project_workflow.get_session_state()}")
    print()

    # Example 3: Reassign and complete a step
    print("=" * 60)
    print(" Example 3: Reassign and Complete Step")
    print("=" * 60)
    project_workflow.print_response(
        input="Reassign 'Create API' to Charlie, then mark it as completed with notes 'API endpoints implemented and tested'"
    )
    print_current_steps(project_workflow)
    print(f" Workflow Session State: {project_workflow.get_session_state()}")
    print()

    # Example 4: Add more steps and manage them
    print("=" * 60)
    print(" Example 4: Add and Manage More Steps")
    print("=" * 60)
    project_workflow.print_response(
        input="Add a low priority step 'Write Tests' assigned to Dave, then mark 'Setup Database' as completed"
    )
    print_current_steps(project_workflow)
    print(f" Workflow Session State: {project_workflow.get_session_state()}")
    print()

    # Example 5: Delete a step
    print("=" * 60)
    print(" Example 5: Delete Step")
    print("=" * 60)
    project_workflow.print_response(
        input="Delete the 'Write Tests' step and add a high priority 'Deploy to Production' step assigned to Eve"
    )
    print_current_steps(project_workflow)
    print(f" Workflow Session State: {project_workflow.get_session_state()}")
```

---

<a name="workflows--_06_advanced_concepts--_05_background_execution--background_execution_pollpy"></a>

### `workflows/_06_advanced_concepts/_05_background_execution/background_execution_poll.py`

```python
import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[GoogleSearchTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[research_step, content_planning_step],
)


async def main():
    print(" Starting Async Background Workflow Test")

    # Start background execution (async)
    bg_response = await content_creation_workflow.arun(
        input="AI trends in 2024", background=True
    )
    print(f" Initial Response: {bg_response.status} - {bg_response.content}")
    print(f" Run ID: {bg_response.run_id}")

    # Poll every 5 seconds until completion
    poll_count = 0

    while True:
        poll_count += 1
        print(f"\n Poll #{poll_count} (every 5s)")

        result = content_creation_workflow.get_run(bg_response.run_id)

        if result is None:
            print(" Workflow not found yet, still waiting...")
            if poll_count > 50:
                print(f" Timeout after {poll_count} attempts")
                break
            await asyncio.sleep(5)
            continue

        if result.has_completed():
            break

        if poll_count > 200:
            print(f" Timeout after {poll_count} attempts")
            break

        await asyncio.sleep(5)

    final_result = content_creation_workflow.get_run(bg_response.run_id)

    print("\n Final Result:")
    print("=" * 50)
    pprint_run_response(final_result, markdown=True)


if __name__ == "__main__":
    asyncio.run(main())
```

---

<a name="workflows--_06_advanced_concepts--_05_background_execution--background_execution_using_websocket--websocket_clientpy"></a>

### `workflows/_06_advanced_concepts/_05_background_execution/background_execution_using_websocket/websocket_client.py`

```python
import asyncio
import json
import sys
from datetime import datetime
from typing import Optional

import websockets
from rich.align import Align
from rich.console import Console
from rich.panel import Panel
from rich.text import Text


class WorkflowWebSocketClient:
    def __init__(
        self,
        server_url: str = "ws://localhost:8000/ws",
        auth_token: Optional[str] = None,
    ):
        self.server_url = server_url
        self.auth_token = auth_token
        self.console = Console()
        self.websocket = None
        self.connection_id = None
        self.events = []
        self.is_running = True
        self.current_step_content = {}  # Track streaming content per step
        self.is_authenticated = False

    def get_event_style(self, event_type: str) -> tuple[str, str]:
        """Get style (emoji, color) for event type"""
        styles = {
            "connected": ("", "cyan"),
            "connection_established": ("", "cyan"),
            "authenticated": ("", "green"),
            "auth_error": ("", "red"),
            "auth_required": ("", "yellow"),
            "workflow_starting": ("", "yellow"),
            "workflow_initiated": ("", "green"),
            "WorkflowStarted": ("", "blue"),
            "StepStarted": ("", "yellow"),
            "StepCompleted": ("", "green"),
            "WorkflowCompleted": ("", "bright_green"),
            "WorkflowError": ("", "red"),
            "workflow_error": ("", "red"),
            "RunStarted": ("", "blue"),
            "RunContent": ("", "white"),
            "RunCompleted": ("", "green"),
            "ToolCallStarted": ("", "magenta"),
            "ToolCallCompleted": ("", "green"),
            "error": ("", "red"),
            "pong": ("", "dim"),
            "echo": ("", "dim"),
        }
        return styles.get(event_type, ("", "white"))

    def parse_sse_message(self, message: str) -> Optional[dict]:
        """Parse SSE format message (event: X \n data: {...})"""
        lines = message.strip().split("\n")
        event_type = None
        data = None

        for line in lines:
            if line.startswith("event: "):
                event_type = line[7:].strip()
            elif line.startswith("data: "):
                data_str = line[6:].strip()
                try:
                    data = json.loads(data_str)
                except json.JSONDecodeError:
                    return None

        if data:
            data["type"] = event_type or data.get("event", "unknown")
            return data
        return None

    def format_event(self, event_data: dict) -> Panel:
        """Format event data into a beautiful panel"""
        event_type = event_data.get("type", event_data.get("event", "unknown"))
        emoji, color = self.get_event_style(event_type)
        timestamp = datetime.now().strftime("%H:%M:%S")

        # Handle streaming content differently
        if event_type == "RunContent":
            return self.format_streaming_content(event_data, emoji, color, timestamp)

        # Build content for other events
        content_lines = []

        # Main message
        message = event_data.get("message", "")
        content = event_data.get("content", "")

        if message:
            content_lines.append(f"[bold]{message}[/bold]")
        elif content and len(content) < 200:
            content_lines.append(f"[bold]{content}[/bold]")
        elif content:
            # For long content, show truncated version
            content_lines.append(f"[bold]{content[:200]}...[/bold]")

        # Additional details
        details = []
        important_fields = [
            "step_name",
            "agent_name",
            "run_id",
            "session_id",
            "step_index",
        ]

        for key in important_fields:
            if key in event_data:
                details.append(f"[dim]{key}:[/dim] {event_data[key]}")

        if details:
            content_lines.extend(details)

        if not content_lines:
            content_lines.append(f"[dim]Event: {event_type}[/dim]")

        content_text = "\n".join(content_lines)

        return Panel(
            content_text,
            title=f"{emoji} [{color}]{event_type}[/{color}] [{timestamp}]",
            border_style=color,
            width=100,
        )

    def format_streaming_content(
        self, event_data: dict, emoji: str, color: str, timestamp: str
    ) -> Optional[Panel]:
        """Handle streaming content with accumulation"""
        step_id = event_data.get("step_id", "unknown")
        step_name = event_data.get("step_name", "unknown")
        agent_name = event_data.get("agent_name", "unknown")
        content = event_data.get("content", "")

        # Accumulate content for this step
        if step_id not in self.current_step_content:
            self.current_step_content[step_id] = {
                "content": "",
                "step_name": step_name,
                "agent_name": agent_name,
                "last_update": timestamp,
            }

        self.current_step_content[step_id]["content"] += content
        self.current_step_content[step_id]["last_update"] = timestamp

        # Only show panels for meaningful content chunks (not single characters)
        if len(content.strip()) > 3 or content.endswith("\n"):
            accumulated_content = self.current_step_content[step_id]["content"]

            # Show last 300 chars if too long
            display_content = accumulated_content
            if len(accumulated_content) > 300:
                display_content = f"...{accumulated_content[-300:]}"

            content_lines = [
                f"[bold]{agent_name}[/bold]  [dim]{step_name}[/dim]",
                f"[white]{display_content}[/white]",
            ]

            return Panel(
                "\n".join(content_lines),
                title=f"{emoji} [{color}]Streaming Content[/{color}] [{timestamp}]",
                border_style=color,
                width=100,
            )

        return None

    async def connect(self):
        """Connect to WebSocket server and authenticate"""
        try:
            self.websocket = await websockets.connect(self.server_url)
            self.console.print(f" [green]Connected to {self.server_url}[/green]")

            # Auto-authenticate if token provided
            if self.auth_token:
                await self.authenticate()
            else:
                self.console.print(
                    "  [yellow]No authentication token provided.[/yellow]"
                )
                self.console.print(
                    " [blue]Use 'auth' command to authenticate interactively[/blue]"
                )

            return True
        except Exception as e:
            self.console.print(f" [red]Failed to connect: {e}[/red]")
            return False

    async def authenticate(self, token: str = None):
        """Send authentication token to server"""
        auth_token = token or self.auth_token

        if not auth_token:
            self.console.print(" [red]No authentication token available[/red]")
            return False

        auth_message = {"action": "authenticate", "token": auth_token}

        await self.websocket.send(json.dumps(auth_message))
        self.console.print(" [blue]Sent authentication token[/blue]")
        return True

    async def prompt_for_auth(self):
        """Interactively prompt for authentication token"""
        try:
            token = await asyncio.get_event_loop().run_in_executor(
                None, lambda: input(" Enter authentication token: ").strip()
            )

            if token:
                self.auth_token = token
                return await self.authenticate(token)
            else:
                self.console.print(" [red]No token provided[/red]")
                return False
        except Exception as e:
            self.console.print(f" [red]Error getting token: {e}[/red]")
            return False

    async def disconnect(self):
        """Disconnect from WebSocket server"""
        if self.websocket:
            await self.websocket.close()
            self.console.print(" [yellow]Disconnected from server[/yellow]")

    async def send_message(self, message_data: dict):
        """Send message to WebSocket server"""
        if self.websocket:
            await self.websocket.send(json.dumps(message_data))

    async def listen_for_events(self):
        """Listen for events from WebSocket server"""
        try:
            async for message in self.websocket:
                if not self.is_running:
                    break

                try:
                    # Try parsing as pure JSON first
                    event_data = json.loads(message)
                    self.events.append(event_data)

                    # Display event immediately
                    panel = self.format_event(event_data)
                    if panel:
                        self.console.print(panel)

                    # Store connection ID and authentication status
                    if (
                        event_data.get("event") == "connected"
                        or event_data.get("type") == "connection_established"
                    ):
                        self.connection_id = event_data.get("connection_id")
                    elif event_data.get("event") == "authenticated":
                        self.is_authenticated = True
                        self.console.print(
                            " [green]Authentication successful![/green]"
                        )
                    elif event_data.get("event") == "auth_error":
                        self.console.print(
                            f" [red]Authentication failed: {event_data.get('error')}[/red]"
                        )
                    elif event_data.get("event") == "auth_required":
                        self.console.print(
                            f" [yellow]Authentication required: {event_data.get('error')}[/yellow]"
                        )

                except json.JSONDecodeError:
                    # Try parsing as SSE format
                    event_data = self.parse_sse_message(message)
                    if event_data:
                        self.events.append(event_data)

                        # Display event immediately
                        panel = self.format_event(event_data)
                        if panel:
                            self.console.print(panel)
                    else:
                        # Only show error for very short messages (real errors)
                        if len(message) < 100:
                            self.console.print(
                                f" [red]Could not parse message: {message[:50]}...[/red]"
                            )

        except websockets.exceptions.ConnectionClosed:
            self.console.print(" [yellow]WebSocket connection closed[/yellow]")
        except Exception as e:
            self.console.print(f" [red]Error listening for events: {e}[/red]")

    async def start_workflow(
        self, workflow_message: str, session_id: Optional[str] = None
    ):
        """Start a workflow via WebSocket"""
        if not self.is_authenticated and self.auth_token:
            self.console.print(
                " [red]Not authenticated. Please authenticate first.[/red]"
            )
            return

        if not session_id:
            session_id = f"cli-session-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

        message_data = {
            "type": "start-workflow",
            "message": workflow_message,
            "session_id": session_id,
        }

        self.console.print(
            f" [blue]Starting workflow with message:[/blue] [bold]{workflow_message}[/bold]"
        )
        await self.send_message(message_data)

    async def ping_server(self):
        """Send ping to server"""
        await self.send_message({"action": "ping"})

    def print_banner(self):
        """Print application banner"""
        banner = Text(" Agno Workflow WebSocket Client", style="bold blue")
        self.console.print(Align.center(banner))
        self.console.print(Align.center(f"Connected to: {self.server_url}"))
        self.console.print()

    async def run_interactive(self):
        """Run interactive mode"""
        if not await self.connect():
            return

        self.print_banner()

        # Start listening for events in background
        listen_task = asyncio.create_task(self.listen_for_events())

        self.console.print("[green]Interactive mode started. Type commands:[/green]")
        self.console.print("  [bold]auth[/bold] - Authenticate with token")
        self.console.print("  [bold]start <message>[/bold] - Start workflow")
        self.console.print("  [bold]ping[/bold] - Ping server")
        self.console.print("  [bold]quit[/bold] - Exit")

        # Prominent auth message if not authenticated
        if not self.is_authenticated:
            if not self.auth_token:
                self.console.print()
                self.console.print(
                    " [yellow bold]AUTHENTICATION REQUIRED[/yellow bold]"
                )
                self.console.print(
                    "   [yellow]Type 'auth' to authenticate with your token[/yellow]"
                )
            else:
                self.console.print(
                    "   [yellow]  Waiting for authentication...[/yellow]"
                )
        self.console.print()

        try:
            while self.is_running:
                try:
                    # Get user input
                    user_input = await asyncio.get_event_loop().run_in_executor(
                        None, input, " Enter command: "
                    )

                    if user_input.lower() in ["quit", "exit", "q"]:
                        self.is_running = False
                        break
                    elif user_input.lower() == "auth":
                        await self.prompt_for_auth()
                    elif user_input.lower() == "ping":
                        if not self.is_authenticated:
                            self.console.print(
                                " [red]Not authenticated. Use 'auth' command first.[/red]"
                            )
                            continue
                        await self.ping_server()
                    elif user_input.lower().startswith("start "):
                        workflow_message = user_input[6:].strip()
                        if workflow_message:
                            await self.start_workflow(workflow_message)
                        else:
                            self.console.print(
                                " [red]Please provide a message for the workflow[/red]"
                            )
                    else:
                        self.console.print(
                            " [red]Unknown command. Use 'auth', 'start <message>', 'ping', or 'quit'[/red]"
                        )

                except KeyboardInterrupt:
                    self.is_running = False
                    break
                except Exception as e:
                    self.console.print(f" [red]Error: {e}[/red]")

        finally:
            self.is_running = False
            listen_task.cancel()
            await self.disconnect()

    async def run_with_message(self, message: str):
        """Run with a single message and listen for events"""
        if not await self.connect():
            return

        self.print_banner()

        # Start listening for events in background
        listen_task = asyncio.create_task(self.listen_for_events())

        # Start workflow
        await self.start_workflow(message)

        # Wait for workflow to complete or timeout
        try:
            self.console.print(
                " [yellow]Listening for workflow events... (Press Ctrl+C to stop)[/yellow]"
            )
            await listen_task
        except KeyboardInterrupt:
            self.console.print("\n [yellow]Stopping...[/yellow]")
            self.is_running = False
            listen_task.cancel()
            await self.disconnect()


async def main():
    """Main CLI function"""
    import argparse

    parser = argparse.ArgumentParser(description="Agno Workflow WebSocket Client")
    parser.add_argument(
        "--server", default="ws://localhost:8000/ws", help="WebSocket server URL"
    )
    parser.add_argument("--message", "-m", help="Workflow message to send")
    parser.add_argument(
        "--interactive", "-i", action="store_true", help="Run in interactive mode"
    )
    parser.add_argument(
        "--token",
        "-t",
        help="Authentication bearer token (or set SECURITY_KEY env var)",
    )

    args = parser.parse_args()

    # Get token from args or environment variable
    import os

    auth_token = args.token or os.getenv("SECURITY_KEY")

    client = WorkflowWebSocketClient(args.server, auth_token)

    if args.interactive or not args.message:
        await client.run_interactive()
    else:
        await client.run_with_message(args.message)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n Goodbye!")
        sys.exit(0)
```

---

<a name="workflows--_06_advanced_concepts--_05_background_execution--background_execution_using_websocket--websocket_serverpy"></a>

### `workflows/_06_advanced_concepts/_05_background_execution/background_execution_using_websocket/websocket_server.py`

```python
import json
import os
from typing import Dict

import uvicorn
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from fastapi import FastAPI, WebSocket, WebSocketDisconnect

# === CONFIGURATION ===
SECURITY_KEY = os.getenv("SECURITY_KEY", "your-secret-key")  # Set your key here

# === WORKFLOW SETUP ===
hackernews_agent = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    instructions="Research tech news and trends from HackerNews",
)

search_agent = Agent(
    name="Search Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Search for additional information on the web",
)

# === FASTAPI APP ===
app = FastAPI(title="Background Workflow WebSocket Server")

# Store active WebSocket connections and their auth status
active_connections: Dict[str, WebSocket] = {}
authenticated_connections: Dict[str, bool] = {}  # {connection_id: is_authenticated}


def validate_token(token: str) -> bool:
    """Validate authentication token"""
    # If no security key set, allow all connections
    if not SECURITY_KEY or SECURITY_KEY == "your-secret-key":
        return True
    return token == SECURITY_KEY


@app.get("/")
async def get():
    """API status endpoint"""
    return {
        "status": "running",
        "message": "Background Workflow WebSocket Server",
        "endpoints": {
            "websocket": "/ws",
            "start-workflow": "/workflow/start",
        },
        "connections": len(active_connections),
        "authenticated": len([c for c in authenticated_connections.values() if c]),
    }


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for background workflow events"""
    await websocket.accept()
    connection_id = f"conn_{len(active_connections)}"
    active_connections[connection_id] = websocket
    authenticated_connections[connection_id] = False  # Start unauthenticated

    print(f" Client connected: {connection_id}")

    try:
        # Send connection confirmation
        await websocket.send_text(
            json.dumps(
                {
                    "event": "connected",
                    "connection_id": connection_id,
                    "message": "Connected to workflow events. Please authenticate to continue.",
                    "requires_auth": True,
                }
            )
        )

        # Keep connection alive
        while True:
            try:
                data = await websocket.receive_text()
                message_data = json.loads(data)
                action = message_data.get("action") or message_data.get("type")

                # Handle authentication
                if action == "authenticate":
                    token = message_data.get("token")
                    if not token:
                        await websocket.send_text(
                            json.dumps(
                                {"event": "auth_error", "error": "Token is required"}
                            )
                        )
                        continue

                    if validate_token(token):
                        authenticated_connections[connection_id] = True
                        await websocket.send_text(
                            json.dumps(
                                {
                                    "event": "authenticated",
                                    "message": "Authentication successful. You can now send commands.",
                                }
                            )
                        )
                        print(f" Client authenticated: {connection_id}")
                    else:
                        await websocket.send_text(
                            json.dumps(
                                {"event": "auth_error", "error": "Invalid token"}
                            )
                        )
                    continue

                # Check authentication for other actions
                if not authenticated_connections.get(connection_id, False):
                    await websocket.send_text(
                        json.dumps(
                            {
                                "event": "auth_required",
                                "error": "Authentication required. Send authenticate action with valid token.",
                            }
                        )
                    )
                    continue

                # Handle authenticated actions
                if action == "start-workflow":
                    await handle_start_workflow(websocket, message_data)
                elif action == "ping":
                    await websocket.send_text(json.dumps({"event": "pong"}))
                else:
                    # Echo back for testing
                    await websocket.send_text(
                        json.dumps({"event": "echo", "original_message": message_data})
                    )

            except WebSocketDisconnect:
                break
            except Exception as e:
                await websocket.send_text(
                    json.dumps(
                        {
                            "event": "error",
                            "message": f"Error processing message: {str(e)}",
                        }
                    )
                )

    except WebSocketDisconnect:
        pass
    finally:
        if connection_id in active_connections:
            del active_connections[connection_id]
        if connection_id in authenticated_connections:
            del authenticated_connections[connection_id]
        print(f" Client disconnected: {connection_id}")


async def handle_start_workflow(websocket: WebSocket, message_data: dict):
    """Handle workflow start request via WebSocket"""
    message = message_data.get("message", "AI trends 2024")
    session_id = message_data.get("session_id", f"ws-session-{len(active_connections)}")

    workflow = Workflow(
        name="Tech Research Pipeline",
        steps=[
            Step(name="hackernews_research", agent=hackernews_agent),
            Step(name="web_search", agent=search_agent),
        ],
        db=SqliteDb(
            db_file="tmp/workflow_bg.db",
            session_table="workflow_bg",
        ),
    )

    try:
        # Send acknowledgment
        await websocket.send_text(
            json.dumps(
                {
                    "event": "workflow_starting",
                    "message": f"Starting workflow with message: {message}",
                    "session_id": session_id,
                }
            )
        )

        # Execute workflow in background with streaming and WebSocket
        result = await workflow.arun(
            input=message,
            session_id=session_id,
            stream=True,
            stream_intermediate_steps=True,
            background=True,
            websocket=websocket,
        )

        # Send completion notification
        await websocket.send_text(
            json.dumps(
                {
                    "event": "workflow_initiated",
                    "run_id": result.run_id,
                    "session_id": result.session_id,
                    "message": "Background streaming workflow initiated successfully",
                }
            )
        )

    except Exception as e:
        await websocket.send_text(
            json.dumps(
                {
                    "event": "workflow_error",
                    "error": str(e),
                    "message": "Failed to start workflow",
                }
            )
        )


# ... rest of the HTTP endpoint code stays the same ...

if __name__ == "__main__":
    print(" Starting Background Workflow WebSocket Server...")
    print(" WebSocket: ws://localhost:8000/ws")
    print(" HTTP API: http://localhost:8000")
    print(" API Docs: http://localhost:8000/docs")
    print(f" Security Key: {SECURITY_KEY}")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info",
    )
```

---

<a name="workflows--_06_advanced_concepts--_06_other--rename_workflow_sessionpy"></a>

### `workflows/_06_advanced_concepts/_06_other/rename_workflow_session.py`

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.workflow import Workflow

# Define agents for different tasks
researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

# Define individual steps
research_step = Step(
    name="research",
    agent=researcher,
    description="Research the topic and gather information",
)

writing_step = Step(
    name="writing",
    agent=writer,
    description="Write an article based on the research",
)


# Create a Steps sequence that chains these above steps together
article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to writing",
    steps=[research_step, writing_step],
)

# Create and use workflow
if __name__ == "__main__":
    article_workflow = Workflow(
        description="Automated article creation from research to writing",
        steps=[article_creation_sequence],
        debug_mode=True,
    )

    article_workflow.print_response(
        input="Write an article about the benefits of renewable energy",
        markdown=True,
    )

    article_workflow.set_session_name(autogenerate=True)

    # print the new session name
    print(f"New session name: {article_workflow.session_name}")
```

---

<a name="workflows--_06_advanced_concepts--_06_other--store_events_and_events_to_skip_in_a_workflowpy"></a>

### `workflows/_06_advanced_concepts/_06_other/store_events_and_events_to_skip_in_a_workflow.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.agent import (
    RunContentEvent,
    RunEvent,
    ToolCallCompletedEvent,
    ToolCallStartedEvent,
)
from agno.run.workflow import WorkflowRunEvent, WorkflowRunOutput
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents for different tasks
news_agent = Agent(
    name="News Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    instructions="You are a news researcher. Get the latest tech news and summarize key points.",
)

search_agent = Agent(
    name="Search Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a search specialist. Find relevant information on given topics.",
)

analysis_agent = Agent(
    name="Analysis Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are an analyst. Analyze the provided information and give insights.",
)

summary_agent = Agent(
    name="Summary Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a summarizer. Create concise summaries of the provided content.",
)

research_step = Step(
    name="Research Step",
    agent=news_agent,
)

search_step = Step(
    name="Search Step",
    agent=search_agent,
)


def print_stored_events(run_response: WorkflowRunOutput, example_name):
    """Helper function to print stored events in a nice format"""
    print(f"\n--- {example_name} - Stored Events ---")
    if run_response.events:
        print(f"Total stored events: {len(run_response.events)}")
        for i, event in enumerate(run_response.events, 1):
            print(f"  {i}. {event.event}")
    else:
        print("No events stored")
    print()


print("=== Simple Step Workflow with Event Storage ===")
step_workflow = Workflow(
    name="Simple Step Workflow",
    description="Basic workflow demonstrating step event storage",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[research_step, search_step],
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.step_started,
        WorkflowRunEvent.workflow_completed,
        RunEvent.run_content,
        RunEvent.run_started,
        RunEvent.run_completed,
    ],  # Skip step started events to reduce noise
)

print("Running Step workflow with streaming...")
for event in step_workflow.run(
    input="AI trends in 2024",
    stream=True,
    stream_intermediate_steps=True,
):
    # Filter out RunContentEvent from printing to reduce noise
    if not isinstance(
        event, (RunContentEvent, ToolCallStartedEvent, ToolCallCompletedEvent)
    ):
        print(
            f"Event: {event.event if hasattr(event, 'event') else type(event).__name__}"
        )
run_response = step_workflow.get_last_run_output()

print("\nStep workflow completed!")
print(
    f"Total events stored: {len(run_response.events) if run_response and run_response.events else 0}"
)

# Print stored events in a nice format
print_stored_events(run_response, "Simple Step Workflow")

# ------------------------------------------------------------------------------------------------ #
# ------------------------------------------------------------------------------------------------ #

# Example 2: Parallel Primitive with Event Storage
print("=== 2. Parallel Example ===")
parallel_workflow = Workflow(
    name="Parallel Research Workflow",
    steps=[
        Parallel(
            Step(name="News Research", agent=news_agent),
            Step(name="Web Search", agent=search_agent),
            name="Parallel Research",
        ),
        Step(name="Combine Results", agent=analysis_agent),
    ],
    db=SqliteDb(
        session_table="workflow_parallel",
        db_file="tmp/workflow_parallel.db",
    ),
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.parallel_execution_started,
        WorkflowRunEvent.parallel_execution_completed,
    ],
)

print("Running Parallel workflow...")
for event in parallel_workflow.run(
    input="Research machine learning developments",
    stream=True,
    stream_intermediate_steps=True,
):
    # Filter out RunContentEvent from printing
    if not isinstance(event, RunContentEvent):
        print(
            f"Event: {event.event if hasattr(event, 'event') else type(event).__name__}"
        )

run_response = parallel_workflow.get_last_run_output()
print(f"Parallel workflow stored {len(run_response.events)} events")
print_stored_events(run_response, "Parallel Workflow")
print()
```

---

<a name="workflows--_06_advanced_concepts--_06_other--workflow_cancel_a_runpy"></a>

### `workflows/_06_advanced_concepts/_06_other/workflow_cancel_a_run.py`

```python
"""
Example demonstrating how to cancel a running workflow execution.

This example shows how to:
1. Start a workflow run in a separate thread
2. Cancel the run from another thread
3. Handle the cancelled response
"""

import threading
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.agent import RunEvent
from agno.run.base import RunStatus
from agno.run.workflow import WorkflowRunEvent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow


def long_running_task(workflow: Workflow, run_id_container: dict):
    """
    Simulate a long-running workflow task that can be cancelled.

    Args:
        workflow: The workflow to run
        run_id_container: Dictionary to store the run_id for cancellation

    Returns:
        Dictionary with run results and status
    """
    try:
        # Start the workflow run - this simulates a long task
        final_response = None
        content_pieces = []

        for chunk in workflow.run(
            "Write a very long story about a dragon who learns to code. "
            "Make it at least 2000 words with detailed descriptions and dialogue. "
            "Take your time and be very thorough.",
            stream=True,
        ):
            if "run_id" not in run_id_container and chunk.run_id:
                print(f" Workflow run started: {chunk.run_id}")
                run_id_container["run_id"] = chunk.run_id

            if chunk.event in [RunEvent.run_content]:
                print(chunk.content, end="", flush=True)
                content_pieces.append(chunk.content)
            elif chunk.event == RunEvent.run_cancelled:
                print(f"\n Workflow run was cancelled: {chunk.run_id}")
                run_id_container["result"] = {
                    "status": "cancelled",
                    "run_id": chunk.run_id,
                    "cancelled": True,
                    "content": "".join(content_pieces)[:200] + "..."
                    if content_pieces
                    else "No content before cancellation",
                }
                return
            elif chunk.event == WorkflowRunEvent.workflow_cancelled:
                print(f"\n Workflow run was cancelled: {chunk.run_id}")
                run_id_container["result"] = {
                    "status": "cancelled",
                    "run_id": chunk.run_id,
                    "cancelled": True,
                    "content": "".join(content_pieces)[:200] + "..."
                    if content_pieces
                    else "No content before cancellation",
                }
                return
            elif hasattr(chunk, "status") and chunk.status == RunStatus.completed:
                final_response = chunk

        # If we get here, the run completed successfully
        if final_response:
            run_id_container["result"] = {
                "status": final_response.status.value
                if final_response.status
                else "completed",
                "run_id": final_response.run_id,
                "cancelled": final_response.status == RunStatus.cancelled,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }
        else:
            run_id_container["result"] = {
                "status": "unknown",
                "run_id": run_id_container.get("run_id"),
                "cancelled": False,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }

    except Exception as e:
        print(f"\n Exception in run: {str(e)}")
        run_id_container["result"] = {
            "status": "error",
            "error": str(e),
            "run_id": run_id_container.get("run_id"),
            "cancelled": True,
            "content": "Error occurred",
        }


def cancel_after_delay(
    workflow: Workflow, run_id_container: dict, delay_seconds: int = 3
):
    """
    Cancel the workflow run after a specified delay.

    Args:
        workflow: The workflow whose run should be cancelled
        run_id_container: Dictionary containing the run_id to cancel
        delay_seconds: How long to wait before cancelling
    """
    print(f" Will cancel workflow run in {delay_seconds} seconds...")
    time.sleep(delay_seconds)

    run_id = run_id_container.get("run_id")
    if run_id:
        print(f" Cancelling workflow run: {run_id}")
        success = workflow.cancel_run(run_id)
        if success:
            print(f" Workflow run {run_id} marked for cancellation")
        else:
            print(
                f" Failed to cancel workflow run {run_id} (may not exist or already completed)"
            )
    else:
        print("  No run_id found to cancel")


def main():
    """Main function demonstrating workflow run cancellation."""

    # Create workflow agents
    researcher = Agent(
        name="Research Agent",
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[DuckDuckGoTools()],
        instructions="Research the given topic and provide key facts and insights.",
    )

    writer = Agent(
        name="Writing Agent",
        model=OpenAIChat(id="gpt-4o"),
        instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
    )
    research_step = Step(
        name="research",
        agent=researcher,
        description="Research the topic and gather information",
    )

    writing_step = Step(
        name="writing",
        agent=writer,
        description="Write an article based on the research",
    )

    # Create a Steps sequence that chains these above steps together
    article_workflow = Workflow(
        description="Automated article creation from research to writing",
        steps=[research_step, writing_step],
        debug_mode=True,
    )

    print(" Starting workflow run cancellation example...")
    print("=" * 50)

    # Container to share run_id between threads
    run_id_container = {}

    # Start the workflow run in a separate thread
    workflow_thread = threading.Thread(
        target=lambda: long_running_task(article_workflow, run_id_container),
        name="WorkflowRunThread",
    )

    # Start the cancellation thread
    cancel_thread = threading.Thread(
        target=cancel_after_delay,
        args=(article_workflow, run_id_container, 8),  # Cancel after 8 seconds
        name="CancelThread",
    )

    # Start both threads
    print(" Starting workflow run thread...")
    workflow_thread.start()

    print(" Starting cancellation thread...")
    cancel_thread.start()

    # Wait for both threads to complete
    print(" Waiting for threads to complete...")
    workflow_thread.join()
    cancel_thread.join()

    # Print the results
    print("\n" + "=" * 50)
    print(" RESULTS:")
    print("=" * 50)

    result = run_id_container.get("result")
    if result:
        print(f"Status: {result['status']}")
        print(f"Run ID: {result['run_id']}")
        print(f"Was Cancelled: {result['cancelled']}")

        if result.get("error"):
            print(f"Error: {result['error']}")
        else:
            print(f"Content Preview: {result['content']}")

        if result["cancelled"]:
            print("\n SUCCESS: Workflow run was successfully cancelled!")
        else:
            print("\n  WARNING: Workflow run completed before cancellation")
    else:
        print(" No result obtained - check if cancellation happened during streaming")

    print("\n Workflow cancellation example completed!")


if __name__ == "__main__":
    # Run the main example
    main()
```

---

<a name="workflows--_06_advanced_concepts--_06_other--workflow_metrics_on_run_responsepy"></a>

### `workflows/_06_advanced_concepts/_06_other/workflow_metrics_on_run_response.py`

```python
import json

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowRunOutput
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

# Define steps
research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

# Create and use workflow
if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
    )
    workflow_run_response: WorkflowRunOutput = content_creation_workflow.run(
        input="AI trends in 2024",
    )

    # Print workflow metrics
    if workflow_run_response.metrics:
        print("\n" + "-" * 60)
        print("WORKFLOW METRICS")
        print("-" * 60)
        print(json.dumps(workflow_run_response.metrics.to_dict(), indent=2))
    else:
        print("\nNo workflow metrics available")
```

---

<a name="workflows--_06_advanced_concepts--_06_other--workflow_toolspy"></a>

### `workflows/_06_advanced_concepts/_06_other/workflow_tools.py`

```python
"""
Here is a tool with reasoning capabilities to allow agents to run workflows.

1. Run: `pip install openai agno lancedb tantivy sqlalchemy` to install the dependencies
2. Export your OPENAI_API_KEY
3. Run: `python cookbook/workflows/_06_advanced_concepts/_06_other/workflow_tools.py` to run the agent
"""

import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.tools.workflow import WorkflowTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

FEW_SHOT_EXAMPLES = dedent(
    """\
    You can refer to the examples below as guidance for how to use each tool.
    ### Examples
    #### Example: Blog Post Workflow
    User: Please create a blog post on the topic: AI Trends in 2024
    Run: input_data="AI trends in 2024", additional_data={"topic": "AI, AI agents, AI workflows", "style": "The blog post should be written in a style that is easy to understand and follow."}
    Final Answer: I've created a blog post on the topic: AI trends in 2024 through the workflow. The blog post shows...

    You HAVE TO USE additional_data to pass the topic and style to the workflow.
"""
)


# Define agents
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

writer_agent = Agent(
    name="Writer Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Write a blog post on the topic",
)


def prepare_input_for_web_search(step_input: StepInput) -> StepOutput:
    title = step_input.input
    topic = step_input.additional_data.get("topic")
    return StepOutput(
        content=dedent(
            f"""\
	I'm writing a blog post with the title: {title}
	<topic>
	{topic}
	</topic>
	Search the web for atleast 10 articles\
	"""
        )
    )


def prepare_input_for_writer(step_input: StepInput) -> StepOutput:
    title = step_input.additional_data.get("title")
    topic = step_input.additional_data.get("topic")
    style = step_input.additional_data.get("style")

    research_team_output = step_input.previous_step_content

    return StepOutput(
        content=dedent(
            f"""\
	I'm writing a blog post with the title: {title}
	<required_style>
	{style}
	</required_style>
	<topic>
	{topic}
	</topic>
	Here is information from the web:
	<research_results>
	{research_team_output}
	<research_results>\
	"""
        )
    )


# Define research team for complex analysis
research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)


# Create and use workflow
content_creation_workflow = Workflow(
    name="Blog Post Workflow",
    description="Automated blog post creation from Hackernews and the web",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[
        prepare_input_for_web_search,
        research_team,
        prepare_input_for_writer,
        writer_agent,
    ],
)

workflow_tools = WorkflowTools(
    workflow=content_creation_workflow,
    add_few_shot=True,
    few_shot_examples=FEW_SHOT_EXAMPLES,
    async_mode=True,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[workflow_tools],
    markdown=True,
)

asyncio.run(
    agent.aprint_response(
        "Create a blog post with the following title: Quantum Computing in 2025",
        instructions="When you run the workflow using the `run_workflow` tool, remember to pass `additional_data` as a dictionary of key-value pairs.",
        markdown=True,
        stream=True,
        debug_mode=True,
    )
)
```

---

<a name="workflows--_06_advanced_concepts--_06_other--workflow_with_image_inputpy"></a>

### `workflows/_06_advanced_concepts/_06_other/workflow_with_image_input.py`

```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

# Define agents
image_analyzer = Agent(
    name="Image Analyzer",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Analyze the provided image and extract key details, objects, and context.",
)

news_researcher = Agent(
    name="News Researcher",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search for latest news and information related to the analyzed image content.",
)

# Define steps
analysis_step = Step(
    name="Image Analysis Step",
    agent=image_analyzer,
)

research_step = Step(
    name="News Research Step",
    agent=news_researcher,
)

# Create workflow with media input
media_workflow = Workflow(
    name="Image Analysis and Research Workflow",
    description="Analyze an image and research related news",
    steps=[analysis_step, research_step],
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
)

# Run workflow with image input
if __name__ == "__main__":
    media_workflow.print_response(
        input="Please analyze this image and find related news",
        images=[
            Image(
                url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
            )
        ],
        markdown=True,
    )
```

---

